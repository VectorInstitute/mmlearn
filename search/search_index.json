{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"mmlearn documentation","text":"<p>mmlearn aims at enabling the evaluation of existing multimodal representation learning methods, as well as facilitating experimentation and research for new techniques.</p>"},{"location":"#contents","title":"Contents","text":"<ul> <li>Installation</li> <li>User Guide</li> <li>Contributing</li> <li>API Reference</li> </ul>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#top-level-module","title":"Top Level Module","text":""},{"location":"api/#mmlearn","title":"mmlearn","text":"<p>Multimodal learning library.</p>"},{"location":"api/#mmlearn.cli","title":"cli","text":"<p>Command Line Interface for mmlearn.</p>"},{"location":"api/#mmlearn.cli.run","title":"run","text":"<p>Main entry point for training and evaluation.</p>"},{"location":"api/#mmlearn.cli.run.CombinedDataset","title":"CombinedDataset","text":"<p>               Bases: <code>Dataset[Example]</code></p> <p>Combine multiple datasets into one.</p> <p>This class is similar to class:<code>~torch.utils.data.ConcatDataset</code> but allows for combining iterable-style datasets with map-style datasets. The iterable-style datasets must implement the :meth:<code>__len__</code> method, which is used to determine the total length of the combined dataset. When an index is passed to the combined dataset, the dataset that contains the example at that index is determined and the example is retrieved from that dataset. Since iterable-style datasets do not support random access, the examples are retrieved sequentially from the iterable-style datasets. When the end of an iterable-style dataset is reached, the iterator is reset and the next example is retrieved from the beginning of the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>datasets</code> <code>Iterable[Union[Dataset, IterableDataset]]</code> <p>Iterable of datasets to combine.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the datasets in the input iterable are not instances of class:<code>~torch.utils.data.Dataset</code> or class:<code>~torch.utils.data.IterableDataset</code>.</p> <code>ValueError</code> <p>If the input iterable of datasets is empty.</p> Source code in <code>mmlearn/datasets/core/combined_dataset.py</code> <pre><code>class CombinedDataset(Dataset[Example]):\n    \"\"\"Combine multiple datasets into one.\n\n    This class is similar to :py:class:`~torch.utils.data.ConcatDataset` but allows\n    for combining iterable-style datasets with map-style datasets. The iterable-style\n    datasets must implement the :meth:`__len__` method, which is used to determine the\n    total length of the combined dataset. When an index is passed to the combined\n    dataset, the dataset that contains the example at that index is determined and\n    the example is retrieved from that dataset. Since iterable-style datasets do\n    not support random access, the examples are retrieved sequentially from the\n    iterable-style datasets. When the end of an iterable-style dataset is reached,\n    the iterator is reset and the next example is retrieved from the beginning of\n    the dataset.\n\n\n    Parameters\n    ----------\n    datasets : Iterable[Union[torch.utils.data.Dataset, torch.utils.data.IterableDataset]]\n        Iterable of datasets to combine.\n\n    Raises\n    ------\n    TypeError\n        If any of the datasets in the input iterable are not instances of\n        :py:class:`~torch.utils.data.Dataset` or :py:class:`~torch.utils.data.IterableDataset`.\n    ValueError\n        If the input iterable of datasets is empty.\n\n    \"\"\"  # noqa: W505\n\n    def __init__(\n        self, datasets: Iterable[Union[Dataset[Example], IterableDataset[Example]]]\n    ) -&gt; None:\n        self.datasets, _ = tree_flatten(datasets)\n        if not all(\n            isinstance(dataset, (Dataset, IterableDataset)) for dataset in self.datasets\n        ):\n            raise TypeError(\n                \"Expected argument `datasets` to be an iterable of `Dataset` or \"\n                f\"`IterableDataset` instances, but found: {self.datasets}\",\n            )\n        if len(self.datasets) == 0:\n            raise ValueError(\n                \"Expected a non-empty iterable of datasets but found an empty iterable\",\n            )\n\n        self._cumulative_sizes: list[int] = np.cumsum(\n            [len(dataset) for dataset in self.datasets]\n        ).tolist()\n        self._iterators: list[Iterator[Example]] = []\n        self._iter_dataset_mapping: dict[int, int] = {}\n\n        # create iterators for iterable datasets and map dataset index to iterator index\n        for idx, dataset in enumerate(self.datasets):\n            if isinstance(dataset, IterableDataset):\n                self._iterators.append(iter(dataset))\n                self._iter_dataset_mapping[idx] = len(self._iterators) - 1\n\n    def __getitem__(self, idx: int) -&gt; Example:\n        \"\"\"Return an example from the combined dataset.\"\"\"\n        if idx &lt; 0:  # handle negative indices\n            if -idx &gt; len(self):\n                raise IndexError(\n                    f\"Index {idx} is out of bounds for the combined dataset with \"\n                    f\"length {len(self)}\",\n                )\n            idx = len(self) + idx\n\n        dataset_idx = bisect.bisect_right(self._cumulative_sizes, idx)\n\n        curr_dataset = self.datasets[dataset_idx]\n        if isinstance(curr_dataset, IterableDataset):\n            iter_idx = self._iter_dataset_mapping[dataset_idx]\n            try:\n                example = next(self._iterators[iter_idx])\n            except StopIteration:\n                self._iterators[iter_idx] = iter(curr_dataset)\n                example = next(self._iterators[iter_idx])\n        else:\n            if dataset_idx == 0:\n                example_idx = idx\n            else:\n                example_idx = idx - self._cumulative_sizes[dataset_idx - 1]\n            example = curr_dataset[example_idx]\n\n        if not isinstance(example, Example):\n            raise TypeError(\n                \"Expected dataset examples to be instances of `Example` \"\n                f\"but found {type(example)}\",\n            )\n\n        if not hasattr(example, \"dataset_index\"):\n            example.dataset_index = dataset_idx\n        if not hasattr(example, \"example_ids\"):\n            example.create_ids()\n\n        return example\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the total number of examples in the combined dataset.\"\"\"\n        return self._cumulative_sizes[-1]\n</code></pre>"},{"location":"api/#mmlearn.cli.run.CombinedDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> <p>Return an example from the combined dataset.</p> Source code in <code>mmlearn/datasets/core/combined_dataset.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; Example:\n    \"\"\"Return an example from the combined dataset.\"\"\"\n    if idx &lt; 0:  # handle negative indices\n        if -idx &gt; len(self):\n            raise IndexError(\n                f\"Index {idx} is out of bounds for the combined dataset with \"\n                f\"length {len(self)}\",\n            )\n        idx = len(self) + idx\n\n    dataset_idx = bisect.bisect_right(self._cumulative_sizes, idx)\n\n    curr_dataset = self.datasets[dataset_idx]\n    if isinstance(curr_dataset, IterableDataset):\n        iter_idx = self._iter_dataset_mapping[dataset_idx]\n        try:\n            example = next(self._iterators[iter_idx])\n        except StopIteration:\n            self._iterators[iter_idx] = iter(curr_dataset)\n            example = next(self._iterators[iter_idx])\n    else:\n        if dataset_idx == 0:\n            example_idx = idx\n        else:\n            example_idx = idx - self._cumulative_sizes[dataset_idx - 1]\n        example = curr_dataset[example_idx]\n\n    if not isinstance(example, Example):\n        raise TypeError(\n            \"Expected dataset examples to be instances of `Example` \"\n            f\"but found {type(example)}\",\n        )\n\n    if not hasattr(example, \"dataset_index\"):\n        example.dataset_index = dataset_idx\n    if not hasattr(example, \"example_ids\"):\n        example.create_ids()\n\n    return example\n</code></pre>"},{"location":"api/#mmlearn.cli.run.CombinedDataset.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the total number of examples in the combined dataset.</p> Source code in <code>mmlearn/datasets/core/combined_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the total number of examples in the combined dataset.\"\"\"\n    return self._cumulative_sizes[-1]\n</code></pre>"},{"location":"api/#mmlearn.cli.run.DefaultDataCollator","title":"DefaultDataCollator  <code>dataclass</code>","text":"<p>Default data collator for batching examples.</p> <p>This data collator will collate a list of class:<code>~mmlearn.datasets.core.example.Example</code> objects into a batch. It can also apply processing functions to specified keys in the batch before returning it.</p> <p>Parameters:</p> Name Type Description Default <code>batch_processors</code> <code>Optional[dict[str, Callable[[Any], Any]]]</code> <p>Dictionary of callables to apply to the batch before returning it.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the batch processor for a key does not return a dictionary with the key in it.</p> Source code in <code>mmlearn/datasets/core/data_collator.py</code> <pre><code>@dataclass\nclass DefaultDataCollator:\n    \"\"\"Default data collator for batching examples.\n\n    This data collator will collate a list of :py:class:`~mmlearn.datasets.core.example.Example`\n    objects into a batch. It can also apply processing functions to specified keys\n    in the batch before returning it.\n\n    Parameters\n    ----------\n    batch_processors : Optional[dict[str, Callable[[Any], Any]]], optional, default=None\n        Dictionary of callables to apply to the batch before returning it.\n\n    Raises\n    ------\n    ValueError\n        If the batch processor for a key does not return a dictionary with the\n        key in it.\n    \"\"\"  # noqa: W505\n\n    #: Dictionary of callables to apply to the batch before returning it.\n    #: The key is the name of the key in the batch, and the value is the processing\n    #: function to apply to the key. The processing function must take a single\n    #: argument and return a single value. If the processing function returns\n    #: a dictionary, it must contain the key that was processed in it (all the\n    #: other keys will also be included in the batch).\n    batch_processors: Optional[dict[str, Callable[[Any], Any]]] = None\n\n    def __call__(self, examples: list[Example]) -&gt; dict[str, Any]:\n        \"\"\"Collate a list of `Example` objects and apply processing functions.\"\"\"\n        batch = collate_example_list(examples)\n\n        if self.batch_processors is not None:\n            for key, processor in self.batch_processors.items():\n                batch_key: str = key\n                if Modalities.has_modality(key):\n                    batch_key = Modalities.get_modality(key).name\n\n                if batch_key in batch:\n                    batch_processed = processor(batch[batch_key])\n                    if isinstance(batch_processed, Mapping):\n                        if batch_key not in batch_processed:\n                            raise ValueError(\n                                f\"Batch processor for '{key}' key must return a dictionary \"\n                                f\"with '{batch_key}' in it.\"\n                            )\n                        batch.update(batch_processed)\n                    else:\n                        batch[batch_key] = batch_processed\n\n        return batch\n</code></pre>"},{"location":"api/#mmlearn.cli.run.DefaultDataCollator.__call__","title":"__call__","text":"<pre><code>__call__(examples)\n</code></pre> <p>Collate a list of <code>Example</code> objects and apply processing functions.</p> Source code in <code>mmlearn/datasets/core/data_collator.py</code> <pre><code>def __call__(self, examples: list[Example]) -&gt; dict[str, Any]:\n    \"\"\"Collate a list of `Example` objects and apply processing functions.\"\"\"\n    batch = collate_example_list(examples)\n\n    if self.batch_processors is not None:\n        for key, processor in self.batch_processors.items():\n            batch_key: str = key\n            if Modalities.has_modality(key):\n                batch_key = Modalities.get_modality(key).name\n\n            if batch_key in batch:\n                batch_processed = processor(batch[batch_key])\n                if isinstance(batch_processed, Mapping):\n                    if batch_key not in batch_processed:\n                        raise ValueError(\n                            f\"Batch processor for '{key}' key must return a dictionary \"\n                            f\"with '{batch_key}' in it.\"\n                        )\n                    batch.update(batch_processed)\n                else:\n                    batch[batch_key] = batch_processed\n\n    return batch\n</code></pre>"},{"location":"api/#mmlearn.cli.run.Example","title":"Example","text":"<p>               Bases: <code>OrderedDict[Any, Any]</code></p> <p>A representation of a single example from a dataset.</p> <p>This class is a subclass of class:<code>~collections.OrderedDict</code> and provides attribute-style access. This means that <code>example[\"text\"]</code> and <code>example.text</code> are equivalent. All datasets in this library return examples as class:<code>~mmlearn.datasets.core.example.Example</code> objects.</p> <p>Parameters:</p> Name Type Description Default <code>init_dict</code> <code>Optional[MutableMapping[Hashable, Any]]</code> <p>Dictionary to init <code>Example</code> class with.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; example = Example({\"text\": torch.tensor(2)})\n&gt;&gt;&gt; example.text.zero_()\ntensor(0)\n&gt;&gt;&gt; example.context = torch.tensor(4)  # set custom attributes after initialization\n</code></pre> Source code in <code>mmlearn/datasets/core/example.py</code> <pre><code>class Example(OrderedDict[Any, Any]):\n    \"\"\"A representation of a single example from a dataset.\n\n    This class is a subclass of :py:class:`~collections.OrderedDict` and provides\n    attribute-style access. This means that `example[\"text\"]` and `example.text`\n    are equivalent. All datasets in this library return examples as\n    :py:class:`~mmlearn.datasets.core.example.Example` objects.\n\n\n    Parameters\n    ----------\n    init_dict : Optional[MutableMapping[Hashable, Any]], optional, default=None\n        Dictionary to init `Example` class with.\n\n    Examples\n    --------\n    &gt;&gt;&gt; example = Example({\"text\": torch.tensor(2)})\n    &gt;&gt;&gt; example.text.zero_()\n    tensor(0)\n    &gt;&gt;&gt; example.context = torch.tensor(4)  # set custom attributes after initialization\n    \"\"\"\n\n    def __init__(\n        self,\n        init_dict: Optional[MutableMapping[Hashable, Any]] = None,\n    ) -&gt; None:\n        if init_dict is None:\n            init_dict = {}\n        super().__init__(init_dict)\n\n    def create_ids(self) -&gt; None:\n        \"\"\"Create a unique id for the example from the dataset and example index.\n\n        This method combines the dataset index and example index to create an\n        attribute called `example_ids`, which is a dictionary of tensors. The\n        dictionary keys are all the keys in the example except for `example_ids`,\n        `example_index`, and `dataset_index`. The values are tensors of shape `(2,)`\n        containing the tuple `(dataset_index, example_index)` for each key.\n        The `example_ids` is used to (re-)identify pairs of examples from different\n        modalities after they have been combined into a batch.\n\n        Warns\n        -----\n        UserWarning\n            If the `example_index` and `dataset_index` attributes are not set.\n\n        Notes\n        -----\n        - The Example must have the following attributes set before calling this\n          this method: `example_index` (usually set/returned by the dataset) and\n          `dataset_index` (usually set by the :py:class:`~mmlearn.datasets.core.combined_dataset.CombinedDataset` object)\n        - The :py:func:`~mmlearn.datasets.core.example.find_matching_indices`\n          function can be used to find matching examples given two tensors of example ids.\n\n        \"\"\"  # noqa: W505\n        if hasattr(self, \"example_index\") and hasattr(self, \"dataset_index\"):\n            self.example_ids = {\n                key: torch.tensor([self.dataset_index, self.example_index])\n                for key in self.keys()\n                if key not in (\"example_ids\", \"example_index\", \"dataset_index\")\n            }\n        else:\n            rank_zero_warn(\n                \"Cannot create `example_ids` without `example_index` and `dataset_index` \"\n                \"attributes. Set these attributes before calling `create_ids`. \"\n                \"No `example_ids` was created.\",\n                stacklevel=2,\n                category=UserWarning,\n            )\n\n    def __getattr__(self, key: str) -&gt; Any:\n        \"\"\"Get attribute by key.\"\"\"\n        try:\n            return self[key]\n        except KeyError:\n            raise AttributeError(key) from None\n\n    def __setattr__(self, key: str, value: Any) -&gt; None:\n        \"\"\"Set attribute by key.\"\"\"\n        if isinstance(value, MutableMapping):\n            value = Example(value)\n        self[key] = value\n\n    def __setitem__(self, key: Hashable, value: Any) -&gt; None:\n        \"\"\"Set item by key.\"\"\"\n        if isinstance(value, MutableMapping):\n            value = Example(value)\n        super().__setitem__(key, value)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.Example.create_ids","title":"create_ids","text":"<pre><code>create_ids()\n</code></pre> <p>Create a unique id for the example from the dataset and example index.</p> <p>This method combines the dataset index and example index to create an attribute called <code>example_ids</code>, which is a dictionary of tensors. The dictionary keys are all the keys in the example except for <code>example_ids</code>, <code>example_index</code>, and <code>dataset_index</code>. The values are tensors of shape <code>(2,)</code> containing the tuple <code>(dataset_index, example_index)</code> for each key. The <code>example_ids</code> is used to (re-)identify pairs of examples from different modalities after they have been combined into a batch.</p> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If the <code>example_index</code> and <code>dataset_index</code> attributes are not set.</p> Notes <ul> <li>The Example must have the following attributes set before calling this   this method: <code>example_index</code> (usually set/returned by the dataset) and   <code>dataset_index</code> (usually set by the class:<code>~mmlearn.datasets.core.combined_dataset.CombinedDataset</code> object)</li> <li>The func:<code>~mmlearn.datasets.core.example.find_matching_indices</code>   function can be used to find matching examples given two tensors of example ids.</li> </ul> Source code in <code>mmlearn/datasets/core/example.py</code> <pre><code>def create_ids(self) -&gt; None:\n    \"\"\"Create a unique id for the example from the dataset and example index.\n\n    This method combines the dataset index and example index to create an\n    attribute called `example_ids`, which is a dictionary of tensors. The\n    dictionary keys are all the keys in the example except for `example_ids`,\n    `example_index`, and `dataset_index`. The values are tensors of shape `(2,)`\n    containing the tuple `(dataset_index, example_index)` for each key.\n    The `example_ids` is used to (re-)identify pairs of examples from different\n    modalities after they have been combined into a batch.\n\n    Warns\n    -----\n    UserWarning\n        If the `example_index` and `dataset_index` attributes are not set.\n\n    Notes\n    -----\n    - The Example must have the following attributes set before calling this\n      this method: `example_index` (usually set/returned by the dataset) and\n      `dataset_index` (usually set by the :py:class:`~mmlearn.datasets.core.combined_dataset.CombinedDataset` object)\n    - The :py:func:`~mmlearn.datasets.core.example.find_matching_indices`\n      function can be used to find matching examples given two tensors of example ids.\n\n    \"\"\"  # noqa: W505\n    if hasattr(self, \"example_index\") and hasattr(self, \"dataset_index\"):\n        self.example_ids = {\n            key: torch.tensor([self.dataset_index, self.example_index])\n            for key in self.keys()\n            if key not in (\"example_ids\", \"example_index\", \"dataset_index\")\n        }\n    else:\n        rank_zero_warn(\n            \"Cannot create `example_ids` without `example_index` and `dataset_index` \"\n            \"attributes. Set these attributes before calling `create_ids`. \"\n            \"No `example_ids` was created.\",\n            stacklevel=2,\n            category=UserWarning,\n        )\n</code></pre>"},{"location":"api/#mmlearn.cli.run.Example.__getattr__","title":"__getattr__","text":"<pre><code>__getattr__(key)\n</code></pre> <p>Get attribute by key.</p> Source code in <code>mmlearn/datasets/core/example.py</code> <pre><code>def __getattr__(self, key: str) -&gt; Any:\n    \"\"\"Get attribute by key.\"\"\"\n    try:\n        return self[key]\n    except KeyError:\n        raise AttributeError(key) from None\n</code></pre>"},{"location":"api/#mmlearn.cli.run.Example.__setattr__","title":"__setattr__","text":"<pre><code>__setattr__(key, value)\n</code></pre> <p>Set attribute by key.</p> Source code in <code>mmlearn/datasets/core/example.py</code> <pre><code>def __setattr__(self, key: str, value: Any) -&gt; None:\n    \"\"\"Set attribute by key.\"\"\"\n    if isinstance(value, MutableMapping):\n        value = Example(value)\n    self[key] = value\n</code></pre>"},{"location":"api/#mmlearn.cli.run.Example.__setitem__","title":"__setitem__","text":"<pre><code>__setitem__(key, value)\n</code></pre> <p>Set item by key.</p> Source code in <code>mmlearn/datasets/core/example.py</code> <pre><code>def __setitem__(self, key: Hashable, value: Any) -&gt; None:\n    \"\"\"Set item by key.\"\"\"\n    if isinstance(value, MutableMapping):\n        value = Example(value)\n    super().__setitem__(key, value)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.CombinedDatasetRatioSampler","title":"CombinedDatasetRatioSampler","text":"<p>               Bases: <code>Sampler[int]</code></p> <p>Sampler for weighted sampling from a class:<code>~mmlearn.datasets.core.combined_dataset.CombinedDataset</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>CombinedDataset</code> <p>An instance of class:<code>~mmlearn.datasets.core.combined_dataset.CombinedDataset</code> to sample from.</p> required <code>ratios</code> <code>Optional[Sequence[float]]</code> <p>A sequence of ratios for sampling from each dataset in the combined dataset. The length of the sequence must be equal to the number of datasets in the combined dataset (<code>dataset</code>). If <code>None</code>, the length of each dataset in the combined dataset is used as the ratio. The ratios are normalized to sum to 1.</p> <code>None</code> <code>num_samples</code> <code>Optional[int]</code> <p>The number of samples to draw from the combined dataset. If <code>None</code>, the sampler will draw as many samples as there are in the combined dataset. This number must yield at least one sample per dataset in the combined dataset, when multiplied by the corresponding ratio.</p> <code>None</code> <code>replacement</code> <code>bool</code> <p>Whether to sample with replacement or not.</p> <code>False</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the sampled indices or not. If <code>False</code>, the indices of each dataset will appear in the order they are stored in the combined dataset. This is similar to sequential sampling from each dataset. The datasets that make up the combined dataset are still sampled randomly.</p> <code>True</code> <code>rank</code> <code>Optional[int]</code> <p>Rank of the current process within :attr:<code>num_replicas</code>. By default, :attr:<code>rank</code> is retrieved from the current distributed group.</p> <code>None</code> <code>num_replicas</code> <code>Optional[int]</code> <p>Number of processes participating in distributed training. By default, :attr:<code>num_replicas</code> is retrieved from the current distributed group.</p> <code>None</code> <code>drop_last</code> <code>bool</code> <p>Whether to drop the last incomplete batch or not. If <code>True</code>, the sampler will drop samples to make the number of samples evenly divisible by the number of replicas in distributed mode.</p> <code>False</code> <code>seed</code> <code>int</code> <p>Random seed used to when sampling from the combined dataset and shuffling the sampled indices.</p> <code>0</code> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>CombinedDataset</code> <p>The dataset to sample from.</p> <code>num_samples</code> <code>int</code> <p>The number of samples to draw from the combined dataset.</p> <code>probs</code> <code>Tensor</code> <p>The probabilities for sampling from each dataset in the combined dataset. This is computed from the <code>ratios</code> argument and is normalized to sum to 1.</p> <code>replacement</code> <code>bool</code> <p>Whether to sample with replacement or not.</p> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the sampled indices or not.</p> <code>rank</code> <code>int</code> <p>Rank of the current process within :attr:<code>num_replicas</code>.</p> <code>num_replicas</code> <code>int</code> <p>Number of processes participating in distributed training.</p> <code>drop_last</code> <code>bool</code> <p>Whether to drop samples to make the number of samples evenly divisible by the number of replicas in distributed mode.</p> <code>seed</code> <code>int</code> <p>Random seed used to when sampling from the combined dataset and shuffling the sampled indices.</p> <code>epoch</code> <code>int</code> <p>Current epoch number. This is used to set the random seed. This is useful in distributed mode to ensure that each process receives a different random ordering of the samples.</p> <code>total_size</code> <code>int</code> <p>The total number of samples across all processes.</p> Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>@store(group=\"dataloader/sampler\", provider=\"mmlearn\")\nclass CombinedDatasetRatioSampler(Sampler[int]):\n    \"\"\"Sampler for weighted sampling from a :py:class:`~mmlearn.datasets.core.combined_dataset.CombinedDataset`.\n\n    Parameters\n    ----------\n    dataset : CombinedDataset\n        An instance of :py:class:`~mmlearn.datasets.core.combined_dataset.CombinedDataset`\n        to sample from.\n    ratios : Optional[Sequence[float]], optional, default=None\n        A sequence of ratios for sampling from each dataset in the combined dataset.\n        The length of the sequence must be equal to the number of datasets in the\n        combined dataset (`dataset`). If `None`, the length of each dataset in the\n        combined dataset is used as the ratio. The ratios are normalized to sum to 1.\n    num_samples : Optional[int], optional, default=None\n        The number of samples to draw from the combined dataset. If `None`, the\n        sampler will draw as many samples as there are in the combined dataset.\n        This number must yield at least one sample per dataset in the combined\n        dataset, when multiplied by the corresponding ratio.\n    replacement : bool, default=False\n        Whether to sample with replacement or not.\n    shuffle : bool, default=True\n        Whether to shuffle the sampled indices or not. If `False`, the indices of\n        each dataset will appear in the order they are stored in the combined dataset.\n        This is similar to sequential sampling from each dataset. The datasets\n        that make up the combined dataset are still sampled randomly.\n    rank : Optional[int], optional, default=None\n        Rank of the current process within :attr:`num_replicas`. By default,\n        :attr:`rank` is retrieved from the current distributed group.\n    num_replicas : Optional[int], optional, default=None\n        Number of processes participating in distributed training. By\n        default, :attr:`num_replicas` is retrieved from the current distributed group.\n    drop_last : bool, default=False\n        Whether to drop the last incomplete batch or not. If `True`, the sampler will\n        drop samples to make the number of samples evenly divisible by the number of\n        replicas in distributed mode.\n    seed : int, default=0\n        Random seed used to when sampling from the combined dataset and shuffling\n        the sampled indices.\n\n    Attributes\n    ----------\n    dataset : CombinedDataset\n        The dataset to sample from.\n    num_samples : int\n        The number of samples to draw from the combined dataset.\n    probs : torch.Tensor\n        The probabilities for sampling from each dataset in the combined dataset.\n        This is computed from the `ratios` argument and is normalized to sum to 1.\n    replacement : bool\n        Whether to sample with replacement or not.\n    shuffle : bool\n        Whether to shuffle the sampled indices or not.\n    rank : int\n        Rank of the current process within :attr:`num_replicas`.\n    num_replicas : int\n        Number of processes participating in distributed training.\n    drop_last : bool\n        Whether to drop samples to make the number of samples evenly divisible by the\n        number of replicas in distributed mode.\n    seed : int\n        Random seed used to when sampling from the combined dataset and shuffling\n        the sampled indices.\n    epoch : int\n        Current epoch number. This is used to set the random seed. This is useful\n        in distributed mode to ensure that each process receives a different random\n        ordering of the samples.\n    total_size : int\n        The total number of samples across all processes.\n    \"\"\"  # noqa: W505\n\n    def __init__(  # noqa: PLR0912\n        self,\n        dataset: CombinedDataset,\n        ratios: Optional[Sequence[float]] = None,\n        num_samples: Optional[int] = None,\n        replacement: bool = False,\n        shuffle: bool = True,\n        rank: Optional[int] = None,\n        num_replicas: Optional[int] = None,\n        drop_last: bool = False,\n        seed: int = 0,\n    ):\n        if not isinstance(dataset, CombinedDataset):\n            raise TypeError(\n                \"Expected argument `dataset` to be of type `CombinedDataset`, \"\n                f\"but got {type(dataset)}.\",\n            )\n        if not isinstance(seed, int):\n            raise TypeError(\n                f\"Expected argument `seed` to be an integer, but got {type(seed)}.\",\n            )\n        if num_replicas is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            num_replicas = dist.get_world_size()\n        if rank is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            rank = dist.get_rank()\n        if rank &gt;= num_replicas or rank &lt; 0:\n            raise ValueError(\n                f\"Invalid rank {rank}, rank should be in the interval [0, {num_replicas - 1}]\"\n            )\n\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.drop_last = drop_last\n        self.replacement = replacement\n        self.shuffle = shuffle\n        self.seed = seed\n        self.epoch = 0\n\n        self._num_samples = num_samples\n        if not isinstance(self.num_samples, int) or self.num_samples &lt;= 0:\n            raise ValueError(\n                \"Expected argument `num_samples` to be a positive integer, but got \"\n                f\"{self.num_samples}.\",\n            )\n\n        if ratios is None:\n            ratios = [len(subset) for subset in self.dataset.datasets]\n\n        num_datasets = len(self.dataset.datasets)\n        if len(ratios) != num_datasets:\n            raise ValueError(\n                f\"Expected argument `ratios` to be of length {num_datasets}, \"\n                f\"but got length {len(ratios)}.\",\n            )\n        prob_sum = sum(ratios)\n        if not all(ratio &gt;= 0 for ratio in ratios) and prob_sum &gt; 0:\n            raise ValueError(\n                \"Expected argument `ratios` to be a sequence of non-negative numbers. \"\n                f\"Got {ratios}.\",\n            )\n        self.probs = torch.tensor(\n            [ratio / prob_sum for ratio in ratios],\n            dtype=torch.double,\n        )\n        if any((prob * self.num_samples) &lt;= 0 for prob in self.probs):\n            raise ValueError(\n                \"Expected dataset ratio to result in at least one sample per dataset. \"\n                f\"Got dataset sizes {self.probs * self.num_samples}.\",\n            )\n\n    @property\n    def num_samples(self) -&gt; int:\n        \"\"\"Return the number of samples managed by the sampler.\"\"\"\n        # dataset size might change at runtime\n        if self._num_samples is None:\n            num_samples = len(self.dataset)\n        else:\n            num_samples = self._num_samples\n\n        if self.drop_last and num_samples % self.num_replicas != 0:\n            # split to nearest available length that is evenly divisible.\n            # This is to ensure each rank receives the same amount of data when\n            # using this Sampler.\n            num_samples = math.ceil(\n                (num_samples - self.num_replicas) / self.num_replicas,\n            )\n        else:\n            num_samples = math.ceil(num_samples / self.num_replicas)\n        return num_samples\n\n    @property\n    def total_size(self) -&gt; int:\n        \"\"\"Return the total size of the dataset.\"\"\"\n        return self.num_samples * self.num_replicas\n\n    def __iter__(self) -&gt; Iterator[int]:\n        \"\"\"Return an iterator that yields sample indices for the combined dataset.\"\"\"\n        generator = torch.Generator()\n        seed = self.seed + self.epoch\n        generator.manual_seed(seed)\n\n        cumulative_sizes = [0] + self.dataset._cumulative_sizes\n        num_samples_per_dataset = [int(prob * self.total_size) for prob in self.probs]\n        indices = []\n        for i in range(len(self.dataset.datasets)):\n            per_dataset_indices: torch.Tensor = torch.multinomial(\n                torch.ones(cumulative_sizes[i + 1] - cumulative_sizes[i]),\n                num_samples_per_dataset[i],\n                replacement=self.replacement,\n                generator=generator,\n            )\n            # adjust indices to reflect position in cumulative dataset\n            per_dataset_indices += cumulative_sizes[i]\n            assert per_dataset_indices.max() &lt; cumulative_sizes[i + 1], (\n                f\"Indices from dataset {i} exceed dataset size. \"\n                f\"Got indices {per_dataset_indices} and dataset size {cumulative_sizes[i + 1]}.\",\n            )\n            indices.append(per_dataset_indices)\n\n        indices = torch.cat(indices)\n        if self.shuffle:\n            rand_indices = torch.randperm(len(indices), generator=generator)\n            indices = indices[rand_indices]\n\n        indices = indices.tolist()  # type: ignore[attr-defined]\n        num_indices = len(indices)\n\n        if num_indices &lt; self.total_size:\n            padding_size = self.total_size - num_indices\n            if padding_size &lt;= num_indices:\n                indices += indices[:padding_size]\n            else:\n                indices += (indices * math.ceil(padding_size / num_indices))[\n                    :padding_size\n                ]\n        elif num_indices &gt; self.total_size:\n            indices = indices[: self.total_size]\n        assert len(indices) == self.total_size\n\n        # subsample\n        indices = indices[self.rank : self.total_size : self.num_replicas]\n        assert len(indices) == self.num_samples, (\n            f\"Expected {self.num_samples} samples, but got {len(indices)}.\",\n        )\n\n        yield from iter(indices)\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the total number of samples in the sampler.\"\"\"\n        return self.num_samples\n\n    def set_epoch(self, epoch: int) -&gt; None:\n        \"\"\"Set the epoch for this sampler.\n\n        When :attr:`shuffle=True`, this ensures all replicas use a different random\n        ordering for each epoch. Otherwise, the next iteration of this sampler\n        will yield the same ordering.\n\n        Parameters\n        ----------\n        epoch : int\n            Epoch number.\n\n        \"\"\"\n        self.epoch = epoch\n\n        # some iterable datasets (especially huggingface iterable datasets) might\n        # require setting the epoch to ensure shuffling works properly\n        for dataset in self.dataset.datasets:\n            if hasattr(dataset, \"set_epoch\"):\n                dataset.set_epoch(epoch)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.CombinedDatasetRatioSampler.num_samples","title":"num_samples  <code>property</code>","text":"<pre><code>num_samples\n</code></pre> <p>Return the number of samples managed by the sampler.</p>"},{"location":"api/#mmlearn.cli.run.CombinedDatasetRatioSampler.total_size","title":"total_size  <code>property</code>","text":"<pre><code>total_size\n</code></pre> <p>Return the total size of the dataset.</p>"},{"location":"api/#mmlearn.cli.run.CombinedDatasetRatioSampler.__iter__","title":"__iter__","text":"<pre><code>__iter__()\n</code></pre> <p>Return an iterator that yields sample indices for the combined dataset.</p> Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>def __iter__(self) -&gt; Iterator[int]:\n    \"\"\"Return an iterator that yields sample indices for the combined dataset.\"\"\"\n    generator = torch.Generator()\n    seed = self.seed + self.epoch\n    generator.manual_seed(seed)\n\n    cumulative_sizes = [0] + self.dataset._cumulative_sizes\n    num_samples_per_dataset = [int(prob * self.total_size) for prob in self.probs]\n    indices = []\n    for i in range(len(self.dataset.datasets)):\n        per_dataset_indices: torch.Tensor = torch.multinomial(\n            torch.ones(cumulative_sizes[i + 1] - cumulative_sizes[i]),\n            num_samples_per_dataset[i],\n            replacement=self.replacement,\n            generator=generator,\n        )\n        # adjust indices to reflect position in cumulative dataset\n        per_dataset_indices += cumulative_sizes[i]\n        assert per_dataset_indices.max() &lt; cumulative_sizes[i + 1], (\n            f\"Indices from dataset {i} exceed dataset size. \"\n            f\"Got indices {per_dataset_indices} and dataset size {cumulative_sizes[i + 1]}.\",\n        )\n        indices.append(per_dataset_indices)\n\n    indices = torch.cat(indices)\n    if self.shuffle:\n        rand_indices = torch.randperm(len(indices), generator=generator)\n        indices = indices[rand_indices]\n\n    indices = indices.tolist()  # type: ignore[attr-defined]\n    num_indices = len(indices)\n\n    if num_indices &lt; self.total_size:\n        padding_size = self.total_size - num_indices\n        if padding_size &lt;= num_indices:\n            indices += indices[:padding_size]\n        else:\n            indices += (indices * math.ceil(padding_size / num_indices))[\n                :padding_size\n            ]\n    elif num_indices &gt; self.total_size:\n        indices = indices[: self.total_size]\n    assert len(indices) == self.total_size\n\n    # subsample\n    indices = indices[self.rank : self.total_size : self.num_replicas]\n    assert len(indices) == self.num_samples, (\n        f\"Expected {self.num_samples} samples, but got {len(indices)}.\",\n    )\n\n    yield from iter(indices)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.CombinedDatasetRatioSampler.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the total number of samples in the sampler.</p> Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the total number of samples in the sampler.\"\"\"\n    return self.num_samples\n</code></pre>"},{"location":"api/#mmlearn.cli.run.CombinedDatasetRatioSampler.set_epoch","title":"set_epoch","text":"<pre><code>set_epoch(epoch)\n</code></pre> <p>Set the epoch for this sampler.</p> <p>When :attr:<code>shuffle=True</code>, this ensures all replicas use a different random ordering for each epoch. Otherwise, the next iteration of this sampler will yield the same ordering.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>Epoch number.</p> required Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>def set_epoch(self, epoch: int) -&gt; None:\n    \"\"\"Set the epoch for this sampler.\n\n    When :attr:`shuffle=True`, this ensures all replicas use a different random\n    ordering for each epoch. Otherwise, the next iteration of this sampler\n    will yield the same ordering.\n\n    Parameters\n    ----------\n    epoch : int\n        Epoch number.\n\n    \"\"\"\n    self.epoch = epoch\n\n    # some iterable datasets (especially huggingface iterable datasets) might\n    # require setting the epoch to ensure shuffling works properly\n    for dataset in self.dataset.datasets:\n        if hasattr(dataset, \"set_epoch\"):\n            dataset.set_epoch(epoch)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.DistributedEvalSampler","title":"DistributedEvalSampler","text":"<p>               Bases: <code>Sampler[int]</code></p> <p>Sampler for distributed evaluation.</p> <p>The main differences between this and class:<code>torch.utils.data.DistributedSampler</code> are that this sampler does not add extra samples to make it evenly divisible and shuffling is disabled by default.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>Dataset used for sampling.</p> required <code>num_replicas</code> <code>Optional[int]</code> <p>Number of processes participating in distributed training. By default, :attr:<code>rank</code> is retrieved from the current distributed group.</p> <code>None</code> <code>rank</code> <code>Optional[int]</code> <p>Rank of the current process within :attr:<code>num_replicas</code>. By default, :attr:<code>rank</code> is retrieved from the current distributed group.</p> <code>None</code> <code>shuffle</code> <code>bool</code> <p>If <code>True</code> (default), sampler will shuffle the indices.</p> <code>False</code> <code>seed</code> <code>int</code> <p>Random seed used to shuffle the sampler if :attr:<code>shuffle=True</code>. This number should be identical across all processes in the distributed group.</p> <code>0</code> Warnings <p>DistributedEvalSampler should NOT be used for training. The distributed processes could hang forever. See [1]_ for details</p> Notes <ul> <li>This sampler is for evaluation purpose where synchronization does not happen   every epoch. Synchronization should be done outside the dataloader loop.   It is especially useful in conjunction with   class:<code>torch.nn.parallel.DistributedDataParallel</code> [2]_.</li> <li>The input Dataset is assumed to be of constant size.</li> <li>This implementation is adapted from [3]_.</li> </ul> References <p>.. [1] https://github.com/pytorch/pytorch/issues/22584 .. [2] https://discuss.pytorch.org/t/how-to-validate-in-distributeddataparallel-correctly/94267/11 .. [3] https://github.com/SeungjunNah/DeepDeblur-PyTorch/blob/master/src/data/sampler.py</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; def example():\n...     start_epoch, n_epochs = 0, 2\n...     sampler = DistributedEvalSampler(dataset) if is_distributed else None\n...     loader = DataLoader(dataset, shuffle=(sampler is None), sampler=sampler)\n...     for epoch in range(start_epoch, n_epochs):\n...         if is_distributed:\n...             sampler.set_epoch(epoch)\n...         evaluate(loader)\n</code></pre> Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>@store(group=\"dataloader/sampler\", provider=\"mmlearn\")\nclass DistributedEvalSampler(Sampler[int]):\n    \"\"\"Sampler for distributed evaluation.\n\n    The main differences between this and :py:class:`torch.utils.data.DistributedSampler`\n    are that this sampler does not add extra samples to make it evenly divisible and\n    shuffling is disabled by default.\n\n    Parameters\n    ----------\n    dataset : torch.utils.data.Dataset\n        Dataset used for sampling.\n    num_replicas : Optional[int], optional, default=None\n        Number of processes participating in distributed training. By\n        default, :attr:`rank` is retrieved from the current distributed group.\n    rank : Optional[int], optional, default=None\n        Rank of the current process within :attr:`num_replicas`. By default,\n        :attr:`rank` is retrieved from the current distributed group.\n    shuffle : bool, optional, default=False\n        If `True` (default), sampler will shuffle the indices.\n    seed : int, optional, default=0\n        Random seed used to shuffle the sampler if :attr:`shuffle=True`.\n        This number should be identical across all processes in the\n        distributed group.\n\n    Warnings\n    --------\n    DistributedEvalSampler should NOT be used for training. The distributed processes\n    could hang forever. See [1]_ for details\n\n    Notes\n    -----\n    - This sampler is for evaluation purpose where synchronization does not happen\n      every epoch. Synchronization should be done outside the dataloader loop.\n      It is especially useful in conjunction with\n      :py:class:`torch.nn.parallel.DistributedDataParallel` [2]_.\n    - The input Dataset is assumed to be of constant size.\n    - This implementation is adapted from [3]_.\n\n    References\n    ----------\n    .. [1] https://github.com/pytorch/pytorch/issues/22584\n    .. [2] https://discuss.pytorch.org/t/how-to-validate-in-distributeddataparallel-correctly/94267/11\n    .. [3] https://github.com/SeungjunNah/DeepDeblur-PyTorch/blob/master/src/data/sampler.py\n\n\n    Examples\n    --------\n    &gt;&gt;&gt; def example():\n    ...     start_epoch, n_epochs = 0, 2\n    ...     sampler = DistributedEvalSampler(dataset) if is_distributed else None\n    ...     loader = DataLoader(dataset, shuffle=(sampler is None), sampler=sampler)\n    ...     for epoch in range(start_epoch, n_epochs):\n    ...         if is_distributed:\n    ...             sampler.set_epoch(epoch)\n    ...         evaluate(loader)\n\n    \"\"\"  # noqa: W505\n\n    def __init__(\n        self,\n        dataset: Dataset[Sized],\n        num_replicas: Optional[int] = None,\n        rank: Optional[int] = None,\n        shuffle: bool = False,\n        seed: int = 0,\n    ) -&gt; None:\n        if num_replicas is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            num_replicas = dist.get_world_size()\n        if rank is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            rank = dist.get_rank()\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.epoch = 0\n        self.shuffle = shuffle\n        self.seed = seed\n\n    @property\n    def total_size(self) -&gt; int:\n        \"\"\"Return the total size of the dataset.\"\"\"\n        return len(self.dataset)\n\n    @property\n    def num_samples(self) -&gt; int:\n        \"\"\"Return the number of samples managed by the sampler.\"\"\"\n        indices = list(range(self.total_size))[\n            self.rank : self.total_size : self.num_replicas\n        ]\n        return len(indices)  # true value without extra samples\n\n    def __iter__(self) -&gt; Iterator[int]:\n        \"\"\"Return an iterator that iterates over the indices of the dataset.\"\"\"\n        if self.shuffle:\n            # deterministically shuffle based on epoch and seed\n            g = torch.Generator()\n            g.manual_seed(self.seed + self.epoch)\n            indices = torch.randperm(self.total_size, generator=g).tolist()\n        else:\n            indices = list(range(self.total_size))\n\n        # subsample\n        indices = indices[self.rank : self.total_size : self.num_replicas]\n        assert len(indices) == self.num_samples\n\n        return iter(indices)\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the number of samples.\"\"\"\n        return self.num_samples\n\n    def set_epoch(self, epoch: int) -&gt; None:\n        \"\"\"Set the epoch for this sampler.\n\n        When :attr:`shuffle=True`, this ensures all replicas use a different random\n        ordering for each epoch. Otherwise, the next iteration of this sampler\n        will yield the same ordering.\n\n        Parameters\n        ----------\n        epoch : int\n            Epoch number.\n\n        \"\"\"\n        self.epoch = epoch\n</code></pre>"},{"location":"api/#mmlearn.cli.run.DistributedEvalSampler.total_size","title":"total_size  <code>property</code>","text":"<pre><code>total_size\n</code></pre> <p>Return the total size of the dataset.</p>"},{"location":"api/#mmlearn.cli.run.DistributedEvalSampler.num_samples","title":"num_samples  <code>property</code>","text":"<pre><code>num_samples\n</code></pre> <p>Return the number of samples managed by the sampler.</p>"},{"location":"api/#mmlearn.cli.run.DistributedEvalSampler.__iter__","title":"__iter__","text":"<pre><code>__iter__()\n</code></pre> <p>Return an iterator that iterates over the indices of the dataset.</p> Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>def __iter__(self) -&gt; Iterator[int]:\n    \"\"\"Return an iterator that iterates over the indices of the dataset.\"\"\"\n    if self.shuffle:\n        # deterministically shuffle based on epoch and seed\n        g = torch.Generator()\n        g.manual_seed(self.seed + self.epoch)\n        indices = torch.randperm(self.total_size, generator=g).tolist()\n    else:\n        indices = list(range(self.total_size))\n\n    # subsample\n    indices = indices[self.rank : self.total_size : self.num_replicas]\n    assert len(indices) == self.num_samples\n\n    return iter(indices)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.DistributedEvalSampler.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the number of samples.</p> Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the number of samples.\"\"\"\n    return self.num_samples\n</code></pre>"},{"location":"api/#mmlearn.cli.run.DistributedEvalSampler.set_epoch","title":"set_epoch","text":"<pre><code>set_epoch(epoch)\n</code></pre> <p>Set the epoch for this sampler.</p> <p>When :attr:<code>shuffle=True</code>, this ensures all replicas use a different random ordering for each epoch. Otherwise, the next iteration of this sampler will yield the same ordering.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>Epoch number.</p> required Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>def set_epoch(self, epoch: int) -&gt; None:\n    \"\"\"Set the epoch for this sampler.\n\n    When :attr:`shuffle=True`, this ensures all replicas use a different random\n    ordering for each epoch. Otherwise, the next iteration of this sampler\n    will yield the same ordering.\n\n    Parameters\n    ----------\n    epoch : int\n        Epoch number.\n\n    \"\"\"\n    self.epoch = epoch\n</code></pre>"},{"location":"api/#mmlearn.cli.run.BlockwiseImagePatchMaskGenerator","title":"BlockwiseImagePatchMaskGenerator","text":"<p>Blockwise image patch mask generator.</p> <p>This is primarily intended for the data2vec method.</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>Union[int, tuple[int, int]]</code> <p>The size of the input image. If an integer is provided, the image is assumed to be square.</p> required <code>num_masking_patches</code> <code>int</code> <p>The number of patches to mask.</p> required <code>min_num_patches</code> <code>int</code> <p>The minimum number of patches to mask.</p> <code>4</code> <code>max_num_patches</code> <code>int</code> <p>The maximum number of patches to mask.</p> <code>None</code> <code>min_aspect_ratio</code> <code>float</code> <p>The minimum aspect ratio of the patch.</p> <code>0.3</code> <code>max_aspect_ratio</code> <code>float</code> <p>The maximum aspect ratio of the patch.</p> <code>None</code> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>@store(group=\"datasets/masking\", provider=\"mmlearn\")\nclass BlockwiseImagePatchMaskGenerator:\n    \"\"\"Blockwise image patch mask generator.\n\n    This is primarily intended for the data2vec method.\n\n    Parameters\n    ----------\n    input_size : Union[int, tuple[int, int]]\n        The size of the input image. If an integer is provided, the image is assumed\n        to be square.\n    num_masking_patches : int\n        The number of patches to mask.\n    min_num_patches : int, default=4\n        The minimum number of patches to mask.\n    max_num_patches : int, default=None\n        The maximum number of patches to mask.\n    min_aspect_ratio : float, default=0.3\n        The minimum aspect ratio of the patch.\n    max_aspect_ratio : float, default=None\n        The maximum aspect ratio of the patch.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_size: Union[int, tuple[int, int]],\n        num_masking_patches: int,\n        min_num_patches: int = 4,\n        max_num_patches: Any = None,\n        min_aspect_ratio: float = 0.3,\n        max_aspect_ratio: Any = None,\n    ):\n        if not isinstance(input_size, tuple):\n            input_size = (input_size,) * 2\n        self.height, self.width = input_size\n\n        self.num_masking_patches = num_masking_patches\n\n        self.min_num_patches = min_num_patches\n        self.max_num_patches = (\n            num_masking_patches if max_num_patches is None else max_num_patches\n        )\n\n        max_aspect_ratio = max_aspect_ratio or 1 / min_aspect_ratio\n        self.log_aspect_ratio = (math.log(min_aspect_ratio), math.log(max_aspect_ratio))\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Generate a printable representation.\n\n        Returns\n        -------\n        str\n            A printable representation of the object.\n\n        \"\"\"\n        return \"Generator(%d, %d -&gt; [%d ~ %d], max = %d, %.3f ~ %.3f)\" % (\n            self.height,\n            self.width,\n            self.min_num_patches,\n            self.max_num_patches,\n            self.num_masking_patches,\n            self.log_aspect_ratio[0],\n            self.log_aspect_ratio[1],\n        )\n\n    def get_shape(self) -&gt; tuple[int, int]:\n        \"\"\"Get the shape of the input.\n\n        Returns\n        -------\n        tuple[int, int]\n            The shape of the input as a tuple `(height, width)`.\n        \"\"\"\n        return self.height, self.width\n\n    def _mask(self, mask: torch.Tensor, max_mask_patches: int) -&gt; int:\n        \"\"\"Masking function.\n\n        This function mask adjacent patches by first selecting a target area and aspect\n        ratio. Since, there might be overlap between selected areas  or the selected\n        area might already be masked, it runs for a  maximum of 10 attempts or until the\n        specified number of patches (max_mask_patches) is achieved.\n\n\n        Parameters\n        ----------\n        mask: torch.Tensor\n            Current mask. The mask to be updated.\n        max_mask_patches: int\n            The maximum number of patches to be masked.\n\n        Returns\n        -------\n        delta: int\n            The number of patches that were successfully masked.\n\n        Notes\n        -----\n        - `target_area`: Randomly chosen target area for the patch.\n        - `aspect_ratio`: Randomly chosen aspect ratio for the patch.\n        - `h`: Height of the patch based on the target area and aspect ratio.\n        - `w`: Width of the patch based on the target area and aspect ratio.\n        - `top`: Randomly chosen top position for the patch.\n        - `left`: Randomly chosen left position for the patch.\n        - `num_masked`: Number of masked pixels within the proposed patch area.\n        - `delta`: Accumulated count of modified pixels.\n        \"\"\"\n        delta = 0\n        for _ in range(10):\n            target_area = random.uniform(self.min_num_patches, max_mask_patches)\n            aspect_ratio = math.exp(random.uniform(*self.log_aspect_ratio))\n            h = int(round(math.sqrt(target_area * aspect_ratio)))\n            w = int(round(math.sqrt(target_area / aspect_ratio)))\n            if w &lt; self.width and h &lt; self.height:\n                top = random.randint(0, self.height - h)\n                left = random.randint(0, self.width - w)\n\n                num_masked = mask[top : top + h, left : left + w].sum()\n                # Overlap\n                if 0 &lt; h * w - num_masked &lt;= max_mask_patches:\n                    for i in range(top, top + h):\n                        for j in range(left, left + w):\n                            if mask[i, j] == 0:\n                                mask[i, j] = 1\n                                delta += 1\n\n                if delta &gt; 0:\n                    break\n        return delta\n\n    def __call__(self) -&gt; torch.Tensor:\n        \"\"\"Generate a random mask.\n\n        Returns a random mask of shape (nb_patches, nb_patches) based on the\n        configuration where the number of patches to be masked is num_masking_patches.\n\n        Returns\n        -------\n        mask: torch.Tensor\n            A mask of shape (nb_patches, nb_patches)\n\n        \"\"\"\n        mask = torch.zeros(self.get_shape(), dtype=torch.int)\n        mask_count = 0\n        while mask_count &lt; self.num_masking_patches:\n            max_mask_patches = self.num_masking_patches - mask_count\n            max_mask_patches = min(max_mask_patches, self.max_num_patches)\n\n            delta = self._mask(mask, max_mask_patches)\n            if delta == 0:\n                break\n            mask_count += delta\n\n        return mask\n</code></pre>"},{"location":"api/#mmlearn.cli.run.BlockwiseImagePatchMaskGenerator.__repr__","title":"__repr__","text":"<pre><code>__repr__()\n</code></pre> <p>Generate a printable representation.</p> <p>Returns:</p> Type Description <code>str</code> <p>A printable representation of the object.</p> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Generate a printable representation.\n\n    Returns\n    -------\n    str\n        A printable representation of the object.\n\n    \"\"\"\n    return \"Generator(%d, %d -&gt; [%d ~ %d], max = %d, %.3f ~ %.3f)\" % (\n        self.height,\n        self.width,\n        self.min_num_patches,\n        self.max_num_patches,\n        self.num_masking_patches,\n        self.log_aspect_ratio[0],\n        self.log_aspect_ratio[1],\n    )\n</code></pre>"},{"location":"api/#mmlearn.cli.run.BlockwiseImagePatchMaskGenerator.get_shape","title":"get_shape","text":"<pre><code>get_shape()\n</code></pre> <p>Get the shape of the input.</p> <p>Returns:</p> Type Description <code>tuple[int, int]</code> <p>The shape of the input as a tuple <code>(height, width)</code>.</p> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>def get_shape(self) -&gt; tuple[int, int]:\n    \"\"\"Get the shape of the input.\n\n    Returns\n    -------\n    tuple[int, int]\n        The shape of the input as a tuple `(height, width)`.\n    \"\"\"\n    return self.height, self.width\n</code></pre>"},{"location":"api/#mmlearn.cli.run.BlockwiseImagePatchMaskGenerator.__call__","title":"__call__","text":"<pre><code>__call__()\n</code></pre> <p>Generate a random mask.</p> <p>Returns a random mask of shape (nb_patches, nb_patches) based on the configuration where the number of patches to be masked is num_masking_patches.</p> <p>Returns:</p> Name Type Description <code>mask</code> <code>Tensor</code> <p>A mask of shape (nb_patches, nb_patches)</p> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>def __call__(self) -&gt; torch.Tensor:\n    \"\"\"Generate a random mask.\n\n    Returns a random mask of shape (nb_patches, nb_patches) based on the\n    configuration where the number of patches to be masked is num_masking_patches.\n\n    Returns\n    -------\n    mask: torch.Tensor\n        A mask of shape (nb_patches, nb_patches)\n\n    \"\"\"\n    mask = torch.zeros(self.get_shape(), dtype=torch.int)\n    mask_count = 0\n    while mask_count &lt; self.num_masking_patches:\n        max_mask_patches = self.num_masking_patches - mask_count\n        max_mask_patches = min(max_mask_patches, self.max_num_patches)\n\n        delta = self._mask(mask, max_mask_patches)\n        if delta == 0:\n            break\n        mask_count += delta\n\n    return mask\n</code></pre>"},{"location":"api/#mmlearn.cli.run.RandomMaskGenerator","title":"RandomMaskGenerator","text":"<p>Random mask generator.</p> <p>Returns a random mask of shape <code>(nb_patches, nb_patches)</code> based on the configuration where the number of patches to be masked is num_masking_patches. This is intended to be used for tasks like masked language modeling.</p> <p>Parameters:</p> Name Type Description Default <code>probability</code> <code>float</code> <p>Probability of masking a token.</p> required Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>@store(group=\"datasets/masking\", provider=\"mmlearn\", probability=0.15)\nclass RandomMaskGenerator:\n    \"\"\"Random mask generator.\n\n    Returns a random mask of shape `(nb_patches, nb_patches)` based on the\n    configuration where the number of patches to be masked is num_masking_patches.\n    **This is intended to be used for tasks like masked language modeling.**\n\n    Parameters\n    ----------\n    probability : float\n        Probability of masking a token.\n    \"\"\"\n\n    def __init__(self, probability: float):\n        self.probability = probability\n\n    def __call__(\n        self,\n        inputs: torch.Tensor,\n        tokenizer: PreTrainedTokenizerBase,\n        special_tokens_mask: Optional[torch.Tensor] = None,\n    ) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Generate a random mask.\n\n        Returns a random mask of shape (nb_patches, nb_patches) based on the\n        configuration where the number of patches to be masked is num_masking_patches.\n\n        Returns\n        -------\n        inputs : torch.Tensor\n            The encoded inputs.\n        tokenizer : PreTrainedTokenizer\n            The tokenizer.\n        special_tokens_mask : Optional[torch.Tensor], default=None\n            Mask for special tokens.\n        \"\"\"\n        inputs = tokenizer.pad(inputs, return_tensors=\"pt\")[\"input_ids\"]\n        labels = inputs.clone()\n        # We sample a few tokens in each sequence for MLM training\n        # (with probability `self.probability`)\n        probability_matrix = torch.full(labels.shape, self.probability)\n        if special_tokens_mask is None:\n            special_tokens_mask = tokenizer.get_special_tokens_mask(\n                labels, already_has_special_tokens=True\n            )\n            special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n        else:\n            special_tokens_mask = special_tokens_mask.bool()\n\n        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n        masked_indices = torch.bernoulli(probability_matrix).bool()\n        labels[~masked_indices] = tokenizer.pad_token_id\n        # 80% of the time, replace masked input tokens with tokenizer.mask_token([MASK])\n        indices_replaced = (\n            torch.bernoulli(torch.full(labels.shape, 0.8)).bool() &amp; masked_indices\n        )\n        inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n\n        # 10% of the time, we replace masked input tokens with random word\n        indices_random = (\n            torch.bernoulli(torch.full(labels.shape, 0.5)).bool()\n            &amp; masked_indices\n            &amp; ~indices_replaced\n        )\n        random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n        inputs[indices_random] = random_words[indices_random]\n\n        # Rest of the time (10% of the time) we keep the masked input tokens unchanged\n        return inputs, labels, masked_indices\n</code></pre>"},{"location":"api/#mmlearn.cli.run.RandomMaskGenerator.__call__","title":"__call__","text":"<pre><code>__call__(inputs, tokenizer, special_tokens_mask=None)\n</code></pre> <p>Generate a random mask.</p> <p>Returns a random mask of shape (nb_patches, nb_patches) based on the configuration where the number of patches to be masked is num_masking_patches.</p> <p>Returns:</p> Name Type Description <code>inputs</code> <code>Tensor</code> <p>The encoded inputs.</p> <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>The tokenizer.</p> <code>special_tokens_mask</code> <code>Optional[torch.Tensor], default=None</code> <p>Mask for special tokens.</p> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>def __call__(\n    self,\n    inputs: torch.Tensor,\n    tokenizer: PreTrainedTokenizerBase,\n    special_tokens_mask: Optional[torch.Tensor] = None,\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Generate a random mask.\n\n    Returns a random mask of shape (nb_patches, nb_patches) based on the\n    configuration where the number of patches to be masked is num_masking_patches.\n\n    Returns\n    -------\n    inputs : torch.Tensor\n        The encoded inputs.\n    tokenizer : PreTrainedTokenizer\n        The tokenizer.\n    special_tokens_mask : Optional[torch.Tensor], default=None\n        Mask for special tokens.\n    \"\"\"\n    inputs = tokenizer.pad(inputs, return_tensors=\"pt\")[\"input_ids\"]\n    labels = inputs.clone()\n    # We sample a few tokens in each sequence for MLM training\n    # (with probability `self.probability`)\n    probability_matrix = torch.full(labels.shape, self.probability)\n    if special_tokens_mask is None:\n        special_tokens_mask = tokenizer.get_special_tokens_mask(\n            labels, already_has_special_tokens=True\n        )\n        special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n    else:\n        special_tokens_mask = special_tokens_mask.bool()\n\n    probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n    masked_indices = torch.bernoulli(probability_matrix).bool()\n    labels[~masked_indices] = tokenizer.pad_token_id\n    # 80% of the time, replace masked input tokens with tokenizer.mask_token([MASK])\n    indices_replaced = (\n        torch.bernoulli(torch.full(labels.shape, 0.8)).bool() &amp; masked_indices\n    )\n    inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n\n    # 10% of the time, we replace masked input tokens with random word\n    indices_random = (\n        torch.bernoulli(torch.full(labels.shape, 0.5)).bool()\n        &amp; masked_indices\n        &amp; ~indices_replaced\n    )\n    random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n    inputs[indices_random] = random_words[indices_random]\n\n    # Rest of the time (10% of the time) we keep the masked input tokens unchanged\n    return inputs, labels, masked_indices\n</code></pre>"},{"location":"api/#mmlearn.cli.run.HFTokenizer","title":"HFTokenizer","text":"<p>A wrapper for loading HuggingFace tokenizers.</p> <p>This class wraps any huggingface tokenizer that can be initialized with meth:<code>transformers.AutoTokenizer.from_pretrained</code>. It preprocesses the input text and returns a dictionary with the tokenized text and other relevant information like attention masks.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>Pretrained model name or path - same as in meth:<code>transformers.AutoTokenizer.from_pretrained</code>.</p> required <code>max_length</code> <code>Optional[int]</code> <p>Maximum length of the tokenized sequence. This is passed to the tokenizer :meth:<code>__call__</code> method.</p> <code>None</code> <code>padding</code> <code>bool or str</code> <p>Padding strategy. Same as in meth:<code>transformers.AutoTokenizer.from_pretrained</code>; passed to the tokenizer :meth:<code>__call__</code> method.</p> <code>False</code> <code>truncation</code> <code>Optional[Union[bool, str]]</code> <p>Truncation strategy. Same as in meth:<code>transformers.AutoTokenizer.from_pretrained</code>; passed to the tokenizer :meth:<code>__call__</code> method.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to meth:<code>transformers.AutoTokenizer.from_pretrained</code>.</p> <code>{}</code> Source code in <code>mmlearn/datasets/processors/tokenizers.py</code> <pre><code>@store(group=\"datasets/tokenizers\", provider=\"mmlearn\")\nclass HFTokenizer:\n    \"\"\"A wrapper for loading HuggingFace tokenizers.\n\n    This class wraps any huggingface tokenizer that can be initialized with\n    :py:meth:`transformers.AutoTokenizer.from_pretrained`. It preprocesses the\n    input text and returns a dictionary with the tokenized text and other\n    relevant information like attention masks.\n\n    Parameters\n    ----------\n    model_name_or_path : str\n        Pretrained model name or path - same as in :py:meth:`transformers.AutoTokenizer.from_pretrained`.\n    max_length : Optional[int], optional, default=None\n        Maximum length of the tokenized sequence. This is passed to the tokenizer\n        :meth:`__call__` method.\n    padding : bool or str, default=False\n        Padding strategy. Same as in :py:meth:`transformers.AutoTokenizer.from_pretrained`;\n        passed to the tokenizer :meth:`__call__` method.\n    truncation : Optional[Union[bool, str]], optional, default=None\n        Truncation strategy. Same as in :py:meth:`transformers.AutoTokenizer.from_pretrained`;\n        passed to the tokenizer :meth:`__call__` method.\n    **kwargs : Any\n        Additional arguments passed to :py:meth:`transformers.AutoTokenizer.from_pretrained`.\n    \"\"\"  # noqa: W505\n\n    def __init__(\n        self,\n        model_name_or_path: str,\n        max_length: Optional[int] = None,\n        padding: Union[bool, str] = False,\n        truncation: Optional[Union[bool, str]] = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, **kwargs)\n        self.max_length = max_length\n        self.padding = padding\n        self.truncation = truncation\n\n    def __call__(\n        self, sentence: Union[str, list[str]], **kwargs: Any\n    ) -&gt; dict[str, torch.Tensor]:\n        \"\"\"Tokenize a text or a list of texts using the HuggingFace tokenizer.\n\n        Parameters\n        ----------\n        sentence : Union[str, list[str]]\n            Sentence(s) to be tokenized.\n        **kwargs : Any\n            Additional arguments passed to the tokenizer :meth:`__call__` method.\n\n        Returns\n        -------\n        dict[str, torch.Tensor]\n            Tokenized sentence(s).\n\n        Notes\n        -----\n        The ``input_ids`` key is replaced with ``Modalities.TEXT`` for consistency.\n        \"\"\"\n        batch_encoding = self.tokenizer(\n            sentence,\n            max_length=self.max_length,\n            padding=self.padding,\n            truncation=self.truncation,\n            return_tensors=\"pt\",\n            **kwargs,\n        )\n\n        if isinstance(\n            sentence, str\n        ):  # remove batch dimension if input is a single sentence\n            for key, value in batch_encoding.items():\n                if isinstance(value, torch.Tensor):\n                    batch_encoding[key] = torch.squeeze(value, 0)\n\n        # use 'Modalities.TEXT' key for input_ids for consistency\n        batch_encoding[Modalities.TEXT.name] = batch_encoding[\"input_ids\"]\n        return dict(batch_encoding)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.HFTokenizer.__call__","title":"__call__","text":"<pre><code>__call__(sentence, **kwargs)\n</code></pre> <p>Tokenize a text or a list of texts using the HuggingFace tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>sentence</code> <code>Union[str, list[str]]</code> <p>Sentence(s) to be tokenized.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to the tokenizer :meth:<code>__call__</code> method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>Tokenized sentence(s).</p> Notes <p>The <code>input_ids</code> key is replaced with <code>Modalities.TEXT</code> for consistency.</p> Source code in <code>mmlearn/datasets/processors/tokenizers.py</code> <pre><code>def __call__(\n    self, sentence: Union[str, list[str]], **kwargs: Any\n) -&gt; dict[str, torch.Tensor]:\n    \"\"\"Tokenize a text or a list of texts using the HuggingFace tokenizer.\n\n    Parameters\n    ----------\n    sentence : Union[str, list[str]]\n        Sentence(s) to be tokenized.\n    **kwargs : Any\n        Additional arguments passed to the tokenizer :meth:`__call__` method.\n\n    Returns\n    -------\n    dict[str, torch.Tensor]\n        Tokenized sentence(s).\n\n    Notes\n    -----\n    The ``input_ids`` key is replaced with ``Modalities.TEXT`` for consistency.\n    \"\"\"\n    batch_encoding = self.tokenizer(\n        sentence,\n        max_length=self.max_length,\n        padding=self.padding,\n        truncation=self.truncation,\n        return_tensors=\"pt\",\n        **kwargs,\n    )\n\n    if isinstance(\n        sentence, str\n    ):  # remove batch dimension if input is a single sentence\n        for key, value in batch_encoding.items():\n            if isinstance(value, torch.Tensor):\n                batch_encoding[key] = torch.squeeze(value, 0)\n\n    # use 'Modalities.TEXT' key for input_ids for consistency\n    batch_encoding[Modalities.TEXT.name] = batch_encoding[\"input_ids\"]\n    return dict(batch_encoding)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.TrimText","title":"TrimText","text":"<p>Trim text strings as a preprocessing step before tokenization.</p> <p>Parameters:</p> Name Type Description Default <code>trim_size</code> <code>int</code> <p>The maximum length of the trimmed text.</p> required Source code in <code>mmlearn/datasets/processors/transforms.py</code> <pre><code>@store(group=\"datasets/transforms\", provider=\"mmlearn\")\nclass TrimText:\n    \"\"\"Trim text strings as a preprocessing step before tokenization.\n\n    Parameters\n    ----------\n    trim_size : int\n        The maximum length of the trimmed text.\n    \"\"\"\n\n    def __init__(self, trim_size: int) -&gt; None:\n        self.trim_size = trim_size\n\n    def __call__(self, sentence: Union[str, list[str]]) -&gt; Union[str, list[str]]:\n        \"\"\"Trim the given sentence(s).\n\n        Parameters\n        ----------\n        sentence : Union[str, list[str]]\n            Sentence(s) to be trimmed.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            Trimmed sentence(s).\n\n        Raises\n        ------\n        TypeError\n            If the input sentence is not a string or list of strings.\n        \"\"\"\n        if not isinstance(sentence, (list, str)):\n            raise TypeError(\n                \"Expected argument `sentence` to be a string or list of strings, \"\n                f\"but got {type(sentence)}\"\n            )\n\n        if isinstance(sentence, str):\n            return sentence[: self.trim_size]\n\n        for i, s in enumerate(sentence):\n            sentence[i] = s[: self.trim_size]\n\n        return sentence\n</code></pre>"},{"location":"api/#mmlearn.cli.run.TrimText.__call__","title":"__call__","text":"<pre><code>__call__(sentence)\n</code></pre> <p>Trim the given sentence(s).</p> <p>Parameters:</p> Name Type Description Default <code>sentence</code> <code>Union[str, list[str]]</code> <p>Sentence(s) to be trimmed.</p> required <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>Trimmed sentence(s).</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input sentence is not a string or list of strings.</p> Source code in <code>mmlearn/datasets/processors/transforms.py</code> <pre><code>def __call__(self, sentence: Union[str, list[str]]) -&gt; Union[str, list[str]]:\n    \"\"\"Trim the given sentence(s).\n\n    Parameters\n    ----------\n    sentence : Union[str, list[str]]\n        Sentence(s) to be trimmed.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        Trimmed sentence(s).\n\n    Raises\n    ------\n    TypeError\n        If the input sentence is not a string or list of strings.\n    \"\"\"\n    if not isinstance(sentence, (list, str)):\n        raise TypeError(\n            \"Expected argument `sentence` to be a string or list of strings, \"\n            f\"but got {type(sentence)}\"\n        )\n\n    if isinstance(sentence, str):\n        return sentence[: self.trim_size]\n\n    for i, s in enumerate(sentence):\n        sentence[i] = s[: self.trim_size]\n\n    return sentence\n</code></pre>"},{"location":"api/#mmlearn.cli.run.HFCLIPTextEncoder","title":"HFCLIPTextEncoder","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper around the <code>CLIPTextModel</code> from HuggingFace.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>The huggingface model name or a local path from which to load the model.</p> required <code>pretrained</code> <code>bool</code> <p>Whether to load the pretrained weights or not.</p> <code>True</code> <code>pooling_layer</code> <code>Optional[Module]</code> <p>Pooling layer to apply to the last hidden state of the model.</p> <code>None</code> <code>freeze_layers</code> <code>Union[int, float, list[int], bool]</code> <p>Whether to freeze layers of the model and which layers to freeze. If <code>True</code>, all model layers are frozen. If it is an integer, the first <code>N</code> layers of the model are frozen. If it is a float, the first <code>N</code> percent of the layers are frozen. If it is a list of integers, the layers at the indices in the list are frozen.</p> <code>False</code> <code>freeze_layer_norm</code> <code>bool</code> <p>Whether to freeze the layer normalization layers of the model.</p> <code>True</code> <code>peft_config</code> <code>Optional[PeftConfig]</code> <p>The configuration from the <code>peft &lt;https://huggingface.co/docs/peft/index&gt;</code>_ library to use to wrap the model for parameter-efficient finetuning.</p> <code>None</code> <code>model_config_kwargs</code> <code>Optional[dict[str, Any]]</code> <p>Additional keyword arguments to pass to the model configuration.</p> <code>None</code> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If both <code>peft_config</code> and <code>freeze_layers</code> are set. The <code>peft_config</code> will override the <code>freeze_layers</code> setting.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>@store(\n    group=\"modules/encoders\",\n    provider=\"mmlearn\",\n    model_name_or_path=\"openai/clip-vit-base-patch16\",\n    hydra_convert=\"object\",  # required for `peft_config` to be converted to a `PeftConfig` object\n)\nclass HFCLIPTextEncoder(nn.Module):\n    \"\"\"Wrapper around the ``CLIPTextModel`` from HuggingFace.\n\n    Parameters\n    ----------\n    model_name_or_path : str\n        The huggingface model name or a local path from which to load the model.\n    pretrained : bool, default=True\n        Whether to load the pretrained weights or not.\n    pooling_layer : Optional[torch.nn.Module], optional, default=None\n        Pooling layer to apply to the last hidden state of the model.\n    freeze_layers : Union[int, float, list[int], bool], default=False\n        Whether to freeze layers of the model and which layers to freeze. If ``True``,\n        all model layers are frozen. If it is an integer, the first ``N`` layers of\n        the model are frozen. If it is a float, the first ``N`` percent of the layers\n        are frozen. If it is a list of integers, the layers at the indices in the\n        list are frozen.\n    freeze_layer_norm : bool, default=True\n        Whether to freeze the layer normalization layers of the model.\n    peft_config : Optional[PeftConfig], optional, default=None\n        The configuration from the `peft &lt;https://huggingface.co/docs/peft/index&gt;`_\n        library to use to wrap the model for parameter-efficient finetuning.\n    model_config_kwargs : Optional[dict[str, Any]], optional, default=None\n        Additional keyword arguments to pass to the model configuration.\n\n    Warns\n    -----\n    UserWarning\n        If both ``peft_config`` and ``freeze_layers`` are set. The ``peft_config``\n        will override the ``freeze_layers`` setting.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name_or_path: str,\n        pretrained: bool = True,\n        pooling_layer: Optional[nn.Module] = None,\n        freeze_layers: Union[int, float, list[int], bool] = False,\n        freeze_layer_norm: bool = True,\n        peft_config: Optional[\"PeftConfig\"] = None,\n        model_config_kwargs: Optional[dict[str, Any]] = None,\n    ) -&gt; None:\n        super().__init__()\n        _warn_freeze_with_peft(peft_config, freeze_layers)\n\n        model = hf_utils.load_huggingface_model(\n            transformers.CLIPTextModel,\n            model_name_or_path=model_name_or_path,\n            load_pretrained_weights=pretrained,\n            model_config_kwargs=model_config_kwargs,\n        )\n        model = _freeze_text_model(model, freeze_layers, freeze_layer_norm)\n        if peft_config is not None:\n            model = hf_utils._wrap_peft_model(model, peft_config)\n\n        self.model = model\n        self.pooling_layer = pooling_layer\n\n    def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The ``input_ids`` will be expected under the\n            ``Modalities.TEXT`` key.\n\n        Returns\n        -------\n        BaseModelOutput\n            The output of the model, including the last hidden state, all hidden\n            states, and the attention weights, if ``output_attentions`` is set\n            to ``True``.\n        \"\"\"\n        outputs = self.model(\n            input_ids=inputs[Modalities.TEXT.name],\n            attention_mask=inputs.get(\"attention_mask\")\n            or inputs.get(Modalities.TEXT.attention_mask),\n            position_ids=inputs.get(\"position_ids\"),\n            output_attentions=inputs.get(\"output_attentions\"),\n            return_dict=True,\n        )\n        last_hidden_state = outputs.hidden_states[-1]  # NOTE: no layer norm applied\n        if self.pooling_layer:\n            last_hidden_state = self.pooling_layer(last_hidden_state)\n\n        return BaseModelOutput(\n            last_hidden_state=last_hidden_state,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n</code></pre>"},{"location":"api/#mmlearn.cli.run.HFCLIPTextEncoder.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The <code>input_ids</code> will be expected under the <code>Modalities.TEXT</code> key.</p> required <p>Returns:</p> Type Description <code>BaseModelOutput</code> <p>The output of the model, including the last hidden state, all hidden states, and the attention weights, if <code>output_attentions</code> is set to <code>True</code>.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The ``input_ids`` will be expected under the\n        ``Modalities.TEXT`` key.\n\n    Returns\n    -------\n    BaseModelOutput\n        The output of the model, including the last hidden state, all hidden\n        states, and the attention weights, if ``output_attentions`` is set\n        to ``True``.\n    \"\"\"\n    outputs = self.model(\n        input_ids=inputs[Modalities.TEXT.name],\n        attention_mask=inputs.get(\"attention_mask\")\n        or inputs.get(Modalities.TEXT.attention_mask),\n        position_ids=inputs.get(\"position_ids\"),\n        output_attentions=inputs.get(\"output_attentions\"),\n        return_dict=True,\n    )\n    last_hidden_state = outputs.hidden_states[-1]  # NOTE: no layer norm applied\n    if self.pooling_layer:\n        last_hidden_state = self.pooling_layer(last_hidden_state)\n\n    return BaseModelOutput(\n        last_hidden_state=last_hidden_state,\n        hidden_states=outputs.hidden_states,\n        attentions=outputs.attentions,\n    )\n</code></pre>"},{"location":"api/#mmlearn.cli.run.HFCLIPTextEncoderWithProjection","title":"HFCLIPTextEncoderWithProjection","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper around the <code>CLIPTextModelWithProjection</code> from HuggingFace.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>The huggingface model name or a local path from which to load the model.</p> required <code>pretrained</code> <code>bool</code> <p>Whether to load the pretrained weights or not.</p> <code>True</code> <code>use_all_token_embeddings</code> <code>bool</code> <p>Whether to use all token embeddings for the text. If <code>False</code> the first token embedding will be used.</p> <code>False</code> <code>freeze_layers</code> <code>Union[int, float, list[int], bool]</code> <p>Whether to freeze layers of the model and which layers to freeze. If <code>True</code>, all model layers are frozen. If it is an integer, the first <code>N</code> layers of the model are frozen. If it is a float, the first <code>N</code> percent of the layers are frozen. If it is a list of integers, the layers at the indices in the list are frozen.</p> <code>False</code> <code>freeze_layer_norm</code> <code>bool</code> <p>Whether to freeze the layer normalization layers of the model.</p> <code>True</code> <code>peft_config</code> <code>Optional[PeftConfig]</code> <p>The configuration from the <code>peft &lt;https://huggingface.co/docs/peft/index&gt;</code>_ library to use to wrap the model for parameter-efficient finetuning.</p> <code>None</code> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If both <code>peft_config</code> and <code>freeze_layers</code> are set. The <code>peft_config</code> will override the <code>freeze_layers</code> setting.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>@store(\n    group=\"modules/encoders\",\n    provider=\"mmlearn\",\n    model_name_or_path=\"openai/clip-vit-base-patch16\",\n    hydra_convert=\"object\",\n)\nclass HFCLIPTextEncoderWithProjection(nn.Module):\n    \"\"\"Wrapper around the ``CLIPTextModelWithProjection`` from HuggingFace.\n\n    Parameters\n    ----------\n    model_name_or_path : str\n        The huggingface model name or a local path from which to load the model.\n    pretrained : bool, default=True\n        Whether to load the pretrained weights or not.\n    use_all_token_embeddings : bool, default=False\n        Whether to use all token embeddings for the text. If ``False`` the first token\n        embedding will be used.\n    freeze_layers : Union[int, float, list[int], bool], default=False\n        Whether to freeze layers of the model and which layers to freeze. If ``True``,\n        all model layers are frozen. If it is an integer, the first ``N`` layers of\n        the model are frozen. If it is a float, the first ``N`` percent of the layers\n        are frozen. If it is a list of integers, the layers at the indices in the\n        list are frozen.\n    freeze_layer_norm : bool, default=True\n        Whether to freeze the layer normalization layers of the model.\n    peft_config : Optional[PeftConfig], optional, default=None\n        The configuration from the `peft &lt;https://huggingface.co/docs/peft/index&gt;`_\n        library to use to wrap the model for parameter-efficient finetuning.\n\n    Warns\n    -----\n    UserWarning\n        If both ``peft_config`` and ``freeze_layers`` are set. The ``peft_config``\n        will override the ``freeze_layers`` setting.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name_or_path: str,\n        pretrained: bool = True,\n        use_all_token_embeddings: bool = False,\n        freeze_layers: Union[int, float, list[int], bool] = False,\n        freeze_layer_norm: bool = True,\n        peft_config: Optional[\"PeftConfig\"] = None,\n        model_config_kwargs: Optional[dict[str, Any]] = None,\n    ) -&gt; None:\n        super().__init__()\n        _warn_freeze_with_peft(peft_config, freeze_layers)\n\n        self.use_all_token_embeddings = use_all_token_embeddings\n\n        model = hf_utils.load_huggingface_model(\n            transformers.CLIPTextModelWithProjection,\n            model_name_or_path,\n            load_pretrained_weights=pretrained,\n            model_config_kwargs=model_config_kwargs,\n            config_type=CLIPTextConfig,\n        )\n\n        model = _freeze_text_model(model, freeze_layers, freeze_layer_norm)\n        if peft_config is not None:\n            model = hf_utils._wrap_peft_model(model, peft_config)\n\n        self.model = model\n\n    def forward(self, inputs: dict[str, Any]) -&gt; tuple[torch.Tensor]:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The ``input_ids`` will be expected under the\n            ``Modalities.TEXT`` key.\n\n        Returns\n        -------\n        tuple[torch.Tensor]\n            The text embeddings. Will be a tuple with a single element.\n        \"\"\"\n        input_ids = inputs[Modalities.TEXT.name]\n        attention_mask: Optional[torch.Tensor] = inputs.get(\n            \"attention_mask\", inputs.get(Modalities.TEXT.attention_mask, None)\n        )\n        position_ids = inputs.get(\"position_ids\")\n\n        if self.use_all_token_embeddings:\n            text_outputs = self.model.text_model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                return_dict=True,\n            )\n            # TODO: add more options for pooling before projection\n            text_embeds = self.model.text_projection(text_outputs.last_hidden_state)\n        else:\n            text_embeds = self.model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                return_dict=True,\n            ).text_embeds\n\n        return (text_embeds,)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.HFCLIPTextEncoderWithProjection.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The <code>input_ids</code> will be expected under the <code>Modalities.TEXT</code> key.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor]</code> <p>The text embeddings. Will be a tuple with a single element.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; tuple[torch.Tensor]:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The ``input_ids`` will be expected under the\n        ``Modalities.TEXT`` key.\n\n    Returns\n    -------\n    tuple[torch.Tensor]\n        The text embeddings. Will be a tuple with a single element.\n    \"\"\"\n    input_ids = inputs[Modalities.TEXT.name]\n    attention_mask: Optional[torch.Tensor] = inputs.get(\n        \"attention_mask\", inputs.get(Modalities.TEXT.attention_mask, None)\n    )\n    position_ids = inputs.get(\"position_ids\")\n\n    if self.use_all_token_embeddings:\n        text_outputs = self.model.text_model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            return_dict=True,\n        )\n        # TODO: add more options for pooling before projection\n        text_embeds = self.model.text_projection(text_outputs.last_hidden_state)\n    else:\n        text_embeds = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            return_dict=True,\n        ).text_embeds\n\n    return (text_embeds,)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.HFCLIPVisionEncoder","title":"HFCLIPVisionEncoder","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper around the <code>CLIPVisionModel</code> from HuggingFace.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>The huggingface model name or a local path from which to load the model.</p> required <code>pretrained</code> <code>bool</code> <p>Whether to load the pretrained weights or not.</p> <code>True</code> <code>pooling_layer</code> <code>Optional[Module]</code> <p>Pooling layer to apply to the last hidden state of the model.</p> <code>None</code> <code>freeze_layers</code> <code>Union[int, float, list[int], bool]</code> <p>Whether to freeze layers of the model and which layers to freeze. If <code>True</code>, all model layers are frozen. If it is an integer, the first <code>N</code> layers of the model are frozen. If it is a float, the first <code>N</code> percent of the layers are frozen. If it is a list of integers, the layers at the indices in the list are frozen.</p> <code>False</code> <code>freeze_layer_norm</code> <code>bool</code> <p>Whether to freeze the layer normalization layers of the model.</p> <code>True</code> <code>patch_dropout_rate</code> <code>float</code> <p>The proportion of patch embeddings to drop out.</p> <code>0.0</code> <code>patch_dropout_shuffle</code> <code>bool</code> <p>Whether to shuffle the patches while applying patch dropout.</p> <code>False</code> <code>patch_dropout_bias</code> <code>Optional[float]</code> <p>The bias to apply to the patch dropout mask.</p> <code>None</code> <code>peft_config</code> <code>Optional[PeftConfig]</code> <p>The configuration from the <code>peft &lt;https://huggingface.co/docs/peft/index&gt;</code>_ library to use to wrap the model for parameter-efficient finetuning.</p> <code>None</code> <code>model_config_kwargs</code> <code>Optional[dict[str, Any]]</code> <p>Additional keyword arguments to pass to the model configuration.</p> <code>None</code> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If both <code>peft_config</code> and <code>freeze_layers</code> are set. The <code>peft_config</code> will override the <code>freeze_layers</code> setting.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>@store(\n    group=\"modules/encoders\",\n    provider=\"mmlearn\",\n    model_name_or_path=\"openai/clip-vit-base-patch16\",\n    hydra_convert=\"object\",\n)\nclass HFCLIPVisionEncoder(nn.Module):\n    \"\"\"Wrapper around the ``CLIPVisionModel`` from HuggingFace.\n\n    Parameters\n    ----------\n    model_name_or_path : str\n        The huggingface model name or a local path from which to load the model.\n    pretrained : bool, default=True\n        Whether to load the pretrained weights or not.\n    pooling_layer : Optional[torch.nn.Module], optional, default=None\n        Pooling layer to apply to the last hidden state of the model.\n    freeze_layers : Union[int, float, list[int], bool], default=False\n        Whether to freeze layers of the model and which layers to freeze. If ``True``,\n        all model layers are frozen. If it is an integer, the first ``N`` layers of\n        the model are frozen. If it is a float, the first ``N`` percent of the layers\n        are frozen. If it is a list of integers, the layers at the indices in the\n        list are frozen.\n    freeze_layer_norm : bool, default=True\n        Whether to freeze the layer normalization layers of the model.\n    patch_dropout_rate : float, default=0.0\n        The proportion of patch embeddings to drop out.\n    patch_dropout_shuffle : bool, default=False\n        Whether to shuffle the patches while applying patch dropout.\n    patch_dropout_bias : Optional[float], optional, default=None\n        The bias to apply to the patch dropout mask.\n    peft_config : Optional[PeftConfig], optional, default=None\n        The configuration from the `peft &lt;https://huggingface.co/docs/peft/index&gt;`_\n        library to use to wrap the model for parameter-efficient finetuning.\n    model_config_kwargs : Optional[dict[str, Any]], optional, default=None\n        Additional keyword arguments to pass to the model configuration.\n\n    Warns\n    -----\n    UserWarning\n        If both ``peft_config`` and ``freeze_layers`` are set. The ``peft_config``\n        will override the ``freeze_layers`` setting.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name_or_path: str,\n        pretrained: bool = True,\n        pooling_layer: Optional[nn.Module] = None,\n        freeze_layers: Union[int, float, list[int], bool] = False,\n        freeze_layer_norm: bool = True,\n        patch_dropout_rate: float = 0.0,\n        patch_dropout_shuffle: bool = False,\n        patch_dropout_bias: Optional[float] = None,\n        peft_config: Optional[\"PeftConfig\"] = None,\n        model_config_kwargs: Optional[dict[str, Any]] = None,\n    ) -&gt; None:\n        super().__init__()\n        _warn_freeze_with_peft(peft_config, freeze_layers)\n\n        model = hf_utils.load_huggingface_model(\n            transformers.CLIPVisionModel,\n            model_name_or_path=model_name_or_path,\n            load_pretrained_weights=pretrained,\n            model_config_kwargs=model_config_kwargs,\n        )\n        model = _freeze_vision_model(model, freeze_layers, freeze_layer_norm)\n        if peft_config is not None:\n            model = hf_utils._wrap_peft_model(model, peft_config)\n\n        self.model = model.vision_model\n        self.pooling_layer = pooling_layer\n        self.patch_dropout = None\n        if patch_dropout_rate &gt; 0:\n            self.patch_dropout = PatchDropout(\n                keep_rate=1 - patch_dropout_rate,\n                token_shuffling=patch_dropout_shuffle,\n                bias=patch_dropout_bias,\n            )\n\n    def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The image tensor will be expected under the\n            ``Modalities.RGB`` key.\n\n        Returns\n        -------\n        BaseModelOutput\n            The output of the model, including the last hidden state, all hidden states,\n            and the attention weights, if ``output_attentions`` is set to ``True``.\n\n        \"\"\"\n        # FIXME: handle other vision modalities\n        pixel_values = inputs[Modalities.RGB.name]\n        hidden_states = self.model.embeddings(pixel_values)\n        if self.patch_dropout is not None:\n            hidden_states = self.patch_dropout(hidden_states)\n        hidden_states = self.model.pre_layrnorm(hidden_states)\n\n        encoder_outputs = self.model.encoder(\n            inputs_embeds=hidden_states,\n            output_attentions=inputs.get(\n                \"output_attentions\", self.model.config.output_attentions\n            ),\n            output_hidden_states=True,\n            return_dict=True,\n        )\n\n        last_hidden_state = encoder_outputs[0]\n        if self.pooling_layer is not None:\n            last_hidden_state = self.pooling_layer(last_hidden_state)\n\n        return BaseModelOutput(\n            last_hidden_state=last_hidden_state,\n            hidden_states=encoder_outputs.hidden_states,\n            attentions=encoder_outputs.attentions,\n        )\n</code></pre>"},{"location":"api/#mmlearn.cli.run.HFCLIPVisionEncoder.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The image tensor will be expected under the <code>Modalities.RGB</code> key.</p> required <p>Returns:</p> Type Description <code>BaseModelOutput</code> <p>The output of the model, including the last hidden state, all hidden states, and the attention weights, if <code>output_attentions</code> is set to <code>True</code>.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The image tensor will be expected under the\n        ``Modalities.RGB`` key.\n\n    Returns\n    -------\n    BaseModelOutput\n        The output of the model, including the last hidden state, all hidden states,\n        and the attention weights, if ``output_attentions`` is set to ``True``.\n\n    \"\"\"\n    # FIXME: handle other vision modalities\n    pixel_values = inputs[Modalities.RGB.name]\n    hidden_states = self.model.embeddings(pixel_values)\n    if self.patch_dropout is not None:\n        hidden_states = self.patch_dropout(hidden_states)\n    hidden_states = self.model.pre_layrnorm(hidden_states)\n\n    encoder_outputs = self.model.encoder(\n        inputs_embeds=hidden_states,\n        output_attentions=inputs.get(\n            \"output_attentions\", self.model.config.output_attentions\n        ),\n        output_hidden_states=True,\n        return_dict=True,\n    )\n\n    last_hidden_state = encoder_outputs[0]\n    if self.pooling_layer is not None:\n        last_hidden_state = self.pooling_layer(last_hidden_state)\n\n    return BaseModelOutput(\n        last_hidden_state=last_hidden_state,\n        hidden_states=encoder_outputs.hidden_states,\n        attentions=encoder_outputs.attentions,\n    )\n</code></pre>"},{"location":"api/#mmlearn.cli.run.HFCLIPVisionEncoderWithProjection","title":"HFCLIPVisionEncoderWithProjection","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper around the <code>CLIPVisionModelWithProjection</code> class from HuggingFace.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>The huggingface model name or a local path from which to load the model.</p> required <code>pretrained</code> <code>bool</code> <p>Whether to load the pretrained weights or not.</p> <code>True</code> <code>use_all_token_embeddings</code> <code>bool</code> <p>Whether to use all token embeddings for the text. If <code>False</code> the first token embedding will be used.</p> <code>False</code> <code>freeze_layers</code> <code>Union[int, float, list[int], bool]</code> <p>Whether to freeze layers of the model and which layers to freeze. If <code>True</code>, all model layers are frozen. If it is an integer, the first <code>N` layers of the model are frozen. If it is a float, the first</code>N`` percent of the layers are frozen. If it is a list of integers, the layers at the indices in the list are frozen.</p> <code>False</code> <code>freeze_layer_norm</code> <code>bool</code> <p>Whether to freeze the layer normalization layers of the model.</p> <code>True</code> <code>patch_dropout_rate</code> <code>float</code> <p>The proportion of patch embeddings to drop out.</p> <code>0.0</code> <code>patch_dropout_shuffle</code> <code>bool</code> <p>Whether to shuffle the patches while applying patch dropout.</p> <code>False</code> <code>patch_dropout_bias</code> <code>float</code> <p>The bias to apply to the patch dropout mask.</p> <code>None</code> <code>peft_config</code> <code>Optional[PeftConfig]</code> <p>The configuration from the <code>peft &lt;https://huggingface.co/docs/peft/index&gt;</code>_ library to use to wrap the model for parameter-efficient finetuning.</p> <code>None</code> <code>model_config_kwargs</code> <code>dict[str, Any]</code> <p>Additional keyword arguments to pass to the model configuration.</p> <code>None</code> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If both <code>peft_config</code> and <code>freeze_layers</code> are set. The <code>peft_config</code> will override the <code>freeze_layers</code> setting.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>@store(\n    group=\"modules/encoders\",\n    provider=\"mmlearn\",\n    model_name_or_path=\"openai/clip-vit-base-patch16\",\n    hydra_convert=\"object\",\n)\nclass HFCLIPVisionEncoderWithProjection(nn.Module):\n    \"\"\"Wrapper around the ``CLIPVisionModelWithProjection`` class from HuggingFace.\n\n    Parameters\n    ----------\n    model_name_or_path : str\n        The huggingface model name or a local path from which to load the model.\n    pretrained : bool, default=True\n        Whether to load the pretrained weights or not.\n    use_all_token_embeddings : bool, default=False\n        Whether to use all token embeddings for the text. If ``False`` the first token\n        embedding will be used.\n    freeze_layers : Union[int, float, list[int], bool], default=False\n        Whether to freeze layers of the model and which layers to freeze. If ``True``,\n        all model layers are frozen. If it is an integer, the first ``N` layers of\n        the model are frozen. If it is a float, the first ``N`` percent of the layers\n        are frozen. If it is a list of integers, the layers at the indices in the\n        list are frozen.\n    freeze_layer_norm : bool, default=True\n        Whether to freeze the layer normalization layers of the model.\n    patch_dropout_rate : float, default=0.0\n        The proportion of patch embeddings to drop out.\n    patch_dropout_shuffle : bool, default=False\n        Whether to shuffle the patches while applying patch dropout.\n    patch_dropout_bias : float, optional, default=None\n        The bias to apply to the patch dropout mask.\n    peft_config : Optional[PeftConfig], optional, default=None\n        The configuration from the `peft &lt;https://huggingface.co/docs/peft/index&gt;`_\n        library to use to wrap the model for parameter-efficient finetuning.\n    model_config_kwargs : dict[str, Any], optional, default=None\n        Additional keyword arguments to pass to the model configuration.\n\n    Warns\n    -----\n    UserWarning\n        If both ``peft_config`` and ``freeze_layers`` are set. The ``peft_config``\n        will override the ``freeze_layers`` setting.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name_or_path: str,\n        pretrained: bool = True,\n        use_all_token_embeddings: bool = False,\n        patch_dropout_rate: float = 0.0,\n        patch_dropout_shuffle: bool = False,\n        patch_dropout_bias: Optional[float] = None,\n        freeze_layers: Union[int, float, list[int], bool] = False,\n        freeze_layer_norm: bool = True,\n        peft_config: Optional[\"PeftConfig\"] = None,\n        model_config_kwargs: Optional[dict[str, Any]] = None,\n    ) -&gt; None:\n        super().__init__()\n        _warn_freeze_with_peft(peft_config, freeze_layers)\n\n        self.use_all_token_embeddings = use_all_token_embeddings\n\n        model = hf_utils.load_huggingface_model(\n            transformers.CLIPVisionModelWithProjection,\n            model_name_or_path,\n            load_pretrained_weights=pretrained,\n            model_config_kwargs=model_config_kwargs,\n            config_type=CLIPVisionConfig,\n        )\n\n        model = _freeze_vision_model(model, freeze_layers, freeze_layer_norm)\n        if peft_config is not None:\n            model = hf_utils._wrap_peft_model(model, peft_config)\n\n        self.model = model\n        self.patch_dropout = None\n        if patch_dropout_rate &gt; 0:\n            self.patch_dropout = PatchDropout(\n                keep_rate=1 - patch_dropout_rate,\n                token_shuffling=patch_dropout_shuffle,\n                bias=patch_dropout_bias,\n            )\n\n    def forward(self, inputs: dict[str, Any]) -&gt; tuple[torch.Tensor]:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The image tensor will be expected under the\n            ``Modalities.RGB`` key.\n\n        Returns\n        -------\n        tuple[torch.Tensor]\n            The image embeddings. Will be a tuple with a single element.\n        \"\"\"\n        pixel_values = inputs[Modalities.RGB.name]\n        hidden_states = self.model.vision_model.embeddings(pixel_values)\n        if self.patch_dropout is not None:\n            hidden_states = self.patch_dropout(hidden_states)\n        hidden_states = self.model.vision_model.pre_layrnorm(hidden_states)\n\n        encoder_outputs = self.model.vision_model.encoder(\n            inputs_embeds=hidden_states, return_dict=True\n        )\n\n        last_hidden_state = encoder_outputs.last_hidden_state\n        if self.use_all_token_embeddings:\n            pooled_output = last_hidden_state\n        else:\n            pooled_output = last_hidden_state[:, 0, :]\n        pooled_output = self.model.vision_model.post_layernorm(pooled_output)\n\n        return (self.model.visual_projection(pooled_output),)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.HFCLIPVisionEncoderWithProjection.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The image tensor will be expected under the <code>Modalities.RGB</code> key.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor]</code> <p>The image embeddings. Will be a tuple with a single element.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; tuple[torch.Tensor]:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The image tensor will be expected under the\n        ``Modalities.RGB`` key.\n\n    Returns\n    -------\n    tuple[torch.Tensor]\n        The image embeddings. Will be a tuple with a single element.\n    \"\"\"\n    pixel_values = inputs[Modalities.RGB.name]\n    hidden_states = self.model.vision_model.embeddings(pixel_values)\n    if self.patch_dropout is not None:\n        hidden_states = self.patch_dropout(hidden_states)\n    hidden_states = self.model.vision_model.pre_layrnorm(hidden_states)\n\n    encoder_outputs = self.model.vision_model.encoder(\n        inputs_embeds=hidden_states, return_dict=True\n    )\n\n    last_hidden_state = encoder_outputs.last_hidden_state\n    if self.use_all_token_embeddings:\n        pooled_output = last_hidden_state\n    else:\n        pooled_output = last_hidden_state[:, 0, :]\n    pooled_output = self.model.vision_model.post_layernorm(pooled_output)\n\n    return (self.model.visual_projection(pooled_output),)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.HFTextEncoder","title":"HFTextEncoder","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper around huggingface models in the <code>AutoModelForTextEncoding</code> class.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>The huggingface model name or a local path from which to load the model.</p> required <code>pretrained</code> <code>bool</code> <p>Whether to load the pretrained weights or not.</p> <code>True</code> <code>pooling_layer</code> <code>Optional[Module]</code> <p>Pooling layer to apply to the last hidden state of the model.</p> <code>None</code> <code>freeze_layers</code> <code>Union[int, float, list[int], bool]</code> <p>Whether to freeze layers of the model and which layers to freeze. If <code>True</code>, all model layers are frozen. If it is an integer, the first <code>N</code> layers of the model are frozen. If it is a float, the first <code>N</code> percent of the layers are frozen. If it is a list of integers, the layers at the indices in the list are frozen.</p> <code>False</code> <code>freeze_layer_norm</code> <code>bool</code> <p>Whether to freeze the layer normalization layers of the model.</p> <code>True</code> <code>peft_config</code> <code>Optional[PeftConfig]</code> <p>The configuration from the <code>peft &lt;https://huggingface.co/docs/peft/index&gt;</code>_ library to use to wrap the model for parameter-efficient finetuning.</p> <code>None</code> <code>model_config_kwargs</code> <code>Optional[dict[str, Any]]</code> <p>Additional keyword arguments to pass to the model configuration.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model is a decoder model or if freezing individual layers is not supported for the model type.</p> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If both <code>peft_config</code> and <code>freeze_layers</code> are set. The <code>peft_config</code> will override the <code>freeze_layers</code> setting.</p> Source code in <code>mmlearn/modules/encoders/text.py</code> <pre><code>@store(group=\"modules/encoders\", provider=\"mmlearn\", hydra_convert=\"object\")\nclass HFTextEncoder(nn.Module):\n    \"\"\"Wrapper around huggingface models in the ``AutoModelForTextEncoding`` class.\n\n    Parameters\n    ----------\n    model_name_or_path : str\n        The huggingface model name or a local path from which to load the model.\n    pretrained : bool, default=True\n        Whether to load the pretrained weights or not.\n    pooling_layer : Optional[torch.nn.Module], optional, default=None\n        Pooling layer to apply to the last hidden state of the model.\n    freeze_layers : Union[int, float, list[int], bool], default=False\n        Whether to freeze layers of the model and which layers to freeze. If ``True``,\n        all model layers are frozen. If it is an integer, the first ``N`` layers of\n        the model are frozen. If it is a float, the first ``N`` percent of the layers\n        are frozen. If it is a list of integers, the layers at the indices in the\n        list are frozen.\n    freeze_layer_norm : bool, default=True\n        Whether to freeze the layer normalization layers of the model.\n    peft_config : Optional[PeftConfig], optional, default=None\n        The configuration from the `peft &lt;https://huggingface.co/docs/peft/index&gt;`_\n        library to use to wrap the model for parameter-efficient finetuning.\n    model_config_kwargs : Optional[dict[str, Any]], optional, default=None\n        Additional keyword arguments to pass to the model configuration.\n\n    Raises\n    ------\n    ValueError\n        If the model is a decoder model or if freezing individual layers is not\n        supported for the model type.\n\n    Warns\n    -----\n    UserWarning\n        If both ``peft_config`` and ``freeze_layers`` are set. The ``peft_config``\n        will override the ``freeze_layers`` setting.\n\n\n    \"\"\"\n\n    def __init__(  # noqa: PLR0912\n        self,\n        model_name_or_path: str,\n        pretrained: bool = True,\n        pooling_layer: Optional[nn.Module] = None,\n        freeze_layers: Union[int, float, list[int], bool] = False,\n        freeze_layer_norm: bool = True,\n        peft_config: Optional[\"PeftConfig\"] = None,\n        model_config_kwargs: Optional[dict[str, Any]] = None,\n    ):\n        super().__init__()\n        if model_config_kwargs is None:\n            model_config_kwargs = {}\n        model_config_kwargs[\"output_hidden_states\"] = True\n        model_config_kwargs[\"add_pooling_layer\"] = False\n        model = hf_utils.load_huggingface_model(\n            AutoModelForTextEncoding,\n            model_name_or_path,\n            load_pretrained_weights=pretrained,\n            model_config_kwargs=model_config_kwargs,\n        )\n        if hasattr(model.config, \"is_decoder\") and model.config.is_decoder:\n            raise ValueError(\"Model is a decoder. Only encoder models are supported.\")\n\n        if not pretrained and freeze_layers:\n            rank_zero_warn(\n                \"Freezing layers when loading a model with random weights may lead to \"\n                \"unexpected behavior. Consider setting `freeze_layers=False` if \"\n                \"`pretrained=False`.\",\n            )\n\n        if isinstance(freeze_layers, bool) and freeze_layers:\n            for name, param in model.named_parameters():\n                param.requires_grad = (\n                    (not freeze_layer_norm) if \"LayerNorm\" in name else False\n                )\n\n        if isinstance(\n            freeze_layers, (float, int, list)\n        ) and model.config.model_type in [\"flaubert\", \"xlm\"]:\n            # flaubert and xlm models have a different architecture that does not\n            # support freezing individual layers in the same way as other models\n            raise ValueError(\n                f\"Freezing individual layers is not supported for {model.config.model_type} \"\n                \"models. Please use `freeze_layers=False` or `freeze_layers=True`.\"\n            )\n\n        # get list of layers\n        embeddings = model.embeddings\n        encoder = getattr(model, \"encoder\", None) or getattr(\n            model, \"transformer\", model\n        )\n        encoder_layers = (\n            getattr(encoder, \"layer\", None)\n            or getattr(encoder, \"layers\", None)\n            or getattr(encoder, \"block\", None)\n        )\n        if encoder_layers is None and hasattr(encoder, \"albert_layer_groups\"):\n            encoder_layers = [\n                layer\n                for group in encoder.albert_layer_groups\n                for layer in group.albert_layers\n            ]\n        modules = [embeddings]\n        if encoder_layers is not None and isinstance(encoder_layers, list):\n            modules.extend(encoder_layers)\n\n        if isinstance(freeze_layers, float):\n            freeze_layers = int(freeze_layers * len(modules))\n        if isinstance(freeze_layers, int):\n            freeze_layers = list(range(freeze_layers))\n\n        if isinstance(freeze_layers, list):\n            for idx, module in enumerate(modules):\n                if idx in freeze_layers:\n                    for name, param in module.named_parameters():\n                        param.requires_grad = (\n                            (not freeze_layer_norm) if \"LayerNorm\" in name else False\n                        )\n\n        if peft_config is not None:\n            model = hf_utils._wrap_peft_model(model, peft_config)\n\n        self.model = model\n        self.pooling_layer = pooling_layer\n\n    def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The ``input_ids`` will be expected under the\n            ``Modalities.TEXT`` key.\n\n        Returns\n        -------\n        BaseModelOutput\n            The output of the model, including the last hidden state, all hidden states,\n            and the attention weights, if ``output_attentions`` is set to ``True``.\n        \"\"\"\n        outputs = self.model(\n            input_ids=inputs[Modalities.TEXT.name],\n            attention_mask=inputs.get(\n                \"attention_mask\", inputs.get(Modalities.TEXT.attention_mask, None)\n            ),\n            position_ids=inputs.get(\"position_ids\"),\n            output_attentions=inputs.get(\"output_attentions\"),\n            return_dict=True,\n        )\n        last_hidden_state = outputs.hidden_states[-1]  # NOTE: no layer norm applied\n        if self.pooling_layer:\n            last_hidden_state = self.pooling_layer(last_hidden_state)\n\n        return BaseModelOutput(\n            last_hidden_state=last_hidden_state,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n</code></pre>"},{"location":"api/#mmlearn.cli.run.HFTextEncoder.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The <code>input_ids</code> will be expected under the <code>Modalities.TEXT</code> key.</p> required <p>Returns:</p> Type Description <code>BaseModelOutput</code> <p>The output of the model, including the last hidden state, all hidden states, and the attention weights, if <code>output_attentions</code> is set to <code>True</code>.</p> Source code in <code>mmlearn/modules/encoders/text.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The ``input_ids`` will be expected under the\n        ``Modalities.TEXT`` key.\n\n    Returns\n    -------\n    BaseModelOutput\n        The output of the model, including the last hidden state, all hidden states,\n        and the attention weights, if ``output_attentions`` is set to ``True``.\n    \"\"\"\n    outputs = self.model(\n        input_ids=inputs[Modalities.TEXT.name],\n        attention_mask=inputs.get(\n            \"attention_mask\", inputs.get(Modalities.TEXT.attention_mask, None)\n        ),\n        position_ids=inputs.get(\"position_ids\"),\n        output_attentions=inputs.get(\"output_attentions\"),\n        return_dict=True,\n    )\n    last_hidden_state = outputs.hidden_states[-1]  # NOTE: no layer norm applied\n    if self.pooling_layer:\n        last_hidden_state = self.pooling_layer(last_hidden_state)\n\n    return BaseModelOutput(\n        last_hidden_state=last_hidden_state,\n        hidden_states=outputs.hidden_states,\n        attentions=outputs.attentions,\n    )\n</code></pre>"},{"location":"api/#mmlearn.cli.run.TimmViT","title":"TimmViT","text":"<p>               Bases: <code>Module</code></p> <p>Vision Transformer model from timm.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the model to use.</p> required <code>modality</code> <code>str</code> <p>The modality of the input data. This allows this model to be used with different image modalities e.g. RGB, Depth, etc.</p> <code>\"RGB\"</code> <code>projection_dim</code> <code>int</code> <p>The dimension of the projection head.</p> <code>768</code> <code>pretrained</code> <code>bool</code> <p>Whether to use the pretrained weights.</p> <code>True</code> <code>freeze_layers</code> <code>Union[int, float, list[int], bool]</code> <p>Whether to freeze the layers.</p> <code>False</code> <code>freeze_layer_norm</code> <code>bool</code> <p>Whether to freeze the layer norm.</p> <code>True</code> <code>peft_config</code> <code>Optional[PeftConfig]</code> <p>The configuration from the <code>peft &lt;https://huggingface.co/docs/peft/index&gt;</code>_ library to use to wrap the model for parameter-efficient finetuning.</p> <code>None</code> <code>model_kwargs</code> <code>Optional[dict[str, Any]]</code> <p>Additional keyword arguments for the model.</p> <code>None</code> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>@store(\n    group=\"modules/encoders\",\n    provider=\"mmlearn\",\n    model_name=\"vit_base_patch16_224\",\n    hydra_convert=\"object\",\n)\nclass TimmViT(nn.Module):\n    \"\"\"Vision Transformer model from timm.\n\n    Parameters\n    ----------\n    model_name : str\n        The name of the model to use.\n    modality : str, default=\"RGB\"\n        The modality of the input data. This allows this model to be used with different\n        image modalities e.g. RGB, Depth, etc.\n    projection_dim : int, default=768\n        The dimension of the projection head.\n    pretrained : bool, default=True\n        Whether to use the pretrained weights.\n    freeze_layers : Union[int, float, list[int], bool], default=False\n        Whether to freeze the layers.\n    freeze_layer_norm : bool, default=True\n        Whether to freeze the layer norm.\n    peft_config : Optional[PeftConfig], optional, default=None\n        The configuration from the `peft &lt;https://huggingface.co/docs/peft/index&gt;`_\n        library to use to wrap the model for parameter-efficient finetuning.\n    model_kwargs : Optional[dict[str, Any]], default=None\n        Additional keyword arguments for the model.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str,\n        modality: str = \"RGB\",\n        projection_dim: int = 768,\n        pretrained: bool = True,\n        freeze_layers: Union[int, float, list[int], bool] = False,\n        freeze_layer_norm: bool = True,\n        peft_config: Optional[\"PeftConfig\"] = None,\n        model_kwargs: Optional[dict[str, Any]] = None,\n    ) -&gt; None:\n        super().__init__()\n        self.modality = Modalities.get_modality(modality)\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        self.model: TimmVisionTransformer = timm.create_model(\n            model_name,\n            pretrained=pretrained,\n            num_classes=projection_dim,\n            **model_kwargs,\n        )\n        assert isinstance(self.model, TimmVisionTransformer), (\n            f\"Model {model_name} is not a Vision Transformer. \"\n            \"Please provide a model name that corresponds to a Vision Transformer.\"\n        )\n\n        self._freeze_layers(freeze_layers, freeze_layer_norm)\n\n        if peft_config is not None:\n            self.model = hf_utils._wrap_peft_model(self.model, peft_config)\n\n    def _freeze_layers(\n        self, freeze_layers: Union[int, float, list[int], bool], freeze_layer_norm: bool\n    ) -&gt; None:\n        \"\"\"Freeze the layers of the model.\n\n        Parameters\n        ----------\n        freeze_layers : Union[int, float, list[int], bool]\n            Whether to freeze the layers.\n        freeze_layer_norm : bool\n            Whether to freeze the layer norm.\n        \"\"\"\n        if isinstance(freeze_layers, bool) and freeze_layers:\n            for name, param in self.model.named_parameters():\n                param.requires_grad = (\n                    (not freeze_layer_norm) if \"norm\" in name else False\n                )\n\n        modules = [self.model.patch_embed, *self.model.blocks, self.model.norm]\n        if isinstance(freeze_layers, float):\n            freeze_layers = int(freeze_layers * len(modules))\n        if isinstance(freeze_layers, int):\n            freeze_layers = list(range(freeze_layers))\n\n        if isinstance(freeze_layers, list):\n            for idx, module in enumerate(modules):\n                if idx in freeze_layers:\n                    for name, param in module.named_parameters():\n                        param.requires_grad = (\n                            (not freeze_layer_norm) if \"norm\" in name else False\n                        )\n\n    def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The ``image`` will be expected under the\n            ``Modalities.RGB`` key.\n\n        Returns\n        -------\n        BaseModelOutput\n            The output of the model.\n        \"\"\"\n        x = inputs[self.modality.name]\n        last_hidden_state, hidden_states = self.model.forward_intermediates(\n            x, output_fmt=\"NLC\"\n        )\n        last_hidden_state = self.model.forward_head(last_hidden_state)\n\n        return BaseModelOutput(\n            last_hidden_state=last_hidden_state, hidden_states=hidden_states\n        )\n\n    def get_intermediate_layers(\n        self, inputs: dict[str, Any], n: int = 1\n    ) -&gt; list[torch.Tensor]:\n        \"\"\"Get the output of the intermediate layers.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The ``image`` will be expected under the ``Modalities.RGB``\n            key.\n        n : int, default=1\n            The number of intermediate layers to return.\n\n        Returns\n        -------\n        list[torch.Tensor]\n            The outputs of the last n intermediate layers.\n        \"\"\"\n        return self.model.get_intermediate_layers(inputs[Modalities.RGB], n)  # type: ignore\n\n    def get_patch_info(self) -&gt; tuple[int, int]:\n        \"\"\"Get patch size and number of patches.\n\n        Returns\n        -------\n        tuple[int, int]\n            Patch size and number of patches.\n        \"\"\"\n        patch_size = self.model.patch_embed.patch_size[0]\n        num_patches = self.model.patch_embed.num_patches\n        return patch_size, num_patches\n</code></pre>"},{"location":"api/#mmlearn.cli.run.TimmViT.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The <code>image</code> will be expected under the <code>Modalities.RGB</code> key.</p> required <p>Returns:</p> Type Description <code>BaseModelOutput</code> <p>The output of the model.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The ``image`` will be expected under the\n        ``Modalities.RGB`` key.\n\n    Returns\n    -------\n    BaseModelOutput\n        The output of the model.\n    \"\"\"\n    x = inputs[self.modality.name]\n    last_hidden_state, hidden_states = self.model.forward_intermediates(\n        x, output_fmt=\"NLC\"\n    )\n    last_hidden_state = self.model.forward_head(last_hidden_state)\n\n    return BaseModelOutput(\n        last_hidden_state=last_hidden_state, hidden_states=hidden_states\n    )\n</code></pre>"},{"location":"api/#mmlearn.cli.run.TimmViT.get_intermediate_layers","title":"get_intermediate_layers","text":"<pre><code>get_intermediate_layers(inputs, n=1)\n</code></pre> <p>Get the output of the intermediate layers.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The <code>image</code> will be expected under the <code>Modalities.RGB</code> key.</p> required <code>n</code> <code>int</code> <p>The number of intermediate layers to return.</p> <code>1</code> <p>Returns:</p> Type Description <code>list[Tensor]</code> <p>The outputs of the last n intermediate layers.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>def get_intermediate_layers(\n    self, inputs: dict[str, Any], n: int = 1\n) -&gt; list[torch.Tensor]:\n    \"\"\"Get the output of the intermediate layers.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The ``image`` will be expected under the ``Modalities.RGB``\n        key.\n    n : int, default=1\n        The number of intermediate layers to return.\n\n    Returns\n    -------\n    list[torch.Tensor]\n        The outputs of the last n intermediate layers.\n    \"\"\"\n    return self.model.get_intermediate_layers(inputs[Modalities.RGB], n)  # type: ignore\n</code></pre>"},{"location":"api/#mmlearn.cli.run.TimmViT.get_patch_info","title":"get_patch_info","text":"<pre><code>get_patch_info()\n</code></pre> <p>Get patch size and number of patches.</p> <p>Returns:</p> Type Description <code>tuple[int, int]</code> <p>Patch size and number of patches.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>def get_patch_info(self) -&gt; tuple[int, int]:\n    \"\"\"Get patch size and number of patches.\n\n    Returns\n    -------\n    tuple[int, int]\n        Patch size and number of patches.\n    \"\"\"\n    patch_size = self.model.patch_embed.patch_size[0]\n    num_patches = self.model.patch_embed.num_patches\n    return patch_size, num_patches\n</code></pre>"},{"location":"api/#mmlearn.cli.run.LearnableLogitScaling","title":"LearnableLogitScaling","text":"<p>               Bases: <code>Module</code></p> <p>Logit scaling layer.</p> <p>Parameters:</p> Name Type Description Default <code>init_logit_scale</code> <code>float</code> <p>Initial value of the logit scale.</p> <code>1/0.07</code> <code>learnable</code> <code>bool</code> <p>If True, the logit scale is learnable. Otherwise, it is fixed.</p> <code>True</code> <code>max_logit_scale</code> <code>float</code> <p>Maximum value of the logit scale.</p> <code>100</code> Source code in <code>mmlearn/modules/layers/logit_scaling.py</code> <pre><code>@store(group=\"modules/layers\", provider=\"mmlearn\")\nclass LearnableLogitScaling(torch.nn.Module):\n    \"\"\"Logit scaling layer.\n\n    Parameters\n    ----------\n    init_logit_scale : float, optional, default=1/0.07\n        Initial value of the logit scale.\n    learnable : bool, optional, default=True\n        If True, the logit scale is learnable. Otherwise, it is fixed.\n    max_logit_scale : float, optional, default=100\n        Maximum value of the logit scale.\n    \"\"\"\n\n    def __init__(\n        self,\n        init_logit_scale: float = 1 / 0.07,\n        max_logit_scale: float = 100,\n        learnable: bool = True,\n    ) -&gt; None:\n        super().__init__()\n        self.max_logit_scale = max_logit_scale\n        self.init_logit_scale = init_logit_scale\n        self.learnable = learnable\n        log_logit_scale = torch.ones([]) * np.log(self.init_logit_scale)\n        if learnable:\n            self.log_logit_scale = torch.nn.Parameter(log_logit_scale)\n        else:\n            self.register_buffer(\"log_logit_scale\", log_logit_scale)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Apply the logit scaling to the input tensor.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor of shape ``(batch_sz, seq_len, dim)``.\n        \"\"\"\n        return torch.clip(self.log_logit_scale.exp(), max=self.max_logit_scale) * x\n\n    def extra_repr(self) -&gt; str:\n        \"\"\"Return the string representation of the layer.\"\"\"\n        return (\n            f\"logit_scale_init={self.init_logit_scale},learnable={self.learnable},\"\n            f\" max_logit_scale={self.max_logit_scale}\"\n        )\n</code></pre>"},{"location":"api/#mmlearn.cli.run.LearnableLogitScaling.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Apply the logit scaling to the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape <code>(batch_sz, seq_len, dim)</code>.</p> required Source code in <code>mmlearn/modules/layers/logit_scaling.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Apply the logit scaling to the input tensor.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape ``(batch_sz, seq_len, dim)``.\n    \"\"\"\n    return torch.clip(self.log_logit_scale.exp(), max=self.max_logit_scale) * x\n</code></pre>"},{"location":"api/#mmlearn.cli.run.LearnableLogitScaling.extra_repr","title":"extra_repr","text":"<pre><code>extra_repr()\n</code></pre> <p>Return the string representation of the layer.</p> Source code in <code>mmlearn/modules/layers/logit_scaling.py</code> <pre><code>def extra_repr(self) -&gt; str:\n    \"\"\"Return the string representation of the layer.\"\"\"\n    return (\n        f\"logit_scale_init={self.init_logit_scale},learnable={self.learnable},\"\n        f\" max_logit_scale={self.max_logit_scale}\"\n    )\n</code></pre>"},{"location":"api/#mmlearn.cli.run.MLP","title":"MLP","text":"<p>               Bases: <code>Sequential</code></p> <p>Multi-layer perceptron (MLP).</p> <p>This module will create a block of <code>Linear -&gt; Normalization -&gt; Activation -&gt; Dropout</code> layers.</p> <p>Parameters:</p> Name Type Description Default <code>in_dim</code> <code>int</code> <p>The input dimension.</p> required <code>out_dim</code> <code>Optional[int]</code> <p>The output dimension. If not specified, it is set to :attr:<code>in_dim</code>.</p> <code>None</code> <code>hidden_dims</code> <code>Optional[list]</code> <p>The dimensions of the hidden layers. The length of the list determines the number of hidden layers. This parameter is mutually exclusive with :attr:<code>hidden_dims_multiplier</code>.</p> <code>None</code> <code>hidden_dims_multiplier</code> <code>Optional[list]</code> <p>The multipliers to apply to the input dimension to get the dimensions of the hidden layers. The length of the list determines the number of hidden layers. The multipliers will be used to get the dimensions of the hidden layers. This parameter is mutually exclusive with <code>hidden_dims</code>.</p> <code>None</code> <code>apply_multiplier_to_in_dim</code> <code>bool</code> <p>Whether to apply the :attr:<code>hidden_dims_multiplier</code> to :attr:<code>in_dim</code> to get the dimensions of the hidden layers. If <code>False</code>, the multipliers will be applied to the dimensions of the previous hidden layer, starting from :attr:<code>in_dim</code>. This parameter is only relevant when :attr:<code>hidden_dims_multiplier</code> is specified.</p> <code>False</code> <code>norm_layer</code> <code>Optional[Callable[..., Module]]</code> <p>The normalization layer to use. If not specified, no normalization is used. Partial functions can be used to specify the normalization layer with specific parameters.</p> <code>None</code> <code>activation_layer</code> <code>Optional[Callable[..., Module]]</code> <p>The activation layer to use. If not specified, ReLU is used. Partial functions can be used to specify the activation layer with specific parameters.</p> <code>torch.nn.ReLU</code> <code>bias</code> <code>Union[bool, list[bool]]</code> <p>Whether to use bias in the linear layers.</p> <code>True</code> <code>dropout</code> <code>Union[float, list[float]]</code> <p>The dropout probability to use.</p> <code>0.0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If both :attr:<code>hidden_dims</code> and :attr:<code>hidden_dims_multiplier</code> are specified or if the lengths of :attr:<code>bias</code> and :attr:<code>hidden_dims</code> do not match or if the lengths of :attr:<code>dropout</code> and :attr:<code>hidden_dims</code> do not match.</p> Source code in <code>mmlearn/modules/layers/mlp.py</code> <pre><code>@store(group=\"modules/layers\", provider=\"mmlearn\")\nclass MLP(torch.nn.Sequential):\n    \"\"\"Multi-layer perceptron (MLP).\n\n    This module will create a block of ``Linear -&gt; Normalization -&gt; Activation -&gt; Dropout``\n    layers.\n\n    Parameters\n    ----------\n    in_dim : int\n        The input dimension.\n    out_dim : Optional[int], optional, default=None\n        The output dimension. If not specified, it is set to :attr:`in_dim`.\n    hidden_dims : Optional[list], optional, default=None\n        The dimensions of the hidden layers. The length of the list determines the\n        number of hidden layers. This parameter is mutually exclusive with\n        :attr:`hidden_dims_multiplier`.\n    hidden_dims_multiplier : Optional[list], optional, default=None\n        The multipliers to apply to the input dimension to get the dimensions of\n        the hidden layers. The length of the list determines the number of hidden\n        layers. The multipliers will be used to get the dimensions of the hidden\n        layers. This parameter is mutually exclusive with `hidden_dims`.\n    apply_multiplier_to_in_dim : bool, optional, default=False\n        Whether to apply the :attr:`hidden_dims_multiplier` to :attr:`in_dim` to get the\n        dimensions of the hidden layers. If ``False``, the multipliers will be applied\n        to the dimensions of the previous hidden layer, starting from :attr:`in_dim`.\n        This parameter is only relevant when :attr:`hidden_dims_multiplier` is\n        specified.\n    norm_layer : Optional[Callable[..., torch.nn.Module]], optional, default=None\n        The normalization layer to use. If not specified, no normalization is used.\n        Partial functions can be used to specify the normalization layer with specific\n        parameters.\n    activation_layer : Optional[Callable[..., torch.nn.Module]], optional, default=torch.nn.ReLU\n        The activation layer to use. If not specified, ReLU is used. Partial functions\n        can be used to specify the activation layer with specific parameters.\n    bias : Union[bool, list[bool]], optional, default=True\n        Whether to use bias in the linear layers.\n    dropout : Union[float, list[float]], optional, default=0.0\n        The dropout probability to use.\n\n    Raises\n    ------\n    ValueError\n        If both :attr:`hidden_dims` and :attr:`hidden_dims_multiplier` are specified\n        or if the lengths of :attr:`bias` and :attr:`hidden_dims` do not match or if\n        the lengths of :attr:`dropout` and :attr:`hidden_dims` do not match.\n\n    \"\"\"  # noqa: W505\n\n    def __init__(  # noqa: PLR0912\n        self,\n        in_dim: int,\n        out_dim: Optional[int] = None,\n        hidden_dims: Optional[list[int]] = None,\n        hidden_dims_multiplier: Optional[list[float]] = None,\n        apply_multiplier_to_in_dim: bool = False,\n        norm_layer: Optional[Callable[..., torch.nn.Module]] = None,\n        activation_layer: Optional[Callable[..., torch.nn.Module]] = torch.nn.ReLU,\n        bias: Union[bool, list[bool]] = True,\n        dropout: Union[float, list[float]] = 0.0,\n    ) -&gt; None:\n        if hidden_dims is None and hidden_dims_multiplier is None:\n            hidden_dims = []\n        if hidden_dims is not None and hidden_dims_multiplier is not None:\n            raise ValueError(\n                \"Only one of `hidden_dims` or `hidden_dims_multiplier` must be specified.\"\n            )\n\n        if hidden_dims is None and hidden_dims_multiplier is not None:\n            if apply_multiplier_to_in_dim:\n                hidden_dims = [\n                    int(in_dim * multiplier) for multiplier in hidden_dims_multiplier\n                ]\n            else:\n                hidden_dims = [int(in_dim * hidden_dims_multiplier[0])]\n                for multiplier in hidden_dims_multiplier[1:]:\n                    hidden_dims.append(int(hidden_dims[-1] * multiplier))\n\n        if isinstance(bias, bool):\n            bias_list: list[bool] = [bias] * (len(hidden_dims) + 1)  # type: ignore[arg-type]\n        else:\n            bias_list = bias\n        if len(bias_list) != len(hidden_dims) + 1:  # type: ignore[arg-type]\n            raise ValueError(\n                \"Expected `bias` to be a boolean or a list of booleans with length \"\n                \"equal to the number of linear layers in the MLP.\"\n            )\n\n        if isinstance(dropout, float):\n            dropout_list: list[float] = [dropout] * (len(hidden_dims) + 1)  # type: ignore[arg-type]\n        else:\n            dropout_list = dropout\n        if len(dropout_list) != len(hidden_dims) + 1:  # type: ignore[arg-type]\n            raise ValueError(\n                \"Expected `dropout` to be a float or a list of floats with length \"\n                \"equal to the number of linear layers in the MLP.\"\n            )\n\n        # construct list of dimensions for the layers\n        dims = [in_dim] + hidden_dims  # type: ignore[operator]\n        layers = []\n        for layer_idx, (in_features, hidden_features) in enumerate(\n            zip(dims[:-1], dims[1:], strict=False)\n        ):\n            layers.append(\n                torch.nn.Linear(in_features, hidden_features, bias=bias_list[layer_idx])\n            )\n            if norm_layer is not None:\n                layers.append(norm_layer(hidden_features))\n            if activation_layer is not None:\n                layers.append(activation_layer())\n            layers.append(torch.nn.Dropout(dropout_list[layer_idx]))\n\n        out_dim = out_dim or in_dim\n\n        layers.append(torch.nn.Linear(dims[-1], out_dim, bias=bias_list[-1]))\n        layers.append(torch.nn.Dropout(dropout_list[-1]))\n\n        super().__init__(*layers)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.L2Norm","title":"L2Norm","text":"<p>               Bases: <code>Module</code></p> <p>L2 normalization.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>The dimension along which to normalize.</p> required Source code in <code>mmlearn/modules/layers/normalization.py</code> <pre><code>@store(group=\"modules/layers\", provider=\"mmlearn\")\nclass L2Norm(torch.nn.Module):\n    \"\"\"L2 normalization.\n\n    Parameters\n    ----------\n    dim : int\n        The dimension along which to normalize.\n    \"\"\"\n\n    def __init__(self, dim: int) -&gt; None:\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Apply L2 normalization to the input tensor.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor of shape ``(batch_sz, seq_len, dim)``.\n\n        Returns\n        -------\n        torch.Tensor\n            Normalized tensor of shape ``(batch_sz, seq_len, dim)``.\n        \"\"\"\n        return torch.nn.functional.normalize(x, dim=self.dim, p=2)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.L2Norm.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Apply L2 normalization to the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape <code>(batch_sz, seq_len, dim)</code>.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized tensor of shape <code>(batch_sz, seq_len, dim)</code>.</p> Source code in <code>mmlearn/modules/layers/normalization.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Apply L2 normalization to the input tensor.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape ``(batch_sz, seq_len, dim)``.\n\n    Returns\n    -------\n    torch.Tensor\n        Normalized tensor of shape ``(batch_sz, seq_len, dim)``.\n    \"\"\"\n    return torch.nn.functional.normalize(x, dim=self.dim, p=2)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.PatchDropout","title":"PatchDropout","text":"<p>               Bases: <code>Module</code></p> <p>Patch dropout layer.</p> <p>Drops patch tokens (after embedding and adding CLS token) from the input tensor. Usually used in vision transformers to reduce the number of tokens. [1]_</p> <p>Parameters:</p> Name Type Description Default <code>keep_rate</code> <code>float</code> <p>The proportion of tokens to keep.</p> <code>0.5</code> <code>bias</code> <code>Optional[float]</code> <p>The bias to add to the random noise before sorting.</p> <code>None</code> <code>token_shuffling</code> <code>bool</code> <p>If True, the tokens are shuffled.</p> <code>False</code> References <p>.. [1] Liu, Y., Matsoukas, C., Strand, F., Azizpour, H., &amp; Smith, K. (2023).    Patchdropout: Economizing vision transformers using patch dropout. In Proceedings    of the IEEE/CVF Winter Conference on Applications of Computer Vision    (pp. 3953-3962).</p> Source code in <code>mmlearn/modules/layers/patch_dropout.py</code> <pre><code>class PatchDropout(torch.nn.Module):\n    \"\"\"Patch dropout layer.\n\n    Drops patch tokens (after embedding and adding CLS token) from the input tensor.\n    Usually used in vision transformers to reduce the number of tokens. [1]_\n\n    Parameters\n    ----------\n    keep_rate : float, optional, default=0.5\n        The proportion of tokens to keep.\n    bias : Optional[float], optional, default=None\n        The bias to add to the random noise before sorting.\n    token_shuffling : bool, optional, default=False\n        If True, the tokens are shuffled.\n\n    References\n    ----------\n    .. [1] Liu, Y., Matsoukas, C., Strand, F., Azizpour, H., &amp; Smith, K. (2023).\n       Patchdropout: Economizing vision transformers using patch dropout. In Proceedings\n       of the IEEE/CVF Winter Conference on Applications of Computer Vision\n       (pp. 3953-3962).\n    \"\"\"\n\n    def __init__(\n        self,\n        keep_rate: float = 0.5,\n        bias: Optional[float] = None,\n        token_shuffling: bool = False,\n    ):\n        super().__init__()\n        assert 0 &lt; keep_rate &lt;= 1, \"The keep_rate must be in (0,1]\"\n\n        self.bias = bias\n        self.keep_rate = keep_rate\n        self.token_shuffling = token_shuffling\n\n    def forward(self, x: torch.Tensor, force_drop: bool = False) -&gt; torch.Tensor:\n        \"\"\"Drop tokens from the input tensor.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor of shape ``(batch_sz, seq_len, dim)``.\n        force_drop : bool, optional, default=False\n            If True, the tokens are always dropped, even when the model is in\n            evaluation mode.\n\n        Returns\n        -------\n        torch.Tensor\n            Tensor of shape ``(batch_sz, keep_len, dim)`` containing the kept tokens.\n        \"\"\"\n        if (not self.training and not force_drop) or self.keep_rate == 1:\n            return x\n\n        batch_sz, _, dim = x.shape\n\n        cls_mask = torch.zeros(  # assumes that CLS is always the 1st element\n            batch_sz, 1, dtype=torch.int64, device=x.device\n        )\n        patch_mask = self.uniform_mask(x)\n        patch_mask = torch.hstack([cls_mask, patch_mask])\n\n        return torch.gather(x, dim=1, index=patch_mask.unsqueeze(-1).repeat(1, 1, dim))\n\n    def uniform_mask(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Generate token ids to keep from uniform random distribution.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor of shape ``(batch_sz, seq_len, dim)``.\n\n        Returns\n        -------\n        torch.Tensor\n            Tensor of shape ``(batch_sz, keep_len)`` containing the token ids to keep.\n\n        \"\"\"\n        batch_sz, seq_len, _ = x.shape\n        seq_len = seq_len - 1  # patch length (without CLS)\n\n        keep_len = int(seq_len * self.keep_rate)\n        noise = torch.rand(batch_sz, seq_len, device=x.device)\n        if self.bias is not None:\n            noise += self.bias\n        ids = torch.argsort(noise, dim=1)\n        keep_ids = ids[:, :keep_len]\n        if not self.token_shuffling:\n            keep_ids = keep_ids.sort(1)[0]\n        return keep_ids\n</code></pre>"},{"location":"api/#mmlearn.cli.run.PatchDropout.forward","title":"forward","text":"<pre><code>forward(x, force_drop=False)\n</code></pre> <p>Drop tokens from the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape <code>(batch_sz, seq_len, dim)</code>.</p> required <code>force_drop</code> <code>bool</code> <p>If True, the tokens are always dropped, even when the model is in evaluation mode.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of shape <code>(batch_sz, keep_len, dim)</code> containing the kept tokens.</p> Source code in <code>mmlearn/modules/layers/patch_dropout.py</code> <pre><code>def forward(self, x: torch.Tensor, force_drop: bool = False) -&gt; torch.Tensor:\n    \"\"\"Drop tokens from the input tensor.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape ``(batch_sz, seq_len, dim)``.\n    force_drop : bool, optional, default=False\n        If True, the tokens are always dropped, even when the model is in\n        evaluation mode.\n\n    Returns\n    -------\n    torch.Tensor\n        Tensor of shape ``(batch_sz, keep_len, dim)`` containing the kept tokens.\n    \"\"\"\n    if (not self.training and not force_drop) or self.keep_rate == 1:\n        return x\n\n    batch_sz, _, dim = x.shape\n\n    cls_mask = torch.zeros(  # assumes that CLS is always the 1st element\n        batch_sz, 1, dtype=torch.int64, device=x.device\n    )\n    patch_mask = self.uniform_mask(x)\n    patch_mask = torch.hstack([cls_mask, patch_mask])\n\n    return torch.gather(x, dim=1, index=patch_mask.unsqueeze(-1).repeat(1, 1, dim))\n</code></pre>"},{"location":"api/#mmlearn.cli.run.PatchDropout.uniform_mask","title":"uniform_mask","text":"<pre><code>uniform_mask(x)\n</code></pre> <p>Generate token ids to keep from uniform random distribution.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape <code>(batch_sz, seq_len, dim)</code>.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of shape <code>(batch_sz, keep_len)</code> containing the token ids to keep.</p> Source code in <code>mmlearn/modules/layers/patch_dropout.py</code> <pre><code>def uniform_mask(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Generate token ids to keep from uniform random distribution.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape ``(batch_sz, seq_len, dim)``.\n\n    Returns\n    -------\n    torch.Tensor\n        Tensor of shape ``(batch_sz, keep_len)`` containing the token ids to keep.\n\n    \"\"\"\n    batch_sz, seq_len, _ = x.shape\n    seq_len = seq_len - 1  # patch length (without CLS)\n\n    keep_len = int(seq_len * self.keep_rate)\n    noise = torch.rand(batch_sz, seq_len, device=x.device)\n    if self.bias is not None:\n        noise += self.bias\n    ids = torch.argsort(noise, dim=1)\n    keep_ids = ids[:, :keep_len]\n    if not self.token_shuffling:\n        keep_ids = keep_ids.sort(1)[0]\n    return keep_ids\n</code></pre>"},{"location":"api/#mmlearn.cli.run.ContrastiveLoss","title":"ContrastiveLoss","text":"<p>               Bases: <code>Module</code></p> <p>Contrastive Loss.</p> <p>Parameters:</p> Name Type Description Default <code>l2_normalize</code> <code>bool</code> <p>Whether to L2 normalize the features.</p> <code>False</code> <code>local_loss</code> <code>bool</code> <p>Whether to calculate the loss locally i.e. <code>local_features@global_features</code>.</p> <code>False</code> <code>gather_with_grad</code> <code>bool</code> <p>Whether to gather tensors with gradients.</p> <code>False</code> <code>modality_alignment</code> <code>bool</code> <p>Whether to include modality alignment loss. This loss considers all features from the same modality as positive pairs and all features from different modalities as negative pairs.</p> <code>False</code> <code>cache_labels</code> <code>bool</code> <p>Whether to cache the labels.</p> <code>False</code> Source code in <code>mmlearn/modules/losses/contrastive.py</code> <pre><code>@store(group=\"modules/losses\", provider=\"mmlearn\")\nclass ContrastiveLoss(nn.Module):\n    \"\"\"Contrastive Loss.\n\n    Parameters\n    ----------\n    l2_normalize : bool, optional, default=False\n        Whether to L2 normalize the features.\n    local_loss : bool, optional, default=False\n        Whether to calculate the loss locally i.e. ``local_features@global_features``.\n    gather_with_grad : bool, optional, default=False\n        Whether to gather tensors with gradients.\n    modality_alignment : bool, optional, default=False\n        Whether to include modality alignment loss. This loss considers all features\n        from the same modality as positive pairs and all features from different\n        modalities as negative pairs.\n    cache_labels : bool, optional, default=False\n        Whether to cache the labels.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        l2_normalize: bool = False,\n        local_loss: bool = False,\n        gather_with_grad: bool = False,\n        modality_alignment: bool = False,\n        cache_labels: bool = False,\n    ):\n        super().__init__()\n        self.local_loss = local_loss\n        self.gather_with_grad = gather_with_grad\n        self.cache_labels = cache_labels\n        self.l2_normalize = l2_normalize\n        self.modality_alignment = modality_alignment\n\n        # cache state\n        self._prev_num_logits = 0\n        self._labels: dict[torch.device, torch.Tensor] = {}\n\n    def forward(\n        self,\n        embeddings: dict[str, torch.Tensor],\n        example_ids: dict[str, torch.Tensor],\n        logit_scale: torch.Tensor,\n        modality_loss_pairs: list[LossPairSpec],\n    ) -&gt; torch.Tensor:\n        \"\"\"Calculate the contrastive loss.\n\n        Parameters\n        ----------\n        embeddings : dict[str, torch.Tensor]\n            Dictionary of embeddings, where the key is the modality name and the value\n            is the corresponding embedding tensor.\n        example_ids : dict[str, torch.Tensor]\n            Dictionary of example IDs, where the key is the modality name and the value\n            is a tensor tuple of the dataset index and the example index.\n        logit_scale : torch.Tensor\n            Scale factor for the logits.\n        modality_loss_pairs : list[LossPairSpec]\n            Specification of the modality pairs for which the loss should be calculated.\n\n        Returns\n        -------\n        torch.Tensor\n            The contrastive loss.\n        \"\"\"\n        world_size = dist.get_world_size() if dist.is_initialized() else 1\n        rank = dist.get_rank() if world_size &gt; 1 else 0\n\n        if self.l2_normalize:\n            embeddings = {k: F.normalize(v, p=2, dim=-1) for k, v in embeddings.items()}\n\n        if world_size &gt; 1:  # gather embeddings and example_ids across all processes\n            # NOTE: gathering dictionaries of tensors across all processes\n            # (keys + values, as opposed to just values) is especially important\n            # for the modality_alignment loss, which requires all embeddings\n            all_embeddings = _gather_dicts(\n                embeddings,\n                local_loss=self.local_loss,\n                gather_with_grad=self.gather_with_grad,\n                rank=rank,\n            )\n            all_example_ids = _gather_dicts(\n                example_ids,\n                local_loss=self.local_loss,\n                gather_with_grad=self.gather_with_grad,\n                rank=rank,\n            )\n        else:\n            all_embeddings = embeddings\n            all_example_ids = example_ids\n\n        losses = []\n        for loss_pairs in modality_loss_pairs:\n            logits_per_feature_a, logits_per_feature_b, skip_flag = self._get_logits(\n                loss_pairs.modalities,\n                per_device_embeddings=embeddings,\n                all_embeddings=all_embeddings,\n                per_device_example_ids=example_ids,\n                all_example_ids=all_example_ids,\n                logit_scale=logit_scale,\n                world_size=world_size,\n            )\n            if logits_per_feature_a is None or logits_per_feature_b is None:\n                continue\n\n            labels = self._get_ground_truth(\n                logits_per_feature_a.shape,\n                device=logits_per_feature_a.device,\n                rank=rank,\n                world_size=world_size,\n                skipped_process=skip_flag,\n            )\n\n            if labels.numel() != 0:\n                losses.append(\n                    (\n                        (\n                            F.cross_entropy(logits_per_feature_a, labels)\n                            + F.cross_entropy(logits_per_feature_b, labels)\n                        )\n                        / 2\n                    )\n                    * loss_pairs.weight\n                )\n\n        if self.modality_alignment:\n            losses.append(\n                self._compute_modality_alignment_loss(all_embeddings, logit_scale)\n            )\n\n        if not losses:  # no loss to compute (e.g. no paired data in batch)\n            losses.append(\n                torch.tensor(\n                    0.0,\n                    device=logit_scale.device,\n                    dtype=next(iter(embeddings.values())).dtype,\n                )\n            )\n\n        return torch.stack(losses).sum()\n\n    def _get_ground_truth(\n        self,\n        logits_shape: tuple[int, int],\n        device: torch.device,\n        rank: int,\n        world_size: int,\n        skipped_process: bool,\n    ) -&gt; torch.Tensor:\n        \"\"\"Return the ground-truth labels.\n\n        Parameters\n        ----------\n        logits_shape : tuple[int, int]\n            Shape of the logits tensor.\n        device : torch.device\n            Device on which the labels should be created.\n        rank : int\n            Rank of the current process.\n        world_size : int\n            Number of processes.\n        skipped_process : bool\n            Whether the current process skipped the computation due to lack of data.\n\n        Returns\n        -------\n        torch.Tensor\n            Ground-truth labels.\n        \"\"\"\n        num_logits = logits_shape[-1]\n\n        # calculate ground-truth and cache if enabled\n        if self._prev_num_logits != num_logits or device not in self._labels:\n            labels = torch.arange(num_logits, device=device, dtype=torch.long)\n\n            if world_size &gt; 1 and self.local_loss:\n                local_size = torch.tensor(\n                    0 if skipped_process else logits_shape[0], device=device\n                )\n                # NOTE: all processes must participate in the all_gather operation\n                # even if they have no data to contribute.\n                sizes = torch.stack(\n                    _simple_gather_all_tensors(\n                        local_size, group=dist.group.WORLD, world_size=world_size\n                    )\n                )\n                sizes = torch.cat(\n                    [torch.tensor([0], device=sizes.device), torch.cumsum(sizes, dim=0)]\n                )\n                labels = labels[\n                    sizes[rank] : sizes[rank + 1] if rank + 1 &lt; world_size else None\n                ]\n\n            if self.cache_labels:\n                self._labels[device] = labels\n                self._prev_num_logits = num_logits\n        else:\n            labels = self._labels[device]\n        return labels\n\n    def _get_logits(  # noqa: PLR0912\n        self,\n        modalities: tuple[str, str],\n        per_device_embeddings: dict[str, torch.Tensor],\n        all_embeddings: dict[str, torch.Tensor],\n        per_device_example_ids: dict[str, torch.Tensor],\n        all_example_ids: dict[str, torch.Tensor],\n        logit_scale: torch.Tensor,\n        world_size: int,\n    ) -&gt; tuple[Optional[torch.Tensor], Optional[torch.Tensor], bool]:\n        \"\"\"Calculate the logits for the given modalities.\n\n        Parameters\n        ----------\n        modalities : tuple[str, str]\n            Tuple of modality names.\n        per_device_embeddings : dict[str, torch.Tensor]\n            Dictionary of embeddings, where the key is the modality name and the value\n            is the corresponding embedding tensor.\n        all_embeddings : dict[str, torch.Tensor]\n            Dictionary of embeddings, where the key is the modality name and the value\n            is the corresponding embedding tensor. In distributed mode, this contains\n            embeddings from all processes.\n        per_device_example_ids : dict[str, torch.Tensor]\n            Dictionary of example IDs, where the key is the modality name and the value\n            is a tensor tuple of the dataset index and the example index.\n        all_example_ids : dict[str, torch.Tensor]\n            Dictionary of example IDs, where the key is the modality name and the value\n            is a tensor tuple of the dataset index and the example index. In distributed\n            mode, this contains example IDs from all processes.\n        logit_scale : torch.Tensor\n            Scale factor for the logits.\n        world_size : int\n            Number of processes.\n\n        Returns\n        -------\n        tuple[Optional[torch.Tensor], Optional[torch.Tensor], bool]\n            Tuple of logits for the given modalities. If embeddings for the given\n            modalities are not available, returns `None` for the logits. The last\n            element is a flag indicating whether the process skipped the computation\n            due to lack of data.\n        \"\"\"\n        modality_a = Modalities.get_modality(modalities[0])\n        modality_b = Modalities.get_modality(modalities[1])\n        skip_flag = False\n\n        if self.local_loss or world_size == 1:\n            if not (\n                modality_a.embedding in per_device_embeddings\n                and modality_b.embedding in per_device_embeddings\n            ):\n                if world_size &gt; 1:  # NOTE: not all processes exit here, hence skip_flag\n                    skip_flag = True\n                else:\n                    return None, None, skip_flag\n\n            if not skip_flag:\n                indices_a, indices_b = find_matching_indices(\n                    per_device_example_ids[modality_a.name],\n                    per_device_example_ids[modality_b.name],\n                )\n                if indices_a.numel() == 0 or indices_b.numel() == 0:\n                    if world_size &gt; 1:  # not all processes exit here\n                        skip_flag = True\n                    else:\n                        return None, None, skip_flag\n\n            if not skip_flag:\n                features_a = per_device_embeddings[modality_a.embedding][indices_a]\n                features_b = per_device_embeddings[modality_b.embedding][indices_b]\n            else:\n                # all processes must participate in the all_gather operation\n                # that follows, even if they have no data to contribute. So,\n                # we create empty tensors here.\n                features_a = torch.empty(\n                    0, device=list(per_device_embeddings.values())[0].device\n                )\n                features_b = torch.empty(\n                    0, device=list(per_device_embeddings.values())[0].device\n                )\n\n        if world_size &gt; 1:\n            if not (\n                modality_a.embedding in all_embeddings\n                and modality_b.embedding in all_embeddings\n            ):  # all processes exit here\n                return None, None, skip_flag\n\n            indices_a, indices_b = find_matching_indices(\n                all_example_ids[modality_a.name],\n                all_example_ids[modality_b.name],\n            )\n            if indices_a.numel() == 0 or indices_b.numel() == 0:\n                # all processes exit here\n                return None, None, skip_flag\n\n            all_features_a = all_embeddings[modality_a.embedding][indices_a]\n            all_features_b = all_embeddings[modality_b.embedding][indices_b]\n\n            if self.local_loss:\n                if features_a.numel() == 0:\n                    features_a = all_features_a\n                if features_b.numel() == 0:\n                    features_b = all_features_b\n\n                logits_per_feature_a = logit_scale * _safe_matmul(\n                    features_a, all_features_b\n                )\n                logits_per_feature_b = logit_scale * _safe_matmul(\n                    features_b, all_features_a\n                )\n            else:\n                logits_per_feature_a = logit_scale * _safe_matmul(\n                    all_features_a, all_features_b\n                )\n                logits_per_feature_b = logits_per_feature_a.T\n        else:\n            logits_per_feature_a = logit_scale * _safe_matmul(features_a, features_b)\n            logits_per_feature_b = logit_scale * _safe_matmul(features_b, features_a)\n\n        return logits_per_feature_a, logits_per_feature_b, skip_flag\n\n    def _compute_modality_alignment_loss(\n        self, all_embeddings: dict[str, torch.Tensor], logit_scale: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"Compute the modality alignment loss.\n\n        This loss considers all features from the same modality as positive pairs\n        and all features from different modalities as negative pairs.\n\n        Parameters\n        ----------\n        all_embeddings : dict[str, torch.Tensor]\n            Dictionary of embeddings, where the key is the modality name and the value\n            is the corresponding embedding tensor.\n        logit_scale : torch.Tensor\n            Scale factor for the logits.\n\n        Returns\n        -------\n        torch.Tensor\n            Modality alignment loss.\n\n        Notes\n        -----\n        This loss does not support `local_loss=True`.\n        \"\"\"\n        available_modalities = list(all_embeddings.keys())\n        # TODO: support local_loss for modality_alignment?\n        # if world_size == 1, all_embeddings == embeddings\n        all_features = torch.cat(list(all_embeddings.values()), dim=0)\n\n        positive_indices = torch.tensor(\n            [\n                (i, j)\n                if idx == 0\n                else (\n                    i + all_embeddings[available_modalities[idx - 1]].size(0),\n                    j + all_embeddings[available_modalities[idx - 1]].size(0),\n                )\n                for idx, k in enumerate(all_embeddings)\n                for i, j in itertools.combinations(range(all_embeddings[k].size(0)), 2)\n            ],\n            device=all_features.device,\n        )\n        logits = logit_scale * _safe_matmul(all_features, all_features)\n\n        target = torch.eye(all_features.size(0), device=all_features.device)\n        target[positive_indices[:, 0], positive_indices[:, 1]] = 1\n\n        modality_loss = torch.nn.functional.binary_cross_entropy_with_logits(\n            logits, target, reduction=\"none\"\n        )\n\n        target_pos = target.bool()\n        target_neg = ~target_pos\n\n        # loss_pos and loss_neg below contain non-zero values only for those\n        # elements that are positive pairs and negative pairs respectively.\n        loss_pos = torch.zeros(\n            logits.size(0), logits.size(0), device=target.device\n        ).masked_scatter(target_pos, modality_loss[target_pos])\n        loss_neg = torch.zeros(\n            logits.size(0), logits.size(0), device=target.device\n        ).masked_scatter(target_neg, modality_loss[target_neg])\n\n        loss_pos = loss_pos.sum(dim=1)\n        loss_neg = loss_neg.sum(dim=1)\n        num_pos = target.sum(dim=1)\n        num_neg = logits.size(0) - num_pos\n\n        return ((loss_pos / num_pos) + (loss_neg / num_neg)).mean()\n</code></pre>"},{"location":"api/#mmlearn.cli.run.ContrastiveLoss.forward","title":"forward","text":"<pre><code>forward(\n    embeddings,\n    example_ids,\n    logit_scale,\n    modality_loss_pairs,\n)\n</code></pre> <p>Calculate the contrastive loss.</p> <p>Parameters:</p> Name Type Description Default <code>embeddings</code> <code>dict[str, Tensor]</code> <p>Dictionary of embeddings, where the key is the modality name and the value is the corresponding embedding tensor.</p> required <code>example_ids</code> <code>dict[str, Tensor]</code> <p>Dictionary of example IDs, where the key is the modality name and the value is a tensor tuple of the dataset index and the example index.</p> required <code>logit_scale</code> <code>Tensor</code> <p>Scale factor for the logits.</p> required <code>modality_loss_pairs</code> <code>list[LossPairSpec]</code> <p>Specification of the modality pairs for which the loss should be calculated.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The contrastive loss.</p> Source code in <code>mmlearn/modules/losses/contrastive.py</code> <pre><code>def forward(\n    self,\n    embeddings: dict[str, torch.Tensor],\n    example_ids: dict[str, torch.Tensor],\n    logit_scale: torch.Tensor,\n    modality_loss_pairs: list[LossPairSpec],\n) -&gt; torch.Tensor:\n    \"\"\"Calculate the contrastive loss.\n\n    Parameters\n    ----------\n    embeddings : dict[str, torch.Tensor]\n        Dictionary of embeddings, where the key is the modality name and the value\n        is the corresponding embedding tensor.\n    example_ids : dict[str, torch.Tensor]\n        Dictionary of example IDs, where the key is the modality name and the value\n        is a tensor tuple of the dataset index and the example index.\n    logit_scale : torch.Tensor\n        Scale factor for the logits.\n    modality_loss_pairs : list[LossPairSpec]\n        Specification of the modality pairs for which the loss should be calculated.\n\n    Returns\n    -------\n    torch.Tensor\n        The contrastive loss.\n    \"\"\"\n    world_size = dist.get_world_size() if dist.is_initialized() else 1\n    rank = dist.get_rank() if world_size &gt; 1 else 0\n\n    if self.l2_normalize:\n        embeddings = {k: F.normalize(v, p=2, dim=-1) for k, v in embeddings.items()}\n\n    if world_size &gt; 1:  # gather embeddings and example_ids across all processes\n        # NOTE: gathering dictionaries of tensors across all processes\n        # (keys + values, as opposed to just values) is especially important\n        # for the modality_alignment loss, which requires all embeddings\n        all_embeddings = _gather_dicts(\n            embeddings,\n            local_loss=self.local_loss,\n            gather_with_grad=self.gather_with_grad,\n            rank=rank,\n        )\n        all_example_ids = _gather_dicts(\n            example_ids,\n            local_loss=self.local_loss,\n            gather_with_grad=self.gather_with_grad,\n            rank=rank,\n        )\n    else:\n        all_embeddings = embeddings\n        all_example_ids = example_ids\n\n    losses = []\n    for loss_pairs in modality_loss_pairs:\n        logits_per_feature_a, logits_per_feature_b, skip_flag = self._get_logits(\n            loss_pairs.modalities,\n            per_device_embeddings=embeddings,\n            all_embeddings=all_embeddings,\n            per_device_example_ids=example_ids,\n            all_example_ids=all_example_ids,\n            logit_scale=logit_scale,\n            world_size=world_size,\n        )\n        if logits_per_feature_a is None or logits_per_feature_b is None:\n            continue\n\n        labels = self._get_ground_truth(\n            logits_per_feature_a.shape,\n            device=logits_per_feature_a.device,\n            rank=rank,\n            world_size=world_size,\n            skipped_process=skip_flag,\n        )\n\n        if labels.numel() != 0:\n            losses.append(\n                (\n                    (\n                        F.cross_entropy(logits_per_feature_a, labels)\n                        + F.cross_entropy(logits_per_feature_b, labels)\n                    )\n                    / 2\n                )\n                * loss_pairs.weight\n            )\n\n    if self.modality_alignment:\n        losses.append(\n            self._compute_modality_alignment_loss(all_embeddings, logit_scale)\n        )\n\n    if not losses:  # no loss to compute (e.g. no paired data in batch)\n        losses.append(\n            torch.tensor(\n                0.0,\n                device=logit_scale.device,\n                dtype=next(iter(embeddings.values())).dtype,\n            )\n        )\n\n    return torch.stack(losses).sum()\n</code></pre>"},{"location":"api/#mmlearn.cli.run.Data2VecLoss","title":"Data2VecLoss","text":"<p>               Bases: <code>Module</code></p> <p>Data2Vec loss function.</p> <p>Parameters:</p> Name Type Description Default <code>beta</code> <code>float</code> <p>Specifies the beta parameter for smooth L1 loss. If <code>0</code>, MSE loss is used.</p> <code>0</code> <code>loss_scale</code> <code>Optional[float]</code> <p>Scaling factor for the loss. If None, uses <code>1 / sqrt(embedding_dim)</code>.</p> <code>None</code> <code>reduction</code> <code>str</code> <p>Specifies the reduction to apply to the output: <code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>.</p> <code>'none'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the reduction mode is not supported.</p> Source code in <code>mmlearn/modules/losses/data2vec.py</code> <pre><code>@store(group=\"modules/losses\", provider=\"mmlearn\")\nclass Data2VecLoss(nn.Module):\n    \"\"\"Data2Vec loss function.\n\n    Parameters\n    ----------\n    beta : float, optional, default=0\n        Specifies the beta parameter for smooth L1 loss. If ``0``, MSE loss is used.\n    loss_scale : Optional[float], optional, default=None\n        Scaling factor for the loss. If None, uses ``1 / sqrt(embedding_dim)``.\n    reduction : str, optional, default='none'\n        Specifies the reduction to apply to the output:\n        ``'none'`` | ``'mean'`` | ``'sum'``.\n\n    Raises\n    ------\n    ValueError\n        If the reduction mode is not supported.\n    \"\"\"\n\n    def __init__(\n        self,\n        beta: float = 0,\n        loss_scale: Optional[float] = None,\n        reduction: str = \"none\",\n    ) -&gt; None:\n        super().__init__()\n        self.beta = beta\n        self.loss_scale = loss_scale\n        if reduction not in [\"none\", \"mean\", \"sum\"]:\n            raise ValueError(f\"Unsupported reduction mode: {reduction}\")\n        self.reduction = reduction\n\n    def forward(self, x: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute the Data2Vec loss.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Predicted embeddings of shape ``(batch_size, num_patches, embedding_dim)``.\n        y : torch.Tensor\n            Target embeddings of shape ``(batch_size, num_patches, embedding_dim)``.\n\n        Returns\n        -------\n        torch.Tensor\n            Data2Vec loss value.\n\n        Raises\n        ------\n        ValueError\n            If the shapes of x and y do not match.\n        \"\"\"\n        if x.shape != y.shape:\n            raise ValueError(f\"Shape mismatch: x: {x.shape}, y: {y.shape}\")\n\n        x = x.view(-1, x.size(-1)).float()\n        y = y.view(-1, y.size(-1))\n\n        if self.beta == 0:\n            loss = mse_loss(x, y, reduction=\"none\")\n        else:\n            loss = smooth_l1_loss(x, y, reduction=\"none\", beta=self.beta)\n\n        if self.loss_scale is not None:\n            scale = self.loss_scale\n        else:\n            scale = 1 / math.sqrt(x.size(-1))\n\n        loss = loss * scale\n\n        if self.reduction == \"mean\":\n            return loss.mean()\n        if self.reduction == \"sum\":\n            return loss.sum()\n        # 'none'\n        return loss.view(x.size(0), -1).sum(1)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.Data2VecLoss.forward","title":"forward","text":"<pre><code>forward(x, y)\n</code></pre> <p>Compute the Data2Vec loss.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Predicted embeddings of shape <code>(batch_size, num_patches, embedding_dim)</code>.</p> required <code>y</code> <code>Tensor</code> <p>Target embeddings of shape <code>(batch_size, num_patches, embedding_dim)</code>.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Data2Vec loss value.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the shapes of x and y do not match.</p> Source code in <code>mmlearn/modules/losses/data2vec.py</code> <pre><code>def forward(self, x: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute the Data2Vec loss.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Predicted embeddings of shape ``(batch_size, num_patches, embedding_dim)``.\n    y : torch.Tensor\n        Target embeddings of shape ``(batch_size, num_patches, embedding_dim)``.\n\n    Returns\n    -------\n    torch.Tensor\n        Data2Vec loss value.\n\n    Raises\n    ------\n    ValueError\n        If the shapes of x and y do not match.\n    \"\"\"\n    if x.shape != y.shape:\n        raise ValueError(f\"Shape mismatch: x: {x.shape}, y: {y.shape}\")\n\n    x = x.view(-1, x.size(-1)).float()\n    y = y.view(-1, y.size(-1))\n\n    if self.beta == 0:\n        loss = mse_loss(x, y, reduction=\"none\")\n    else:\n        loss = smooth_l1_loss(x, y, reduction=\"none\", beta=self.beta)\n\n    if self.loss_scale is not None:\n        scale = self.loss_scale\n    else:\n        scale = 1 / math.sqrt(x.size(-1))\n\n    loss = loss * scale\n\n    if self.reduction == \"mean\":\n        return loss.mean()\n    if self.reduction == \"sum\":\n        return loss.sum()\n    # 'none'\n    return loss.view(x.size(0), -1).sum(1)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.RetrievalRecallAtK","title":"RetrievalRecallAtK","text":"<p>               Bases: <code>Metric</code></p> <p>Retrieval Recall@K metric.</p> <p>Computes the Recall@K for retrieval tasks. The metric is computed as follows:</p> <ol> <li>Compute the cosine similarity between the query and the database.</li> <li>For each query, sort the database in decreasing order of similarity.</li> <li>Compute the Recall@K as the number of true positives among the top K elements.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>top_k</code> <code>int</code> <p>The number of top elements to consider for computing the Recall@K.</p> required <code>reduction</code> <code>(mean, sum, none, None)</code> <p>Specifies the reduction to apply after computing the pairwise cosine similarity scores.</p> <code>\"mean\"</code> <code>aggregation</code> <code>(mean, median, min, max)</code> <p>Specifies the aggregation function to apply to the Recall@K values computed in batches. If a callable is provided, it should accept a tensor of values and a keyword argument <code>'dim'</code> and return a single scalar value.</p> <code>\"mean\"</code> <code>kwargs</code> <code>Any</code> <p>Additional arguments to be passed to the class:<code>torchmetrics.Metric</code> class.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <ul> <li>If the <code>top_k</code> is not a positive integer or None.</li> <li>If the <code>reduction</code> is not one of {\"mean\", \"sum\", \"none\", None}.</li> <li>If the <code>aggregation</code> is not one of {\"mean\", \"median\", \"min\", \"max\"} or a   custom callable function.</li> </ul> Source code in <code>mmlearn/modules/metrics/retrieval_recall.py</code> <pre><code>@store(group=\"modules/metrics\", provider=\"mmlearn\")\nclass RetrievalRecallAtK(Metric):\n    \"\"\"Retrieval Recall@K metric.\n\n    Computes the Recall@K for retrieval tasks. The metric is computed as follows:\n\n    1. Compute the cosine similarity between the query and the database.\n    2. For each query, sort the database in decreasing order of similarity.\n    3. Compute the Recall@K as the number of true positives among the top K elements.\n\n    Parameters\n    ----------\n    top_k : int\n        The number of top elements to consider for computing the Recall@K.\n    reduction : {\"mean\", \"sum\", \"none\", None}, optional, default=\"sum\"\n        Specifies the reduction to apply after computing the pairwise cosine similarity\n        scores.\n    aggregation : {\"mean\", \"median\", \"min\", \"max\"} or callable, default=\"mean\"\n        Specifies the aggregation function to apply to the Recall@K values computed\n        in batches. If a callable is provided, it should accept a tensor of values\n        and a keyword argument ``'dim'`` and return a single scalar value.\n    kwargs : Any\n        Additional arguments to be passed to the :py:class:`torchmetrics.Metric` class.\n\n    Raises\n    ------\n    ValueError\n\n        - If the `top_k` is not a positive integer or None.\n        - If the `reduction` is not one of {\"mean\", \"sum\", \"none\", None}.\n        - If the `aggregation` is not one of {\"mean\", \"median\", \"min\", \"max\"} or a\n          custom callable function.\n\n    \"\"\"\n\n    is_differentiable: bool = False\n    higher_is_better: bool = True\n    full_state_update: bool = False\n\n    indexes: list[torch.Tensor]\n    x: list[torch.Tensor]\n    y: list[torch.Tensor]\n    num_samples: torch.Tensor\n\n    def __init__(\n        self,\n        top_k: int,\n        reduction: Optional[Literal[\"mean\", \"sum\", \"none\"]] = \"sum\",\n        aggregation: Union[\n            Literal[\"mean\", \"median\", \"min\", \"max\"],\n            Callable[[torch.Tensor, int], torch.Tensor],\n        ] = \"mean\",\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Initialize the metric.\"\"\"\n        super().__init__(**kwargs)\n\n        if top_k is not None and not (isinstance(top_k, int) and top_k &gt; 0):\n            raise ValueError(\"`top_k` has to be a positive integer or None\")\n        self.top_k = top_k\n\n        allowed_reduction = (\"sum\", \"mean\", \"none\", None)\n        if reduction not in allowed_reduction:\n            raise ValueError(\n                f\"Expected argument `reduction` to be one of {allowed_reduction} but got {reduction}\"\n            )\n        self.reduction = reduction\n\n        if not (\n            aggregation in (\"mean\", \"median\", \"min\", \"max\") or callable(aggregation)\n        ):\n            raise ValueError(\n                \"Argument `aggregation` must be one of `mean`, `median`, `min`, `max` or a custom callable function\"\n                f\"which takes tensor of values, but got {aggregation}.\"\n            )\n        self.aggregation = aggregation\n\n        self.add_state(\"x\", default=[], dist_reduce_fx=\"cat\")\n        self.add_state(\"y\", default=[], dist_reduce_fx=\"cat\")\n        self.add_state(\"indexes\", default=[], dist_reduce_fx=\"cat\")\n        self.add_state(\"num_samples\", default=torch.tensor(0), dist_reduce_fx=\"cat\")\n\n        self._batch_size: int = -1\n\n        self.compute_on_cpu = True\n        self.sync_on_compute = False\n        self.dist_sync_on_step = False\n        self._to_sync = self.sync_on_compute\n        self._should_unsync = False\n\n    def update(self, x: torch.Tensor, y: torch.Tensor, indexes: torch.Tensor) -&gt; None:\n        \"\"\"Check shape, convert dtypes and add to accumulators.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Embeddings (unnormalized) of shape ``(N, D)`` where ``N`` is the number\n            of samples and `D` is the number of dimensions.\n        y : torch.Tensor\n            Embeddings (unnormalized) of shape ``(M, D)`` where ``M`` is the number\n            of samples and ``D`` is the number of dimensions.\n        indexes : torch.Tensor\n            Index tensor of shape ``(N,)`` where ``N`` is the number of samples.\n            This specifies which sample in ``y`` is the positive match for each\n            sample in ``x``.\n\n        Raises\n        ------\n        ValueError\n            If `indexes` is None.\n\n        \"\"\"\n        if indexes is None:\n            raise ValueError(\"Argument `indexes` cannot be None\")\n\n        x, y, indexes = _update_batch_inputs(x.clone(), y.clone(), indexes.clone())\n\n        # offset batch indexes by the number of samples seen so far\n        indexes += self.num_samples\n\n        local_batch_size = torch.zeros_like(self.num_samples) + x.size(0)\n        if self._is_distributed():\n            x = dim_zero_cat(gather_all_tensors(x, self.process_group))\n            y = dim_zero_cat(gather_all_tensors(y, self.process_group))\n            indexes = dim_zero_cat(\n                gather_all_tensors(indexes.clone(), self.process_group)\n            )\n\n            # offset indexes for each device\n            bsz_per_device = dim_zero_cat(\n                gather_all_tensors(local_batch_size, self.process_group)\n            )\n            cum_local_bsz = torch.cumsum(bsz_per_device, dim=0)\n            for device_idx in range(1, bsz_per_device.numel()):\n                indexes[cum_local_bsz[device_idx - 1] : cum_local_bsz[device_idx]] += (\n                    cum_local_bsz[device_idx - 1]\n                )\n\n            # update the global sample count\n            self.num_samples += cum_local_bsz[-1]\n\n            self._is_synced = True\n        else:\n            self.num_samples += x.size(0)\n\n        self.x.append(x)\n        self.y.append(y)\n        self.indexes.append(indexes)\n\n        if self._batch_size == -1:\n            self._batch_size = x.size(0)  # global batch size\n\n    def compute(self) -&gt; torch.Tensor:\n        \"\"\"Compute the metric.\n\n        Returns\n        -------\n        torch.Tensor\n            The computed metric.\n        \"\"\"\n        x = dim_zero_cat(self.x)\n        y = dim_zero_cat(self.y)\n\n        # normalize embeddings\n        x /= x.norm(dim=-1, p=2, keepdim=True)\n        y /= y.norm(dim=-1, p=2, keepdim=True)\n\n        # instantiate reduction function\n        reduction_mapping: Dict[\n            Optional[str], Callable[[torch.Tensor], torch.Tensor]\n        ] = {\n            \"sum\": partial(torch.sum, dim=-1),\n            \"mean\": partial(torch.mean, dim=-1),\n            \"none\": lambda x: x,\n            None: lambda x: x,\n        }\n\n        # concatenate indexes of true pairs\n        indexes = dim_zero_cat(self.indexes)\n\n        results: list[torch.Tensor] = []\n        with concurrent.futures.ThreadPoolExecutor(\n            max_workers=os.cpu_count() or 1  # use all available CPUs\n        ) as executor:\n            futures = [\n                executor.submit(\n                    self._process_batch,\n                    start,\n                    x,\n                    y,\n                    indexes,\n                    reduction_mapping,\n                    self.top_k,\n                )\n                for start in tqdm(\n                    range(0, len(x), self._batch_size), desc=f\"Recall@{self.top_k}\"\n                )\n            ]\n            for future in concurrent.futures.as_completed(futures):\n                results.append(future.result())\n\n        return _retrieval_aggregate(\n            (torch.cat([x.float() for x in results]) &gt; 0).float(), self.aggregation\n        )\n\n    def forward(self, *args: Any, **kwargs: Any) -&gt; Any:\n        \"\"\"Forward method is not supported.\n\n        Raises\n        ------\n        NotImplementedError\n            The forward method is not supported for this metric.\n        \"\"\"\n        raise NotImplementedError(\n            \"RetrievalRecallAtK metric does not support forward method\"\n        )\n\n    def _is_distributed(self) -&gt; bool:\n        if self.distributed_available_fn is not None:\n            distributed_available = self.distributed_available_fn\n\n        return distributed_available() if callable(distributed_available) else False\n\n    def _process_batch(\n        self,\n        start: int,\n        x_norm: torch.Tensor,\n        y_norm: torch.Tensor,\n        indexes: torch.Tensor,\n        reduction_mapping: Dict[Optional[str], Callable[[torch.Tensor], torch.Tensor]],\n        top_k: int,\n    ) -&gt; torch.Tensor:\n        \"\"\"Compute the Recall@K for a batch of samples.\"\"\"\n        end = start + self._batch_size\n        x_norm_batch = x_norm[start:end]\n        indexes_batch = indexes[start:end]\n\n        similarity = _safe_matmul(x_norm_batch, y_norm)\n        scores: torch.Tensor = reduction_mapping[self.reduction](similarity)\n\n        with torch.inference_mode():\n            positive_pairs = torch.zeros_like(scores, dtype=torch.bool)\n            positive_pairs[torch.arange(len(scores)), indexes_batch] = True\n\n        return _recall_at_k(scores, positive_pairs, top_k)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.RetrievalRecallAtK.__init__","title":"__init__","text":"<pre><code>__init__(\n    top_k, reduction=\"sum\", aggregation=\"mean\", **kwargs\n)\n</code></pre> <p>Initialize the metric.</p> Source code in <code>mmlearn/modules/metrics/retrieval_recall.py</code> <pre><code>def __init__(\n    self,\n    top_k: int,\n    reduction: Optional[Literal[\"mean\", \"sum\", \"none\"]] = \"sum\",\n    aggregation: Union[\n        Literal[\"mean\", \"median\", \"min\", \"max\"],\n        Callable[[torch.Tensor, int], torch.Tensor],\n    ] = \"mean\",\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Initialize the metric.\"\"\"\n    super().__init__(**kwargs)\n\n    if top_k is not None and not (isinstance(top_k, int) and top_k &gt; 0):\n        raise ValueError(\"`top_k` has to be a positive integer or None\")\n    self.top_k = top_k\n\n    allowed_reduction = (\"sum\", \"mean\", \"none\", None)\n    if reduction not in allowed_reduction:\n        raise ValueError(\n            f\"Expected argument `reduction` to be one of {allowed_reduction} but got {reduction}\"\n        )\n    self.reduction = reduction\n\n    if not (\n        aggregation in (\"mean\", \"median\", \"min\", \"max\") or callable(aggregation)\n    ):\n        raise ValueError(\n            \"Argument `aggregation` must be one of `mean`, `median`, `min`, `max` or a custom callable function\"\n            f\"which takes tensor of values, but got {aggregation}.\"\n        )\n    self.aggregation = aggregation\n\n    self.add_state(\"x\", default=[], dist_reduce_fx=\"cat\")\n    self.add_state(\"y\", default=[], dist_reduce_fx=\"cat\")\n    self.add_state(\"indexes\", default=[], dist_reduce_fx=\"cat\")\n    self.add_state(\"num_samples\", default=torch.tensor(0), dist_reduce_fx=\"cat\")\n\n    self._batch_size: int = -1\n\n    self.compute_on_cpu = True\n    self.sync_on_compute = False\n    self.dist_sync_on_step = False\n    self._to_sync = self.sync_on_compute\n    self._should_unsync = False\n</code></pre>"},{"location":"api/#mmlearn.cli.run.RetrievalRecallAtK.update","title":"update","text":"<pre><code>update(x, y, indexes)\n</code></pre> <p>Check shape, convert dtypes and add to accumulators.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Embeddings (unnormalized) of shape <code>(N, D)</code> where <code>N</code> is the number of samples and <code>D</code> is the number of dimensions.</p> required <code>y</code> <code>Tensor</code> <p>Embeddings (unnormalized) of shape <code>(M, D)</code> where <code>M</code> is the number of samples and <code>D</code> is the number of dimensions.</p> required <code>indexes</code> <code>Tensor</code> <p>Index tensor of shape <code>(N,)</code> where <code>N</code> is the number of samples. This specifies which sample in <code>y</code> is the positive match for each sample in <code>x</code>.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>indexes</code> is None.</p> Source code in <code>mmlearn/modules/metrics/retrieval_recall.py</code> <pre><code>def update(self, x: torch.Tensor, y: torch.Tensor, indexes: torch.Tensor) -&gt; None:\n    \"\"\"Check shape, convert dtypes and add to accumulators.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Embeddings (unnormalized) of shape ``(N, D)`` where ``N`` is the number\n        of samples and `D` is the number of dimensions.\n    y : torch.Tensor\n        Embeddings (unnormalized) of shape ``(M, D)`` where ``M`` is the number\n        of samples and ``D`` is the number of dimensions.\n    indexes : torch.Tensor\n        Index tensor of shape ``(N,)`` where ``N`` is the number of samples.\n        This specifies which sample in ``y`` is the positive match for each\n        sample in ``x``.\n\n    Raises\n    ------\n    ValueError\n        If `indexes` is None.\n\n    \"\"\"\n    if indexes is None:\n        raise ValueError(\"Argument `indexes` cannot be None\")\n\n    x, y, indexes = _update_batch_inputs(x.clone(), y.clone(), indexes.clone())\n\n    # offset batch indexes by the number of samples seen so far\n    indexes += self.num_samples\n\n    local_batch_size = torch.zeros_like(self.num_samples) + x.size(0)\n    if self._is_distributed():\n        x = dim_zero_cat(gather_all_tensors(x, self.process_group))\n        y = dim_zero_cat(gather_all_tensors(y, self.process_group))\n        indexes = dim_zero_cat(\n            gather_all_tensors(indexes.clone(), self.process_group)\n        )\n\n        # offset indexes for each device\n        bsz_per_device = dim_zero_cat(\n            gather_all_tensors(local_batch_size, self.process_group)\n        )\n        cum_local_bsz = torch.cumsum(bsz_per_device, dim=0)\n        for device_idx in range(1, bsz_per_device.numel()):\n            indexes[cum_local_bsz[device_idx - 1] : cum_local_bsz[device_idx]] += (\n                cum_local_bsz[device_idx - 1]\n            )\n\n        # update the global sample count\n        self.num_samples += cum_local_bsz[-1]\n\n        self._is_synced = True\n    else:\n        self.num_samples += x.size(0)\n\n    self.x.append(x)\n    self.y.append(y)\n    self.indexes.append(indexes)\n\n    if self._batch_size == -1:\n        self._batch_size = x.size(0)  # global batch size\n</code></pre>"},{"location":"api/#mmlearn.cli.run.RetrievalRecallAtK.compute","title":"compute","text":"<pre><code>compute()\n</code></pre> <p>Compute the metric.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>The computed metric.</p> Source code in <code>mmlearn/modules/metrics/retrieval_recall.py</code> <pre><code>def compute(self) -&gt; torch.Tensor:\n    \"\"\"Compute the metric.\n\n    Returns\n    -------\n    torch.Tensor\n        The computed metric.\n    \"\"\"\n    x = dim_zero_cat(self.x)\n    y = dim_zero_cat(self.y)\n\n    # normalize embeddings\n    x /= x.norm(dim=-1, p=2, keepdim=True)\n    y /= y.norm(dim=-1, p=2, keepdim=True)\n\n    # instantiate reduction function\n    reduction_mapping: Dict[\n        Optional[str], Callable[[torch.Tensor], torch.Tensor]\n    ] = {\n        \"sum\": partial(torch.sum, dim=-1),\n        \"mean\": partial(torch.mean, dim=-1),\n        \"none\": lambda x: x,\n        None: lambda x: x,\n    }\n\n    # concatenate indexes of true pairs\n    indexes = dim_zero_cat(self.indexes)\n\n    results: list[torch.Tensor] = []\n    with concurrent.futures.ThreadPoolExecutor(\n        max_workers=os.cpu_count() or 1  # use all available CPUs\n    ) as executor:\n        futures = [\n            executor.submit(\n                self._process_batch,\n                start,\n                x,\n                y,\n                indexes,\n                reduction_mapping,\n                self.top_k,\n            )\n            for start in tqdm(\n                range(0, len(x), self._batch_size), desc=f\"Recall@{self.top_k}\"\n            )\n        ]\n        for future in concurrent.futures.as_completed(futures):\n            results.append(future.result())\n\n    return _retrieval_aggregate(\n        (torch.cat([x.float() for x in results]) &gt; 0).float(), self.aggregation\n    )\n</code></pre>"},{"location":"api/#mmlearn.cli.run.RetrievalRecallAtK.forward","title":"forward","text":"<pre><code>forward(*args, **kwargs)\n</code></pre> <p>Forward method is not supported.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>The forward method is not supported for this metric.</p> Source code in <code>mmlearn/modules/metrics/retrieval_recall.py</code> <pre><code>def forward(self, *args: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"Forward method is not supported.\n\n    Raises\n    ------\n    NotImplementedError\n        The forward method is not supported for this metric.\n    \"\"\"\n    raise NotImplementedError(\n        \"RetrievalRecallAtK metric does not support forward method\"\n    )\n</code></pre>"},{"location":"api/#mmlearn.cli.run.ContrastivePretraining","title":"ContrastivePretraining","text":"<p>               Bases: <code>TrainingTask</code></p> <p>Contrastive pretraining task.</p> <p>This class supports contrastive pretraining with <code>N</code> modalities of data. It allows the sharing of encoders, heads, and postprocessors across modalities. It also supports computing the contrastive loss between specified pairs of modalities, as well as training auxiliary tasks alongside the main contrastive pretraining task.</p> <p>Parameters:</p> Name Type Description Default <code>encoders</code> <code>dict[str, Module]</code> <p>A dictionary of encoders. The keys can be any string, including the names of any supported modalities. If the keys are not supported modalities, the <code>modality_module_mapping</code> parameter must be provided to map the encoders to specific modalities. The encoders are expected to take a dictionary of input values and return a list-like object with the first element being the encoded values. This first element is passed on to the heads or postprocessors and the remaining elements are ignored.</p> required <code>heads</code> <code>Optional[dict[str, Union[Module, dict[str, Module]]]]</code> <p>A dictionary of modules that process the encoder outputs, usually projection heads. If the keys do not correspond to the name of a supported modality, the <code>modality_module_mapping</code> parameter must be provided. If any of the values are dictionaries, they will be wrapped in a class:<code>torch.nn.Sequential</code> module. All head modules are expected to take a single input tensor and return a single output tensor.</p> <code>None</code> <code>postprocessors</code> <code>Optional[dict[str, Union[Module, dict[str, Module]]]]</code> <p>A dictionary of modules that process the head outputs. If the keys do not correspond to the name of a supported modality, the <code>modality_module_mapping</code> parameter must be provided. If any of the values are dictionaries, they will be wrapped in a <code>nn.Sequential</code> module. All postprocessor modules are expected to take a single input tensor and return a single output tensor.</p> <code>None</code> <code>modality_module_mapping</code> <code>Optional[dict[str, ModuleKeySpec]]</code> <p>A dictionary mapping modalities to encoders, heads, and postprocessors. Useful for reusing the same instance of a module across multiple modalities.</p> <code>None</code> <code>optimizer</code> <code>Optional[partial[Optimizer]]</code> <p>The optimizer to use for training. This is expected to be a func:<code>~functools.partial</code> function that takes the model parameters as the only required argument. If not provided, training will continue without an optimizer.</p> <code>None</code> <code>lr_scheduler</code> <code>Optional[Union[dict[str, Union[partial[LRScheduler], Any]], partial[LRScheduler]]]</code> <p>The learning rate scheduler to use for training. This can be a func:<code>~functools.partial</code> function that takes the optimizer as the only required argument or a dictionary with a <code>'scheduler'</code> key that specifies the scheduler and an optional <code>'extras'</code> key that specifies additional arguments to pass to the scheduler. If not provided, the learning rate will not be adjusted during training.</p> <code>None</code> <code>init_logit_scale</code> <code>float</code> <p>The initial value of the logit scale parameter. This is the log of the scale factor applied to the logits before computing the contrastive loss.</p> <code>1 / 0.07</code> <code>max_logit_scale</code> <code>float</code> <p>The maximum value of the logit scale parameter. The logit scale parameter is clamped to the range <code>[0, log(max_logit_scale)]</code>.</p> <code>100</code> <code>learnable_logit_scale</code> <code>bool</code> <p>Whether the logit scale parameter is learnable. If set to False, the logit scale parameter is treated as a constant.</p> <code>True</code> <code>loss</code> <code>Optional[Module]</code> <p>The loss function to use.</p> <code>None</code> <code>modality_loss_pairs</code> <code>Optional[list[LossPairSpec]]</code> <p>A list of pairs of modalities to compute the contrastive loss between and the weight to apply to each pair.</p> <code>None</code> <code>auxiliary_tasks</code> <code>dict[str, AuxiliaryTaskSpec]</code> <p>Auxiliary tasks to run alongside the main contrastive pretraining task.</p> <ul> <li>The auxiliary task module is expected to be a partially-initialized instance   of a class:<code>~lightning.pytorch.core.LightningModule</code> created using   func:<code>functools.partial</code>, such that an initialized encoder can be   passed as the only argument.</li> <li>The <code>modality</code> parameter specifies the modality of the encoder to use   for the auxiliary task. The <code>loss_weight</code> parameter specifies the weight   to apply to the auxiliary task loss.</li> </ul> <code>None</code> <code>log_auxiliary_tasks_loss</code> <code>bool</code> <p>Whether to log the loss of auxiliary tasks to the main logger.</p> <code>False</code> <code>compute_validation_loss</code> <code>bool</code> <p>Whether to compute the validation loss if a validation dataloader is provided. The loss function must be provided to compute the validation loss.</p> <code>True</code> <code>compute_test_loss</code> <code>bool</code> <p>Whether to compute the test loss if a test dataloader is provided. The loss function must be provided to compute the test loss.</p> <code>True</code> <code>evaluation_tasks</code> <code>Optional[dict[str, EvaluationSpec]]</code> <p>Evaluation tasks to run during validation, while training, and during testing.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <ul> <li>If the loss function is not provided and either the validation or test loss   needs to be computed.</li> <li>If the given modality is not supported.</li> <li>If the encoder, head, or postprocessor is not mapped to a modality.</li> <li>If an unsupported modality is found in the loss pair specification.</li> <li>If an unsupported modality is found in the auxiliary tasks.</li> <li>If the auxiliary task is not a partial function.</li> <li>If the evaluation task is not an instance of class:<code>~mmlearn.tasks.hooks.EvaluationHooks</code>.</li> </ul> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>@store(group=\"task\", provider=\"mmlearn\")\nclass ContrastivePretraining(TrainingTask):\n    \"\"\"Contrastive pretraining task.\n\n    This class supports contrastive pretraining with ``N`` modalities of data. It\n    allows the sharing of encoders, heads, and postprocessors across modalities.\n    It also supports computing the contrastive loss between specified pairs of\n    modalities, as well as training auxiliary tasks alongside the main contrastive\n    pretraining task.\n\n    Parameters\n    ----------\n    encoders : dict[str, torch.nn.Module]\n        A dictionary of encoders. The keys can be any string, including the names of\n        any supported modalities. If the keys are not supported modalities, the\n        ``modality_module_mapping`` parameter must be provided to map the encoders to\n        specific modalities. The encoders are expected to take a dictionary of input\n        values and return a list-like object with the first element being the encoded\n        values. This first element is passed on to the heads or postprocessors and\n        the remaining elements are ignored.\n    heads : Optional[dict[str, Union[torch.nn.Module, dict[str, torch.nn.Module]]]], optional, default=None\n        A dictionary of modules that process the encoder outputs, usually projection\n        heads. If the keys do not correspond to the name of a supported modality,\n        the ``modality_module_mapping`` parameter must be provided. If any of the values\n        are dictionaries, they will be wrapped in a :py:class:`torch.nn.Sequential`\n        module. All head modules are expected to take a single input tensor and\n        return a single output tensor.\n    postprocessors : Optional[dict[str, Union[torch.nn.Module, dict[str, torch.nn.Module]]]], optional, default=None\n        A dictionary of modules that process the head outputs. If the keys do not\n        correspond to the name of a supported modality, the `modality_module_mapping`\n        parameter must be provided. If any of the values are dictionaries, they will\n        be wrapped in a `nn.Sequential` module. All postprocessor modules are expected\n        to take a single input tensor and return a single output tensor.\n    modality_module_mapping : Optional[dict[str, ModuleKeySpec]], optional, default=None\n        A dictionary mapping modalities to encoders, heads, and postprocessors.\n        Useful for reusing the same instance of a module across multiple modalities.\n    optimizer : Optional[partial[torch.optim.Optimizer]], optional, default=None\n        The optimizer to use for training. This is expected to be a :py:func:`~functools.partial`\n        function that takes the model parameters as the only required argument.\n        If not provided, training will continue without an optimizer.\n    lr_scheduler : Optional[Union[dict[str, Union[partial[torch.optim.lr_scheduler.LRScheduler], Any]], partial[torch.optim.lr_scheduler.LRScheduler]]], optional, default=None\n        The learning rate scheduler to use for training. This can be a\n        :py:func:`~functools.partial` function that takes the optimizer as the only\n        required argument or a dictionary with a ``'scheduler'`` key that specifies\n        the scheduler and an optional ``'extras'`` key that specifies additional\n        arguments to pass to the scheduler. If not provided, the learning rate will\n        not be adjusted during training.\n    init_logit_scale : float, optional, default=1 / 0.07\n        The initial value of the logit scale parameter. This is the log of the scale\n        factor applied to the logits before computing the contrastive loss.\n    max_logit_scale : float, optional, default=100\n        The maximum value of the logit scale parameter. The logit scale parameter\n        is clamped to the range ``[0, log(max_logit_scale)]``.\n    learnable_logit_scale : bool, optional, default=True\n        Whether the logit scale parameter is learnable. If set to False, the logit\n        scale parameter is treated as a constant.\n    loss : Optional[torch.nn.Module], optional, default=None\n        The loss function to use.\n    modality_loss_pairs : Optional[list[LossPairSpec]], optional, default=None\n        A list of pairs of modalities to compute the contrastive loss between and\n        the weight to apply to each pair.\n    auxiliary_tasks : dict[str, AuxiliaryTaskSpec], optional, default=None\n        Auxiliary tasks to run alongside the main contrastive pretraining task.\n\n        - The auxiliary task module is expected to be a partially-initialized instance\n          of a :py:class:`~lightning.pytorch.core.LightningModule` created using\n          :py:func:`functools.partial`, such that an initialized encoder can be\n          passed as the only argument.\n        - The ``modality`` parameter specifies the modality of the encoder to use\n          for the auxiliary task. The ``loss_weight`` parameter specifies the weight\n          to apply to the auxiliary task loss.\n    log_auxiliary_tasks_loss : bool, optional, default=False\n        Whether to log the loss of auxiliary tasks to the main logger.\n    compute_validation_loss : bool, optional, default=True\n        Whether to compute the validation loss if a validation dataloader is provided.\n        The loss function must be provided to compute the validation loss.\n    compute_test_loss : bool, optional, default=True\n        Whether to compute the test loss if a test dataloader is provided. The loss\n        function must be provided to compute the test loss.\n    evaluation_tasks : Optional[dict[str, EvaluationSpec]], optional, default=None\n        Evaluation tasks to run during validation, while training, and during testing.\n\n    Raises\n    ------\n    ValueError\n\n        - If the loss function is not provided and either the validation or test loss\n          needs to be computed.\n        - If the given modality is not supported.\n        - If the encoder, head, or postprocessor is not mapped to a modality.\n        - If an unsupported modality is found in the loss pair specification.\n        - If an unsupported modality is found in the auxiliary tasks.\n        - If the auxiliary task is not a partial function.\n        - If the evaluation task is not an instance of :py:class:`~mmlearn.tasks.hooks.EvaluationHooks`.\n\n    \"\"\"  # noqa: W505\n\n    def __init__(  # noqa: PLR0912, PLR0915\n        self,\n        encoders: dict[str, nn.Module],\n        heads: Optional[dict[str, Union[nn.Module, dict[str, nn.Module]]]] = None,\n        postprocessors: Optional[\n            dict[str, Union[nn.Module, dict[str, nn.Module]]]\n        ] = None,\n        modality_module_mapping: Optional[dict[str, ModuleKeySpec]] = None,\n        optimizer: Optional[partial[torch.optim.Optimizer]] = None,\n        lr_scheduler: Optional[\n            Union[\n                dict[str, Union[partial[torch.optim.lr_scheduler.LRScheduler], Any]],\n                partial[torch.optim.lr_scheduler.LRScheduler],\n            ]\n        ] = None,\n        init_logit_scale: float = 1 / 0.07,\n        max_logit_scale: float = 100,\n        learnable_logit_scale: bool = True,\n        loss: Optional[nn.Module] = None,\n        modality_loss_pairs: Optional[list[LossPairSpec]] = None,\n        auxiliary_tasks: Optional[dict[str, AuxiliaryTaskSpec]] = None,\n        log_auxiliary_tasks_loss: bool = False,\n        compute_validation_loss: bool = True,\n        compute_test_loss: bool = True,\n        evaluation_tasks: Optional[dict[str, EvaluationSpec]] = None,\n    ) -&gt; None:\n        super().__init__(\n            optimizer=optimizer,\n            lr_scheduler=lr_scheduler,\n            loss_fn=loss,\n            compute_validation_loss=compute_validation_loss,\n            compute_test_loss=compute_test_loss,\n        )\n\n        self.save_hyperparameters(\n            ignore=[\n                \"encoders\",\n                \"heads\",\n                \"postprocessors\",\n                \"modality_module_mapping\",\n                \"loss\",\n                \"auxiliary_tasks\",\n                \"evaluation_tasks\",\n                \"modality_loss_pairs\",\n            ]\n        )\n\n        if modality_module_mapping is None:\n            # assume all the module dictionaries use the same keys corresponding\n            # to modalities\n            modality_module_mapping = {}\n            for key in encoders:\n                modality_module_mapping[key] = ModuleKeySpec(\n                    encoder_key=key,\n                    head_key=key,\n                    postprocessor_key=key,\n                )\n\n        # match modalities to encoders, heads, and postprocessors\n        modality_encoder_mapping: dict[str, Optional[str]] = {}\n        modality_head_mapping: dict[str, Optional[str]] = {}\n        modality_postprocessor_mapping: dict[str, Optional[str]] = {}\n        for modality_key, module_mapping in modality_module_mapping.items():\n            if not Modalities.has_modality(modality_key):\n                raise ValueError(_unsupported_modality_error.format(modality_key))\n            modality_encoder_mapping[modality_key] = module_mapping.encoder_key\n            modality_head_mapping[modality_key] = module_mapping.head_key\n            modality_postprocessor_mapping[modality_key] = (\n                module_mapping.postprocessor_key\n            )\n\n        # ensure all modules are mapped to a modality\n        for key in encoders:\n            if key not in modality_encoder_mapping.values():\n                if not Modalities.has_modality(key):\n                    raise ValueError(_unsupported_modality_error.format(key))\n                modality_encoder_mapping[key] = key\n\n        if heads is not None:\n            for key in heads:\n                if key not in modality_head_mapping.values():\n                    if not Modalities.has_modality(key):\n                        raise ValueError(_unsupported_modality_error.format(key))\n                    modality_head_mapping[key] = key\n\n        if postprocessors is not None:\n            for key in postprocessors:\n                if key not in modality_postprocessor_mapping.values():\n                    if not Modalities.has_modality(key):\n                        raise ValueError(_unsupported_modality_error.format(key))\n                    modality_postprocessor_mapping[key] = key\n\n        self._available_modalities: list[Modality] = [\n            Modalities.get_modality(modality_key)\n            for modality_key in modality_encoder_mapping\n        ]\n        assert len(self._available_modalities) &gt;= 2, (\n            \"Expected at least two modalities to be available. \"\n        )\n\n        #: A :py:class:`~torch.nn.ModuleDict`, where the keys are the names of the\n        #: modalities and the values are the encoder modules.\n        self.encoders = nn.ModuleDict(\n            {\n                Modalities.get_modality(modality_key).name: encoders[encoder_key]\n                for modality_key, encoder_key in modality_encoder_mapping.items()\n                if encoder_key is not None\n            }\n        )\n\n        #: A :py:class:`~torch.nn.ModuleDict`, where the keys are the names of the\n        #: modalities and the values are the projection head modules. This can be\n        #: ``None`` if no heads modules are provided.\n        self.heads = None\n        if heads is not None:\n            self.heads = nn.ModuleDict(\n                {\n                    Modalities.get_modality(modality_key).name: heads[head_key]\n                    if isinstance(heads[head_key], nn.Module)\n                    else nn.Sequential(*heads[head_key].values())\n                    for modality_key, head_key in modality_head_mapping.items()\n                    if head_key is not None and head_key in heads\n                }\n            )\n\n        #: A :py:class:`~torch.nn.ModuleDict`, where the keys are the names of the\n        #: modalities and the values are the postprocessor modules. This can be\n        #: ``None`` if no postprocessor modules are provided.\n        self.postprocessors = None\n        if postprocessors is not None:\n            self.postprocessors = nn.ModuleDict(\n                {\n                    Modalities.get_modality(modality_key).name: postprocessors[\n                        postprocessor_key\n                    ]\n                    if isinstance(postprocessors[postprocessor_key], nn.Module)\n                    else nn.Sequential(*postprocessors[postprocessor_key].values())\n                    for modality_key, postprocessor_key in modality_postprocessor_mapping.items()\n                    if postprocessor_key is not None\n                    and postprocessor_key in postprocessors\n                }\n            )\n\n        # set up logit scaling\n        log_logit_scale = torch.ones([]) * np.log(init_logit_scale)\n        self.max_logit_scale = max_logit_scale\n        self.learnable_logit_scale = learnable_logit_scale\n\n        if self.learnable_logit_scale:\n            self.log_logit_scale = torch.nn.Parameter(\n                log_logit_scale, requires_grad=True\n            )\n        else:\n            self.register_buffer(\"log_logit_scale\", log_logit_scale)\n\n        # set up contrastive loss pairs\n        if modality_loss_pairs is None:\n            modality_loss_pairs = [\n                LossPairSpec(modalities=(m1.name, m2.name))\n                for m1, m2 in itertools.combinations(self._available_modalities, 2)\n            ]\n\n        for modality_pair in modality_loss_pairs:\n            if not all(\n                Modalities.get_modality(modality) in self._available_modalities\n                for modality in modality_pair.modalities\n            ):\n                raise ValueError(\n                    \"Found unspecified modality in the loss pair specification \"\n                    f\"{modality_pair.modalities}. Available modalities are \"\n                    f\"{self._available_modalities}.\"\n                )\n\n        #: A list :py:class:`LossPairSpec` instances specifying the pairs of\n        #: modalities to compute the contrastive loss between and the weight to\n        #: apply to each pair.\n        self.modality_loss_pairs = modality_loss_pairs\n\n        # set up auxiliary tasks\n        self.aux_task_specs = auxiliary_tasks or {}\n        self.auxiliary_tasks: nn.ModuleDict[str, L.LightningModule] = nn.ModuleDict()\n        for task_name, task_spec in self.aux_task_specs.items():\n            if not Modalities.has_modality(task_spec.modality):\n                raise ValueError(\n                    f\"Found unsupported modality `{task_spec.modality}` in the auxiliary tasks. \"\n                    f\"Available modalities are {self._available_modalities}.\"\n                )\n            if not isinstance(task_spec.task, partial):\n                raise TypeError(\n                    f\"Expected auxiliary task to be a partial function, but got {type(task_spec.task)}.\"\n                )\n\n            self.auxiliary_tasks[task_name] = task_spec.task(\n                self.encoders[Modalities.get_modality(task_spec.modality).name]\n            )\n\n        self.log_auxiliary_tasks_loss = log_auxiliary_tasks_loss\n\n        if evaluation_tasks is not None:\n            for eval_task_spec in evaluation_tasks.values():\n                if not isinstance(eval_task_spec.task, EvaluationHooks):\n                    raise TypeError(\n                        f\"Expected {eval_task_spec.task} to be an instance of `EvaluationHooks` \"\n                        f\"but got {type(eval_task_spec.task)}.\"\n                    )\n\n        #: A dictionary of evaluation tasks to run during validation, while training,\n        #: or during testing.\n        self.evaluation_tasks = evaluation_tasks\n\n    def configure_model(self) -&gt; None:\n        \"\"\"Configure the model.\"\"\"\n        if self.auxiliary_tasks:\n            for task_name in self.auxiliary_tasks:\n                self.auxiliary_tasks[task_name].configure_model()\n\n    def encode(\n        self, inputs: dict[str, Any], modality: Modality, normalize: bool = False\n    ) -&gt; torch.Tensor:\n        \"\"\"Encode the input values for the given modality.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            Input values.\n        modality : Modality\n            The modality to encode.\n        normalize : bool, optional, default=False\n            Whether to apply L2 normalization to the output (after the head and\n            postprocessor layers, if present).\n\n        Returns\n        -------\n        torch.Tensor\n            The encoded values for the specified modality.\n        \"\"\"\n        output = self.encoders[modality.name](inputs)[0]\n\n        if self.postprocessors and modality.name in self.postprocessors:\n            output = self.postprocessors[modality.name](output)\n\n        if self.heads and modality.name in self.heads:\n            output = self.heads[modality.name](output)\n\n        if normalize:\n            output = torch.nn.functional.normalize(output, p=2, dim=-1)\n\n        return output\n\n    def forward(self, inputs: dict[str, Any]) -&gt; dict[str, torch.Tensor]:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input tensors to encode.\n\n        Returns\n        -------\n        dict[str, torch.Tensor]\n            The encodings for each modality.\n        \"\"\"\n        outputs = {\n            modality.embedding: self.encode(inputs, modality, normalize=True)\n            for modality in self._available_modalities\n            if modality.name in inputs\n        }\n\n        if not all(\n            output.size(-1) == list(outputs.values())[0].size(-1)\n            for output in outputs.values()\n        ):\n            raise ValueError(\"Expected all model outputs to have the same dimension.\")\n\n        return outputs\n\n    def on_train_epoch_start(self) -&gt; None:\n        \"\"\"Prepare for the training epoch.\n\n        This method sets the modules to training mode.\n        \"\"\"\n        self.encoders.train()\n        if self.heads:\n            self.heads.train()\n        if self.postprocessors:\n            self.postprocessors.train()\n\n    def training_step(self, batch: dict[str, Any], batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"Compute the loss for the batch.\n\n        Parameters\n        ----------\n        batch : dict[str, Any]\n            The batch of data to process.\n        batch_idx : int\n            The index of the batch.\n\n        Returns\n        -------\n        torch.Tensor\n            The loss for the batch.\n        \"\"\"\n        outputs = self(batch)\n\n        with torch.no_grad():\n            self.log_logit_scale.clamp_(0, math.log(self.max_logit_scale))\n\n        loss = self._compute_loss(batch, batch_idx, outputs)\n\n        if loss is None:\n            raise ValueError(\"The loss function must be provided for training.\")\n\n        self.log(\"train/loss\", loss, prog_bar=True, sync_dist=True)\n        self.log(\n            \"train/logit_scale\",\n            self.log_logit_scale.exp(),\n            prog_bar=True,\n            on_step=True,\n            on_epoch=False,\n        )\n\n        return loss\n\n    def on_before_zero_grad(self, optimizer: torch.optim.Optimizer) -&gt; None:\n        \"\"\"Zero out the gradients of the model.\"\"\"\n        if self.auxiliary_tasks:\n            for task in self.auxiliary_tasks.values():\n                task.on_before_zero_grad(optimizer)\n\n    def on_validation_epoch_start(self) -&gt; None:\n        \"\"\"Prepare for the validation epoch.\n\n        This method sets the modules to evaluation mode and calls the\n        ``on_evaluation_epoch_start`` method of each evaluation task.\n        \"\"\"\n        self._on_eval_epoch_start(\"val\")\n\n    def validation_step(\n        self, batch: dict[str, torch.Tensor], batch_idx: int\n    ) -&gt; Optional[torch.Tensor]:\n        \"\"\"Run a single validation step.\n\n        Parameters\n        ----------\n        batch : dict[str, torch.Tensor]\n            The batch of data to process.\n        batch_idx : int\n            The index of the batch.\n\n        Returns\n        -------\n        Optional[torch.Tensor]\n            The loss for the batch or ``None`` if the loss function is not provided.\n        \"\"\"\n        return self._shared_eval_step(batch, batch_idx, \"val\")\n\n    def on_validation_epoch_end(self) -&gt; None:\n        \"\"\"Compute and log epoch-level metrics at the end of the validation epoch.\"\"\"\n        self._on_eval_epoch_end(\"val\")\n\n    def on_test_epoch_start(self) -&gt; None:\n        \"\"\"Prepare for the test epoch.\"\"\"\n        self._on_eval_epoch_start(\"test\")\n\n    def test_step(\n        self, batch: dict[str, torch.Tensor], batch_idx: int\n    ) -&gt; Optional[torch.Tensor]:\n        \"\"\"Run a single test step.\n\n        Parameters\n        ----------\n        batch : dict[str, torch.Tensor]\n            The batch of data to process.\n        batch_idx : int\n            The index of the batch.\n\n        Returns\n        -------\n        Optional[torch.Tensor]\n            The loss for the batch or ``None`` if the loss function is not provided.\n        \"\"\"\n        return self._shared_eval_step(batch, batch_idx, \"test\")\n\n    def on_test_epoch_end(self) -&gt; None:\n        \"\"\"Compute and log epoch-level metrics at the end of the test epoch.\"\"\"\n        self._on_eval_epoch_end(\"test\")\n\n    def on_load_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n        \"\"\"Modify the model checkpoint after loading.\n\n        The `on_load_checkpoint` method of auxiliary tasks is called here to allow\n        them to modify the checkpoint after loading.\n\n        Parameters\n        ----------\n        checkpoint : Dict[str, Any]\n            The loaded checkpoint.\n        \"\"\"\n        if self.auxiliary_tasks:\n            for task in self.auxiliary_tasks.values():\n                task.on_load_checkpoint(checkpoint)\n\n    def on_save_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n        \"\"\"Modify the checkpoint before saving.\n\n        The `on_save_checkpoint` method of auxiliary tasks is called here to allow\n        them to modify the checkpoint before saving.\n\n        Parameters\n        ----------\n        checkpoint : Dict[str, Any]\n            The checkpoint to save.\n        \"\"\"\n        if self.auxiliary_tasks:\n            for task in self.auxiliary_tasks.values():\n                task.on_save_checkpoint(checkpoint)\n\n    def _compute_loss(\n        self, batch: dict[str, Any], batch_idx: int, outputs: dict[str, torch.Tensor]\n    ) -&gt; Optional[torch.Tensor]:\n        if self.loss_fn is None:\n            return None\n\n        contrastive_loss = self.loss_fn(\n            outputs,\n            batch[\"example_ids\"],\n            self.log_logit_scale.exp(),\n            self.modality_loss_pairs,\n        )\n\n        auxiliary_losses: list[torch.Tensor] = []\n        if self.auxiliary_tasks:\n            for task_name, task_spec in self.aux_task_specs.items():\n                auxiliary_task_output = self.auxiliary_tasks[task_name].training_step(\n                    batch, batch_idx\n                )\n                if isinstance(auxiliary_task_output, torch.Tensor):\n                    auxiliary_task_loss = auxiliary_task_output\n                elif isinstance(auxiliary_task_output, Mapping):\n                    auxiliary_task_loss = auxiliary_task_output[\"loss\"]\n                else:\n                    raise ValueError(\n                        \"Expected auxiliary task output to be a tensor or a mapping \"\n                        f\"containing a 'loss' key, but got {type(auxiliary_task_output)}.\"\n                    )\n\n                auxiliary_task_loss *= task_spec.loss_weight\n                auxiliary_losses.append(auxiliary_task_loss)\n                if self.log_auxiliary_tasks_loss:\n                    self.log(\n                        f\"train/{task_name}_loss\", auxiliary_task_loss, sync_dist=True\n                    )\n\n        if not auxiliary_losses:\n            return contrastive_loss\n\n        return torch.stack(auxiliary_losses).sum() + contrastive_loss\n\n    def _on_eval_epoch_start(self, eval_type: Literal[\"val\", \"test\"]) -&gt; None:\n        \"\"\"Prepare for the evaluation epoch.\"\"\"\n        self.encoders.eval()\n        if self.heads:\n            self.heads.eval()\n        if self.postprocessors:\n            self.postprocessors.eval()\n        if self.evaluation_tasks:\n            for task_spec in self.evaluation_tasks.values():\n                if (eval_type == \"val\" and task_spec.run_on_validation) or (\n                    eval_type == \"test\" and task_spec.run_on_test\n                ):\n                    task_spec.task.on_evaluation_epoch_start(self)\n\n    def _shared_eval_step(\n        self,\n        batch: dict[str, torch.Tensor],\n        batch_idx: int,\n        eval_type: Literal[\"val\", \"test\"],\n    ) -&gt; Optional[torch.Tensor]:\n        \"\"\"Run a single evaluation step.\n\n        Parameters\n        ----------\n        batch : dict[str, torch.Tensor]\n            The batch of data to process.\n        batch_idx : int\n            The index of the batch.\n\n        Returns\n        -------\n        Optional[torch.Tensor]\n            The loss for the batch or ``None`` if the loss function is not provided.\n        \"\"\"\n        loss: Optional[torch.Tensor] = None\n        if (eval_type == \"val\" and self.compute_validation_loss) or (\n            eval_type == \"test\" and self.compute_test_loss\n        ):\n            outputs = self(batch)\n            loss = self._compute_loss(batch, batch_idx, outputs)\n            if loss is not None and not self.trainer.sanity_checking:\n                self.log(f\"{eval_type}/loss\", loss, prog_bar=True, sync_dist=True)\n\n        if self.evaluation_tasks:\n            for task_spec in self.evaluation_tasks.values():\n                if (eval_type == \"val\" and task_spec.run_on_validation) or (\n                    eval_type == \"test\" and task_spec.run_on_test\n                ):\n                    task_spec.task.evaluation_step(self, batch, batch_idx)\n\n        return loss\n\n    def _on_eval_epoch_end(self, eval_type: Literal[\"val\", \"test\"]) -&gt; None:\n        \"\"\"Compute and log epoch-level metrics at the end of the evaluation epoch.\"\"\"\n        if self.evaluation_tasks:\n            for task_spec in self.evaluation_tasks.values():\n                if (eval_type == \"val\" and task_spec.run_on_validation) or (\n                    eval_type == \"test\" and task_spec.run_on_test\n                ):\n                    task_spec.task.on_evaluation_epoch_end(self)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.ContrastivePretraining.configure_model","title":"configure_model","text":"<pre><code>configure_model()\n</code></pre> <p>Configure the model.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def configure_model(self) -&gt; None:\n    \"\"\"Configure the model.\"\"\"\n    if self.auxiliary_tasks:\n        for task_name in self.auxiliary_tasks:\n            self.auxiliary_tasks[task_name].configure_model()\n</code></pre>"},{"location":"api/#mmlearn.cli.run.ContrastivePretraining.encode","title":"encode","text":"<pre><code>encode(inputs, modality, normalize=False)\n</code></pre> <p>Encode the input values for the given modality.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>Input values.</p> required <code>modality</code> <code>Modality</code> <p>The modality to encode.</p> required <code>normalize</code> <code>bool</code> <p>Whether to apply L2 normalization to the output (after the head and postprocessor layers, if present).</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The encoded values for the specified modality.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def encode(\n    self, inputs: dict[str, Any], modality: Modality, normalize: bool = False\n) -&gt; torch.Tensor:\n    \"\"\"Encode the input values for the given modality.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        Input values.\n    modality : Modality\n        The modality to encode.\n    normalize : bool, optional, default=False\n        Whether to apply L2 normalization to the output (after the head and\n        postprocessor layers, if present).\n\n    Returns\n    -------\n    torch.Tensor\n        The encoded values for the specified modality.\n    \"\"\"\n    output = self.encoders[modality.name](inputs)[0]\n\n    if self.postprocessors and modality.name in self.postprocessors:\n        output = self.postprocessors[modality.name](output)\n\n    if self.heads and modality.name in self.heads:\n        output = self.heads[modality.name](output)\n\n    if normalize:\n        output = torch.nn.functional.normalize(output, p=2, dim=-1)\n\n    return output\n</code></pre>"},{"location":"api/#mmlearn.cli.run.ContrastivePretraining.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input tensors to encode.</p> required <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>The encodings for each modality.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; dict[str, torch.Tensor]:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input tensors to encode.\n\n    Returns\n    -------\n    dict[str, torch.Tensor]\n        The encodings for each modality.\n    \"\"\"\n    outputs = {\n        modality.embedding: self.encode(inputs, modality, normalize=True)\n        for modality in self._available_modalities\n        if modality.name in inputs\n    }\n\n    if not all(\n        output.size(-1) == list(outputs.values())[0].size(-1)\n        for output in outputs.values()\n    ):\n        raise ValueError(\"Expected all model outputs to have the same dimension.\")\n\n    return outputs\n</code></pre>"},{"location":"api/#mmlearn.cli.run.ContrastivePretraining.on_train_epoch_start","title":"on_train_epoch_start","text":"<pre><code>on_train_epoch_start()\n</code></pre> <p>Prepare for the training epoch.</p> <p>This method sets the modules to training mode.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def on_train_epoch_start(self) -&gt; None:\n    \"\"\"Prepare for the training epoch.\n\n    This method sets the modules to training mode.\n    \"\"\"\n    self.encoders.train()\n    if self.heads:\n        self.heads.train()\n    if self.postprocessors:\n        self.postprocessors.train()\n</code></pre>"},{"location":"api/#mmlearn.cli.run.ContrastivePretraining.training_step","title":"training_step","text":"<pre><code>training_step(batch, batch_idx)\n</code></pre> <p>Compute the loss for the batch.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict[str, Any]</code> <p>The batch of data to process.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The loss for the batch.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def training_step(self, batch: dict[str, Any], batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"Compute the loss for the batch.\n\n    Parameters\n    ----------\n    batch : dict[str, Any]\n        The batch of data to process.\n    batch_idx : int\n        The index of the batch.\n\n    Returns\n    -------\n    torch.Tensor\n        The loss for the batch.\n    \"\"\"\n    outputs = self(batch)\n\n    with torch.no_grad():\n        self.log_logit_scale.clamp_(0, math.log(self.max_logit_scale))\n\n    loss = self._compute_loss(batch, batch_idx, outputs)\n\n    if loss is None:\n        raise ValueError(\"The loss function must be provided for training.\")\n\n    self.log(\"train/loss\", loss, prog_bar=True, sync_dist=True)\n    self.log(\n        \"train/logit_scale\",\n        self.log_logit_scale.exp(),\n        prog_bar=True,\n        on_step=True,\n        on_epoch=False,\n    )\n\n    return loss\n</code></pre>"},{"location":"api/#mmlearn.cli.run.ContrastivePretraining.on_before_zero_grad","title":"on_before_zero_grad","text":"<pre><code>on_before_zero_grad(optimizer)\n</code></pre> <p>Zero out the gradients of the model.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def on_before_zero_grad(self, optimizer: torch.optim.Optimizer) -&gt; None:\n    \"\"\"Zero out the gradients of the model.\"\"\"\n    if self.auxiliary_tasks:\n        for task in self.auxiliary_tasks.values():\n            task.on_before_zero_grad(optimizer)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.ContrastivePretraining.on_validation_epoch_start","title":"on_validation_epoch_start","text":"<pre><code>on_validation_epoch_start()\n</code></pre> <p>Prepare for the validation epoch.</p> <p>This method sets the modules to evaluation mode and calls the <code>on_evaluation_epoch_start</code> method of each evaluation task.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def on_validation_epoch_start(self) -&gt; None:\n    \"\"\"Prepare for the validation epoch.\n\n    This method sets the modules to evaluation mode and calls the\n    ``on_evaluation_epoch_start`` method of each evaluation task.\n    \"\"\"\n    self._on_eval_epoch_start(\"val\")\n</code></pre>"},{"location":"api/#mmlearn.cli.run.ContrastivePretraining.validation_step","title":"validation_step","text":"<pre><code>validation_step(batch, batch_idx)\n</code></pre> <p>Run a single validation step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict[str, Tensor]</code> <p>The batch of data to process.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required <p>Returns:</p> Type Description <code>Optional[Tensor]</code> <p>The loss for the batch or <code>None</code> if the loss function is not provided.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def validation_step(\n    self, batch: dict[str, torch.Tensor], batch_idx: int\n) -&gt; Optional[torch.Tensor]:\n    \"\"\"Run a single validation step.\n\n    Parameters\n    ----------\n    batch : dict[str, torch.Tensor]\n        The batch of data to process.\n    batch_idx : int\n        The index of the batch.\n\n    Returns\n    -------\n    Optional[torch.Tensor]\n        The loss for the batch or ``None`` if the loss function is not provided.\n    \"\"\"\n    return self._shared_eval_step(batch, batch_idx, \"val\")\n</code></pre>"},{"location":"api/#mmlearn.cli.run.ContrastivePretraining.on_validation_epoch_end","title":"on_validation_epoch_end","text":"<pre><code>on_validation_epoch_end()\n</code></pre> <p>Compute and log epoch-level metrics at the end of the validation epoch.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def on_validation_epoch_end(self) -&gt; None:\n    \"\"\"Compute and log epoch-level metrics at the end of the validation epoch.\"\"\"\n    self._on_eval_epoch_end(\"val\")\n</code></pre>"},{"location":"api/#mmlearn.cli.run.ContrastivePretraining.on_test_epoch_start","title":"on_test_epoch_start","text":"<pre><code>on_test_epoch_start()\n</code></pre> <p>Prepare for the test epoch.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def on_test_epoch_start(self) -&gt; None:\n    \"\"\"Prepare for the test epoch.\"\"\"\n    self._on_eval_epoch_start(\"test\")\n</code></pre>"},{"location":"api/#mmlearn.cli.run.ContrastivePretraining.test_step","title":"test_step","text":"<pre><code>test_step(batch, batch_idx)\n</code></pre> <p>Run a single test step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict[str, Tensor]</code> <p>The batch of data to process.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required <p>Returns:</p> Type Description <code>Optional[Tensor]</code> <p>The loss for the batch or <code>None</code> if the loss function is not provided.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def test_step(\n    self, batch: dict[str, torch.Tensor], batch_idx: int\n) -&gt; Optional[torch.Tensor]:\n    \"\"\"Run a single test step.\n\n    Parameters\n    ----------\n    batch : dict[str, torch.Tensor]\n        The batch of data to process.\n    batch_idx : int\n        The index of the batch.\n\n    Returns\n    -------\n    Optional[torch.Tensor]\n        The loss for the batch or ``None`` if the loss function is not provided.\n    \"\"\"\n    return self._shared_eval_step(batch, batch_idx, \"test\")\n</code></pre>"},{"location":"api/#mmlearn.cli.run.ContrastivePretraining.on_test_epoch_end","title":"on_test_epoch_end","text":"<pre><code>on_test_epoch_end()\n</code></pre> <p>Compute and log epoch-level metrics at the end of the test epoch.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def on_test_epoch_end(self) -&gt; None:\n    \"\"\"Compute and log epoch-level metrics at the end of the test epoch.\"\"\"\n    self._on_eval_epoch_end(\"test\")\n</code></pre>"},{"location":"api/#mmlearn.cli.run.ContrastivePretraining.on_load_checkpoint","title":"on_load_checkpoint","text":"<pre><code>on_load_checkpoint(checkpoint)\n</code></pre> <p>Modify the model checkpoint after loading.</p> <p>The <code>on_load_checkpoint</code> method of auxiliary tasks is called here to allow them to modify the checkpoint after loading.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <code>Dict[str, Any]</code> <p>The loaded checkpoint.</p> required Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def on_load_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n    \"\"\"Modify the model checkpoint after loading.\n\n    The `on_load_checkpoint` method of auxiliary tasks is called here to allow\n    them to modify the checkpoint after loading.\n\n    Parameters\n    ----------\n    checkpoint : Dict[str, Any]\n        The loaded checkpoint.\n    \"\"\"\n    if self.auxiliary_tasks:\n        for task in self.auxiliary_tasks.values():\n            task.on_load_checkpoint(checkpoint)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.ContrastivePretraining.on_save_checkpoint","title":"on_save_checkpoint","text":"<pre><code>on_save_checkpoint(checkpoint)\n</code></pre> <p>Modify the checkpoint before saving.</p> <p>The <code>on_save_checkpoint</code> method of auxiliary tasks is called here to allow them to modify the checkpoint before saving.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <code>Dict[str, Any]</code> <p>The checkpoint to save.</p> required Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def on_save_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n    \"\"\"Modify the checkpoint before saving.\n\n    The `on_save_checkpoint` method of auxiliary tasks is called here to allow\n    them to modify the checkpoint before saving.\n\n    Parameters\n    ----------\n    checkpoint : Dict[str, Any]\n        The checkpoint to save.\n    \"\"\"\n    if self.auxiliary_tasks:\n        for task in self.auxiliary_tasks.values():\n            task.on_save_checkpoint(checkpoint)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.IJEPA","title":"IJEPA","text":"<p>               Bases: <code>TrainingTask</code></p> <p>Pretraining module for IJEPA.</p> <p>This class implements the IJEPA (Image Joint-Embedding Predictive Architecture) pretraining task using PyTorch Lightning. It trains an encoder and a predictor to reconstruct masked regions of an image based on its unmasked context.</p> <p>Parameters:</p> Name Type Description Default <code>encoder</code> <code>VisionTransformer</code> <p>Vision transformer encoder.</p> required <code>predictor</code> <code>VisionTransformerPredictor</code> <p>Vision transformer predictor.</p> required <code>optimizer</code> <code>Optional[partial[Optimizer]]</code> <p>The optimizer to use for training. This is expected to be a func:<code>~functools.partial</code> function that takes the model parameters as the only required argument. If not provided, training will continue without an optimizer.</p> <code>None</code> <code>lr_scheduler</code> <code>Optional[Union[dict[str, Union[partial[LRScheduler], Any]], partial[LRScheduler]]]</code> <p>The learning rate scheduler to use for training. This can be a func:<code>~functools.partial</code> function that takes the optimizer as the only required argument or a dictionary with a <code>'scheduler'</code> key that specifies the scheduler and an optional <code>'extras'</code> key that specifies additional arguments to pass to the scheduler. If not provided, the learning rate will not be adjusted during training.</p> <code>None</code> <code>ema_decay</code> <code>float</code> <p>Initial momentum for EMA of target encoder.</p> <code>0.996</code> <code>ema_decay_end</code> <code>float</code> <p>Final momentum for EMA of target encoder.</p> <code>1.0</code> <code>ema_anneal_end_step</code> <code>int</code> <p>Number of steps to anneal EMA momentum to <code>ema_decay_end</code>.</p> <code>1000</code> <code>loss_fn</code> <code>Optional[Callable[[Tensor, Tensor], Tensor]]</code> <p>Loss function to use. If not provided, defaults to func:<code>~torch.nn.functional.smooth_l1_loss</code>.</p> <code>None</code> <code>compute_validation_loss</code> <code>bool</code> <p>Whether to compute validation loss.</p> <code>True</code> <code>compute_test_loss</code> <code>bool</code> <p>Whether to compute test loss.</p> <code>True</code> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>@store(group=\"task\", provider=\"mmlearn\", zen_partial=False)\nclass IJEPA(TrainingTask):\n    \"\"\"Pretraining module for IJEPA.\n\n    This class implements the IJEPA (Image Joint-Embedding Predictive Architecture)\n    pretraining task using PyTorch Lightning. It trains an encoder and a predictor to\n    reconstruct masked regions of an image based on its unmasked context.\n\n    Parameters\n    ----------\n    encoder : VisionTransformer\n        Vision transformer encoder.\n    predictor : VisionTransformerPredictor\n        Vision transformer predictor.\n    optimizer : Optional[partial[torch.optim.Optimizer]], optional, default=None\n        The optimizer to use for training. This is expected to be a :py:func:`~functools.partial`\n        function that takes the model parameters as the only required argument.\n        If not provided, training will continue without an optimizer.\n    lr_scheduler : Optional[Union[dict[str, Union[partial[torch.optim.lr_scheduler.LRScheduler], Any]], partial[torch.optim.lr_scheduler.LRScheduler]]], optional, default=None\n        The learning rate scheduler to use for training. This can be a\n        :py:func:`~functools.partial` function that takes the optimizer as the only\n        required argument or a dictionary with a ``'scheduler'`` key that specifies\n        the scheduler and an optional ``'extras'`` key that specifies additional\n        arguments to pass to the scheduler. If not provided, the learning rate will\n        not be adjusted during training.\n    ema_decay : float, optional, default=0.996\n        Initial momentum for EMA of target encoder.\n    ema_decay_end : float, optional, default=1.0\n        Final momentum for EMA of target encoder.\n    ema_anneal_end_step : int, optional, default=1000\n        Number of steps to anneal EMA momentum to ``ema_decay_end``.\n    loss_fn : Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]], optional\n        Loss function to use. If not provided, defaults to\n        :py:func:`~torch.nn.functional.smooth_l1_loss`.\n    compute_validation_loss : bool, optional, default=True\n        Whether to compute validation loss.\n    compute_test_loss : bool, optional, default=True\n        Whether to compute test loss.\n    \"\"\"  # noqa: W505\n\n    def __init__(\n        self,\n        encoder: VisionTransformer,\n        predictor: VisionTransformerPredictor,\n        modality: str = \"RGB\",\n        optimizer: Optional[partial[torch.optim.Optimizer]] = None,\n        lr_scheduler: Optional[\n            Union[\n                dict[str, Union[partial[torch.optim.lr_scheduler.LRScheduler], Any]],\n                partial[torch.optim.lr_scheduler.LRScheduler],\n            ]\n        ] = None,\n        ema_decay: float = 0.996,\n        ema_decay_end: float = 1.0,\n        ema_anneal_end_step: int = 1000,\n        loss_fn: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None,\n        compute_validation_loss: bool = True,\n        compute_test_loss: bool = True,\n    ):\n        super().__init__(\n            optimizer=optimizer,\n            lr_scheduler=lr_scheduler,\n            loss_fn=loss_fn if loss_fn is not None else F.smooth_l1_loss,\n            compute_validation_loss=compute_validation_loss,\n            compute_test_loss=compute_test_loss,\n        )\n        self.modality = Modalities.get_modality(modality)\n        self.mask_generator = IJEPAMaskGenerator()\n\n        self.encoder = encoder\n        self.predictor = predictor\n\n        self.predictor.num_patches = encoder.patch_embed.num_patches\n        self.predictor.embed_dim = encoder.embed_dim\n        self.predictor.num_heads = encoder.num_heads\n\n        self.target_encoder = ExponentialMovingAverage(\n            self.encoder, ema_decay, ema_decay_end, ema_anneal_end_step\n        )\n\n    def configure_model(self) -&gt; None:\n        \"\"\"Configure the model.\"\"\"\n        self.target_encoder.configure_model(self.device)\n\n    def on_before_zero_grad(self, optimizer: torch.optim.Optimizer) -&gt; None:\n        \"\"\"Perform exponential moving average update of target encoder.\n\n        This is done right after the ``optimizer.step()`, which comes just before\n        ``optimizer.zero_grad()`` to account for gradient accumulation.\n        \"\"\"\n        if self.target_encoder is not None:\n            self.target_encoder.step(self.encoder)\n\n    def training_step(self, batch: dict[str, Any], batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"Perform a single training step.\n\n        Parameters\n        ----------\n        batch : dict[str, Any]\n            A batch of data.\n        batch_idx : int\n            Index of the batch.\n\n        Returns\n        -------\n        torch.Tensor\n            Loss value.\n        \"\"\"\n        return self._shared_step(batch, batch_idx, step_type=\"train\")\n\n    def validation_step(\n        self, batch: dict[str, Any], batch_idx: int\n    ) -&gt; Optional[torch.Tensor]:\n        \"\"\"Run a single validation step.\n\n        Parameters\n        ----------\n        batch : dict[str, Any]\n            A batch of data.\n        batch_idx : int\n            Index of the batch.\n\n        Returns\n        -------\n        Optional[torch.Tensor]\n            Loss value or ``None`` if no loss is computed.\n        \"\"\"\n        return self._shared_step(batch, batch_idx, step_type=\"val\")\n\n    def test_step(\n        self, batch: dict[str, Any], batch_idx: int\n    ) -&gt; Optional[torch.Tensor]:\n        \"\"\"Run a single test step.\n\n        Parameters\n        ----------\n        batch : dict[str, Any]\n            A batch of data.\n        batch_idx : int\n            Index of the batch.\n\n        Returns\n        -------\n        Optional[torch.Tensor]\n            Loss value or ``None`` if no loss is computed\n        \"\"\"\n        return self._shared_step(batch, batch_idx, step_type=\"test\")\n\n    def on_validation_epoch_start(self) -&gt; None:\n        \"\"\"Prepare for the validation epoch.\"\"\"\n        self._on_eval_epoch_start(\"val\")\n\n    def on_validation_epoch_end(self) -&gt; None:\n        \"\"\"Actions at the end of the validation epoch.\"\"\"\n        self._on_eval_epoch_end(\"val\")\n\n    def on_test_epoch_start(self) -&gt; None:\n        \"\"\"Prepare for the test epoch.\"\"\"\n        self._on_eval_epoch_start(\"test\")\n\n    def on_test_epoch_end(self) -&gt; None:\n        \"\"\"Actions at the end of the test epoch.\"\"\"\n        self._on_eval_epoch_end(\"test\")\n\n    def on_save_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n        \"\"\"Add relevant EMA state to the checkpoint.\n\n        Parameters\n        ----------\n        checkpoint : dict[str, Any]\n            The state dictionary to save the EMA state to.\n        \"\"\"\n        if self.target_encoder is not None:\n            checkpoint[\"ema_params\"] = {\n                \"decay\": self.target_encoder.decay,\n                \"num_updates\": self.target_encoder.num_updates,\n            }\n\n    def on_load_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n        \"\"\"Restore EMA state from the checkpoint.\n\n        Parameters\n        ----------\n        checkpoint : dict[str, Any]\n            The state dictionary to restore the EMA state from.\n        \"\"\"\n        if \"ema_params\" in checkpoint and self.target_encoder is not None:\n            ema_params = checkpoint.pop(\"ema_params\")\n            self.target_encoder.decay = ema_params[\"decay\"]\n            self.target_encoder.num_updates = ema_params[\"num_updates\"]\n\n            self.target_encoder.restore(self.encoder)\n\n    def _shared_step(\n        self, batch: dict[str, Any], batch_idx: int, step_type: str\n    ) -&gt; Optional[torch.Tensor]:\n        images = batch[self.modality.name]\n\n        # Generate masks\n        batch_size = images.size(0)\n        mask_info = self.mask_generator(batch_size=batch_size)\n\n        # Extract masks and move to device\n        device = images.device\n        encoder_masks = [mask.to(device) for mask in mask_info[\"encoder_masks\"]]\n        predictor_masks = [mask.to(device) for mask in mask_info[\"predictor_masks\"]]\n\n        # Forward pass through target encoder to get h\n        with torch.no_grad():\n            h = self.target_encoder.model(batch)[0]\n            h = F.layer_norm(h, h.size()[-1:])\n            h_masked = apply_masks(h, predictor_masks)\n            h_masked = repeat_interleave_batch(\n                h_masked, images.size(0), repeat=len(encoder_masks)\n            )\n\n        # Forward pass through encoder with encoder_masks\n        batch[self.modality.mask] = encoder_masks\n        z = self.encoder(batch)[0]\n\n        # Pass z through predictor with encoder_masks and predictor_masks\n        z_pred = self.predictor(z, encoder_masks, predictor_masks)\n\n        if step_type == \"train\":\n            self.log(\"train/ema_decay\", self.target_encoder.decay, prog_bar=True)\n\n        if self.loss_fn is not None and (\n            step_type == \"train\"\n            or (step_type == \"val\" and self.compute_validation_loss)\n            or (step_type == \"test\" and self.compute_test_loss)\n        ):\n            # Compute loss between z_pred and h_masked\n            loss = self.loss_fn(z_pred, h_masked)\n\n            # Log loss\n            self.log(f\"{step_type}/loss\", loss, prog_bar=True, sync_dist=True)\n\n            return loss\n\n        return None\n\n    def _on_eval_epoch_start(self, step_type: str) -&gt; None:\n        \"\"\"Initialize states or configurations at the start of an evaluation epoch.\n\n        Parameters\n        ----------\n        step_type : str\n            Type of the evaluation phase (\"val\" or \"test\").\n        \"\"\"\n        if (\n            step_type == \"val\"\n            and self.compute_validation_loss\n            or step_type == \"test\"\n            and self.compute_test_loss\n        ):\n            self.log(f\"{step_type}/start\", 1, prog_bar=True, sync_dist=True)\n\n    def _on_eval_epoch_end(self, step_type: str) -&gt; None:\n        \"\"\"Finalize states or logging at the end of an evaluation epoch.\n\n        Parameters\n        ----------\n        step_type : str\n            Type of the evaluation phase (\"val\" or \"test\").\n        \"\"\"\n        if (\n            step_type == \"val\"\n            and self.compute_validation_loss\n            or step_type == \"test\"\n            and self.compute_test_loss\n        ):\n            self.log(f\"{step_type}/end\", 1, prog_bar=True, sync_dist=True)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.IJEPA.configure_model","title":"configure_model","text":"<pre><code>configure_model()\n</code></pre> <p>Configure the model.</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def configure_model(self) -&gt; None:\n    \"\"\"Configure the model.\"\"\"\n    self.target_encoder.configure_model(self.device)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.IJEPA.on_before_zero_grad","title":"on_before_zero_grad","text":"<pre><code>on_before_zero_grad(optimizer)\n</code></pre> <p>Perform exponential moving average update of target encoder.</p> <p>This is done right after the <code>optimizer.step()`, which comes just before</code>optimizer.zero_grad()`` to account for gradient accumulation.</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def on_before_zero_grad(self, optimizer: torch.optim.Optimizer) -&gt; None:\n    \"\"\"Perform exponential moving average update of target encoder.\n\n    This is done right after the ``optimizer.step()`, which comes just before\n    ``optimizer.zero_grad()`` to account for gradient accumulation.\n    \"\"\"\n    if self.target_encoder is not None:\n        self.target_encoder.step(self.encoder)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.IJEPA.training_step","title":"training_step","text":"<pre><code>training_step(batch, batch_idx)\n</code></pre> <p>Perform a single training step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict[str, Any]</code> <p>A batch of data.</p> required <code>batch_idx</code> <code>int</code> <p>Index of the batch.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Loss value.</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def training_step(self, batch: dict[str, Any], batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"Perform a single training step.\n\n    Parameters\n    ----------\n    batch : dict[str, Any]\n        A batch of data.\n    batch_idx : int\n        Index of the batch.\n\n    Returns\n    -------\n    torch.Tensor\n        Loss value.\n    \"\"\"\n    return self._shared_step(batch, batch_idx, step_type=\"train\")\n</code></pre>"},{"location":"api/#mmlearn.cli.run.IJEPA.validation_step","title":"validation_step","text":"<pre><code>validation_step(batch, batch_idx)\n</code></pre> <p>Run a single validation step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict[str, Any]</code> <p>A batch of data.</p> required <code>batch_idx</code> <code>int</code> <p>Index of the batch.</p> required <p>Returns:</p> Type Description <code>Optional[Tensor]</code> <p>Loss value or <code>None</code> if no loss is computed.</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def validation_step(\n    self, batch: dict[str, Any], batch_idx: int\n) -&gt; Optional[torch.Tensor]:\n    \"\"\"Run a single validation step.\n\n    Parameters\n    ----------\n    batch : dict[str, Any]\n        A batch of data.\n    batch_idx : int\n        Index of the batch.\n\n    Returns\n    -------\n    Optional[torch.Tensor]\n        Loss value or ``None`` if no loss is computed.\n    \"\"\"\n    return self._shared_step(batch, batch_idx, step_type=\"val\")\n</code></pre>"},{"location":"api/#mmlearn.cli.run.IJEPA.test_step","title":"test_step","text":"<pre><code>test_step(batch, batch_idx)\n</code></pre> <p>Run a single test step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict[str, Any]</code> <p>A batch of data.</p> required <code>batch_idx</code> <code>int</code> <p>Index of the batch.</p> required <p>Returns:</p> Type Description <code>Optional[Tensor]</code> <p>Loss value or <code>None</code> if no loss is computed</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def test_step(\n    self, batch: dict[str, Any], batch_idx: int\n) -&gt; Optional[torch.Tensor]:\n    \"\"\"Run a single test step.\n\n    Parameters\n    ----------\n    batch : dict[str, Any]\n        A batch of data.\n    batch_idx : int\n        Index of the batch.\n\n    Returns\n    -------\n    Optional[torch.Tensor]\n        Loss value or ``None`` if no loss is computed\n    \"\"\"\n    return self._shared_step(batch, batch_idx, step_type=\"test\")\n</code></pre>"},{"location":"api/#mmlearn.cli.run.IJEPA.on_validation_epoch_start","title":"on_validation_epoch_start","text":"<pre><code>on_validation_epoch_start()\n</code></pre> <p>Prepare for the validation epoch.</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def on_validation_epoch_start(self) -&gt; None:\n    \"\"\"Prepare for the validation epoch.\"\"\"\n    self._on_eval_epoch_start(\"val\")\n</code></pre>"},{"location":"api/#mmlearn.cli.run.IJEPA.on_validation_epoch_end","title":"on_validation_epoch_end","text":"<pre><code>on_validation_epoch_end()\n</code></pre> <p>Actions at the end of the validation epoch.</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def on_validation_epoch_end(self) -&gt; None:\n    \"\"\"Actions at the end of the validation epoch.\"\"\"\n    self._on_eval_epoch_end(\"val\")\n</code></pre>"},{"location":"api/#mmlearn.cli.run.IJEPA.on_test_epoch_start","title":"on_test_epoch_start","text":"<pre><code>on_test_epoch_start()\n</code></pre> <p>Prepare for the test epoch.</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def on_test_epoch_start(self) -&gt; None:\n    \"\"\"Prepare for the test epoch.\"\"\"\n    self._on_eval_epoch_start(\"test\")\n</code></pre>"},{"location":"api/#mmlearn.cli.run.IJEPA.on_test_epoch_end","title":"on_test_epoch_end","text":"<pre><code>on_test_epoch_end()\n</code></pre> <p>Actions at the end of the test epoch.</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def on_test_epoch_end(self) -&gt; None:\n    \"\"\"Actions at the end of the test epoch.\"\"\"\n    self._on_eval_epoch_end(\"test\")\n</code></pre>"},{"location":"api/#mmlearn.cli.run.IJEPA.on_save_checkpoint","title":"on_save_checkpoint","text":"<pre><code>on_save_checkpoint(checkpoint)\n</code></pre> <p>Add relevant EMA state to the checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <code>dict[str, Any]</code> <p>The state dictionary to save the EMA state to.</p> required Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def on_save_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n    \"\"\"Add relevant EMA state to the checkpoint.\n\n    Parameters\n    ----------\n    checkpoint : dict[str, Any]\n        The state dictionary to save the EMA state to.\n    \"\"\"\n    if self.target_encoder is not None:\n        checkpoint[\"ema_params\"] = {\n            \"decay\": self.target_encoder.decay,\n            \"num_updates\": self.target_encoder.num_updates,\n        }\n</code></pre>"},{"location":"api/#mmlearn.cli.run.IJEPA.on_load_checkpoint","title":"on_load_checkpoint","text":"<pre><code>on_load_checkpoint(checkpoint)\n</code></pre> <p>Restore EMA state from the checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <code>dict[str, Any]</code> <p>The state dictionary to restore the EMA state from.</p> required Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def on_load_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n    \"\"\"Restore EMA state from the checkpoint.\n\n    Parameters\n    ----------\n    checkpoint : dict[str, Any]\n        The state dictionary to restore the EMA state from.\n    \"\"\"\n    if \"ema_params\" in checkpoint and self.target_encoder is not None:\n        ema_params = checkpoint.pop(\"ema_params\")\n        self.target_encoder.decay = ema_params[\"decay\"]\n        self.target_encoder.num_updates = ema_params[\"num_updates\"]\n\n        self.target_encoder.restore(self.encoder)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.ZeroShotClassification","title":"ZeroShotClassification","text":"<p>               Bases: <code>EvaluationHooks</code></p> <p>Zero-shot classification evaluation task.</p> <p>This task evaluates the zero-shot classification performance.</p> <p>Parameters:</p> Name Type Description Default <code>task_specs</code> <code>list[ClassificationTaskSpec]</code> <p>A list of classification task specifications.</p> required <code>tokenizer</code> <code>Callable[[Union[str, list[str]]], Union[Tensor, dict[str, Tensor]]]</code> <p>A function to tokenize text inputs.</p> required Source code in <code>mmlearn/tasks/zero_shot_classification.py</code> <pre><code>@store(group=\"eval_task\", provider=\"mmlearn\")\nclass ZeroShotClassification(EvaluationHooks):\n    \"\"\"Zero-shot classification evaluation task.\n\n    This task evaluates the zero-shot classification performance.\n\n    Parameters\n    ----------\n    task_specs : list[ClassificationTaskSpec]\n        A list of classification task specifications.\n    tokenizer : Callable[[Union[str, list[str]]], Union[torch.Tensor, dict[str, torch.Tensor]]]\n        A function to tokenize text inputs.\n    \"\"\"  # noqa: W505\n\n    def __init__(\n        self,\n        task_specs: list[ClassificationTaskSpec],\n        tokenizer: Callable[\n            [Union[str, list[str]]], Union[torch.Tensor, dict[str, torch.Tensor]]\n        ],\n    ) -&gt; None:\n        super().__init__()\n        self.tokenizer = tokenizer\n        self.task_specs = task_specs\n        for spec in self.task_specs:\n            assert Modalities.has_modality(spec.query_modality)\n\n        self.metrics: dict[tuple[str, int], MetricCollection] = {}\n        self._embeddings_store: dict[int, torch.Tensor] = {}\n\n    def on_evaluation_epoch_start(self, pl_module: LightningModule) -&gt; None:\n        \"\"\"Set up the evaluation task.\n\n        Parameters\n        ----------\n        pl_module : pl.LightningModule\n            A reference to the Lightning module being evaluated.\n\n        Raises\n        ------\n        ValueError\n            - If the task is not being run for validation or testing.\n            - If the dataset does not have the required attributes to perform zero-shot\n              classification (i.e ``id2label`` and ``zero_shot_prompt_templates``).\n        \"\"\"\n        if pl_module.trainer.validating:\n            eval_dataset: CombinedDataset = pl_module.trainer.val_dataloaders.dataset\n        elif pl_module.trainer.testing:\n            eval_dataset = pl_module.trainer.test_dataloaders.dataset\n        else:\n            raise ValueError(\n                \"ZeroShotClassification task is only supported for validation and testing.\"\n            )\n\n        self.all_dataset_info = {}\n\n        # create metrics for each dataset/query_modality combination\n        if not self.metrics:\n            for dataset_index, dataset in enumerate(eval_dataset.datasets):\n                dataset_name = getattr(dataset, \"name\", dataset.__class__.__name__)\n                try:\n                    id2label: dict[int, str] = dataset.id2label\n                except AttributeError:\n                    raise ValueError(\n                        f\"Dataset '{dataset_name}' must have a `id2label` attribute \"\n                        \"to perform zero-shot classification.\"\n                    ) from None\n\n                try:\n                    zero_shot_prompt_templates: list[str] = (\n                        dataset.zero_shot_prompt_templates\n                    )\n                except AttributeError:\n                    raise ValueError(\n                        \"Dataset must have a `zero_shot_prompt_templates` attribute to perform zero-shot classification.\"\n                    ) from None\n\n                num_classes = len(id2label)\n\n                self.all_dataset_info[dataset_index] = {\n                    \"name\": dataset_name,\n                    \"id2label\": id2label,\n                    \"prompt_templates\": zero_shot_prompt_templates,\n                    \"num_classes\": num_classes,\n                }\n\n                for spec in self.task_specs:\n                    query_modality = Modalities.get_modality(spec.query_modality).name\n                    self.metrics[(query_modality, dataset_index)] = (\n                        self._create_metrics(\n                            num_classes,\n                            spec.top_k,\n                            prefix=f\"{dataset_name}/{query_modality}_\",\n                            postfix=\"\",\n                        )\n                    )\n\n        for metric in self.metrics.values():\n            metric.to(pl_module.device)\n\n        for dataset_index, dataset_info in self.all_dataset_info.items():\n            id2label = dataset_info[\"id2label\"]\n            prompt_templates: list[str] = dataset_info[\"prompt_templates\"]\n            labels = list(id2label.values())\n\n            with torch.no_grad():\n                chunk_size = 10\n                all_embeddings = []\n\n                for i in tqdm(\n                    range(0, len(labels), chunk_size),\n                    desc=\"Encoding class descriptions\",\n                ):\n                    batch_labels = labels[i : min(i + chunk_size, len(labels))]\n                    descriptions = [\n                        template.format(label)\n                        for label in batch_labels\n                        for template in prompt_templates\n                    ]\n                    tokenized_descriptions = move_data_to_device(\n                        self.tokenizer(descriptions),\n                        pl_module.device,\n                    )\n\n                    # Encode the chunk using the pl_module's encode method\n                    chunk_embeddings = pl_module.encode(\n                        tokenized_descriptions, Modalities.TEXT\n                    )  # shape: [chunk_size x len(prompt_templates), embed_dim]\n                    chunk_embeddings /= chunk_embeddings.norm(p=2, dim=-1, keepdim=True)\n                    chunk_embeddings = chunk_embeddings.reshape(\n                        len(batch_labels), len(prompt_templates), -1\n                    ).mean(dim=1)  # shape: [chunk_size, embed_dim]\n                    chunk_embeddings /= chunk_embeddings.norm(p=2, dim=-1, keepdim=True)\n\n                    # Append the chunk embeddings to the list\n                    all_embeddings.append(chunk_embeddings)\n\n                # Concatenate all chunk embeddings into a single tensor\n                class_embeddings = torch.cat(all_embeddings, dim=0)\n\n            self._embeddings_store[dataset_index] = class_embeddings\n\n    def evaluation_step(\n        self, pl_module: LightningModule, batch: dict[str, torch.Tensor], batch_idx: int\n    ) -&gt; None:\n        \"\"\"Compute logits and update metrics.\n\n        Parameters\n        ----------\n        pl_module : pl.LightningModule\n            A reference to the Lightning module being evaluated.\n        batch : dict[str, torch.Tensor]\n            A batch of data.\n        batch_idx : int\n            The index of the batch.\n        \"\"\"\n        if pl_module.trainer.sanity_checking:\n            return\n\n        for (query_modality, dataset_index), metric_collection in self.metrics.items():\n            matching_indices = torch.where(batch[\"dataset_index\"] == dataset_index)[0]\n\n            if not matching_indices.numel():\n                continue\n\n            class_embeddings = self._embeddings_store[dataset_index]\n            query_embeddings: torch.Tensor = pl_module.encode(\n                batch, Modalities.get_modality(query_modality)\n            )\n            query_embeddings /= query_embeddings.norm(p=2, dim=-1, keepdim=True)\n            query_embeddings = query_embeddings[matching_indices]\n\n            if self.all_dataset_info[dataset_index][\"num_classes\"] == 2:\n                softmax_output = _safe_matmul(\n                    query_embeddings, class_embeddings\n                ).softmax(dim=-1)\n                logits = softmax_output[:, 1] - softmax_output[:, 0]\n            else:\n                logits = 100.0 * _safe_matmul(query_embeddings, class_embeddings)\n            targets = batch[Modalities.get_modality(query_modality).target][\n                matching_indices\n            ]\n\n            metric_collection.update(logits, targets)\n\n    def on_evaluation_epoch_end(self, pl_module: LightningModule) -&gt; dict[str, Any]:\n        \"\"\"Compute and reset metrics.\n\n        Parameters\n        ----------\n        pl_module : pl.LightningModule\n            A reference to the Lightning module being evaluated.\n\n        Returns\n        -------\n        dict[str, Any]\n            The computed metrics.\n        \"\"\"\n        results = {}\n        for metric_collection in self.metrics.values():\n            results.update(metric_collection.compute())\n            metric_collection.reset()\n\n        self._embeddings_store.clear()\n\n        eval_type = \"val\" if pl_module.trainer.validating else \"test\"\n        for key, value in results.items():\n            pl_module.log(f\"{eval_type}/{key}\", value)\n\n        return results\n\n    @staticmethod\n    def _create_metrics(\n        num_classes: int, top_k: list[int], prefix: str, postfix: str\n    ) -&gt; MetricCollection:\n        \"\"\"Create a collection of classification metrics.\"\"\"\n        task_type = \"binary\" if num_classes == 2 else \"multiclass\"\n        acc_metrics = (\n            {\n                f\"top{k}_accuracy\": Accuracy(\n                    task=task_type, num_classes=num_classes, top_k=k, average=\"micro\"\n                )\n                for k in top_k\n            }\n            if num_classes &gt; 2\n            else {\"accuracy\": Accuracy(task=task_type, num_classes=num_classes)}\n        )\n        return MetricCollection(\n            {\n                \"precision\": Precision(\n                    task=task_type,\n                    num_classes=num_classes,\n                    average=\"macro\" if num_classes &gt; 2 else \"micro\",\n                ),\n                \"recall\": Recall(\n                    task=task_type,\n                    num_classes=num_classes,\n                    average=\"macro\" if num_classes &gt; 2 else \"micro\",\n                ),\n                \"f1_score_macro\": F1Score(\n                    task=task_type,\n                    num_classes=num_classes,\n                    average=\"macro\" if num_classes &gt; 2 else \"micro\",\n                ),\n                \"aucroc\": AUROC(task=task_type, num_classes=num_classes),\n                **acc_metrics,\n            },\n            prefix=prefix,\n            postfix=postfix,\n            compute_groups=True,\n        )\n</code></pre>"},{"location":"api/#mmlearn.cli.run.ZeroShotClassification.on_evaluation_epoch_start","title":"on_evaluation_epoch_start","text":"<pre><code>on_evaluation_epoch_start(pl_module)\n</code></pre> <p>Set up the evaluation task.</p> <p>Parameters:</p> Name Type Description Default <code>pl_module</code> <code>LightningModule</code> <p>A reference to the Lightning module being evaluated.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <ul> <li>If the task is not being run for validation or testing.</li> <li>If the dataset does not have the required attributes to perform zero-shot   classification (i.e <code>id2label</code> and <code>zero_shot_prompt_templates</code>).</li> </ul> Source code in <code>mmlearn/tasks/zero_shot_classification.py</code> <pre><code>def on_evaluation_epoch_start(self, pl_module: LightningModule) -&gt; None:\n    \"\"\"Set up the evaluation task.\n\n    Parameters\n    ----------\n    pl_module : pl.LightningModule\n        A reference to the Lightning module being evaluated.\n\n    Raises\n    ------\n    ValueError\n        - If the task is not being run for validation or testing.\n        - If the dataset does not have the required attributes to perform zero-shot\n          classification (i.e ``id2label`` and ``zero_shot_prompt_templates``).\n    \"\"\"\n    if pl_module.trainer.validating:\n        eval_dataset: CombinedDataset = pl_module.trainer.val_dataloaders.dataset\n    elif pl_module.trainer.testing:\n        eval_dataset = pl_module.trainer.test_dataloaders.dataset\n    else:\n        raise ValueError(\n            \"ZeroShotClassification task is only supported for validation and testing.\"\n        )\n\n    self.all_dataset_info = {}\n\n    # create metrics for each dataset/query_modality combination\n    if not self.metrics:\n        for dataset_index, dataset in enumerate(eval_dataset.datasets):\n            dataset_name = getattr(dataset, \"name\", dataset.__class__.__name__)\n            try:\n                id2label: dict[int, str] = dataset.id2label\n            except AttributeError:\n                raise ValueError(\n                    f\"Dataset '{dataset_name}' must have a `id2label` attribute \"\n                    \"to perform zero-shot classification.\"\n                ) from None\n\n            try:\n                zero_shot_prompt_templates: list[str] = (\n                    dataset.zero_shot_prompt_templates\n                )\n            except AttributeError:\n                raise ValueError(\n                    \"Dataset must have a `zero_shot_prompt_templates` attribute to perform zero-shot classification.\"\n                ) from None\n\n            num_classes = len(id2label)\n\n            self.all_dataset_info[dataset_index] = {\n                \"name\": dataset_name,\n                \"id2label\": id2label,\n                \"prompt_templates\": zero_shot_prompt_templates,\n                \"num_classes\": num_classes,\n            }\n\n            for spec in self.task_specs:\n                query_modality = Modalities.get_modality(spec.query_modality).name\n                self.metrics[(query_modality, dataset_index)] = (\n                    self._create_metrics(\n                        num_classes,\n                        spec.top_k,\n                        prefix=f\"{dataset_name}/{query_modality}_\",\n                        postfix=\"\",\n                    )\n                )\n\n    for metric in self.metrics.values():\n        metric.to(pl_module.device)\n\n    for dataset_index, dataset_info in self.all_dataset_info.items():\n        id2label = dataset_info[\"id2label\"]\n        prompt_templates: list[str] = dataset_info[\"prompt_templates\"]\n        labels = list(id2label.values())\n\n        with torch.no_grad():\n            chunk_size = 10\n            all_embeddings = []\n\n            for i in tqdm(\n                range(0, len(labels), chunk_size),\n                desc=\"Encoding class descriptions\",\n            ):\n                batch_labels = labels[i : min(i + chunk_size, len(labels))]\n                descriptions = [\n                    template.format(label)\n                    for label in batch_labels\n                    for template in prompt_templates\n                ]\n                tokenized_descriptions = move_data_to_device(\n                    self.tokenizer(descriptions),\n                    pl_module.device,\n                )\n\n                # Encode the chunk using the pl_module's encode method\n                chunk_embeddings = pl_module.encode(\n                    tokenized_descriptions, Modalities.TEXT\n                )  # shape: [chunk_size x len(prompt_templates), embed_dim]\n                chunk_embeddings /= chunk_embeddings.norm(p=2, dim=-1, keepdim=True)\n                chunk_embeddings = chunk_embeddings.reshape(\n                    len(batch_labels), len(prompt_templates), -1\n                ).mean(dim=1)  # shape: [chunk_size, embed_dim]\n                chunk_embeddings /= chunk_embeddings.norm(p=2, dim=-1, keepdim=True)\n\n                # Append the chunk embeddings to the list\n                all_embeddings.append(chunk_embeddings)\n\n            # Concatenate all chunk embeddings into a single tensor\n            class_embeddings = torch.cat(all_embeddings, dim=0)\n\n        self._embeddings_store[dataset_index] = class_embeddings\n</code></pre>"},{"location":"api/#mmlearn.cli.run.ZeroShotClassification.evaluation_step","title":"evaluation_step","text":"<pre><code>evaluation_step(pl_module, batch, batch_idx)\n</code></pre> <p>Compute logits and update metrics.</p> <p>Parameters:</p> Name Type Description Default <code>pl_module</code> <code>LightningModule</code> <p>A reference to the Lightning module being evaluated.</p> required <code>batch</code> <code>dict[str, Tensor]</code> <p>A batch of data.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required Source code in <code>mmlearn/tasks/zero_shot_classification.py</code> <pre><code>def evaluation_step(\n    self, pl_module: LightningModule, batch: dict[str, torch.Tensor], batch_idx: int\n) -&gt; None:\n    \"\"\"Compute logits and update metrics.\n\n    Parameters\n    ----------\n    pl_module : pl.LightningModule\n        A reference to the Lightning module being evaluated.\n    batch : dict[str, torch.Tensor]\n        A batch of data.\n    batch_idx : int\n        The index of the batch.\n    \"\"\"\n    if pl_module.trainer.sanity_checking:\n        return\n\n    for (query_modality, dataset_index), metric_collection in self.metrics.items():\n        matching_indices = torch.where(batch[\"dataset_index\"] == dataset_index)[0]\n\n        if not matching_indices.numel():\n            continue\n\n        class_embeddings = self._embeddings_store[dataset_index]\n        query_embeddings: torch.Tensor = pl_module.encode(\n            batch, Modalities.get_modality(query_modality)\n        )\n        query_embeddings /= query_embeddings.norm(p=2, dim=-1, keepdim=True)\n        query_embeddings = query_embeddings[matching_indices]\n\n        if self.all_dataset_info[dataset_index][\"num_classes\"] == 2:\n            softmax_output = _safe_matmul(\n                query_embeddings, class_embeddings\n            ).softmax(dim=-1)\n            logits = softmax_output[:, 1] - softmax_output[:, 0]\n        else:\n            logits = 100.0 * _safe_matmul(query_embeddings, class_embeddings)\n        targets = batch[Modalities.get_modality(query_modality).target][\n            matching_indices\n        ]\n\n        metric_collection.update(logits, targets)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.ZeroShotClassification.on_evaluation_epoch_end","title":"on_evaluation_epoch_end","text":"<pre><code>on_evaluation_epoch_end(pl_module)\n</code></pre> <p>Compute and reset metrics.</p> <p>Parameters:</p> Name Type Description Default <code>pl_module</code> <code>LightningModule</code> <p>A reference to the Lightning module being evaluated.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>The computed metrics.</p> Source code in <code>mmlearn/tasks/zero_shot_classification.py</code> <pre><code>def on_evaluation_epoch_end(self, pl_module: LightningModule) -&gt; dict[str, Any]:\n    \"\"\"Compute and reset metrics.\n\n    Parameters\n    ----------\n    pl_module : pl.LightningModule\n        A reference to the Lightning module being evaluated.\n\n    Returns\n    -------\n    dict[str, Any]\n        The computed metrics.\n    \"\"\"\n    results = {}\n    for metric_collection in self.metrics.values():\n        results.update(metric_collection.compute())\n        metric_collection.reset()\n\n    self._embeddings_store.clear()\n\n    eval_type = \"val\" if pl_module.trainer.validating else \"test\"\n    for key, value in results.items():\n        pl_module.log(f\"{eval_type}/{key}\", value)\n\n    return results\n</code></pre>"},{"location":"api/#mmlearn.cli.run.ZeroShotCrossModalRetrieval","title":"ZeroShotCrossModalRetrieval","text":"<p>               Bases: <code>EvaluationHooks</code></p> <p>Zero-shot cross-modal retrieval evaluation task.</p> <p>This task evaluates the retrieval performance of a model on a set of query-target pairs. The model is expected to produce embeddings for both the query and target modalities. The task computes the retrieval recall at <code>k</code> for each pair of modalities.</p> <p>Parameters:</p> Name Type Description Default <code>task_specs</code> <code>list[RetrievalTaskSpec]</code> <p>A list of retrieval task specifications. Each specification defines the query and target modalities, as well as the top-k values for which to compute the retrieval recall metrics.</p> required Source code in <code>mmlearn/tasks/zero_shot_retrieval.py</code> <pre><code>@store(group=\"eval_task\", provider=\"mmlearn\")\nclass ZeroShotCrossModalRetrieval(EvaluationHooks):\n    \"\"\"Zero-shot cross-modal retrieval evaluation task.\n\n    This task evaluates the retrieval performance of a model on a set of query-target\n    pairs. The model is expected to produce embeddings for both the query and target\n    modalities. The task computes the retrieval recall at `k` for each pair of\n    modalities.\n\n    Parameters\n    ----------\n    task_specs : list[RetrievalTaskSpec]\n        A list of retrieval task specifications. Each specification defines the query\n        and target modalities, as well as the top-k values for which to compute the\n        retrieval recall metrics.\n\n    \"\"\"\n\n    def __init__(self, task_specs: list[RetrievalTaskSpec]) -&gt; None:\n        super().__init__()\n\n        self.task_specs = task_specs\n        self.metrics: dict[tuple[str, str], MetricCollection] = {}\n        self._available_modalities = set()\n\n        for spec in self.task_specs:\n            query_modality = spec.query_modality\n            target_modality = spec.target_modality\n            assert Modalities.has_modality(query_modality)\n            assert Modalities.has_modality(target_modality)\n\n            self.metrics[(query_modality, target_modality)] = MetricCollection(\n                {\n                    f\"{query_modality}_to_{target_modality}_R@{k}\": RetrievalRecallAtK(\n                        top_k=k, aggregation=\"mean\", reduction=\"none\"\n                    )\n                    for k in spec.top_k\n                }\n            )\n            self._available_modalities.add(query_modality)\n            self._available_modalities.add(target_modality)\n\n    def on_evaluation_epoch_start(self, pl_module: pl.LightningModule) -&gt; None:\n        \"\"\"Move the metrics to the device of the Lightning module.\"\"\"\n        for metric in self.metrics.values():\n            metric.to(pl_module.device)\n\n    def evaluation_step(\n        self,\n        pl_module: pl.LightningModule,\n        batch: dict[str, torch.Tensor],\n        batch_idx: int,\n    ) -&gt; None:\n        \"\"\"Run the forward pass and update retrieval recall metrics.\n\n        Parameters\n        ----------\n        pl_module : pl.LightningModule\n            A reference to the Lightning module being evaluated.\n        batch : dict[str, torch.Tensor]\n            A dictionary of batched input tensors.\n        batch_idx : int\n            The index of the batch.\n\n        \"\"\"\n        if pl_module.trainer.sanity_checking:\n            return\n\n        outputs: dict[str, Any] = {}\n        for modality_name in self._available_modalities:\n            if modality_name in batch:\n                outputs[modality_name] = pl_module.encode(\n                    batch, Modalities.get_modality(modality_name), normalize=False\n                )\n        for (query_modality, target_modality), metric in self.metrics.items():\n            if query_modality not in outputs or target_modality not in outputs:\n                continue\n            query_embeddings: torch.Tensor = outputs[query_modality]\n            target_embeddings: torch.Tensor = outputs[target_modality]\n            indexes = torch.arange(query_embeddings.size(0), device=pl_module.device)\n\n            metric.update(query_embeddings, target_embeddings, indexes)\n\n    def on_evaluation_epoch_end(\n        self, pl_module: pl.LightningModule\n    ) -&gt; Optional[dict[str, Any]]:\n        \"\"\"Compute the retrieval recall metrics.\n\n        Parameters\n        ----------\n        pl_module : pl.LightningModule\n            A reference to the Lightning module being evaluated.\n\n        Returns\n        -------\n        Optional[dict[str, Any]]\n            A dictionary of evaluation results or `None` if no results are available.\n        \"\"\"\n        if pl_module.trainer.sanity_checking:\n            return None\n\n        results = {}\n        for metric in self.metrics.values():\n            results.update(metric.compute())\n            metric.reset()\n\n        eval_type = \"val\" if pl_module.trainer.validating else \"test\"\n\n        for key, value in results.items():\n            pl_module.log(f\"{eval_type}/{key}\", value)\n\n        return results\n</code></pre>"},{"location":"api/#mmlearn.cli.run.ZeroShotCrossModalRetrieval.on_evaluation_epoch_start","title":"on_evaluation_epoch_start","text":"<pre><code>on_evaluation_epoch_start(pl_module)\n</code></pre> <p>Move the metrics to the device of the Lightning module.</p> Source code in <code>mmlearn/tasks/zero_shot_retrieval.py</code> <pre><code>def on_evaluation_epoch_start(self, pl_module: pl.LightningModule) -&gt; None:\n    \"\"\"Move the metrics to the device of the Lightning module.\"\"\"\n    for metric in self.metrics.values():\n        metric.to(pl_module.device)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.ZeroShotCrossModalRetrieval.evaluation_step","title":"evaluation_step","text":"<pre><code>evaluation_step(pl_module, batch, batch_idx)\n</code></pre> <p>Run the forward pass and update retrieval recall metrics.</p> <p>Parameters:</p> Name Type Description Default <code>pl_module</code> <code>LightningModule</code> <p>A reference to the Lightning module being evaluated.</p> required <code>batch</code> <code>dict[str, Tensor]</code> <p>A dictionary of batched input tensors.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required Source code in <code>mmlearn/tasks/zero_shot_retrieval.py</code> <pre><code>def evaluation_step(\n    self,\n    pl_module: pl.LightningModule,\n    batch: dict[str, torch.Tensor],\n    batch_idx: int,\n) -&gt; None:\n    \"\"\"Run the forward pass and update retrieval recall metrics.\n\n    Parameters\n    ----------\n    pl_module : pl.LightningModule\n        A reference to the Lightning module being evaluated.\n    batch : dict[str, torch.Tensor]\n        A dictionary of batched input tensors.\n    batch_idx : int\n        The index of the batch.\n\n    \"\"\"\n    if pl_module.trainer.sanity_checking:\n        return\n\n    outputs: dict[str, Any] = {}\n    for modality_name in self._available_modalities:\n        if modality_name in batch:\n            outputs[modality_name] = pl_module.encode(\n                batch, Modalities.get_modality(modality_name), normalize=False\n            )\n    for (query_modality, target_modality), metric in self.metrics.items():\n        if query_modality not in outputs or target_modality not in outputs:\n            continue\n        query_embeddings: torch.Tensor = outputs[query_modality]\n        target_embeddings: torch.Tensor = outputs[target_modality]\n        indexes = torch.arange(query_embeddings.size(0), device=pl_module.device)\n\n        metric.update(query_embeddings, target_embeddings, indexes)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.ZeroShotCrossModalRetrieval.on_evaluation_epoch_end","title":"on_evaluation_epoch_end","text":"<pre><code>on_evaluation_epoch_end(pl_module)\n</code></pre> <p>Compute the retrieval recall metrics.</p> <p>Parameters:</p> Name Type Description Default <code>pl_module</code> <code>LightningModule</code> <p>A reference to the Lightning module being evaluated.</p> required <p>Returns:</p> Type Description <code>Optional[dict[str, Any]]</code> <p>A dictionary of evaluation results or <code>None</code> if no results are available.</p> Source code in <code>mmlearn/tasks/zero_shot_retrieval.py</code> <pre><code>def on_evaluation_epoch_end(\n    self, pl_module: pl.LightningModule\n) -&gt; Optional[dict[str, Any]]:\n    \"\"\"Compute the retrieval recall metrics.\n\n    Parameters\n    ----------\n    pl_module : pl.LightningModule\n        A reference to the Lightning module being evaluated.\n\n    Returns\n    -------\n    Optional[dict[str, Any]]\n        A dictionary of evaluation results or `None` if no results are available.\n    \"\"\"\n    if pl_module.trainer.sanity_checking:\n        return None\n\n    results = {}\n    for metric in self.metrics.values():\n        results.update(metric.compute())\n        metric.reset()\n\n    eval_type = \"val\" if pl_module.trainer.validating else \"test\"\n\n    for key, value in results.items():\n        pl_module.log(f\"{eval_type}/{key}\", value)\n\n    return results\n</code></pre>"},{"location":"api/#mmlearn.cli.run.find_matching_indices","title":"find_matching_indices","text":"<pre><code>find_matching_indices(\n    first_example_ids, second_example_ids\n)\n</code></pre> <p>Find the indices of matching examples given two tensors of example ids.</p> <p>Matching examples are defined as examples with the same value in both tensors. This method is useful for finding pairs of examples from different modalities that are related to each other in a batch.</p> <p>Parameters:</p> Name Type Description Default <code>first_example_ids</code> <code>Tensor</code> <p>A tensor of example ids of shape <code>(N, 2)</code>, where <code>N</code> is the number of examples.</p> required <code>second_example_ids</code> <code>Tensor</code> <p>A tensor of example ids of shape <code>(M, 2)</code>, where <code>M</code> is the number of examples.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>A tuple of tensors containing the indices of matching examples in the first and second tensor, respectively.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If either <code>first_example_ids</code> or <code>second_example_ids</code> is not a tensor.</p> <code>ValueError</code> <p>If either <code>first_example_ids</code> or <code>second_example_ids</code> is not a 2D tensor with the second dimension having a size of <code>2</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; img_example_ids = torch.tensor([(0, 0), (0, 1), (1, 0), (1, 1)])\n&gt;&gt;&gt; text_example_ids = torch.tensor([(1, 0), (1, 1), (2, 0), (2, 1), (2, 2)])\n&gt;&gt;&gt; find_matching_indices(img_example_ids, text_example_ids)\n(tensor([2, 3]), tensor([0, 1]))\n</code></pre> Source code in <code>mmlearn/datasets/core/example.py</code> <pre><code>def find_matching_indices(\n    first_example_ids: torch.Tensor, second_example_ids: torch.Tensor\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Find the indices of matching examples given two tensors of example ids.\n\n    Matching examples are defined as examples with the same value in both tensors.\n    This method is useful for finding pairs of examples from different modalities\n    that are related to each other in a batch.\n\n    Parameters\n    ----------\n    first_example_ids : torch.Tensor\n        A tensor of example ids of shape `(N, 2)`, where `N` is the number of examples.\n    second_example_ids : torch.Tensor\n        A tensor of example ids of shape `(M, 2)`, where `M` is the number of examples.\n\n    Returns\n    -------\n    tuple[torch.Tensor, torch.Tensor]\n        A tuple of tensors containing the indices of matching examples in the first and\n        second tensor, respectively.\n\n    Raises\n    ------\n    TypeError\n        If either `first_example_ids` or `second_example_ids` is not a tensor.\n    ValueError\n        If either `first_example_ids` or `second_example_ids` is not a 2D tensor\n        with the second dimension having a size of `2`.\n\n    Examples\n    --------\n    &gt;&gt;&gt; img_example_ids = torch.tensor([(0, 0), (0, 1), (1, 0), (1, 1)])\n    &gt;&gt;&gt; text_example_ids = torch.tensor([(1, 0), (1, 1), (2, 0), (2, 1), (2, 2)])\n    &gt;&gt;&gt; find_matching_indices(img_example_ids, text_example_ids)\n    (tensor([2, 3]), tensor([0, 1]))\n\n\n    \"\"\"\n    if not isinstance(first_example_ids, torch.Tensor) or not isinstance(\n        second_example_ids,\n        torch.Tensor,\n    ):\n        raise TypeError(\n            f\"Expected inputs to be tensors, but got {type(first_example_ids)} \"\n            f\"and {type(second_example_ids)}.\",\n        )\n    val = 2\n    if not (first_example_ids.ndim == val and first_example_ids.shape[1] == val):\n        raise ValueError(\n            \"Expected argument `first_example_ids` to be a tensor of shape (N, 2), \"\n            f\"but got shape {first_example_ids.shape}.\",\n        )\n    if not (second_example_ids.ndim == val and second_example_ids.shape[1] == val):\n        raise ValueError(\n            \"Expected argument `second_example_ids` to be a tensor of shape (N, 2), \"\n            f\"but got shape {second_example_ids.shape}.\",\n        )\n\n    first_example_ids = first_example_ids.unsqueeze(1)  # shape=(N, 1, 2)\n    second_example_ids = second_example_ids.unsqueeze(0)  # shape=(1, M, 2)\n\n    # compare all elements; results in a shape (N, M) tensor\n    matches = torch.all(first_example_ids == second_example_ids, dim=-1)\n    first_indices, second_indices = torch.where(matches)\n    return first_indices, second_indices\n</code></pre>"},{"location":"api/#mmlearn.cli.run.linear_warmup_cosine_annealing_lr","title":"linear_warmup_cosine_annealing_lr","text":"<pre><code>linear_warmup_cosine_annealing_lr(\n    optimizer,\n    warmup_steps,\n    max_steps,\n    start_factor=1 / 3,\n    eta_min=0.0,\n    last_epoch=-1,\n)\n</code></pre> <p>Create a linear warmup cosine annealing learning rate scheduler.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>Optimizer</code> <p>The optimizer for which to schedule the learning rate.</p> required <code>warmup_steps</code> <code>int</code> <p>Maximum number of iterations for linear warmup.</p> required <code>max_steps</code> <code>int</code> <p>Maximum number of iterations.</p> required <code>start_factor</code> <code>float</code> <p>Multiplicative factor for the learning rate at the start of the warmup phase.</p> <code>1/3</code> <code>eta_min</code> <code>float</code> <p>Minimum learning rate.</p> <code>0</code> <code>last_epoch</code> <code>int</code> <p>The index of last epoch. If set to <code>-1</code>, it initializes the learning rate as the base learning rate</p> <code>-1</code> <p>Returns:</p> Type Description <code>LRScheduler</code> <p>The learning rate scheduler.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>warmup_steps</code> is greater than or equal to <code>max_steps</code> or if <code>warmup_steps</code> is less than or equal to 0.</p> Source code in <code>mmlearn/modules/lr_schedulers/linear_warmup_cosine_lr.py</code> <pre><code>@store(  # type: ignore[misc]\n    group=\"modules/lr_schedulers\",\n    provider=\"mmlearn\",\n    zen_partial=True,\n    warmup_steps=MISSING,\n    max_steps=MISSING,\n)\ndef linear_warmup_cosine_annealing_lr(\n    optimizer: Optimizer,\n    warmup_steps: int,\n    max_steps: int,\n    start_factor: float = 1 / 3,\n    eta_min: float = 0.0,\n    last_epoch: int = -1,\n) -&gt; LRScheduler:\n    \"\"\"Create a linear warmup cosine annealing learning rate scheduler.\n\n    Parameters\n    ----------\n    optimizer : Optimizer\n        The optimizer for which to schedule the learning rate.\n    warmup_steps : int\n        Maximum number of iterations for linear warmup.\n    max_steps : int\n        Maximum number of iterations.\n    start_factor : float, optional, default=1/3\n        Multiplicative factor for the learning rate at the start of the warmup phase.\n    eta_min : float, optional, default=0\n        Minimum learning rate.\n    last_epoch : int, optional, default=-1\n        The index of last epoch. If set to ``-1``, it initializes the learning rate\n        as the base learning rate\n\n    Returns\n    -------\n    LRScheduler\n        The learning rate scheduler.\n\n    Raises\n    ------\n    ValueError\n        If `warmup_steps` is greater than or equal to `max_steps` or if `warmup_steps`\n        is less than or equal to 0.\n    \"\"\"\n    if warmup_steps &gt;= max_steps:\n        raise ValueError(\n            \"Expected `warmup_steps` to be less than `max_steps` but got \"\n            f\"`warmup_steps={warmup_steps}` and `max_steps={max_steps}`.\"\n        )\n    if warmup_steps &lt;= 0:\n        raise ValueError(\n            \"Expected `warmup_steps` to be positive but got \"\n            f\"`warmup_steps={warmup_steps}`.\"\n        )\n\n    linear_lr = LinearLR(\n        optimizer,\n        start_factor=start_factor,\n        total_iters=warmup_steps,\n        last_epoch=last_epoch,\n    )\n    cosine_lr = CosineAnnealingLR(\n        optimizer,\n        T_max=max_steps - warmup_steps,\n        eta_min=eta_min,\n        last_epoch=last_epoch,\n    )\n    return SequentialLR(\n        optimizer,\n        schedulers=[linear_lr, cosine_lr],\n        milestones=[warmup_steps],\n        last_epoch=last_epoch,\n    )\n</code></pre>"},{"location":"api/#mmlearn.cli.run.main","title":"main","text":"<pre><code>main(cfg)\n</code></pre> <p>Entry point for training or evaluation.</p> Source code in <code>mmlearn/cli/run.py</code> <pre><code>@_hydra_main(\n    config_path=\"pkg://mmlearn.conf\", config_name=\"base_config\", version_base=None\n)\ndef main(cfg: MMLearnConf) -&gt; None:  # noqa: PLR0912\n    \"\"\"Entry point for training or evaluation.\"\"\"\n    cfg_copy = copy.deepcopy(cfg)  # copy of the config for logging\n\n    L.seed_everything(cfg.seed, workers=True)\n\n    if is_torch_tf32_available():\n        torch.backends.cuda.matmul.allow_tf32 = True\n        if \"16-mixed\" in str(cfg.trainer.precision):\n            cfg.trainer.precision = \"bf16-mixed\"\n\n    # setup trainer first so that we can get some variables for distributed training\n    callbacks = instantiate_callbacks(cfg.trainer.get(\"callbacks\"))\n    cfg.trainer[\"callbacks\"] = None  # will be replaced with the instantiated object\n    loggers = instantiate_loggers(cfg.trainer.get(\"logger\"))\n    cfg.trainer[\"logger\"] = None\n    trainer: Trainer = hydra.utils.instantiate(\n        cfg.trainer, callbacks=callbacks, logger=loggers, _convert_=\"all\"\n    )\n    assert isinstance(trainer, Trainer), (\n        \"Trainer must be an instance of `lightning.pytorch.trainer.Trainer`\"\n    )\n\n    if rank_zero_only.rank == 0 and loggers is not None:  # update wandb config\n        for trainer_logger in loggers:\n            if isinstance(trainer_logger, WandbLogger):\n                trainer_logger.experiment.config.update(\n                    OmegaConf.to_container(cfg_copy, resolve=True, enum_to_str=True),\n                    allow_val_change=True,\n                )\n    trainer.print(OmegaConf.to_yaml(cfg_copy, resolve=True))\n\n    requires_distributed_sampler = (\n        trainer.distributed_sampler_kwargs is not None\n        and trainer._accelerator_connector.use_distributed_sampler\n    )\n    if requires_distributed_sampler:  # we handle distributed samplers\n        trainer._accelerator_connector.use_distributed_sampler = False\n\n    # prepare dataloaders\n    if cfg.job_type == JobType.train:\n        train_dataset = instantiate_datasets(cfg.datasets.train)\n        assert train_dataset is not None, (\n            \"Train dataset (`cfg.datasets.train`) is required for training.\"\n        )\n\n        train_sampler = instantiate_sampler(\n            cfg.dataloader.train.get(\"sampler\"),\n            train_dataset,\n            requires_distributed_sampler=requires_distributed_sampler,\n            distributed_sampler_kwargs=trainer.distributed_sampler_kwargs,\n        )\n        cfg.dataloader.train[\"sampler\"] = None  # replaced with the instantiated object\n        train_loader: DataLoader = hydra.utils.instantiate(\n            cfg.dataloader.train, dataset=train_dataset, sampler=train_sampler\n        )\n\n        val_loader: Optional[DataLoader] = None\n        val_dataset = instantiate_datasets(cfg.datasets.val)\n        if val_dataset is not None:\n            val_sampler = instantiate_sampler(\n                cfg.dataloader.val.get(\"sampler\"),\n                val_dataset,\n                requires_distributed_sampler=requires_distributed_sampler,\n                distributed_sampler_kwargs=trainer.distributed_sampler_kwargs,\n            )\n            cfg.dataloader.val[\"sampler\"] = None\n            val_loader = hydra.utils.instantiate(\n                cfg.dataloader.val, dataset=val_dataset, sampler=val_sampler\n            )\n    else:\n        test_dataset = instantiate_datasets(cfg.datasets.test)\n        assert test_dataset is not None, (\n            \"Test dataset (`cfg.datasets.test`) is required for evaluation.\"\n        )\n\n        test_sampler = instantiate_sampler(\n            cfg.dataloader.test.get(\"sampler\"),\n            test_dataset,\n            requires_distributed_sampler=requires_distributed_sampler,\n            distributed_sampler_kwargs=trainer.distributed_sampler_kwargs,\n        )\n        cfg.dataloader.test[\"sampler\"] = None\n        test_loader = hydra.utils.instantiate(\n            cfg.dataloader.test, dataset=test_dataset, sampler=test_sampler\n        )\n\n    # setup task module\n    if cfg.task is None or \"_target_\" not in cfg.task:\n        raise ValueError(\n            \"Expected a non-empty config for `cfg.task` with a `_target_` key. \"\n            f\"But got: {cfg.task}\"\n        )\n    logger.info(f\"Instantiating task module: {cfg.task['_target_']}\")\n    model: L.LightningModule = hydra.utils.instantiate(cfg.task, _convert_=\"partial\")\n    assert isinstance(model, L.LightningModule), \"Task must be a `LightningModule`\"\n    model.strict_loading = cfg.strict_loading\n\n    # compile model\n    model = torch.compile(model, **OmegaConf.to_object(cfg.torch_compile_kwargs))\n\n    if cfg.job_type == JobType.train:\n        trainer.fit(\n            model, train_loader, val_loader, ckpt_path=cfg.resume_from_checkpoint\n        )\n    elif cfg.job_type == JobType.eval:\n        trainer.test(model, test_loader, ckpt_path=cfg.resume_from_checkpoint)\n</code></pre>"},{"location":"api/#mmlearn.conf","title":"conf","text":"<p>Hydra/Hydra-zen-based configurations.</p>"},{"location":"api/#mmlearn.conf.JobType","title":"JobType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Type of the job.</p> Source code in <code>mmlearn/conf/__init__.py</code> <pre><code>class JobType(str, Enum):\n    \"\"\"Type of the job.\"\"\"\n\n    train = \"train\"\n    eval = \"eval\"\n</code></pre>"},{"location":"api/#mmlearn.conf.DatasetConf","title":"DatasetConf  <code>dataclass</code>","text":"<p>Configuration template for the datasets.</p> Source code in <code>mmlearn/conf/__init__.py</code> <pre><code>@dataclass\nclass DatasetConf:\n    \"\"\"Configuration template for the datasets.\"\"\"\n\n    #: Configuration for the training dataset.\n    train: Optional[Any] = field(\n        default=None,\n        metadata={\"help\": \"Configuration for the training dataset.\"},\n    )\n    #: Configuration for the validation dataset.\n    val: Optional[Any] = field(\n        default=None, metadata={\"help\": \"Configuration for the validation dataset.\"}\n    )\n    #: Configuration for the test dataset.\n    test: Optional[Any] = field(\n        default=None,\n        metadata={\"help\": \"Configuration for the test dataset.\"},\n    )\n</code></pre>"},{"location":"api/#mmlearn.conf.DataLoaderConf","title":"DataLoaderConf  <code>dataclass</code>","text":"<p>Configuration for the dataloader.</p> Source code in <code>mmlearn/conf/__init__.py</code> <pre><code>@dataclass\nclass DataLoaderConf:\n    \"\"\"Configuration for the dataloader.\"\"\"\n\n    #: Configuration for the training dataloader.\n    train: Any = field(\n        default_factory=_DataLoaderConf,\n        metadata={\"help\": \"Configuration for the training dataloader.\"},\n    )\n    #: Configuration for the validation dataloader.\n    val: Any = field(\n        default_factory=_DataLoaderConf,\n        metadata={\"help\": \"Configuration for the validation dataloader.\"},\n    )\n    #: Configuration for the test dataloader.\n    test: Any = field(\n        default_factory=_DataLoaderConf,\n        metadata={\"help\": \"Configuration for the test dataloader.\"},\n    )\n</code></pre>"},{"location":"api/#mmlearn.conf.MMLearnConf","title":"MMLearnConf  <code>dataclass</code>","text":"<p>Top-level configuration for mmlearn experiments.</p> Source code in <code>mmlearn/conf/__init__.py</code> <pre><code>@dataclass\nclass MMLearnConf:\n    \"\"\"Top-level configuration for mmlearn experiments.\"\"\"\n\n    defaults: list[Any] = field(\n        default_factory=lambda: [\n            \"_self_\",  # See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information\n            {\"task\": MISSING},\n            {\"override hydra/launcher\": \"submitit_slurm\"},\n        ]\n    )\n    #: Name of the experiment. This must be specified for any experiment to run.\n    experiment_name: str = field(default=MISSING)\n    #: Type of the job.\n    job_type: JobType = field(default=JobType.train)\n    #: Seed for the random number generators. This is set for Python, Numpy and PyTorch,\n    #: including the workers in PyTorch Dataloaders.\n    seed: Optional[int] = field(default=None)\n    #: Configuration for the datasets.\n    datasets: DatasetConf = field(default_factory=DatasetConf)\n    #: Configuration for the dataloaders.\n    dataloader: DataLoaderConf = field(default_factory=DataLoaderConf)\n    #: Configuration for the task. This is required to run any experiment.\n    task: Any = field(default=MISSING)\n    #: Configuration for the trainer. The options here are the same as in\n    #: :py:class:`~lightning.pytorch.trainer.trainer.Trainer`\n    trainer: Any = field(\n        default_factory=builds(\n            lightning_trainer.Trainer,\n            populate_full_signature=True,\n            enable_model_summary=True,\n            enable_progress_bar=True,\n            enable_checkpointing=True,\n            default_root_dir=_get_default_ckpt_dir(),\n        )\n    )\n    #: Tags for the experiment. This is useful for `wandb &lt;https://docs.wandb.ai/ref/python/init&gt;`_\n    #: logging.\n    tags: Optional[list[str]] = field(default_factory=lambda: [II(\"experiment_name\")])\n    #: Path to the checkpoint to resume training from.\n    resume_from_checkpoint: Optional[Path] = field(default=None)\n    #: Whether to strictly enforce loading of model weights i.e. `strict=True` in\n    #: :py:meth:`~lightning.pytorch.core.module.LightningModule.load_from_checkpoint`.\n    strict_loading: bool = field(default=True)\n    #: Configuration for torch.compile. These are essentially the same as the\n    #: arguments for :py:func:`torch.compile`.\n    torch_compile_kwargs: dict[str, Any] = field(\n        default_factory=lambda: {\n            \"disable\": True,\n            \"fullgraph\": False,\n            \"dynamic\": None,\n            \"backend\": \"inductor\",\n            \"mode\": None,\n            \"options\": None,\n        }\n    )\n    #: Hydra configuration.\n    hydra: HydraConf = field(\n        default_factory=lambda: HydraConf(\n            searchpath=[\"pkg://mmlearn.conf\"],\n            run=RunDir(\n                dir=SI(\"./outputs/${experiment_name}/${now:%Y-%m-%d}/${now:%H-%M-%S}\")\n            ),\n            sweep=SweepDir(\n                dir=SI(\"./outputs/${experiment_name}/${now:%Y-%m-%d}/${now:%H-%M-%S}\"),\n                subdir=SI(\"${hydra.job.num}_${hydra.job.id}\"),\n            ),\n            help=HelpConf(\n                app_name=\"mmlearn\",\n                header=\"mmlearn: A modular framework for research on multimodal representation learning.\",\n            ),\n            job=JobConf(\n                name=II(\"experiment_name\"),\n                env_set={\n                    \"TORCH_NCCL_ASYNC_ERROR_HANDLING\": \"1\",\n                    \"HYDRA_FULL_ERROR\": \"1\",\n                },\n            ),\n        )\n    )\n</code></pre>"},{"location":"api/#mmlearn.conf.register_external_modules","title":"register_external_modules","text":"<pre><code>register_external_modules(\n    module,\n    group,\n    name=None,\n    package=None,\n    provider=None,\n    base_cls=None,\n    ignore_cls=None,\n    ignore_prefix=None,\n    **kwargs_for_builds\n)\n</code></pre> <p>Add all classes in an external module to a ZenStore.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>ModuleType</code> <p>The module to add classes from.</p> required <code>group</code> <code>str</code> <p>The config group to add the classes to.</p> required <code>name</code> <code>Optional[str]</code> <p>The name to give to the dynamically-generated configs. If <code>None</code>, the class name is used.</p> <code>None</code> <code>package</code> <code>Optional[str]</code> <p>The package to add the configs to.</p> <code>None</code> <code>provider</code> <code>Optional[str]</code> <p>The provider to add the configs to.</p> <code>None</code> <code>base_cls</code> <code>Optional[type]</code> <p>The base class to filter classes by. The base class is also excluded from the configs.</p> <code>None</code> <code>ignore_cls</code> <code>Optional[list[type]]</code> <p>list of classes to ignore.</p> <code>None</code> <code>ignore_prefix</code> <code>Optional[str]</code> <p>Ignore classes whose names start with this prefix.</p> <code>None</code> <code>kwargs_for_builds</code> <code>Any</code> <p>Additional keyword arguments to pass to <code>hydra_zen.builds</code>.</p> <code>{}</code> Source code in <code>mmlearn/conf/__init__.py</code> <pre><code>def register_external_modules(\n    module: ModuleType,\n    group: str,\n    name: Optional[str] = None,\n    package: Optional[str] = None,\n    provider: Optional[str] = None,\n    base_cls: Optional[type] = None,\n    ignore_cls: Optional[list[type]] = None,\n    ignore_prefix: Optional[str] = None,\n    **kwargs_for_builds: Any,\n) -&gt; None:\n    \"\"\"Add all classes in an external module to a ZenStore.\n\n    Parameters\n    ----------\n    module : ModuleType\n        The module to add classes from.\n    group : str\n        The config group to add the classes to.\n    name : Optional[str], optional, default=None\n        The name to give to the dynamically-generated configs. If `None`, the\n        class name is used.\n    package : Optional[str], optional, default=None\n        The package to add the configs to.\n    provider : Optional[str], optional, default=None\n        The provider to add the configs to.\n    base_cls : Optional[type], optional, default=None\n        The base class to filter classes by. The base class is also excluded from\n        the configs.\n    ignore_cls : Optional[list[type]], optional, default=None\n        list of classes to ignore.\n    ignore_prefix : Optional[str], optional, default=None\n        Ignore classes whose names start with this prefix.\n    kwargs_for_builds : Any\n        Additional keyword arguments to pass to ``hydra_zen.builds``.\n\n    \"\"\"\n    for key, cls in module.__dict__.items():\n        if (\n            isinstance(cls, type)\n            and (base_cls is None or issubclass(cls, base_cls))\n            and cls != base_cls\n            and (ignore_cls is None or cls not in ignore_cls)\n            and (ignore_prefix is None or not key.startswith(ignore_prefix))\n        ):\n            external_store(\n                builds(cls, populate_full_signature=True, **kwargs_for_builds),\n                name=name or key,\n                group=group,\n                package=package,\n                provider=provider,\n            )\n</code></pre>"},{"location":"api/#mmlearn.constants","title":"constants","text":"<p>Constants.</p>"},{"location":"api/#mmlearn.datasets","title":"datasets","text":"<p>Datasets.</p>"},{"location":"api/#mmlearn.datasets.CheXpert","title":"CheXpert","text":"<p>               Bases: <code>Dataset[Example]</code></p> <p>CheXpert dataset.</p> <p>Each datapoint is a pair of <code>(image, target label)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>Directory which contains <code>.json</code> files stating all dataset entries.</p> required <code>split</code> <code>(train, valid)</code> <p>Dataset split.</p> <code>\"train\"</code> <code>labeler</code> <code>Optional[{chexpert, chexbert, vchexbert}]</code> <p>Labeler used to extract labels from the training images. \"valid\" split has no labeler, labeling for valid split was done by human radiologists.</p> <code>None</code> <code>transform</code> <code>Optional[Callable[[PIL.Image], torch.Tensor]</code> <p>A callable that takes in a PIL image and returns a transformed version of the image as a PyTorch tensor.</p> <code>None</code> Source code in <code>mmlearn/datasets/chexpert.py</code> <pre><code>@store(\n    group=\"datasets\",\n    provider=\"mmlearn\",\n    root_dir=os.getenv(\"CHEXPERT_ROOT_DIR\", MISSING),\n    split=\"train\",\n)\nclass CheXpert(Dataset[Example]):\n    \"\"\"CheXpert dataset.\n\n    Each datapoint is a pair of `(image, target label)`.\n\n    Parameters\n    ----------\n    root_dir : str\n        Directory which contains `.json` files stating all dataset entries.\n    split : {\"train\", \"valid\"}\n        Dataset split.\n    labeler : Optional[{\"chexpert\", \"chexbert\", \"vchexbert\"}], optional, default=None\n        Labeler used to extract labels from the training images. \"valid\" split\n        has no labeler, labeling for valid split was done by human radiologists.\n    transform : Optional[Callable[[PIL.Image], torch.Tensor], optional, default=None\n        A callable that takes in a PIL image and returns a transformed version\n        of the image as a PyTorch tensor.\n    \"\"\"\n\n    def __init__(\n        self,\n        root_dir: str,\n        split: Literal[\"train\", \"valid\"],\n        labeler: Optional[Literal[\"chexpert\", \"chexbert\", \"vchexbert\"]] = None,\n        transform: Optional[Callable[[Image.Image], torch.Tensor]] = None,\n    ) -&gt; None:\n        assert split in [\"train\", \"valid\"], f\"split {split} is not available.\"\n        assert labeler in [\"chexpert\", \"chexbert\", \"vchexbert\"] or labeler is None, (\n            f\"labeler {labeler} is not available.\"\n        )\n        assert callable(transform) or transform is None, (\n            \"transform is not callable or None.\"\n        )\n\n        if split == \"valid\":\n            data_file = f\"{split}_data.json\"\n        elif split == \"train\":\n            data_file = f\"{labeler}_{split}_data.json\"\n        data_path = os.path.join(root_dir, data_file)\n\n        assert os.path.isfile(data_path), f\"entries file does not exist: {data_path}.\"\n\n        with open(data_path, \"rb\") as file:\n            entries = json.load(file)\n        self.entries = entries\n\n        if transform is not None:\n            self.transform = transform\n        else:\n            self.transform = Compose([Resize(224), CenterCrop(224), ToTensor()])\n\n    def __getitem__(self, idx: int) -&gt; Example:\n        \"\"\"Return the idx'th datapoint.\"\"\"\n        entry = self.entries[idx]\n        image = Image.open(entry[\"image_path\"]).convert(\"RGB\")\n        image = self.transform(image)\n        label = torch.tensor(entry[\"label\"])\n\n        return Example(\n            {\n                Modalities.RGB.name: image,\n                Modalities.RGB.target: label,\n                \"qid\": entry[\"qid\"],\n                EXAMPLE_INDEX_KEY: idx,\n            }\n        )\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the length of the dataset.\"\"\"\n        return len(self.entries)\n</code></pre>"},{"location":"api/#mmlearn.datasets.CheXpert.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> <p>Return the idx'th datapoint.</p> Source code in <code>mmlearn/datasets/chexpert.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; Example:\n    \"\"\"Return the idx'th datapoint.\"\"\"\n    entry = self.entries[idx]\n    image = Image.open(entry[\"image_path\"]).convert(\"RGB\")\n    image = self.transform(image)\n    label = torch.tensor(entry[\"label\"])\n\n    return Example(\n        {\n            Modalities.RGB.name: image,\n            Modalities.RGB.target: label,\n            \"qid\": entry[\"qid\"],\n            EXAMPLE_INDEX_KEY: idx,\n        }\n    )\n</code></pre>"},{"location":"api/#mmlearn.datasets.CheXpert.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the length of the dataset.</p> Source code in <code>mmlearn/datasets/chexpert.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the length of the dataset.\"\"\"\n    return len(self.entries)\n</code></pre>"},{"location":"api/#mmlearn.datasets.ImageNet","title":"ImageNet","text":"<p>               Bases: <code>ImageFolder</code></p> <p>ImageNet dataset.</p> <p>This is a wrapper around the class:<code>~torchvision.datasets.ImageFolder</code> class that returns an class:<code>~mmlearn.datasets.core.example.Example</code> object.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>Path to the root directory of the dataset.</p> required <code>split</code> <code>(train, val)</code> <p>The split of the dataset to use.</p> <code>\"train\"</code> <code>transform</code> <code>Optional[Callable]</code> <p>A callable that takes in a PIL image and returns a transformed version of the image as a PyTorch tensor.</p> <code>None</code> <code>target_transform</code> <code>Optional[Callable]</code> <p>A callable that takes in the target and transforms it.</p> <code>None</code> <code>mask_generator</code> <code>Optional[Callable]</code> <p>A callable that generates a mask for the image.</p> <code>None</code> Source code in <code>mmlearn/datasets/imagenet.py</code> <pre><code>@store(\n    group=\"datasets\",\n    provider=\"mmlearn\",\n    root_dir=os.getenv(\"IMAGENET_ROOT_DIR\", MISSING),\n)\nclass ImageNet(ImageFolder):\n    \"\"\"ImageNet dataset.\n\n    This is a wrapper around the :py:class:`~torchvision.datasets.ImageFolder` class\n    that returns an :py:class:`~mmlearn.datasets.core.example.Example` object.\n\n    Parameters\n    ----------\n    root_dir : str\n        Path to the root directory of the dataset.\n    split : {\"train\", \"val\"}, default=\"train\"\n        The split of the dataset to use.\n    transform : Optional[Callable], optional, default=None\n        A callable that takes in a PIL image and returns a transformed version\n        of the image as a PyTorch tensor.\n    target_transform : Optional[Callable], optional, default=None\n        A callable that takes in the target and transforms it.\n    mask_generator : Optional[Callable], optional, default=None\n        A callable that generates a mask for the image.\n    \"\"\"\n\n    def __init__(\n        self,\n        root_dir: str,\n        split: Literal[\"train\", \"val\"] = \"train\",\n        transform: Optional[Callable[..., Any]] = None,\n        target_transform: Optional[Callable[..., Any]] = None,\n        mask_generator: Optional[Callable[..., Any]] = None,\n    ) -&gt; None:\n        split = \"train\" if split == \"train\" else \"val\"\n        root_dir = os.path.join(root_dir, split)\n        super().__init__(\n            root=root_dir, transform=transform, target_transform=target_transform\n        )\n        self.mask_generator = mask_generator\n\n    def __getitem__(self, index: int) -&gt; Example:\n        \"\"\"Get an example at the given index.\"\"\"\n        image, target = super().__getitem__(index)\n        example = Example(\n            {\n                Modalities.RGB.name: image,\n                Modalities.RGB.target: target,\n                EXAMPLE_INDEX_KEY: index,\n            }\n        )\n        mask = self.mask_generator() if self.mask_generator else None\n        if mask is not None:  # error will be raised during collation if `None`\n            example[Modalities.RGB.mask] = mask\n        return example\n\n    @property\n    def zero_shot_prompt_templates(self) -&gt; list[str]:\n        \"\"\"Return the zero-shot prompt templates.\"\"\"\n        return [\n            \"a bad photo of a {}.\",\n            \"a photo of many {}.\",\n            \"a sculpture of a {}.\",\n            \"a photo of the hard to see {}.\",\n            \"a low resolution photo of the {}.\",\n            \"a rendering of a {}.\",\n            \"graffiti of a {}.\",\n            \"a bad photo of the {}.\",\n            \"a cropped photo of the {}.\",\n            \"a tattoo of a {}.\",\n            \"the embroidered {}.\",\n            \"a photo of a hard to see {}.\",\n            \"a bright photo of a {}.\",\n            \"a photo of a clean {}.\",\n            \"a photo of a dirty {}.\",\n            \"a dark photo of the {}.\",\n            \"a drawing of a {}.\",\n            \"a photo of my {}.\",\n            \"the plastic {}.\",\n            \"a photo of the cool {}.\",\n            \"a close-up photo of a {}.\",\n            \"a black and white photo of the {}.\",\n            \"a painting of the {}.\",\n            \"a painting of a {}.\",\n            \"a pixelated photo of the {}.\",\n            \"a sculpture of the {}.\",\n            \"a bright photo of the {}.\",\n            \"a cropped photo of a {}.\",\n            \"a plastic {}.\",\n            \"a photo of the dirty {}.\",\n            \"a jpeg corrupted photo of a {}.\",\n            \"a blurry photo of the {}.\",\n            \"a photo of the {}.\",\n            \"a good photo of the {}.\",\n            \"a rendering of the {}.\",\n            \"a {} in a video game.\",\n            \"a photo of one {}.\",\n            \"a doodle of a {}.\",\n            \"a close-up photo of the {}.\",\n            \"a photo of a {}.\",\n            \"the origami {}.\",\n            \"the {} in a video game.\",\n            \"a sketch of a {}.\",\n            \"a doodle of the {}.\",\n            \"a origami {}.\",\n            \"a low resolution photo of a {}.\",\n            \"the toy {}.\",\n            \"a rendition of the {}.\",\n            \"a photo of the clean {}.\",\n            \"a photo of a large {}.\",\n            \"a rendition of a {}.\",\n            \"a photo of a nice {}.\",\n            \"a photo of a weird {}.\",\n            \"a blurry photo of a {}.\",\n            \"a cartoon {}.\",\n            \"art of a {}.\",\n            \"a sketch of the {}.\",\n            \"a embroidered {}.\",\n            \"a pixelated photo of a {}.\",\n            \"itap of the {}.\",\n            \"a jpeg corrupted photo of the {}.\",\n            \"a good photo of a {}.\",\n            \"a plushie {}.\",\n            \"a photo of the nice {}.\",\n            \"a photo of the small {}.\",\n            \"a photo of the weird {}.\",\n            \"the cartoon {}.\",\n            \"art of the {}.\",\n            \"a drawing of the {}.\",\n            \"a photo of the large {}.\",\n            \"a black and white photo of a {}.\",\n            \"the plushie {}.\",\n            \"a dark photo of a {}.\",\n            \"itap of a {}.\",\n            \"graffiti of the {}.\",\n            \"a toy {}.\",\n            \"itap of my {}.\",\n            \"a photo of a cool {}.\",\n            \"a photo of a small {}.\",\n            \"a tattoo of the {}.\",\n        ]\n\n    @property\n    def id2label(self) -&gt; dict[int, str]:\n        \"\"\"Return the label mapping.\"\"\"\n        return {\n            0: \"tench\",\n            1: \"goldfish\",\n            2: \"great white shark\",\n            3: \"tiger shark\",\n            4: \"hammerhead shark\",\n            5: \"electric ray\",\n            6: \"stingray\",\n            7: \"rooster\",\n            8: \"hen\",\n            9: \"ostrich\",\n            10: \"brambling\",\n            11: \"goldfinch\",\n            12: \"house finch\",\n            13: \"junco\",\n            14: \"indigo bunting\",\n            15: \"American robin\",\n            16: \"bulbul\",\n            17: \"jay\",\n            18: \"magpie\",\n            19: \"chickadee\",\n            20: \"American dipper\",\n            21: \"kite (bird of prey)\",\n            22: \"bald eagle\",\n            23: \"vulture\",\n            24: \"great grey owl\",\n            25: \"fire salamander\",\n            26: \"smooth newt\",\n            27: \"newt\",\n            28: \"spotted salamander\",\n            29: \"axolotl\",\n            30: \"American bullfrog\",\n            31: \"tree frog\",\n            32: \"tailed frog\",\n            33: \"loggerhead sea turtle\",\n            34: \"leatherback sea turtle\",\n            35: \"mud turtle\",\n            36: \"terrapin\",\n            37: \"box turtle\",\n            38: \"banded gecko\",\n            39: \"green iguana\",\n            40: \"Carolina anole\",\n            41: \"desert grassland whiptail lizard\",\n            42: \"agama\",\n            43: \"frilled-necked lizard\",\n            44: \"alligator lizard\",\n            45: \"Gila monster\",\n            46: \"European green lizard\",\n            47: \"chameleon\",\n            48: \"Komodo dragon\",\n            49: \"Nile crocodile\",\n            50: \"American alligator\",\n            51: \"triceratops\",\n            52: \"worm snake\",\n            53: \"ring-necked snake\",\n            54: \"eastern hog-nosed snake\",\n            55: \"smooth green snake\",\n            56: \"kingsnake\",\n            57: \"garter snake\",\n            58: \"water snake\",\n            59: \"vine snake\",\n            60: \"night snake\",\n            61: \"boa constrictor\",\n            62: \"African rock python\",\n            63: \"Indian cobra\",\n            64: \"green mamba\",\n            65: \"sea snake\",\n            66: \"Saharan horned viper\",\n            67: \"eastern diamondback rattlesnake\",\n            68: \"sidewinder rattlesnake\",\n            69: \"trilobite\",\n            70: \"harvestman\",\n            71: \"scorpion\",\n            72: \"yellow garden spider\",\n            73: \"barn spider\",\n            74: \"European garden spider\",\n            75: \"southern black widow\",\n            76: \"tarantula\",\n            77: \"wolf spider\",\n            78: \"tick\",\n            79: \"centipede\",\n            80: \"black grouse\",\n            81: \"ptarmigan\",\n            82: \"ruffed grouse\",\n            83: \"prairie grouse\",\n            84: \"peafowl\",\n            85: \"quail\",\n            86: \"partridge\",\n            87: \"african grey parrot\",\n            88: \"macaw\",\n            89: \"sulphur-crested cockatoo\",\n            90: \"lorikeet\",\n            91: \"coucal\",\n            92: \"bee eater\",\n            93: \"hornbill\",\n            94: \"hummingbird\",\n            95: \"jacamar\",\n            96: \"toucan\",\n            97: \"duck\",\n            98: \"red-breasted merganser\",\n            99: \"goose\",\n            100: \"black swan\",\n            101: \"tusker\",\n            102: \"echidna\",\n            103: \"platypus\",\n            104: \"wallaby\",\n            105: \"koala\",\n            106: \"wombat\",\n            107: \"jellyfish\",\n            108: \"sea anemone\",\n            109: \"brain coral\",\n            110: \"flatworm\",\n            111: \"nematode\",\n            112: \"conch\",\n            113: \"snail\",\n            114: \"slug\",\n            115: \"sea slug\",\n            116: \"chiton\",\n            117: \"chambered nautilus\",\n            118: \"Dungeness crab\",\n            119: \"rock crab\",\n            120: \"fiddler crab\",\n            121: \"red king crab\",\n            122: \"American lobster\",\n            123: \"spiny lobster\",\n            124: \"crayfish\",\n            125: \"hermit crab\",\n            126: \"isopod\",\n            127: \"white stork\",\n            128: \"black stork\",\n            129: \"spoonbill\",\n            130: \"flamingo\",\n            131: \"little blue heron\",\n            132: \"great egret\",\n            133: \"bittern bird\",\n            134: \"crane bird\",\n            135: \"limpkin\",\n            136: \"common gallinule\",\n            137: \"American coot\",\n            138: \"bustard\",\n            139: \"ruddy turnstone\",\n            140: \"dunlin\",\n            141: \"common redshank\",\n            142: \"dowitcher\",\n            143: \"oystercatcher\",\n            144: \"pelican\",\n            145: \"king penguin\",\n            146: \"albatross\",\n            147: \"grey whale\",\n            148: \"killer whale\",\n            149: \"dugong\",\n            150: \"sea lion\",\n            151: \"Chihuahua\",\n            152: \"Japanese Chin\",\n            153: \"Maltese\",\n            154: \"Pekingese\",\n            155: \"Shih Tzu\",\n            156: \"King Charles Spaniel\",\n            157: \"Papillon\",\n            158: \"toy terrier\",\n            159: \"Rhodesian Ridgeback\",\n            160: \"Afghan Hound\",\n            161: \"Basset Hound\",\n            162: \"Beagle\",\n            163: \"Bloodhound\",\n            164: \"Bluetick Coonhound\",\n            165: \"Black and Tan Coonhound\",\n            166: \"Treeing Walker Coonhound\",\n            167: \"English foxhound\",\n            168: \"Redbone Coonhound\",\n            169: \"borzoi\",\n            170: \"Irish Wolfhound\",\n            171: \"Italian Greyhound\",\n            172: \"Whippet\",\n            173: \"Ibizan Hound\",\n            174: \"Norwegian Elkhound\",\n            175: \"Otterhound\",\n            176: \"Saluki\",\n            177: \"Scottish Deerhound\",\n            178: \"Weimaraner\",\n            179: \"Staffordshire Bull Terrier\",\n            180: \"American Staffordshire Terrier\",\n            181: \"Bedlington Terrier\",\n            182: \"Border Terrier\",\n            183: \"Kerry Blue Terrier\",\n            184: \"Irish Terrier\",\n            185: \"Norfolk Terrier\",\n            186: \"Norwich Terrier\",\n            187: \"Yorkshire Terrier\",\n            188: \"Wire Fox Terrier\",\n            189: \"Lakeland Terrier\",\n            190: \"Sealyham Terrier\",\n            191: \"Airedale Terrier\",\n            192: \"Cairn Terrier\",\n            193: \"Australian Terrier\",\n            194: \"Dandie Dinmont Terrier\",\n            195: \"Boston Terrier\",\n            196: \"Miniature Schnauzer\",\n            197: \"Giant Schnauzer\",\n            198: \"Standard Schnauzer\",\n            199: \"Scottish Terrier\",\n            200: \"Tibetan Terrier\",\n            201: \"Australian Silky Terrier\",\n            202: \"Soft-coated Wheaten Terrier\",\n            203: \"West Highland White Terrier\",\n            204: \"Lhasa Apso\",\n            205: \"Flat-Coated Retriever\",\n            206: \"Curly-coated Retriever\",\n            207: \"Golden Retriever\",\n            208: \"Labrador Retriever\",\n            209: \"Chesapeake Bay Retriever\",\n            210: \"German Shorthaired Pointer\",\n            211: \"Vizsla\",\n            212: \"English Setter\",\n            213: \"Irish Setter\",\n            214: \"Gordon Setter\",\n            215: \"Brittany dog\",\n            216: \"Clumber Spaniel\",\n            217: \"English Springer Spaniel\",\n            218: \"Welsh Springer Spaniel\",\n            219: \"Cocker Spaniel\",\n            220: \"Sussex Spaniel\",\n            221: \"Irish Water Spaniel\",\n            222: \"Kuvasz\",\n            223: \"Schipperke\",\n            224: \"Groenendael dog\",\n            225: \"Malinois\",\n            226: \"Briard\",\n            227: \"Australian Kelpie\",\n            228: \"Komondor\",\n            229: \"Old English Sheepdog\",\n            230: \"Shetland Sheepdog\",\n            231: \"collie\",\n            232: \"Border Collie\",\n            233: \"Bouvier des Flandres dog\",\n            234: \"Rottweiler\",\n            235: \"German Shepherd Dog\",\n            236: \"Dobermann\",\n            237: \"Miniature Pinscher\",\n            238: \"Greater Swiss Mountain Dog\",\n            239: \"Bernese Mountain Dog\",\n            240: \"Appenzeller Sennenhund\",\n            241: \"Entlebucher Sennenhund\",\n            242: \"Boxer\",\n            243: \"Bullmastiff\",\n            244: \"Tibetan Mastiff\",\n            245: \"French Bulldog\",\n            246: \"Great Dane\",\n            247: \"St. Bernard\",\n            248: \"husky\",\n            249: \"Alaskan Malamute\",\n            250: \"Siberian Husky\",\n            251: \"Dalmatian\",\n            252: \"Affenpinscher\",\n            253: \"Basenji\",\n            254: \"pug\",\n            255: \"Leonberger\",\n            256: \"Newfoundland dog\",\n            257: \"Great Pyrenees dog\",\n            258: \"Samoyed\",\n            259: \"Pomeranian\",\n            260: \"Chow Chow\",\n            261: \"Keeshond\",\n            262: \"brussels griffon\",\n            263: \"Pembroke Welsh Corgi\",\n            264: \"Cardigan Welsh Corgi\",\n            265: \"Toy Poodle\",\n            266: \"Miniature Poodle\",\n            267: \"Standard Poodle\",\n            268: \"Mexican hairless dog (xoloitzcuintli)\",\n            269: \"grey wolf\",\n            270: \"Alaskan tundra wolf\",\n            271: \"red wolf or maned wolf\",\n            272: \"coyote\",\n            273: \"dingo\",\n            274: \"dhole\",\n            275: \"African wild dog\",\n            276: \"hyena\",\n            277: \"red fox\",\n            278: \"kit fox\",\n            279: \"Arctic fox\",\n            280: \"grey fox\",\n            281: \"tabby cat\",\n            282: \"tiger cat\",\n            283: \"Persian cat\",\n            284: \"Siamese cat\",\n            285: \"Egyptian Mau\",\n            286: \"cougar\",\n            287: \"lynx\",\n            288: \"leopard\",\n            289: \"snow leopard\",\n            290: \"jaguar\",\n            291: \"lion\",\n            292: \"tiger\",\n            293: \"cheetah\",\n            294: \"brown bear\",\n            295: \"American black bear\",\n            296: \"polar bear\",\n            297: \"sloth bear\",\n            298: \"mongoose\",\n            299: \"meerkat\",\n            300: \"tiger beetle\",\n            301: \"ladybug\",\n            302: \"ground beetle\",\n            303: \"longhorn beetle\",\n            304: \"leaf beetle\",\n            305: \"dung beetle\",\n            306: \"rhinoceros beetle\",\n            307: \"weevil\",\n            308: \"fly\",\n            309: \"bee\",\n            310: \"ant\",\n            311: \"grasshopper\",\n            312: \"cricket insect\",\n            313: \"stick insect\",\n            314: \"cockroach\",\n            315: \"praying mantis\",\n            316: \"cicada\",\n            317: \"leafhopper\",\n            318: \"lacewing\",\n            319: \"dragonfly\",\n            320: \"damselfly\",\n            321: \"red admiral butterfly\",\n            322: \"ringlet butterfly\",\n            323: \"monarch butterfly\",\n            324: \"small white butterfly\",\n            325: \"sulphur butterfly\",\n            326: \"gossamer-winged butterfly\",\n            327: \"starfish\",\n            328: \"sea urchin\",\n            329: \"sea cucumber\",\n            330: \"cottontail rabbit\",\n            331: \"hare\",\n            332: \"Angora rabbit\",\n            333: \"hamster\",\n            334: \"porcupine\",\n            335: \"fox squirrel\",\n            336: \"marmot\",\n            337: \"beaver\",\n            338: \"guinea pig\",\n            339: \"common sorrel horse\",\n            340: \"zebra\",\n            341: \"pig\",\n            342: \"wild boar\",\n            343: \"warthog\",\n            344: \"hippopotamus\",\n            345: \"ox\",\n            346: \"water buffalo\",\n            347: \"bison\",\n            348: \"ram (adult male sheep)\",\n            349: \"bighorn sheep\",\n            350: \"Alpine ibex\",\n            351: \"hartebeest\",\n            352: \"impala (antelope)\",\n            353: \"gazelle\",\n            354: \"arabian camel\",\n            355: \"llama\",\n            356: \"weasel\",\n            357: \"mink\",\n            358: \"European polecat\",\n            359: \"black-footed ferret\",\n            360: \"otter\",\n            361: \"skunk\",\n            362: \"badger\",\n            363: \"armadillo\",\n            364: \"three-toed sloth\",\n            365: \"orangutan\",\n            366: \"gorilla\",\n            367: \"chimpanzee\",\n            368: \"gibbon\",\n            369: \"siamang\",\n            370: \"guenon\",\n            371: \"patas monkey\",\n            372: \"baboon\",\n            373: \"macaque\",\n            374: \"langur\",\n            375: \"black-and-white colobus\",\n            376: \"proboscis monkey\",\n            377: \"marmoset\",\n            378: \"white-headed capuchin\",\n            379: \"howler monkey\",\n            380: \"titi monkey\",\n            381: \"Geoffroy's spider monkey\",\n            382: \"common squirrel monkey\",\n            383: \"ring-tailed lemur\",\n            384: \"indri\",\n            385: \"Asian elephant\",\n            386: \"African bush elephant\",\n            387: \"red panda\",\n            388: \"giant panda\",\n            389: \"snoek fish\",\n            390: \"eel\",\n            391: \"silver salmon\",\n            392: \"rock beauty fish\",\n            393: \"clownfish\",\n            394: \"sturgeon\",\n            395: \"gar fish\",\n            396: \"lionfish\",\n            397: \"pufferfish\",\n            398: \"abacus\",\n            399: \"abaya\",\n            400: \"academic gown\",\n            401: \"accordion\",\n            402: \"acoustic guitar\",\n            403: \"aircraft carrier\",\n            404: \"airliner\",\n            405: \"airship\",\n            406: \"altar\",\n            407: \"ambulance\",\n            408: \"amphibious vehicle\",\n            409: \"analog clock\",\n            410: \"apiary\",\n            411: \"apron\",\n            412: \"trash can\",\n            413: \"assault rifle\",\n            414: \"backpack\",\n            415: \"bakery\",\n            416: \"balance beam\",\n            417: \"balloon\",\n            418: \"ballpoint pen\",\n            419: \"Band-Aid\",\n            420: \"banjo\",\n            421: \"baluster / handrail\",\n            422: \"barbell\",\n            423: \"barber chair\",\n            424: \"barbershop\",\n            425: \"barn\",\n            426: \"barometer\",\n            427: \"barrel\",\n            428: \"wheelbarrow\",\n            429: \"baseball\",\n            430: \"basketball\",\n            431: \"bassinet\",\n            432: \"bassoon\",\n            433: \"swimming cap\",\n            434: \"bath towel\",\n            435: \"bathtub\",\n            436: \"station wagon\",\n            437: \"lighthouse\",\n            438: \"beaker\",\n            439: \"military hat (bearskin or shako)\",\n            440: \"beer bottle\",\n            441: \"beer glass\",\n            442: \"bell tower\",\n            443: \"baby bib\",\n            444: \"tandem bicycle\",\n            445: \"bikini\",\n            446: \"ring binder\",\n            447: \"binoculars\",\n            448: \"birdhouse\",\n            449: \"boathouse\",\n            450: \"bobsleigh\",\n            451: \"bolo tie\",\n            452: \"poke bonnet\",\n            453: \"bookcase\",\n            454: \"bookstore\",\n            455: \"bottle cap\",\n            456: \"hunting bow\",\n            457: \"bow tie\",\n            458: \"brass memorial plaque\",\n            459: \"bra\",\n            460: \"breakwater\",\n            461: \"breastplate\",\n            462: \"broom\",\n            463: \"bucket\",\n            464: \"buckle\",\n            465: \"bulletproof vest\",\n            466: \"high-speed train\",\n            467: \"butcher shop\",\n            468: \"taxicab\",\n            469: \"cauldron\",\n            470: \"candle\",\n            471: \"cannon\",\n            472: \"canoe\",\n            473: \"can opener\",\n            474: \"cardigan\",\n            475: \"car mirror\",\n            476: \"carousel\",\n            477: \"tool kit\",\n            478: \"cardboard box / carton\",\n            479: \"car wheel\",\n            480: \"automated teller machine\",\n            481: \"cassette\",\n            482: \"cassette player\",\n            483: \"castle\",\n            484: \"catamaran\",\n            485: \"CD player\",\n            486: \"cello\",\n            487: \"mobile phone\",\n            488: \"chain\",\n            489: \"chain-link fence\",\n            490: \"chain mail\",\n            491: \"chainsaw\",\n            492: \"storage chest\",\n            493: \"chiffonier\",\n            494: \"bell or wind chime\",\n            495: \"china cabinet\",\n            496: \"Christmas stocking\",\n            497: \"church\",\n            498: \"movie theater\",\n            499: \"cleaver\",\n            500: \"cliff dwelling\",\n            501: \"cloak\",\n            502: \"clogs\",\n            503: \"cocktail shaker\",\n            504: \"coffee mug\",\n            505: \"coffeemaker\",\n            506: \"spiral or coil\",\n            507: \"combination lock\",\n            508: \"computer keyboard\",\n            509: \"candy store\",\n            510: \"container ship\",\n            511: \"convertible\",\n            512: \"corkscrew\",\n            513: \"cornet\",\n            514: \"cowboy boot\",\n            515: \"cowboy hat\",\n            516: \"cradle\",\n            517: \"construction crane\",\n            518: \"crash helmet\",\n            519: \"crate\",\n            520: \"infant bed\",\n            521: \"Crock Pot\",\n            522: \"croquet ball\",\n            523: \"crutch\",\n            524: \"cuirass\",\n            525: \"dam\",\n            526: \"desk\",\n            527: \"desktop computer\",\n            528: \"rotary dial telephone\",\n            529: \"diaper\",\n            530: \"digital clock\",\n            531: \"digital watch\",\n            532: \"dining table\",\n            533: \"dishcloth\",\n            534: \"dishwasher\",\n            535: \"disc brake\",\n            536: \"dock\",\n            537: \"dog sled\",\n            538: \"dome\",\n            539: \"doormat\",\n            540: \"drilling rig\",\n            541: \"drum\",\n            542: \"drumstick\",\n            543: \"dumbbell\",\n            544: \"Dutch oven\",\n            545: \"electric fan\",\n            546: \"electric guitar\",\n            547: \"electric locomotive\",\n            548: \"entertainment center\",\n            549: \"envelope\",\n            550: \"espresso machine\",\n            551: \"face powder\",\n            552: \"feather boa\",\n            553: \"filing cabinet\",\n            554: \"fireboat\",\n            555: \"fire truck\",\n            556: \"fire screen\",\n            557: \"flagpole\",\n            558: \"flute\",\n            559: \"folding chair\",\n            560: \"football helmet\",\n            561: \"forklift\",\n            562: \"fountain\",\n            563: \"fountain pen\",\n            564: \"four-poster bed\",\n            565: \"freight car\",\n            566: \"French horn\",\n            567: \"frying pan\",\n            568: \"fur coat\",\n            569: \"garbage truck\",\n            570: \"gas mask or respirator\",\n            571: \"gas pump\",\n            572: \"goblet\",\n            573: \"go-kart\",\n            574: \"golf ball\",\n            575: \"golf cart\",\n            576: \"gondola\",\n            577: \"gong\",\n            578: \"gown\",\n            579: \"grand piano\",\n            580: \"greenhouse\",\n            581: \"radiator grille\",\n            582: \"grocery store\",\n            583: \"guillotine\",\n            584: \"hair clip\",\n            585: \"hair spray\",\n            586: \"half-track\",\n            587: \"hammer\",\n            588: \"hamper\",\n            589: \"hair dryer\",\n            590: \"hand-held computer\",\n            591: \"handkerchief\",\n            592: \"hard disk drive\",\n            593: \"harmonica\",\n            594: \"harp\",\n            595: \"combine harvester\",\n            596: \"hatchet\",\n            597: \"holster\",\n            598: \"home theater\",\n            599: \"honeycomb\",\n            600: \"hook\",\n            601: \"hoop skirt\",\n            602: \"gymnastic horizontal bar\",\n            603: \"horse-drawn vehicle\",\n            604: \"hourglass\",\n            605: \"iPod\",\n            606: \"clothes iron\",\n            607: \"carved pumpkin\",\n            608: \"jeans\",\n            609: \"jeep\",\n            610: \"T-shirt\",\n            611: \"jigsaw puzzle\",\n            612: \"rickshaw\",\n            613: \"joystick\",\n            614: \"kimono\",\n            615: \"knee pad\",\n            616: \"knot\",\n            617: \"lab coat\",\n            618: \"ladle\",\n            619: \"lampshade\",\n            620: \"laptop computer\",\n            621: \"lawn mower\",\n            622: \"lens cap\",\n            623: \"letter opener\",\n            624: \"library\",\n            625: \"lifeboat\",\n            626: \"lighter\",\n            627: \"limousine\",\n            628: \"ocean liner\",\n            629: \"lipstick\",\n            630: \"slip-on shoe\",\n            631: \"lotion\",\n            632: \"music speaker\",\n            633: \"loupe magnifying glass\",\n            634: \"sawmill\",\n            635: \"magnetic compass\",\n            636: \"messenger bag\",\n            637: \"mailbox\",\n            638: \"tights\",\n            639: \"one-piece bathing suit\",\n            640: \"manhole cover\",\n            641: \"maraca\",\n            642: \"marimba\",\n            643: \"mask\",\n            644: \"matchstick\",\n            645: \"maypole\",\n            646: \"maze\",\n            647: \"measuring cup\",\n            648: \"medicine cabinet\",\n            649: \"megalith\",\n            650: \"microphone\",\n            651: \"microwave oven\",\n            652: \"military uniform\",\n            653: \"milk can\",\n            654: \"minibus\",\n            655: \"miniskirt\",\n            656: \"minivan\",\n            657: \"missile\",\n            658: \"mitten\",\n            659: \"mixing bowl\",\n            660: \"mobile home\",\n            661: \"ford model t\",\n            662: \"modem\",\n            663: \"monastery\",\n            664: \"monitor\",\n            665: \"moped\",\n            666: \"mortar and pestle\",\n            667: \"graduation cap\",\n            668: \"mosque\",\n            669: \"mosquito net\",\n            670: \"vespa\",\n            671: \"mountain bike\",\n            672: \"tent\",\n            673: \"computer mouse\",\n            674: \"mousetrap\",\n            675: \"moving van\",\n            676: \"muzzle\",\n            677: \"metal nail\",\n            678: \"neck brace\",\n            679: \"necklace\",\n            680: \"baby pacifier\",\n            681: \"notebook computer\",\n            682: \"obelisk\",\n            683: \"oboe\",\n            684: \"ocarina\",\n            685: \"odometer\",\n            686: \"oil filter\",\n            687: \"pipe organ\",\n            688: \"oscilloscope\",\n            689: \"overskirt\",\n            690: \"bullock cart\",\n            691: \"oxygen mask\",\n            692: \"product packet / packaging\",\n            693: \"paddle\",\n            694: \"paddle wheel\",\n            695: \"padlock\",\n            696: \"paintbrush\",\n            697: \"pajamas\",\n            698: \"palace\",\n            699: \"pan flute\",\n            700: \"paper towel\",\n            701: \"parachute\",\n            702: \"parallel bars\",\n            703: \"park bench\",\n            704: \"parking meter\",\n            705: \"railroad car\",\n            706: \"patio\",\n            707: \"payphone\",\n            708: \"pedestal\",\n            709: \"pencil case\",\n            710: \"pencil sharpener\",\n            711: \"perfume\",\n            712: \"Petri dish\",\n            713: \"photocopier\",\n            714: \"plectrum\",\n            715: \"Pickelhaube\",\n            716: \"picket fence\",\n            717: \"pickup truck\",\n            718: \"pier\",\n            719: \"piggy bank\",\n            720: \"pill bottle\",\n            721: \"pillow\",\n            722: \"ping-pong ball\",\n            723: \"pinwheel\",\n            724: \"pirate ship\",\n            725: \"drink pitcher\",\n            726: \"block plane\",\n            727: \"planetarium\",\n            728: \"plastic bag\",\n            729: \"plate rack\",\n            730: \"farm plow\",\n            731: \"plunger\",\n            732: \"Polaroid camera\",\n            733: \"pole\",\n            734: \"police van\",\n            735: \"poncho\",\n            736: \"pool table\",\n            737: \"soda bottle\",\n            738: \"plant pot\",\n            739: \"potter's wheel\",\n            740: \"power drill\",\n            741: \"prayer rug\",\n            742: \"printer\",\n            743: \"prison\",\n            744: \"missile\",\n            745: \"projector\",\n            746: \"hockey puck\",\n            747: \"punching bag\",\n            748: \"purse\",\n            749: \"quill\",\n            750: \"quilt\",\n            751: \"race car\",\n            752: \"racket\",\n            753: \"radiator\",\n            754: \"radio\",\n            755: \"radio telescope\",\n            756: \"rain barrel\",\n            757: \"recreational vehicle\",\n            758: \"fishing casting reel\",\n            759: \"reflex camera\",\n            760: \"refrigerator\",\n            761: \"remote control\",\n            762: \"restaurant\",\n            763: \"revolver\",\n            764: \"rifle\",\n            765: \"rocking chair\",\n            766: \"rotisserie\",\n            767: \"eraser\",\n            768: \"rugby ball\",\n            769: \"ruler measuring stick\",\n            770: \"sneaker\",\n            771: \"safe\",\n            772: \"safety pin\",\n            773: \"salt shaker\",\n            774: \"sandal\",\n            775: \"sarong\",\n            776: \"saxophone\",\n            777: \"scabbard\",\n            778: \"weighing scale\",\n            779: \"school bus\",\n            780: \"schooner\",\n            781: \"scoreboard\",\n            782: \"CRT monitor\",\n            783: \"screw\",\n            784: \"screwdriver\",\n            785: \"seat belt\",\n            786: \"sewing machine\",\n            787: \"shield\",\n            788: \"shoe store\",\n            789: \"shoji screen / room divider\",\n            790: \"shopping basket\",\n            791: \"shopping cart\",\n            792: \"shovel\",\n            793: \"shower cap\",\n            794: \"shower curtain\",\n            795: \"ski\",\n            796: \"balaclava ski mask\",\n            797: \"sleeping bag\",\n            798: \"slide rule\",\n            799: \"sliding door\",\n            800: \"slot machine\",\n            801: \"snorkel\",\n            802: \"snowmobile\",\n            803: \"snowplow\",\n            804: \"soap dispenser\",\n            805: \"soccer ball\",\n            806: \"sock\",\n            807: \"solar thermal collector\",\n            808: \"sombrero\",\n            809: \"soup bowl\",\n            810: \"keyboard space bar\",\n            811: \"space heater\",\n            812: \"space shuttle\",\n            813: \"spatula\",\n            814: \"motorboat\",\n            815: \"spider web\",\n            816: \"spindle\",\n            817: \"sports car\",\n            818: \"spotlight\",\n            819: \"stage\",\n            820: \"steam locomotive\",\n            821: \"through arch bridge\",\n            822: \"steel drum\",\n            823: \"stethoscope\",\n            824: \"scarf\",\n            825: \"stone wall\",\n            826: \"stopwatch\",\n            827: \"stove\",\n            828: \"strainer\",\n            829: \"tram\",\n            830: \"stretcher\",\n            831: \"couch\",\n            832: \"stupa\",\n            833: \"submarine\",\n            834: \"suit\",\n            835: \"sundial\",\n            836: \"sunglasses\",\n            837: \"sunglasses\",\n            838: \"sunscreen\",\n            839: \"suspension bridge\",\n            840: \"mop\",\n            841: \"sweatshirt\",\n            842: \"swim trunks / shorts\",\n            843: \"swing\",\n            844: \"electrical switch\",\n            845: \"syringe\",\n            846: \"table lamp\",\n            847: \"tank\",\n            848: \"tape player\",\n            849: \"teapot\",\n            850: \"teddy bear\",\n            851: \"television\",\n            852: \"tennis ball\",\n            853: \"thatched roof\",\n            854: \"front curtain\",\n            855: \"thimble\",\n            856: \"threshing machine\",\n            857: \"throne\",\n            858: \"tile roof\",\n            859: \"toaster\",\n            860: \"tobacco shop\",\n            861: \"toilet seat\",\n            862: \"torch\",\n            863: \"totem pole\",\n            864: \"tow truck\",\n            865: \"toy store\",\n            866: \"tractor\",\n            867: \"semi-trailer truck\",\n            868: \"tray\",\n            869: \"trench coat\",\n            870: \"tricycle\",\n            871: \"trimaran\",\n            872: \"tripod\",\n            873: \"triumphal arch\",\n            874: \"trolleybus\",\n            875: \"trombone\",\n            876: \"hot tub\",\n            877: \"turnstile\",\n            878: \"typewriter keyboard\",\n            879: \"umbrella\",\n            880: \"unicycle\",\n            881: \"upright piano\",\n            882: \"vacuum cleaner\",\n            883: \"vase\",\n            884: \"vaulted or arched ceiling\",\n            885: \"velvet fabric\",\n            886: \"vending machine\",\n            887: \"vestment\",\n            888: \"viaduct\",\n            889: \"violin\",\n            890: \"volleyball\",\n            891: \"waffle iron\",\n            892: \"wall clock\",\n            893: \"wallet\",\n            894: \"wardrobe\",\n            895: \"military aircraft\",\n            896: \"sink\",\n            897: \"washing machine\",\n            898: \"water bottle\",\n            899: \"water jug\",\n            900: \"water tower\",\n            901: \"whiskey jug\",\n            902: \"whistle\",\n            903: \"hair wig\",\n            904: \"window screen\",\n            905: \"window shade\",\n            906: \"Windsor tie\",\n            907: \"wine bottle\",\n            908: \"airplane wing\",\n            909: \"wok\",\n            910: \"wooden spoon\",\n            911: \"wool\",\n            912: \"split-rail fence\",\n            913: \"shipwreck\",\n            914: \"sailboat\",\n            915: \"yurt\",\n            916: \"website\",\n            917: \"comic book\",\n            918: \"crossword\",\n            919: \"traffic or street sign\",\n            920: \"traffic light\",\n            921: \"dust jacket\",\n            922: \"menu\",\n            923: \"plate\",\n            924: \"guacamole\",\n            925: \"consomme\",\n            926: \"hot pot\",\n            927: \"trifle\",\n            928: \"ice cream\",\n            929: \"popsicle\",\n            930: \"baguette\",\n            931: \"bagel\",\n            932: \"pretzel\",\n            933: \"cheeseburger\",\n            934: \"hot dog\",\n            935: \"mashed potatoes\",\n            936: \"cabbage\",\n            937: \"broccoli\",\n            938: \"cauliflower\",\n            939: \"zucchini\",\n            940: \"spaghetti squash\",\n            941: \"acorn squash\",\n            942: \"butternut squash\",\n            943: \"cucumber\",\n            944: \"artichoke\",\n            945: \"bell pepper\",\n            946: \"cardoon\",\n            947: \"mushroom\",\n            948: \"Granny Smith apple\",\n            949: \"strawberry\",\n            950: \"orange\",\n            951: \"lemon\",\n            952: \"fig\",\n            953: \"pineapple\",\n            954: \"banana\",\n            955: \"jackfruit\",\n            956: \"cherimoya (custard apple)\",\n            957: \"pomegranate\",\n            958: \"hay\",\n            959: \"carbonara\",\n            960: \"chocolate syrup\",\n            961: \"dough\",\n            962: \"meatloaf\",\n            963: \"pizza\",\n            964: \"pot pie\",\n            965: \"burrito\",\n            966: \"red wine\",\n            967: \"espresso\",\n            968: \"tea cup\",\n            969: \"eggnog\",\n            970: \"mountain\",\n            971: \"bubble\",\n            972: \"cliff\",\n            973: \"coral reef\",\n            974: \"geyser\",\n            975: \"lakeshore\",\n            976: \"promontory\",\n            977: \"sandbar\",\n            978: \"beach\",\n            979: \"valley\",\n            980: \"volcano\",\n            981: \"baseball player\",\n            982: \"bridegroom\",\n            983: \"scuba diver\",\n            984: \"rapeseed\",\n            985: \"daisy\",\n            986: \"yellow lady's slipper\",\n            987: \"corn\",\n            988: \"acorn\",\n            989: \"rose hip\",\n            990: \"horse chestnut seed\",\n            991: \"coral fungus\",\n            992: \"agaric\",\n            993: \"gyromitra\",\n            994: \"stinkhorn mushroom\",\n            995: \"earth star fungus\",\n            996: \"hen of the woods mushroom\",\n            997: \"bolete\",\n            998: \"corn cob\",\n            999: \"toilet paper\",\n        }\n</code></pre>"},{"location":"api/#mmlearn.datasets.ImageNet.zero_shot_prompt_templates","title":"zero_shot_prompt_templates  <code>property</code>","text":"<pre><code>zero_shot_prompt_templates\n</code></pre> <p>Return the zero-shot prompt templates.</p>"},{"location":"api/#mmlearn.datasets.ImageNet.id2label","title":"id2label  <code>property</code>","text":"<pre><code>id2label\n</code></pre> <p>Return the label mapping.</p>"},{"location":"api/#mmlearn.datasets.ImageNet.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(index)\n</code></pre> <p>Get an example at the given index.</p> Source code in <code>mmlearn/datasets/imagenet.py</code> <pre><code>def __getitem__(self, index: int) -&gt; Example:\n    \"\"\"Get an example at the given index.\"\"\"\n    image, target = super().__getitem__(index)\n    example = Example(\n        {\n            Modalities.RGB.name: image,\n            Modalities.RGB.target: target,\n            EXAMPLE_INDEX_KEY: index,\n        }\n    )\n    mask = self.mask_generator() if self.mask_generator else None\n    if mask is not None:  # error will be raised during collation if `None`\n        example[Modalities.RGB.mask] = mask\n    return example\n</code></pre>"},{"location":"api/#mmlearn.datasets.LibriSpeech","title":"LibriSpeech","text":"<p>               Bases: <code>Dataset[Example]</code></p> <p>LibriSpeech dataset.</p> <p>This is a wrapper around class:<code>torchaudio.datasets.LIBRISPEECH</code> that assumes that the dataset is already downloaded and the top-level directory of the dataset in the root directory is <code>librispeech</code>.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>Root directory of dataset.</p> required <code>split</code> <code>(train - clean - 100, train - clean - 360, train - other - 500, dev - clean, dev - other, test - clean, test - other)</code> <p>Split of the dataset to use.</p> <code>\"train-clean-100\"</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If <code>torchaudio</code> is not installed.</p> Notes <p>This dataset only returns the audio and transcript from the dataset.</p> Source code in <code>mmlearn/datasets/librispeech.py</code> <pre><code>@store(\n    group=\"datasets\",\n    provider=\"mmlearn\",\n    root_dir=os.getenv(\"LIBRISPEECH_ROOT_DIR\", MISSING),\n)\nclass LibriSpeech(Dataset[Example]):\n    \"\"\"LibriSpeech dataset.\n\n    This is a wrapper around :py:class:`torchaudio.datasets.LIBRISPEECH` that assumes\n    that the dataset is already downloaded and the top-level directory of the dataset\n    in the root directory is `librispeech`.\n\n    Parameters\n    ----------\n    root_dir : str\n        Root directory of dataset.\n    split : {\"train-clean-100\", \"train-clean-360\", \"train-other-500\", \"dev-clean\", \"dev-other\", \"test-clean\", \"test-other\"}, default=\"train-clean-100\"\n        Split of the dataset to use.\n\n    Raises\n    ------\n    ImportError\n        If ``torchaudio`` is not installed.\n\n    Notes\n    -----\n    This dataset only returns the audio and transcript from the dataset.\n\n    \"\"\"  # noqa: W505\n\n    def __init__(self, root_dir: str, split: str = \"train-clean-100\") -&gt; None:\n        super().__init__()\n        if not _TORCHAUDIO_AVAILABLE:\n            raise ImportError(\n                \"LibriSpeech dataset requires `torchaudio`, which is not installed.\"\n            )\n        from torchaudio.datasets import LIBRISPEECH\n\n        self.dataset = LIBRISPEECH(\n            root=root_dir,\n            url=split,\n            download=False,\n            folder_in_archive=\"librispeech\",\n        )\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the length of the dataset.\"\"\"\n        return len(self.dataset)\n\n    def __getitem__(self, idx: int) -&gt; Example:\n        \"\"\"Return an example from the dataset.\"\"\"\n        waveform, sample_rate, transcript, _, _, _ = self.dataset[idx]\n        assert sample_rate == SAMPLE_RATE, (\n            f\"Expected sample rate to be `16000`, got {sample_rate}.\"\n        )\n        waveform = pad_or_trim(waveform.flatten())\n\n        return Example(\n            {\n                Modalities.AUDIO.name: waveform,\n                Modalities.TEXT.name: transcript,\n                EXAMPLE_INDEX_KEY: idx,\n            },\n        )\n</code></pre>"},{"location":"api/#mmlearn.datasets.LibriSpeech.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the length of the dataset.</p> Source code in <code>mmlearn/datasets/librispeech.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the length of the dataset.\"\"\"\n    return len(self.dataset)\n</code></pre>"},{"location":"api/#mmlearn.datasets.LibriSpeech.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> <p>Return an example from the dataset.</p> Source code in <code>mmlearn/datasets/librispeech.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; Example:\n    \"\"\"Return an example from the dataset.\"\"\"\n    waveform, sample_rate, transcript, _, _, _ = self.dataset[idx]\n    assert sample_rate == SAMPLE_RATE, (\n        f\"Expected sample rate to be `16000`, got {sample_rate}.\"\n    )\n    waveform = pad_or_trim(waveform.flatten())\n\n    return Example(\n        {\n            Modalities.AUDIO.name: waveform,\n            Modalities.TEXT.name: transcript,\n            EXAMPLE_INDEX_KEY: idx,\n        },\n    )\n</code></pre>"},{"location":"api/#mmlearn.datasets.LLVIPDataset","title":"LLVIPDataset","text":"<p>               Bases: <code>Dataset[Example]</code></p> <p>Low-Light Visible-Infrared Pair (LLVIP) dataset.</p> <p>Loads pairs of <code>RGB</code> and <code>THERMAL</code> images from the LLVIP dataset.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>Path to the root directory of the dataset. The directory should contain 'visible' and 'infrared' subdirectories.</p> required <code>train</code> <code>bool</code> <p>Flag to indicate whether to load the training or test set.</p> <code>True</code> <code>transform</code> <code>Optional[Callable[[Image], Tensor]]</code> <p>A callable that takes in a PIL image and returns a transformed version of the image as a PyTorch tensor. This is applied to both RGB and thermal images.</p> <code>None</code> Source code in <code>mmlearn/datasets/llvip.py</code> <pre><code>@store(\n    name=\"LLVIP\",\n    group=\"datasets\",\n    provider=\"mmlearn\",\n    root_dir=os.getenv(\"LLVIP_ROOT_DIR\", MISSING),\n)\nclass LLVIPDataset(Dataset[Example]):\n    \"\"\"Low-Light Visible-Infrared Pair (LLVIP) dataset.\n\n    Loads pairs of `RGB` and `THERMAL` images from the LLVIP dataset.\n\n    Parameters\n    ----------\n    root_dir : str\n        Path to the root directory of the dataset. The directory should contain\n        'visible' and 'infrared' subdirectories.\n    train : bool, default=True\n        Flag to indicate whether to load the training or test set.\n    transform : Optional[Callable[[PIL.Image], torch.Tensor]], optional, default=None\n        A callable that takes in a PIL image and returns a transformed version\n        of the image as a PyTorch tensor. This is applied to both RGB and thermal\n        images.\n    \"\"\"\n\n    def __init__(\n        self,\n        root_dir: str,\n        train: bool = True,\n        transform: Optional[Callable[[PILImage], torch.Tensor]] = None,\n    ):\n        self.path_images_rgb = os.path.join(\n            root_dir,\n            \"visible\",\n            \"train\" if train else \"test\",\n        )\n        self.path_images_ir = os.path.join(\n            root_dir, \"infrared\", \"train\" if train else \"test\"\n        )\n        self.train = train\n        self.transform = transform or transforms.ToTensor()\n\n        self.rgb_images = sorted(glob.glob(os.path.join(self.path_images_rgb, \"*.jpg\")))\n        self.ir_images = sorted(glob.glob(os.path.join(self.path_images_ir, \"*.jpg\")))\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the length of the dataset.\"\"\"\n        return len(self.rgb_images)\n\n    def __getitem__(self, idx: int) -&gt; Example:\n        \"\"\"Return an example from the dataset.\"\"\"\n        rgb_image_path = self.rgb_images[idx]\n        ir_image_path = self.ir_images[idx]\n\n        rgb_image = PILImage.open(rgb_image_path).convert(\"RGB\")\n        ir_image = PILImage.open(ir_image_path).convert(\"L\")\n\n        example = Example(\n            {\n                Modalities.RGB.name: self.transform(rgb_image),\n                Modalities.THERMAL.name: self.transform(ir_image),\n                EXAMPLE_INDEX_KEY: idx,\n            },\n        )\n\n        if self.train:\n            annot_path = (\n                rgb_image_path.replace(\"visible\", \"Annotations\")\n                .replace(\".jpg\", \".xml\")\n                .replace(\"train\", \"\")\n            )\n            annot = self._get_bbox(annot_path)\n            example[\"annotation\"] = {\n                \"bboxes\": torch.from_numpy(annot[\"bboxes\"]),\n                \"labels\": torch.from_numpy(annot[\"labels\"]),\n            }\n        return example\n\n    def _get_bbox(self, filename: str) -&gt; dict[str, np.ndarray]:\n        \"\"\"Parse the XML file to get bounding boxes and labels.\n\n        Parameters\n        ----------\n        filename : str\n            Path to the annotation XML file.\n\n        Returns\n        -------\n        dict\n            A dictionary containing bounding boxes and labels.\n        \"\"\"\n        try:\n            root = ET.parse(filename).getroot()\n\n            bboxes, labels = [], []\n            for obj in root.findall(\"object\"):\n                bbox_obj = obj.find(\"bndbox\")\n                bbox = [\n                    int(bbox_obj.find(dim).text)  # type: ignore[union-attr,arg-type]\n                    for dim in [\"xmin\", \"ymin\", \"xmax\", \"ymax\"]\n                ]\n                bboxes.append(bbox)\n                labels.append(1)  # Assuming 'person' is the only label\n            return {\n                \"bboxes\": np.array(bboxes).astype(\"float\"),\n                \"labels\": np.array(labels).astype(\"int\"),\n            }\n        except ET.ParseError as e:\n            raise ValueError(f\"Error parsing XML: {e}\") from None\n        except Exception as e:\n            raise RuntimeError(\n                f\"Error processing annotation file {filename}: {e}\",\n            ) from None\n</code></pre>"},{"location":"api/#mmlearn.datasets.LLVIPDataset.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the length of the dataset.</p> Source code in <code>mmlearn/datasets/llvip.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the length of the dataset.\"\"\"\n    return len(self.rgb_images)\n</code></pre>"},{"location":"api/#mmlearn.datasets.LLVIPDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> <p>Return an example from the dataset.</p> Source code in <code>mmlearn/datasets/llvip.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; Example:\n    \"\"\"Return an example from the dataset.\"\"\"\n    rgb_image_path = self.rgb_images[idx]\n    ir_image_path = self.ir_images[idx]\n\n    rgb_image = PILImage.open(rgb_image_path).convert(\"RGB\")\n    ir_image = PILImage.open(ir_image_path).convert(\"L\")\n\n    example = Example(\n        {\n            Modalities.RGB.name: self.transform(rgb_image),\n            Modalities.THERMAL.name: self.transform(ir_image),\n            EXAMPLE_INDEX_KEY: idx,\n        },\n    )\n\n    if self.train:\n        annot_path = (\n            rgb_image_path.replace(\"visible\", \"Annotations\")\n            .replace(\".jpg\", \".xml\")\n            .replace(\"train\", \"\")\n        )\n        annot = self._get_bbox(annot_path)\n        example[\"annotation\"] = {\n            \"bboxes\": torch.from_numpy(annot[\"bboxes\"]),\n            \"labels\": torch.from_numpy(annot[\"labels\"]),\n        }\n    return example\n</code></pre>"},{"location":"api/#mmlearn.datasets.NIHCXR","title":"NIHCXR","text":"<p>               Bases: <code>Dataset[Example]</code></p> <p>NIH Chest X-ray dataset.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>Directory which contains <code>.json</code> files stating all dataset entries.</p> required <code>split</code> <code>(train, test, bbox)</code> <p>Dataset split. \"bbox\" is a subset of \"test\" which contains bounding box info.</p> <code>\"train\"</code> <code>transform</code> <code>Optional[Callable[[Image], Tensor]]</code> <p>A callable that takes in a PIL image and returns a transformed version of the image as a PyTorch tensor.</p> <code>None</code> Source code in <code>mmlearn/datasets/nihcxr.py</code> <pre><code>@store(\n    group=\"datasets\",\n    provider=\"mmlearn\",\n    root_dir=os.getenv(\"NIH_CXR_DIR\", MISSING),\n    split=\"train\",\n)\nclass NIHCXR(Dataset[Example]):\n    \"\"\"NIH Chest X-ray dataset.\n\n    Parameters\n    ----------\n    root_dir : str\n        Directory which contains `.json` files stating all dataset entries.\n    split : {\"train\", \"test\", \"bbox\"}\n        Dataset split. \"bbox\" is a subset of \"test\" which contains bounding box info.\n    transform : Optional[Callable[[PIL.Image], torch.Tensor]], optional, default=None\n        A callable that takes in a PIL image and returns a transformed version\n        of the image as a PyTorch tensor.\n    \"\"\"\n\n    def __init__(\n        self,\n        root_dir: str,\n        split: Literal[\"train\", \"test\", \"bbox\"],\n        transform: Optional[Callable[[Image.Image], torch.Tensor]] = None,\n    ) -&gt; None:\n        assert split in [\"train\", \"test\", \"bbox\"], f\"split {split} is not available.\"\n        assert callable(transform) or transform is None, (\n            \"transform is not callable or None.\"\n        )\n\n        data_path = os.path.join(root_dir, split + \"_data.json\")\n\n        assert os.path.isfile(data_path), f\"entries file does not exist: {data_path}.\"\n\n        with open(data_path, \"rb\") as file:\n            entries = json.load(file)\n        self.entries = entries\n\n        if transform is not None:\n            self.transform = transform\n        else:\n            self.transform = Compose([Resize(224), CenterCrop(224), ToTensor()])\n\n        self.bbox = split == \"bbox\"\n\n    def __getitem__(self, idx: int) -&gt; Example:\n        \"\"\"Return image-label or image-label-tabular(bbox).\"\"\"\n        entry = self.entries[idx]\n        image = Image.open(entry[\"image_path\"]).convert(\"RGB\")\n        image = self.transform(image)\n        label = torch.tensor(entry[\"label\"])\n\n        example = Example(\n            {\n                Modalities.RGB.name: image,\n                Modalities.RGB.target: label,\n                \"qid\": entry[\"qid\"],\n                EXAMPLE_INDEX_KEY: idx,\n            }\n        )\n\n        if self.bbox:\n            example[\"bbox\"] = entry[\"bbox\"]\n\n        return example\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the length of the dataset.\"\"\"\n        return len(self.entries)\n</code></pre>"},{"location":"api/#mmlearn.datasets.NIHCXR.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> <p>Return image-label or image-label-tabular(bbox).</p> Source code in <code>mmlearn/datasets/nihcxr.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; Example:\n    \"\"\"Return image-label or image-label-tabular(bbox).\"\"\"\n    entry = self.entries[idx]\n    image = Image.open(entry[\"image_path\"]).convert(\"RGB\")\n    image = self.transform(image)\n    label = torch.tensor(entry[\"label\"])\n\n    example = Example(\n        {\n            Modalities.RGB.name: image,\n            Modalities.RGB.target: label,\n            \"qid\": entry[\"qid\"],\n            EXAMPLE_INDEX_KEY: idx,\n        }\n    )\n\n    if self.bbox:\n        example[\"bbox\"] = entry[\"bbox\"]\n\n    return example\n</code></pre>"},{"location":"api/#mmlearn.datasets.NIHCXR.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the length of the dataset.</p> Source code in <code>mmlearn/datasets/nihcxr.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the length of the dataset.\"\"\"\n    return len(self.entries)\n</code></pre>"},{"location":"api/#mmlearn.datasets.NYUv2Dataset","title":"NYUv2Dataset","text":"<p>               Bases: <code>Dataset[Example]</code></p> <p>NYUv2 dataset.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>Path to the root directory of the dataset.</p> required <code>split</code> <code>(train, test)</code> <p>Split of the dataset to use.</p> <code>\"train\"</code> <code>return_type</code> <code>(disparity, image)</code> <p>Return type of the depth images.</p> <ul> <li><code>\"disparity\"</code>: Return the depth image as disparity map.</li> <li><code>\"image\"</code>: Return the depth image as a 3-channel image.</li> </ul> <code>\"disparity\"</code> <code>rgb_transform</code> <code>Optional[Callable[[Image], Tensor]]</code> <p>A callable that takes in an RGB PIL image and returns a transformed version of the image as a PyTorch tensor.</p> <code>None</code> <code>depth_transform</code> <code>Optional[Callable[[Image], Tensor]]</code> <p>A callable that takes in a depth PIL image and returns a transformed version of the image as a PyTorch tensor.</p> <code>None</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If <code>opencv-python</code> is not installed.</p> Source code in <code>mmlearn/datasets/nyuv2.py</code> <pre><code>@store(\n    name=\"NYUv2\",\n    group=\"datasets\",\n    provider=\"mmlearn\",\n    root_dir=os.getenv(\"NYUV2_ROOT_DIR\", MISSING),\n)\nclass NYUv2Dataset(Dataset[Example]):\n    \"\"\"NYUv2 dataset.\n\n    Parameters\n    ----------\n    root_dir : str\n        Path to the root directory of the dataset.\n    split : {\"train\", \"test\"}, default=\"train\"\n        Split of the dataset to use.\n    return_type : {\"disparity\", \"image\"}, default=\"disparity\"\n        Return type of the depth images.\n\n        - `\"disparity\"`: Return the depth image as disparity map.\n        - `\"image\"`: Return the depth image as a 3-channel image.\n    rgb_transform: Callable[[PIL.Image], torch.Tensor], default=None\n        A callable that takes in an RGB PIL image and returns a transformed version\n        of the image as a PyTorch tensor.\n    depth_transform: Callable[[PIL.Image], torch.Tensor], default=None\n        A callable that takes in a depth PIL image and returns a transformed version\n        of the image as a PyTorch tensor.\n\n    Raises\n    ------\n    ImportError\n        If `opencv-python` is not installed.\n    \"\"\"\n\n    def __init__(\n        self,\n        root_dir: str,\n        split: Literal[\"train\", \"test\"] = \"train\",\n        return_type: Literal[\"disparity\", \"image\"] = \"disparity\",\n        rgb_transform: Optional[Callable[[PILImage], torch.Tensor]] = None,\n        depth_transform: Optional[Callable[[PILImage], torch.Tensor]] = None,\n    ) -&gt; None:\n        super().__init__()\n        if not _OPENCV_AVAILABLE:\n            raise ImportError(\n                \"NYUv2 dataset requires `opencv-python` which is not installed.\",\n            )\n        self._validate_args(root_dir, split, rgb_transform, depth_transform)\n        self.return_type = return_type\n\n        self.root_dir = root_dir\n        with open(os.path.join(root_dir, f\"{split}.txt\"), \"r\") as f:\n            file_ids = f.readlines()\n        file_ids = [f.strip() for f in file_ids]\n\n        root_dir = os.path.join(root_dir, split)\n        depth_files = [os.path.join(root_dir, \"depth\", f\"{f}.png\") for f in file_ids]\n        rgb_files = [os.path.join(root_dir, \"rgb\", f\"{f}.png\") for f in file_ids]\n\n        label_files = [\n            os.path.join(root_dir, \"scene_class\", f\"{f}.txt\") for f in file_ids\n        ]\n        labels = [str(open(f).read().strip()) for f in label_files]  # noqa: SIM115\n        labels = [label.replace(\"_\", \" \") for label in labels]\n        labels = [\n            _LABELS.index(label) if label in _LABELS else len(_LABELS)  # type: ignore\n            for label in labels\n        ]\n\n        # remove the samples with classes not in _LABELS\n        # this is to follow the same classes used in ImageBind\n        if split == \"test\":\n            valid_indices = [\n                i\n                for i, label in enumerate(labels)\n                if label &lt; len(_LABELS)  # type: ignore\n            ]\n            rgb_files = [rgb_files[i] for i in valid_indices]\n            depth_files = [depth_files[i] for i in valid_indices]\n            labels = [labels[i] for i in valid_indices]\n\n        self.samples = list(zip(rgb_files, depth_files, labels, strict=False))\n\n        self.rgb_transform = rgb_transform\n        self.depth_transform = depth_transform\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the length of the dataset.\"\"\"\n        return len(self.samples)\n\n    def _validate_args(\n        self,\n        root_dir: str,\n        split: str,\n        rgb_transform: Optional[Callable[[PILImage], torch.Tensor]],\n        depth_transform: Optional[Callable[[PILImage], torch.Tensor]],\n    ) -&gt; None:\n        \"\"\"Validate arguments.\"\"\"\n        if not os.path.isdir(root_dir):\n            raise NotADirectoryError(\n                f\"The given `root_dir` {root_dir} is not a directory\",\n            )\n        if split not in [\"train\", \"test\"]:\n            raise ValueError(\n                f\"Expected `split` to be one of `'train'` or `'test'`, but got {split}\",\n            )\n        if rgb_transform is not None and not callable(rgb_transform):\n            raise TypeError(\n                f\"Expected argument `rgb_transform` to be callable, but got {type(rgb_transform)}\",\n            )\n        if depth_transform is not None and not callable(depth_transform):\n            raise TypeError(\n                f\"Expected `depth_transform` to be callable, but got {type(depth_transform)}\",\n            )\n\n    def __getitem__(self, idx: int) -&gt; Example:\n        \"\"\"Return RGB and depth images at index `idx`.\"\"\"\n        # Read images\n        rgb_image = cv2.imread(self.samples[idx][0], cv2.IMREAD_UNCHANGED)\n        if self.rgb_transform is not None:\n            rgb_image = self.rgb_transform(to_pil_image(rgb_image))\n\n        if self.return_type == \"disparity\":\n            depth_image = depth_normalize(\n                self.samples[idx][1],\n            )\n        else:\n            # Using cv2 instead of PIL Image since we use PNG grayscale images.\n            depth_image = cv2.imread(\n                self.samples[idx][1],\n                cv2.IMREAD_GRAYSCALE,\n            )\n            # Make a 3-channel depth image to enable passing to a pretrained ViT.\n            depth_image = np.repeat(depth_image[:, :, np.newaxis], 3, axis=-1)\n\n        if self.depth_transform is not None:\n            depth_image = self.depth_transform(to_pil_image(depth_image))\n\n        return Example(\n            {\n                Modalities.RGB.name: rgb_image,\n                Modalities.DEPTH.name: depth_image,\n                EXAMPLE_INDEX_KEY: idx,\n                Modalities.DEPTH.target: self.samples[idx][2],\n            }\n        )\n</code></pre>"},{"location":"api/#mmlearn.datasets.NYUv2Dataset.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the length of the dataset.</p> Source code in <code>mmlearn/datasets/nyuv2.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the length of the dataset.\"\"\"\n    return len(self.samples)\n</code></pre>"},{"location":"api/#mmlearn.datasets.NYUv2Dataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> <p>Return RGB and depth images at index <code>idx</code>.</p> Source code in <code>mmlearn/datasets/nyuv2.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; Example:\n    \"\"\"Return RGB and depth images at index `idx`.\"\"\"\n    # Read images\n    rgb_image = cv2.imread(self.samples[idx][0], cv2.IMREAD_UNCHANGED)\n    if self.rgb_transform is not None:\n        rgb_image = self.rgb_transform(to_pil_image(rgb_image))\n\n    if self.return_type == \"disparity\":\n        depth_image = depth_normalize(\n            self.samples[idx][1],\n        )\n    else:\n        # Using cv2 instead of PIL Image since we use PNG grayscale images.\n        depth_image = cv2.imread(\n            self.samples[idx][1],\n            cv2.IMREAD_GRAYSCALE,\n        )\n        # Make a 3-channel depth image to enable passing to a pretrained ViT.\n        depth_image = np.repeat(depth_image[:, :, np.newaxis], 3, axis=-1)\n\n    if self.depth_transform is not None:\n        depth_image = self.depth_transform(to_pil_image(depth_image))\n\n    return Example(\n        {\n            Modalities.RGB.name: rgb_image,\n            Modalities.DEPTH.name: depth_image,\n            EXAMPLE_INDEX_KEY: idx,\n            Modalities.DEPTH.target: self.samples[idx][2],\n        }\n    )\n</code></pre>"},{"location":"api/#mmlearn.datasets.SUNRGBDDataset","title":"SUNRGBDDataset","text":"<p>               Bases: <code>Dataset[Example]</code></p> <p>SUN RGB-D dataset.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>Path to the root directory of the dataset.</p> required <code>split</code> <code>(train, test)</code> <p>Split of the dataset to use.</p> <code>\"train\"</code> <code>return_type</code> <code>(disparity, image)</code> <p>Return type of the depth images. If \"disparity\", the depth images are converted to disparity similar to the ImageBind implementation. Otherwise, return the depth image as a 3-channel image.</p> <code>\"disparity\"</code> <code>rgb_transform</code> <code>Optional[Callable[[Image], Tensor]]</code> <p>A callable that takes in an RGB PIL image and returns a transformed version of the image as a PyTorch tensor.</p> <code>None</code> <code>depth_transform</code> <code>Optional[Callable[[Image], Tensor]]</code> <p>A callable that takes in a depth PIL image and returns a transformed version of the image as a PyTorch tensor.</p> <code>None</code> References <p>.. [1] Repo followed to extract the dataset: https://github.com/TUI-NICR/nicr-scene-analysis-datasets</p> Source code in <code>mmlearn/datasets/sunrgbd.py</code> <pre><code>@store(\n    name=\"SUNRGBD\",\n    group=\"datasets\",\n    provider=\"mmlearn\",\n    root_dir=os.getenv(\"SUNRGBD_ROOT_DIR\", MISSING),\n)\nclass SUNRGBDDataset(Dataset[Example]):\n    \"\"\"SUN RGB-D dataset.\n\n    Parameters\n    ----------\n    root_dir : str\n        Path to the root directory of the dataset.\n    split : {\"train\", \"test\"}, default=\"train\"\n        Split of the dataset to use.\n    return_type : {\"disparity\", \"image\"}, default=\"disparity\"\n        Return type of the depth images. If \"disparity\", the depth images are\n        converted to disparity similar to the ImageBind implementation.\n        Otherwise, return the depth image as a 3-channel image.\n    rgb_transform: Callable[[PIL.Image], torch.Tensor], default=None\n        A callable that takes in an RGB PIL image and returns a transformed version\n        of the image as a PyTorch tensor.\n    depth_transform: Callable[[PIL.Image], torch.Tensor], default=None\n        A callable that takes in a depth PIL image and returns a transformed version\n        of the image as a PyTorch tensor.\n\n    References\n    ----------\n    .. [1] Repo followed to extract the dataset: https://github.com/TUI-NICR/nicr-scene-analysis-datasets\n    \"\"\"\n\n    def __init__(\n        self,\n        root_dir: str,\n        split: Literal[\"train\", \"test\"] = \"train\",\n        return_type: Literal[\"disparity\", \"image\"] = \"disparity\",\n        rgb_transform: Optional[Callable[[PILImage], torch.Tensor]] = None,\n        depth_transform: Optional[Callable[[PILImage], torch.Tensor]] = None,\n    ) -&gt; None:\n        super().__init__()\n        if not _OPENCV_AVAILABLE:\n            raise ImportError(\n                \"SUN RGB-D dataset requires `opencv-python` which is not installed.\",\n            )\n\n        self._validate_args(root_dir, split, rgb_transform, depth_transform)\n        self.return_type = return_type\n\n        self.root_dir = root_dir\n        with open(os.path.join(root_dir, f\"{split}.txt\"), \"r\") as f:\n            file_ids = f.readlines()\n        file_ids = [f.strip() for f in file_ids]\n\n        root_dir = os.path.join(root_dir, split)\n        depth_files = [os.path.join(root_dir, \"depth\", f\"{f}.png\") for f in file_ids]\n        rgb_files = [os.path.join(root_dir, \"rgb\", f\"{f}.jpg\") for f in file_ids]\n        intrinsic_files = [\n            os.path.join(root_dir, \"intrinsics\", f\"{f}.txt\") for f in file_ids\n        ]\n\n        sensor_types = [\n            file.removeprefix(os.path.join(root_dir, \"depth\")).split(os.sep)[1]\n            for file in depth_files\n        ]\n\n        label_files = [\n            os.path.join(root_dir, \"scene_class\", f\"{f}.txt\") for f in file_ids\n        ]\n        labels = []\n        for label_file in label_files:\n            with open(label_file, \"r\") as file:  # noqa: SIM115\n                labels.append(file.read().strip())\n        labels = [label.replace(\"_\", \" \") for label in labels]\n        labels = [\n            _LABELS.index(label) if label in _LABELS else len(_LABELS)  # type: ignore\n            for label in labels\n        ]\n\n        # remove the samples with classes not in _LABELS\n        # this is to follow the same classes used in ImageBind\n        if split == \"test\":\n            valid_indices = [\n                i\n                for i, label in enumerate(labels)\n                if label &lt; len(_LABELS)  # type: ignore\n            ]\n            rgb_files = [rgb_files[i] for i in valid_indices]\n            depth_files = [depth_files[i] for i in valid_indices]\n            labels = [labels[i] for i in valid_indices]\n            intrinsic_files = [intrinsic_files[i] for i in valid_indices]\n            sensor_types = [sensor_types[i] for i in valid_indices]\n\n        self.samples = list(\n            zip(\n                rgb_files,\n                depth_files,\n                labels,\n                intrinsic_files,\n                sensor_types,\n                strict=False,\n            )\n        )\n\n        self.rgb_transform = rgb_transform\n        self.depth_transform = depth_transform\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the length of the dataset.\"\"\"\n        return len(self.samples)\n\n    def _validate_args(\n        self,\n        root_dir: str,\n        split: str,\n        rgb_transform: Optional[Callable[[PILImage], torch.Tensor]],\n        depth_transform: Optional[Callable[[PILImage], torch.Tensor]],\n    ) -&gt; None:\n        \"\"\"Validate arguments.\"\"\"\n        if not os.path.isdir(root_dir):\n            raise NotADirectoryError(\n                f\"The given `root_dir` {root_dir} is not a directory\",\n            )\n        if split not in [\"train\", \"test\"]:\n            raise ValueError(\n                f\"Expected `split` to be one of `'train'` or `'test'`, but got {split}\",\n            )\n        if rgb_transform is not None and not callable(rgb_transform):\n            raise TypeError(\n                f\"Expected argument `rgb_transform` to be callable, but got {type(rgb_transform)}\",\n            )\n        if depth_transform is not None and not callable(depth_transform):\n            raise TypeError(\n                f\"Expected `depth_transform` to be callable, but got {type(depth_transform)}\",\n            )\n\n    def __getitem__(self, idx: int) -&gt; Example:\n        \"\"\"Return RGB and depth images at index `idx`.\"\"\"\n        # Read images\n        rgb_image = cv2.imread(self.samples[idx][0], cv2.IMREAD_UNCHANGED)\n        if self.rgb_transform is not None:\n            rgb_image = self.rgb_transform(to_pil_image(rgb_image))\n\n        if self.return_type == \"disparity\":\n            depth_image = convert_depth_to_disparity(\n                self.samples[idx][1],\n                self.samples[idx][3],\n                self.samples[idx][4],\n            )\n        else:\n            # Using cv2 instead of PIL Image since we use PNG grayscale images.\n            depth_image = cv2.imread(\n                self.samples[idx][1],\n                cv2.IMREAD_GRAYSCALE,\n            )\n            # Make a 3-channel depth image to enable passing to a pretrained ViT.\n            depth_image = np.repeat(depth_image[:, :, np.newaxis], 3, axis=-1)\n\n        if self.depth_transform is not None:\n            depth_image = self.depth_transform(to_pil_image(depth_image))\n\n        return Example(\n            {\n                Modalities.RGB.name: rgb_image,\n                Modalities.DEPTH.name: depth_image,\n                EXAMPLE_INDEX_KEY: idx,\n                Modalities.DEPTH.target: self.samples[idx][2],\n            }\n        )\n</code></pre>"},{"location":"api/#mmlearn.datasets.SUNRGBDDataset.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the length of the dataset.</p> Source code in <code>mmlearn/datasets/sunrgbd.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the length of the dataset.\"\"\"\n    return len(self.samples)\n</code></pre>"},{"location":"api/#mmlearn.datasets.SUNRGBDDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> <p>Return RGB and depth images at index <code>idx</code>.</p> Source code in <code>mmlearn/datasets/sunrgbd.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; Example:\n    \"\"\"Return RGB and depth images at index `idx`.\"\"\"\n    # Read images\n    rgb_image = cv2.imread(self.samples[idx][0], cv2.IMREAD_UNCHANGED)\n    if self.rgb_transform is not None:\n        rgb_image = self.rgb_transform(to_pil_image(rgb_image))\n\n    if self.return_type == \"disparity\":\n        depth_image = convert_depth_to_disparity(\n            self.samples[idx][1],\n            self.samples[idx][3],\n            self.samples[idx][4],\n        )\n    else:\n        # Using cv2 instead of PIL Image since we use PNG grayscale images.\n        depth_image = cv2.imread(\n            self.samples[idx][1],\n            cv2.IMREAD_GRAYSCALE,\n        )\n        # Make a 3-channel depth image to enable passing to a pretrained ViT.\n        depth_image = np.repeat(depth_image[:, :, np.newaxis], 3, axis=-1)\n\n    if self.depth_transform is not None:\n        depth_image = self.depth_transform(to_pil_image(depth_image))\n\n    return Example(\n        {\n            Modalities.RGB.name: rgb_image,\n            Modalities.DEPTH.name: depth_image,\n            EXAMPLE_INDEX_KEY: idx,\n            Modalities.DEPTH.target: self.samples[idx][2],\n        }\n    )\n</code></pre>"},{"location":"api/#mmlearn.datasets.chexpert","title":"chexpert","text":"<p>CheXpert Dataset.</p>"},{"location":"api/#mmlearn.datasets.chexpert.CheXpert","title":"CheXpert","text":"<p>               Bases: <code>Dataset[Example]</code></p> <p>CheXpert dataset.</p> <p>Each datapoint is a pair of <code>(image, target label)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>Directory which contains <code>.json</code> files stating all dataset entries.</p> required <code>split</code> <code>(train, valid)</code> <p>Dataset split.</p> <code>\"train\"</code> <code>labeler</code> <code>Optional[{chexpert, chexbert, vchexbert}]</code> <p>Labeler used to extract labels from the training images. \"valid\" split has no labeler, labeling for valid split was done by human radiologists.</p> <code>None</code> <code>transform</code> <code>Optional[Callable[[PIL.Image], torch.Tensor]</code> <p>A callable that takes in a PIL image and returns a transformed version of the image as a PyTorch tensor.</p> <code>None</code> Source code in <code>mmlearn/datasets/chexpert.py</code> <pre><code>@store(\n    group=\"datasets\",\n    provider=\"mmlearn\",\n    root_dir=os.getenv(\"CHEXPERT_ROOT_DIR\", MISSING),\n    split=\"train\",\n)\nclass CheXpert(Dataset[Example]):\n    \"\"\"CheXpert dataset.\n\n    Each datapoint is a pair of `(image, target label)`.\n\n    Parameters\n    ----------\n    root_dir : str\n        Directory which contains `.json` files stating all dataset entries.\n    split : {\"train\", \"valid\"}\n        Dataset split.\n    labeler : Optional[{\"chexpert\", \"chexbert\", \"vchexbert\"}], optional, default=None\n        Labeler used to extract labels from the training images. \"valid\" split\n        has no labeler, labeling for valid split was done by human radiologists.\n    transform : Optional[Callable[[PIL.Image], torch.Tensor], optional, default=None\n        A callable that takes in a PIL image and returns a transformed version\n        of the image as a PyTorch tensor.\n    \"\"\"\n\n    def __init__(\n        self,\n        root_dir: str,\n        split: Literal[\"train\", \"valid\"],\n        labeler: Optional[Literal[\"chexpert\", \"chexbert\", \"vchexbert\"]] = None,\n        transform: Optional[Callable[[Image.Image], torch.Tensor]] = None,\n    ) -&gt; None:\n        assert split in [\"train\", \"valid\"], f\"split {split} is not available.\"\n        assert labeler in [\"chexpert\", \"chexbert\", \"vchexbert\"] or labeler is None, (\n            f\"labeler {labeler} is not available.\"\n        )\n        assert callable(transform) or transform is None, (\n            \"transform is not callable or None.\"\n        )\n\n        if split == \"valid\":\n            data_file = f\"{split}_data.json\"\n        elif split == \"train\":\n            data_file = f\"{labeler}_{split}_data.json\"\n        data_path = os.path.join(root_dir, data_file)\n\n        assert os.path.isfile(data_path), f\"entries file does not exist: {data_path}.\"\n\n        with open(data_path, \"rb\") as file:\n            entries = json.load(file)\n        self.entries = entries\n\n        if transform is not None:\n            self.transform = transform\n        else:\n            self.transform = Compose([Resize(224), CenterCrop(224), ToTensor()])\n\n    def __getitem__(self, idx: int) -&gt; Example:\n        \"\"\"Return the idx'th datapoint.\"\"\"\n        entry = self.entries[idx]\n        image = Image.open(entry[\"image_path\"]).convert(\"RGB\")\n        image = self.transform(image)\n        label = torch.tensor(entry[\"label\"])\n\n        return Example(\n            {\n                Modalities.RGB.name: image,\n                Modalities.RGB.target: label,\n                \"qid\": entry[\"qid\"],\n                EXAMPLE_INDEX_KEY: idx,\n            }\n        )\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the length of the dataset.\"\"\"\n        return len(self.entries)\n</code></pre>"},{"location":"api/#mmlearn.datasets.chexpert.CheXpert.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> <p>Return the idx'th datapoint.</p> Source code in <code>mmlearn/datasets/chexpert.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; Example:\n    \"\"\"Return the idx'th datapoint.\"\"\"\n    entry = self.entries[idx]\n    image = Image.open(entry[\"image_path\"]).convert(\"RGB\")\n    image = self.transform(image)\n    label = torch.tensor(entry[\"label\"])\n\n    return Example(\n        {\n            Modalities.RGB.name: image,\n            Modalities.RGB.target: label,\n            \"qid\": entry[\"qid\"],\n            EXAMPLE_INDEX_KEY: idx,\n        }\n    )\n</code></pre>"},{"location":"api/#mmlearn.datasets.chexpert.CheXpert.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the length of the dataset.</p> Source code in <code>mmlearn/datasets/chexpert.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the length of the dataset.\"\"\"\n    return len(self.entries)\n</code></pre>"},{"location":"api/#mmlearn.datasets.core","title":"core","text":"<p>Modules for core dataloading functionality.</p>"},{"location":"api/#mmlearn.datasets.core.CombinedDataset","title":"CombinedDataset","text":"<p>               Bases: <code>Dataset[Example]</code></p> <p>Combine multiple datasets into one.</p> <p>This class is similar to class:<code>~torch.utils.data.ConcatDataset</code> but allows for combining iterable-style datasets with map-style datasets. The iterable-style datasets must implement the :meth:<code>__len__</code> method, which is used to determine the total length of the combined dataset. When an index is passed to the combined dataset, the dataset that contains the example at that index is determined and the example is retrieved from that dataset. Since iterable-style datasets do not support random access, the examples are retrieved sequentially from the iterable-style datasets. When the end of an iterable-style dataset is reached, the iterator is reset and the next example is retrieved from the beginning of the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>datasets</code> <code>Iterable[Union[Dataset, IterableDataset]]</code> <p>Iterable of datasets to combine.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the datasets in the input iterable are not instances of class:<code>~torch.utils.data.Dataset</code> or class:<code>~torch.utils.data.IterableDataset</code>.</p> <code>ValueError</code> <p>If the input iterable of datasets is empty.</p> Source code in <code>mmlearn/datasets/core/combined_dataset.py</code> <pre><code>class CombinedDataset(Dataset[Example]):\n    \"\"\"Combine multiple datasets into one.\n\n    This class is similar to :py:class:`~torch.utils.data.ConcatDataset` but allows\n    for combining iterable-style datasets with map-style datasets. The iterable-style\n    datasets must implement the :meth:`__len__` method, which is used to determine the\n    total length of the combined dataset. When an index is passed to the combined\n    dataset, the dataset that contains the example at that index is determined and\n    the example is retrieved from that dataset. Since iterable-style datasets do\n    not support random access, the examples are retrieved sequentially from the\n    iterable-style datasets. When the end of an iterable-style dataset is reached,\n    the iterator is reset and the next example is retrieved from the beginning of\n    the dataset.\n\n\n    Parameters\n    ----------\n    datasets : Iterable[Union[torch.utils.data.Dataset, torch.utils.data.IterableDataset]]\n        Iterable of datasets to combine.\n\n    Raises\n    ------\n    TypeError\n        If any of the datasets in the input iterable are not instances of\n        :py:class:`~torch.utils.data.Dataset` or :py:class:`~torch.utils.data.IterableDataset`.\n    ValueError\n        If the input iterable of datasets is empty.\n\n    \"\"\"  # noqa: W505\n\n    def __init__(\n        self, datasets: Iterable[Union[Dataset[Example], IterableDataset[Example]]]\n    ) -&gt; None:\n        self.datasets, _ = tree_flatten(datasets)\n        if not all(\n            isinstance(dataset, (Dataset, IterableDataset)) for dataset in self.datasets\n        ):\n            raise TypeError(\n                \"Expected argument `datasets` to be an iterable of `Dataset` or \"\n                f\"`IterableDataset` instances, but found: {self.datasets}\",\n            )\n        if len(self.datasets) == 0:\n            raise ValueError(\n                \"Expected a non-empty iterable of datasets but found an empty iterable\",\n            )\n\n        self._cumulative_sizes: list[int] = np.cumsum(\n            [len(dataset) for dataset in self.datasets]\n        ).tolist()\n        self._iterators: list[Iterator[Example]] = []\n        self._iter_dataset_mapping: dict[int, int] = {}\n\n        # create iterators for iterable datasets and map dataset index to iterator index\n        for idx, dataset in enumerate(self.datasets):\n            if isinstance(dataset, IterableDataset):\n                self._iterators.append(iter(dataset))\n                self._iter_dataset_mapping[idx] = len(self._iterators) - 1\n\n    def __getitem__(self, idx: int) -&gt; Example:\n        \"\"\"Return an example from the combined dataset.\"\"\"\n        if idx &lt; 0:  # handle negative indices\n            if -idx &gt; len(self):\n                raise IndexError(\n                    f\"Index {idx} is out of bounds for the combined dataset with \"\n                    f\"length {len(self)}\",\n                )\n            idx = len(self) + idx\n\n        dataset_idx = bisect.bisect_right(self._cumulative_sizes, idx)\n\n        curr_dataset = self.datasets[dataset_idx]\n        if isinstance(curr_dataset, IterableDataset):\n            iter_idx = self._iter_dataset_mapping[dataset_idx]\n            try:\n                example = next(self._iterators[iter_idx])\n            except StopIteration:\n                self._iterators[iter_idx] = iter(curr_dataset)\n                example = next(self._iterators[iter_idx])\n        else:\n            if dataset_idx == 0:\n                example_idx = idx\n            else:\n                example_idx = idx - self._cumulative_sizes[dataset_idx - 1]\n            example = curr_dataset[example_idx]\n\n        if not isinstance(example, Example):\n            raise TypeError(\n                \"Expected dataset examples to be instances of `Example` \"\n                f\"but found {type(example)}\",\n            )\n\n        if not hasattr(example, \"dataset_index\"):\n            example.dataset_index = dataset_idx\n        if not hasattr(example, \"example_ids\"):\n            example.create_ids()\n\n        return example\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the total number of examples in the combined dataset.\"\"\"\n        return self._cumulative_sizes[-1]\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.CombinedDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> <p>Return an example from the combined dataset.</p> Source code in <code>mmlearn/datasets/core/combined_dataset.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; Example:\n    \"\"\"Return an example from the combined dataset.\"\"\"\n    if idx &lt; 0:  # handle negative indices\n        if -idx &gt; len(self):\n            raise IndexError(\n                f\"Index {idx} is out of bounds for the combined dataset with \"\n                f\"length {len(self)}\",\n            )\n        idx = len(self) + idx\n\n    dataset_idx = bisect.bisect_right(self._cumulative_sizes, idx)\n\n    curr_dataset = self.datasets[dataset_idx]\n    if isinstance(curr_dataset, IterableDataset):\n        iter_idx = self._iter_dataset_mapping[dataset_idx]\n        try:\n            example = next(self._iterators[iter_idx])\n        except StopIteration:\n            self._iterators[iter_idx] = iter(curr_dataset)\n            example = next(self._iterators[iter_idx])\n    else:\n        if dataset_idx == 0:\n            example_idx = idx\n        else:\n            example_idx = idx - self._cumulative_sizes[dataset_idx - 1]\n        example = curr_dataset[example_idx]\n\n    if not isinstance(example, Example):\n        raise TypeError(\n            \"Expected dataset examples to be instances of `Example` \"\n            f\"but found {type(example)}\",\n        )\n\n    if not hasattr(example, \"dataset_index\"):\n        example.dataset_index = dataset_idx\n    if not hasattr(example, \"example_ids\"):\n        example.create_ids()\n\n    return example\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.CombinedDataset.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the total number of examples in the combined dataset.</p> Source code in <code>mmlearn/datasets/core/combined_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the total number of examples in the combined dataset.\"\"\"\n    return self._cumulative_sizes[-1]\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.DefaultDataCollator","title":"DefaultDataCollator  <code>dataclass</code>","text":"<p>Default data collator for batching examples.</p> <p>This data collator will collate a list of class:<code>~mmlearn.datasets.core.example.Example</code> objects into a batch. It can also apply processing functions to specified keys in the batch before returning it.</p> <p>Parameters:</p> Name Type Description Default <code>batch_processors</code> <code>Optional[dict[str, Callable[[Any], Any]]]</code> <p>Dictionary of callables to apply to the batch before returning it.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the batch processor for a key does not return a dictionary with the key in it.</p> Source code in <code>mmlearn/datasets/core/data_collator.py</code> <pre><code>@dataclass\nclass DefaultDataCollator:\n    \"\"\"Default data collator for batching examples.\n\n    This data collator will collate a list of :py:class:`~mmlearn.datasets.core.example.Example`\n    objects into a batch. It can also apply processing functions to specified keys\n    in the batch before returning it.\n\n    Parameters\n    ----------\n    batch_processors : Optional[dict[str, Callable[[Any], Any]]], optional, default=None\n        Dictionary of callables to apply to the batch before returning it.\n\n    Raises\n    ------\n    ValueError\n        If the batch processor for a key does not return a dictionary with the\n        key in it.\n    \"\"\"  # noqa: W505\n\n    #: Dictionary of callables to apply to the batch before returning it.\n    #: The key is the name of the key in the batch, and the value is the processing\n    #: function to apply to the key. The processing function must take a single\n    #: argument and return a single value. If the processing function returns\n    #: a dictionary, it must contain the key that was processed in it (all the\n    #: other keys will also be included in the batch).\n    batch_processors: Optional[dict[str, Callable[[Any], Any]]] = None\n\n    def __call__(self, examples: list[Example]) -&gt; dict[str, Any]:\n        \"\"\"Collate a list of `Example` objects and apply processing functions.\"\"\"\n        batch = collate_example_list(examples)\n\n        if self.batch_processors is not None:\n            for key, processor in self.batch_processors.items():\n                batch_key: str = key\n                if Modalities.has_modality(key):\n                    batch_key = Modalities.get_modality(key).name\n\n                if batch_key in batch:\n                    batch_processed = processor(batch[batch_key])\n                    if isinstance(batch_processed, Mapping):\n                        if batch_key not in batch_processed:\n                            raise ValueError(\n                                f\"Batch processor for '{key}' key must return a dictionary \"\n                                f\"with '{batch_key}' in it.\"\n                            )\n                        batch.update(batch_processed)\n                    else:\n                        batch[batch_key] = batch_processed\n\n        return batch\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.DefaultDataCollator.__call__","title":"__call__","text":"<pre><code>__call__(examples)\n</code></pre> <p>Collate a list of <code>Example</code> objects and apply processing functions.</p> Source code in <code>mmlearn/datasets/core/data_collator.py</code> <pre><code>def __call__(self, examples: list[Example]) -&gt; dict[str, Any]:\n    \"\"\"Collate a list of `Example` objects and apply processing functions.\"\"\"\n    batch = collate_example_list(examples)\n\n    if self.batch_processors is not None:\n        for key, processor in self.batch_processors.items():\n            batch_key: str = key\n            if Modalities.has_modality(key):\n                batch_key = Modalities.get_modality(key).name\n\n            if batch_key in batch:\n                batch_processed = processor(batch[batch_key])\n                if isinstance(batch_processed, Mapping):\n                    if batch_key not in batch_processed:\n                        raise ValueError(\n                            f\"Batch processor for '{key}' key must return a dictionary \"\n                            f\"with '{batch_key}' in it.\"\n                        )\n                    batch.update(batch_processed)\n                else:\n                    batch[batch_key] = batch_processed\n\n    return batch\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.Example","title":"Example","text":"<p>               Bases: <code>OrderedDict[Any, Any]</code></p> <p>A representation of a single example from a dataset.</p> <p>This class is a subclass of class:<code>~collections.OrderedDict</code> and provides attribute-style access. This means that <code>example[\"text\"]</code> and <code>example.text</code> are equivalent. All datasets in this library return examples as class:<code>~mmlearn.datasets.core.example.Example</code> objects.</p> <p>Parameters:</p> Name Type Description Default <code>init_dict</code> <code>Optional[MutableMapping[Hashable, Any]]</code> <p>Dictionary to init <code>Example</code> class with.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; example = Example({\"text\": torch.tensor(2)})\n&gt;&gt;&gt; example.text.zero_()\ntensor(0)\n&gt;&gt;&gt; example.context = torch.tensor(4)  # set custom attributes after initialization\n</code></pre> Source code in <code>mmlearn/datasets/core/example.py</code> <pre><code>class Example(OrderedDict[Any, Any]):\n    \"\"\"A representation of a single example from a dataset.\n\n    This class is a subclass of :py:class:`~collections.OrderedDict` and provides\n    attribute-style access. This means that `example[\"text\"]` and `example.text`\n    are equivalent. All datasets in this library return examples as\n    :py:class:`~mmlearn.datasets.core.example.Example` objects.\n\n\n    Parameters\n    ----------\n    init_dict : Optional[MutableMapping[Hashable, Any]], optional, default=None\n        Dictionary to init `Example` class with.\n\n    Examples\n    --------\n    &gt;&gt;&gt; example = Example({\"text\": torch.tensor(2)})\n    &gt;&gt;&gt; example.text.zero_()\n    tensor(0)\n    &gt;&gt;&gt; example.context = torch.tensor(4)  # set custom attributes after initialization\n    \"\"\"\n\n    def __init__(\n        self,\n        init_dict: Optional[MutableMapping[Hashable, Any]] = None,\n    ) -&gt; None:\n        if init_dict is None:\n            init_dict = {}\n        super().__init__(init_dict)\n\n    def create_ids(self) -&gt; None:\n        \"\"\"Create a unique id for the example from the dataset and example index.\n\n        This method combines the dataset index and example index to create an\n        attribute called `example_ids`, which is a dictionary of tensors. The\n        dictionary keys are all the keys in the example except for `example_ids`,\n        `example_index`, and `dataset_index`. The values are tensors of shape `(2,)`\n        containing the tuple `(dataset_index, example_index)` for each key.\n        The `example_ids` is used to (re-)identify pairs of examples from different\n        modalities after they have been combined into a batch.\n\n        Warns\n        -----\n        UserWarning\n            If the `example_index` and `dataset_index` attributes are not set.\n\n        Notes\n        -----\n        - The Example must have the following attributes set before calling this\n          this method: `example_index` (usually set/returned by the dataset) and\n          `dataset_index` (usually set by the :py:class:`~mmlearn.datasets.core.combined_dataset.CombinedDataset` object)\n        - The :py:func:`~mmlearn.datasets.core.example.find_matching_indices`\n          function can be used to find matching examples given two tensors of example ids.\n\n        \"\"\"  # noqa: W505\n        if hasattr(self, \"example_index\") and hasattr(self, \"dataset_index\"):\n            self.example_ids = {\n                key: torch.tensor([self.dataset_index, self.example_index])\n                for key in self.keys()\n                if key not in (\"example_ids\", \"example_index\", \"dataset_index\")\n            }\n        else:\n            rank_zero_warn(\n                \"Cannot create `example_ids` without `example_index` and `dataset_index` \"\n                \"attributes. Set these attributes before calling `create_ids`. \"\n                \"No `example_ids` was created.\",\n                stacklevel=2,\n                category=UserWarning,\n            )\n\n    def __getattr__(self, key: str) -&gt; Any:\n        \"\"\"Get attribute by key.\"\"\"\n        try:\n            return self[key]\n        except KeyError:\n            raise AttributeError(key) from None\n\n    def __setattr__(self, key: str, value: Any) -&gt; None:\n        \"\"\"Set attribute by key.\"\"\"\n        if isinstance(value, MutableMapping):\n            value = Example(value)\n        self[key] = value\n\n    def __setitem__(self, key: Hashable, value: Any) -&gt; None:\n        \"\"\"Set item by key.\"\"\"\n        if isinstance(value, MutableMapping):\n            value = Example(value)\n        super().__setitem__(key, value)\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.Example.create_ids","title":"create_ids","text":"<pre><code>create_ids()\n</code></pre> <p>Create a unique id for the example from the dataset and example index.</p> <p>This method combines the dataset index and example index to create an attribute called <code>example_ids</code>, which is a dictionary of tensors. The dictionary keys are all the keys in the example except for <code>example_ids</code>, <code>example_index</code>, and <code>dataset_index</code>. The values are tensors of shape <code>(2,)</code> containing the tuple <code>(dataset_index, example_index)</code> for each key. The <code>example_ids</code> is used to (re-)identify pairs of examples from different modalities after they have been combined into a batch.</p> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If the <code>example_index</code> and <code>dataset_index</code> attributes are not set.</p> Notes <ul> <li>The Example must have the following attributes set before calling this   this method: <code>example_index</code> (usually set/returned by the dataset) and   <code>dataset_index</code> (usually set by the class:<code>~mmlearn.datasets.core.combined_dataset.CombinedDataset</code> object)</li> <li>The func:<code>~mmlearn.datasets.core.example.find_matching_indices</code>   function can be used to find matching examples given two tensors of example ids.</li> </ul> Source code in <code>mmlearn/datasets/core/example.py</code> <pre><code>def create_ids(self) -&gt; None:\n    \"\"\"Create a unique id for the example from the dataset and example index.\n\n    This method combines the dataset index and example index to create an\n    attribute called `example_ids`, which is a dictionary of tensors. The\n    dictionary keys are all the keys in the example except for `example_ids`,\n    `example_index`, and `dataset_index`. The values are tensors of shape `(2,)`\n    containing the tuple `(dataset_index, example_index)` for each key.\n    The `example_ids` is used to (re-)identify pairs of examples from different\n    modalities after they have been combined into a batch.\n\n    Warns\n    -----\n    UserWarning\n        If the `example_index` and `dataset_index` attributes are not set.\n\n    Notes\n    -----\n    - The Example must have the following attributes set before calling this\n      this method: `example_index` (usually set/returned by the dataset) and\n      `dataset_index` (usually set by the :py:class:`~mmlearn.datasets.core.combined_dataset.CombinedDataset` object)\n    - The :py:func:`~mmlearn.datasets.core.example.find_matching_indices`\n      function can be used to find matching examples given two tensors of example ids.\n\n    \"\"\"  # noqa: W505\n    if hasattr(self, \"example_index\") and hasattr(self, \"dataset_index\"):\n        self.example_ids = {\n            key: torch.tensor([self.dataset_index, self.example_index])\n            for key in self.keys()\n            if key not in (\"example_ids\", \"example_index\", \"dataset_index\")\n        }\n    else:\n        rank_zero_warn(\n            \"Cannot create `example_ids` without `example_index` and `dataset_index` \"\n            \"attributes. Set these attributes before calling `create_ids`. \"\n            \"No `example_ids` was created.\",\n            stacklevel=2,\n            category=UserWarning,\n        )\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.Example.__getattr__","title":"__getattr__","text":"<pre><code>__getattr__(key)\n</code></pre> <p>Get attribute by key.</p> Source code in <code>mmlearn/datasets/core/example.py</code> <pre><code>def __getattr__(self, key: str) -&gt; Any:\n    \"\"\"Get attribute by key.\"\"\"\n    try:\n        return self[key]\n    except KeyError:\n        raise AttributeError(key) from None\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.Example.__setattr__","title":"__setattr__","text":"<pre><code>__setattr__(key, value)\n</code></pre> <p>Set attribute by key.</p> Source code in <code>mmlearn/datasets/core/example.py</code> <pre><code>def __setattr__(self, key: str, value: Any) -&gt; None:\n    \"\"\"Set attribute by key.\"\"\"\n    if isinstance(value, MutableMapping):\n        value = Example(value)\n    self[key] = value\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.Example.__setitem__","title":"__setitem__","text":"<pre><code>__setitem__(key, value)\n</code></pre> <p>Set item by key.</p> Source code in <code>mmlearn/datasets/core/example.py</code> <pre><code>def __setitem__(self, key: Hashable, value: Any) -&gt; None:\n    \"\"\"Set item by key.\"\"\"\n    if isinstance(value, MutableMapping):\n        value = Example(value)\n    super().__setitem__(key, value)\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.CombinedDatasetRatioSampler","title":"CombinedDatasetRatioSampler","text":"<p>               Bases: <code>Sampler[int]</code></p> <p>Sampler for weighted sampling from a class:<code>~mmlearn.datasets.core.combined_dataset.CombinedDataset</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>CombinedDataset</code> <p>An instance of class:<code>~mmlearn.datasets.core.combined_dataset.CombinedDataset</code> to sample from.</p> required <code>ratios</code> <code>Optional[Sequence[float]]</code> <p>A sequence of ratios for sampling from each dataset in the combined dataset. The length of the sequence must be equal to the number of datasets in the combined dataset (<code>dataset</code>). If <code>None</code>, the length of each dataset in the combined dataset is used as the ratio. The ratios are normalized to sum to 1.</p> <code>None</code> <code>num_samples</code> <code>Optional[int]</code> <p>The number of samples to draw from the combined dataset. If <code>None</code>, the sampler will draw as many samples as there are in the combined dataset. This number must yield at least one sample per dataset in the combined dataset, when multiplied by the corresponding ratio.</p> <code>None</code> <code>replacement</code> <code>bool</code> <p>Whether to sample with replacement or not.</p> <code>False</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the sampled indices or not. If <code>False</code>, the indices of each dataset will appear in the order they are stored in the combined dataset. This is similar to sequential sampling from each dataset. The datasets that make up the combined dataset are still sampled randomly.</p> <code>True</code> <code>rank</code> <code>Optional[int]</code> <p>Rank of the current process within :attr:<code>num_replicas</code>. By default, :attr:<code>rank</code> is retrieved from the current distributed group.</p> <code>None</code> <code>num_replicas</code> <code>Optional[int]</code> <p>Number of processes participating in distributed training. By default, :attr:<code>num_replicas</code> is retrieved from the current distributed group.</p> <code>None</code> <code>drop_last</code> <code>bool</code> <p>Whether to drop the last incomplete batch or not. If <code>True</code>, the sampler will drop samples to make the number of samples evenly divisible by the number of replicas in distributed mode.</p> <code>False</code> <code>seed</code> <code>int</code> <p>Random seed used to when sampling from the combined dataset and shuffling the sampled indices.</p> <code>0</code> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>CombinedDataset</code> <p>The dataset to sample from.</p> <code>num_samples</code> <code>int</code> <p>The number of samples to draw from the combined dataset.</p> <code>probs</code> <code>Tensor</code> <p>The probabilities for sampling from each dataset in the combined dataset. This is computed from the <code>ratios</code> argument and is normalized to sum to 1.</p> <code>replacement</code> <code>bool</code> <p>Whether to sample with replacement or not.</p> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the sampled indices or not.</p> <code>rank</code> <code>int</code> <p>Rank of the current process within :attr:<code>num_replicas</code>.</p> <code>num_replicas</code> <code>int</code> <p>Number of processes participating in distributed training.</p> <code>drop_last</code> <code>bool</code> <p>Whether to drop samples to make the number of samples evenly divisible by the number of replicas in distributed mode.</p> <code>seed</code> <code>int</code> <p>Random seed used to when sampling from the combined dataset and shuffling the sampled indices.</p> <code>epoch</code> <code>int</code> <p>Current epoch number. This is used to set the random seed. This is useful in distributed mode to ensure that each process receives a different random ordering of the samples.</p> <code>total_size</code> <code>int</code> <p>The total number of samples across all processes.</p> Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>@store(group=\"dataloader/sampler\", provider=\"mmlearn\")\nclass CombinedDatasetRatioSampler(Sampler[int]):\n    \"\"\"Sampler for weighted sampling from a :py:class:`~mmlearn.datasets.core.combined_dataset.CombinedDataset`.\n\n    Parameters\n    ----------\n    dataset : CombinedDataset\n        An instance of :py:class:`~mmlearn.datasets.core.combined_dataset.CombinedDataset`\n        to sample from.\n    ratios : Optional[Sequence[float]], optional, default=None\n        A sequence of ratios for sampling from each dataset in the combined dataset.\n        The length of the sequence must be equal to the number of datasets in the\n        combined dataset (`dataset`). If `None`, the length of each dataset in the\n        combined dataset is used as the ratio. The ratios are normalized to sum to 1.\n    num_samples : Optional[int], optional, default=None\n        The number of samples to draw from the combined dataset. If `None`, the\n        sampler will draw as many samples as there are in the combined dataset.\n        This number must yield at least one sample per dataset in the combined\n        dataset, when multiplied by the corresponding ratio.\n    replacement : bool, default=False\n        Whether to sample with replacement or not.\n    shuffle : bool, default=True\n        Whether to shuffle the sampled indices or not. If `False`, the indices of\n        each dataset will appear in the order they are stored in the combined dataset.\n        This is similar to sequential sampling from each dataset. The datasets\n        that make up the combined dataset are still sampled randomly.\n    rank : Optional[int], optional, default=None\n        Rank of the current process within :attr:`num_replicas`. By default,\n        :attr:`rank` is retrieved from the current distributed group.\n    num_replicas : Optional[int], optional, default=None\n        Number of processes participating in distributed training. By\n        default, :attr:`num_replicas` is retrieved from the current distributed group.\n    drop_last : bool, default=False\n        Whether to drop the last incomplete batch or not. If `True`, the sampler will\n        drop samples to make the number of samples evenly divisible by the number of\n        replicas in distributed mode.\n    seed : int, default=0\n        Random seed used to when sampling from the combined dataset and shuffling\n        the sampled indices.\n\n    Attributes\n    ----------\n    dataset : CombinedDataset\n        The dataset to sample from.\n    num_samples : int\n        The number of samples to draw from the combined dataset.\n    probs : torch.Tensor\n        The probabilities for sampling from each dataset in the combined dataset.\n        This is computed from the `ratios` argument and is normalized to sum to 1.\n    replacement : bool\n        Whether to sample with replacement or not.\n    shuffle : bool\n        Whether to shuffle the sampled indices or not.\n    rank : int\n        Rank of the current process within :attr:`num_replicas`.\n    num_replicas : int\n        Number of processes participating in distributed training.\n    drop_last : bool\n        Whether to drop samples to make the number of samples evenly divisible by the\n        number of replicas in distributed mode.\n    seed : int\n        Random seed used to when sampling from the combined dataset and shuffling\n        the sampled indices.\n    epoch : int\n        Current epoch number. This is used to set the random seed. This is useful\n        in distributed mode to ensure that each process receives a different random\n        ordering of the samples.\n    total_size : int\n        The total number of samples across all processes.\n    \"\"\"  # noqa: W505\n\n    def __init__(  # noqa: PLR0912\n        self,\n        dataset: CombinedDataset,\n        ratios: Optional[Sequence[float]] = None,\n        num_samples: Optional[int] = None,\n        replacement: bool = False,\n        shuffle: bool = True,\n        rank: Optional[int] = None,\n        num_replicas: Optional[int] = None,\n        drop_last: bool = False,\n        seed: int = 0,\n    ):\n        if not isinstance(dataset, CombinedDataset):\n            raise TypeError(\n                \"Expected argument `dataset` to be of type `CombinedDataset`, \"\n                f\"but got {type(dataset)}.\",\n            )\n        if not isinstance(seed, int):\n            raise TypeError(\n                f\"Expected argument `seed` to be an integer, but got {type(seed)}.\",\n            )\n        if num_replicas is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            num_replicas = dist.get_world_size()\n        if rank is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            rank = dist.get_rank()\n        if rank &gt;= num_replicas or rank &lt; 0:\n            raise ValueError(\n                f\"Invalid rank {rank}, rank should be in the interval [0, {num_replicas - 1}]\"\n            )\n\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.drop_last = drop_last\n        self.replacement = replacement\n        self.shuffle = shuffle\n        self.seed = seed\n        self.epoch = 0\n\n        self._num_samples = num_samples\n        if not isinstance(self.num_samples, int) or self.num_samples &lt;= 0:\n            raise ValueError(\n                \"Expected argument `num_samples` to be a positive integer, but got \"\n                f\"{self.num_samples}.\",\n            )\n\n        if ratios is None:\n            ratios = [len(subset) for subset in self.dataset.datasets]\n\n        num_datasets = len(self.dataset.datasets)\n        if len(ratios) != num_datasets:\n            raise ValueError(\n                f\"Expected argument `ratios` to be of length {num_datasets}, \"\n                f\"but got length {len(ratios)}.\",\n            )\n        prob_sum = sum(ratios)\n        if not all(ratio &gt;= 0 for ratio in ratios) and prob_sum &gt; 0:\n            raise ValueError(\n                \"Expected argument `ratios` to be a sequence of non-negative numbers. \"\n                f\"Got {ratios}.\",\n            )\n        self.probs = torch.tensor(\n            [ratio / prob_sum for ratio in ratios],\n            dtype=torch.double,\n        )\n        if any((prob * self.num_samples) &lt;= 0 for prob in self.probs):\n            raise ValueError(\n                \"Expected dataset ratio to result in at least one sample per dataset. \"\n                f\"Got dataset sizes {self.probs * self.num_samples}.\",\n            )\n\n    @property\n    def num_samples(self) -&gt; int:\n        \"\"\"Return the number of samples managed by the sampler.\"\"\"\n        # dataset size might change at runtime\n        if self._num_samples is None:\n            num_samples = len(self.dataset)\n        else:\n            num_samples = self._num_samples\n\n        if self.drop_last and num_samples % self.num_replicas != 0:\n            # split to nearest available length that is evenly divisible.\n            # This is to ensure each rank receives the same amount of data when\n            # using this Sampler.\n            num_samples = math.ceil(\n                (num_samples - self.num_replicas) / self.num_replicas,\n            )\n        else:\n            num_samples = math.ceil(num_samples / self.num_replicas)\n        return num_samples\n\n    @property\n    def total_size(self) -&gt; int:\n        \"\"\"Return the total size of the dataset.\"\"\"\n        return self.num_samples * self.num_replicas\n\n    def __iter__(self) -&gt; Iterator[int]:\n        \"\"\"Return an iterator that yields sample indices for the combined dataset.\"\"\"\n        generator = torch.Generator()\n        seed = self.seed + self.epoch\n        generator.manual_seed(seed)\n\n        cumulative_sizes = [0] + self.dataset._cumulative_sizes\n        num_samples_per_dataset = [int(prob * self.total_size) for prob in self.probs]\n        indices = []\n        for i in range(len(self.dataset.datasets)):\n            per_dataset_indices: torch.Tensor = torch.multinomial(\n                torch.ones(cumulative_sizes[i + 1] - cumulative_sizes[i]),\n                num_samples_per_dataset[i],\n                replacement=self.replacement,\n                generator=generator,\n            )\n            # adjust indices to reflect position in cumulative dataset\n            per_dataset_indices += cumulative_sizes[i]\n            assert per_dataset_indices.max() &lt; cumulative_sizes[i + 1], (\n                f\"Indices from dataset {i} exceed dataset size. \"\n                f\"Got indices {per_dataset_indices} and dataset size {cumulative_sizes[i + 1]}.\",\n            )\n            indices.append(per_dataset_indices)\n\n        indices = torch.cat(indices)\n        if self.shuffle:\n            rand_indices = torch.randperm(len(indices), generator=generator)\n            indices = indices[rand_indices]\n\n        indices = indices.tolist()  # type: ignore[attr-defined]\n        num_indices = len(indices)\n\n        if num_indices &lt; self.total_size:\n            padding_size = self.total_size - num_indices\n            if padding_size &lt;= num_indices:\n                indices += indices[:padding_size]\n            else:\n                indices += (indices * math.ceil(padding_size / num_indices))[\n                    :padding_size\n                ]\n        elif num_indices &gt; self.total_size:\n            indices = indices[: self.total_size]\n        assert len(indices) == self.total_size\n\n        # subsample\n        indices = indices[self.rank : self.total_size : self.num_replicas]\n        assert len(indices) == self.num_samples, (\n            f\"Expected {self.num_samples} samples, but got {len(indices)}.\",\n        )\n\n        yield from iter(indices)\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the total number of samples in the sampler.\"\"\"\n        return self.num_samples\n\n    def set_epoch(self, epoch: int) -&gt; None:\n        \"\"\"Set the epoch for this sampler.\n\n        When :attr:`shuffle=True`, this ensures all replicas use a different random\n        ordering for each epoch. Otherwise, the next iteration of this sampler\n        will yield the same ordering.\n\n        Parameters\n        ----------\n        epoch : int\n            Epoch number.\n\n        \"\"\"\n        self.epoch = epoch\n\n        # some iterable datasets (especially huggingface iterable datasets) might\n        # require setting the epoch to ensure shuffling works properly\n        for dataset in self.dataset.datasets:\n            if hasattr(dataset, \"set_epoch\"):\n                dataset.set_epoch(epoch)\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.CombinedDatasetRatioSampler.num_samples","title":"num_samples  <code>property</code>","text":"<pre><code>num_samples\n</code></pre> <p>Return the number of samples managed by the sampler.</p>"},{"location":"api/#mmlearn.datasets.core.CombinedDatasetRatioSampler.total_size","title":"total_size  <code>property</code>","text":"<pre><code>total_size\n</code></pre> <p>Return the total size of the dataset.</p>"},{"location":"api/#mmlearn.datasets.core.CombinedDatasetRatioSampler.__iter__","title":"__iter__","text":"<pre><code>__iter__()\n</code></pre> <p>Return an iterator that yields sample indices for the combined dataset.</p> Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>def __iter__(self) -&gt; Iterator[int]:\n    \"\"\"Return an iterator that yields sample indices for the combined dataset.\"\"\"\n    generator = torch.Generator()\n    seed = self.seed + self.epoch\n    generator.manual_seed(seed)\n\n    cumulative_sizes = [0] + self.dataset._cumulative_sizes\n    num_samples_per_dataset = [int(prob * self.total_size) for prob in self.probs]\n    indices = []\n    for i in range(len(self.dataset.datasets)):\n        per_dataset_indices: torch.Tensor = torch.multinomial(\n            torch.ones(cumulative_sizes[i + 1] - cumulative_sizes[i]),\n            num_samples_per_dataset[i],\n            replacement=self.replacement,\n            generator=generator,\n        )\n        # adjust indices to reflect position in cumulative dataset\n        per_dataset_indices += cumulative_sizes[i]\n        assert per_dataset_indices.max() &lt; cumulative_sizes[i + 1], (\n            f\"Indices from dataset {i} exceed dataset size. \"\n            f\"Got indices {per_dataset_indices} and dataset size {cumulative_sizes[i + 1]}.\",\n        )\n        indices.append(per_dataset_indices)\n\n    indices = torch.cat(indices)\n    if self.shuffle:\n        rand_indices = torch.randperm(len(indices), generator=generator)\n        indices = indices[rand_indices]\n\n    indices = indices.tolist()  # type: ignore[attr-defined]\n    num_indices = len(indices)\n\n    if num_indices &lt; self.total_size:\n        padding_size = self.total_size - num_indices\n        if padding_size &lt;= num_indices:\n            indices += indices[:padding_size]\n        else:\n            indices += (indices * math.ceil(padding_size / num_indices))[\n                :padding_size\n            ]\n    elif num_indices &gt; self.total_size:\n        indices = indices[: self.total_size]\n    assert len(indices) == self.total_size\n\n    # subsample\n    indices = indices[self.rank : self.total_size : self.num_replicas]\n    assert len(indices) == self.num_samples, (\n        f\"Expected {self.num_samples} samples, but got {len(indices)}.\",\n    )\n\n    yield from iter(indices)\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.CombinedDatasetRatioSampler.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the total number of samples in the sampler.</p> Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the total number of samples in the sampler.\"\"\"\n    return self.num_samples\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.CombinedDatasetRatioSampler.set_epoch","title":"set_epoch","text":"<pre><code>set_epoch(epoch)\n</code></pre> <p>Set the epoch for this sampler.</p> <p>When :attr:<code>shuffle=True</code>, this ensures all replicas use a different random ordering for each epoch. Otherwise, the next iteration of this sampler will yield the same ordering.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>Epoch number.</p> required Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>def set_epoch(self, epoch: int) -&gt; None:\n    \"\"\"Set the epoch for this sampler.\n\n    When :attr:`shuffle=True`, this ensures all replicas use a different random\n    ordering for each epoch. Otherwise, the next iteration of this sampler\n    will yield the same ordering.\n\n    Parameters\n    ----------\n    epoch : int\n        Epoch number.\n\n    \"\"\"\n    self.epoch = epoch\n\n    # some iterable datasets (especially huggingface iterable datasets) might\n    # require setting the epoch to ensure shuffling works properly\n    for dataset in self.dataset.datasets:\n        if hasattr(dataset, \"set_epoch\"):\n            dataset.set_epoch(epoch)\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.DistributedEvalSampler","title":"DistributedEvalSampler","text":"<p>               Bases: <code>Sampler[int]</code></p> <p>Sampler for distributed evaluation.</p> <p>The main differences between this and class:<code>torch.utils.data.DistributedSampler</code> are that this sampler does not add extra samples to make it evenly divisible and shuffling is disabled by default.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>Dataset used for sampling.</p> required <code>num_replicas</code> <code>Optional[int]</code> <p>Number of processes participating in distributed training. By default, :attr:<code>rank</code> is retrieved from the current distributed group.</p> <code>None</code> <code>rank</code> <code>Optional[int]</code> <p>Rank of the current process within :attr:<code>num_replicas</code>. By default, :attr:<code>rank</code> is retrieved from the current distributed group.</p> <code>None</code> <code>shuffle</code> <code>bool</code> <p>If <code>True</code> (default), sampler will shuffle the indices.</p> <code>False</code> <code>seed</code> <code>int</code> <p>Random seed used to shuffle the sampler if :attr:<code>shuffle=True</code>. This number should be identical across all processes in the distributed group.</p> <code>0</code> Warnings <p>DistributedEvalSampler should NOT be used for training. The distributed processes could hang forever. See [1]_ for details</p> Notes <ul> <li>This sampler is for evaluation purpose where synchronization does not happen   every epoch. Synchronization should be done outside the dataloader loop.   It is especially useful in conjunction with   class:<code>torch.nn.parallel.DistributedDataParallel</code> [2]_.</li> <li>The input Dataset is assumed to be of constant size.</li> <li>This implementation is adapted from [3]_.</li> </ul> References <p>.. [1] https://github.com/pytorch/pytorch/issues/22584 .. [2] https://discuss.pytorch.org/t/how-to-validate-in-distributeddataparallel-correctly/94267/11 .. [3] https://github.com/SeungjunNah/DeepDeblur-PyTorch/blob/master/src/data/sampler.py</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; def example():\n...     start_epoch, n_epochs = 0, 2\n...     sampler = DistributedEvalSampler(dataset) if is_distributed else None\n...     loader = DataLoader(dataset, shuffle=(sampler is None), sampler=sampler)\n...     for epoch in range(start_epoch, n_epochs):\n...         if is_distributed:\n...             sampler.set_epoch(epoch)\n...         evaluate(loader)\n</code></pre> Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>@store(group=\"dataloader/sampler\", provider=\"mmlearn\")\nclass DistributedEvalSampler(Sampler[int]):\n    \"\"\"Sampler for distributed evaluation.\n\n    The main differences between this and :py:class:`torch.utils.data.DistributedSampler`\n    are that this sampler does not add extra samples to make it evenly divisible and\n    shuffling is disabled by default.\n\n    Parameters\n    ----------\n    dataset : torch.utils.data.Dataset\n        Dataset used for sampling.\n    num_replicas : Optional[int], optional, default=None\n        Number of processes participating in distributed training. By\n        default, :attr:`rank` is retrieved from the current distributed group.\n    rank : Optional[int], optional, default=None\n        Rank of the current process within :attr:`num_replicas`. By default,\n        :attr:`rank` is retrieved from the current distributed group.\n    shuffle : bool, optional, default=False\n        If `True` (default), sampler will shuffle the indices.\n    seed : int, optional, default=0\n        Random seed used to shuffle the sampler if :attr:`shuffle=True`.\n        This number should be identical across all processes in the\n        distributed group.\n\n    Warnings\n    --------\n    DistributedEvalSampler should NOT be used for training. The distributed processes\n    could hang forever. See [1]_ for details\n\n    Notes\n    -----\n    - This sampler is for evaluation purpose where synchronization does not happen\n      every epoch. Synchronization should be done outside the dataloader loop.\n      It is especially useful in conjunction with\n      :py:class:`torch.nn.parallel.DistributedDataParallel` [2]_.\n    - The input Dataset is assumed to be of constant size.\n    - This implementation is adapted from [3]_.\n\n    References\n    ----------\n    .. [1] https://github.com/pytorch/pytorch/issues/22584\n    .. [2] https://discuss.pytorch.org/t/how-to-validate-in-distributeddataparallel-correctly/94267/11\n    .. [3] https://github.com/SeungjunNah/DeepDeblur-PyTorch/blob/master/src/data/sampler.py\n\n\n    Examples\n    --------\n    &gt;&gt;&gt; def example():\n    ...     start_epoch, n_epochs = 0, 2\n    ...     sampler = DistributedEvalSampler(dataset) if is_distributed else None\n    ...     loader = DataLoader(dataset, shuffle=(sampler is None), sampler=sampler)\n    ...     for epoch in range(start_epoch, n_epochs):\n    ...         if is_distributed:\n    ...             sampler.set_epoch(epoch)\n    ...         evaluate(loader)\n\n    \"\"\"  # noqa: W505\n\n    def __init__(\n        self,\n        dataset: Dataset[Sized],\n        num_replicas: Optional[int] = None,\n        rank: Optional[int] = None,\n        shuffle: bool = False,\n        seed: int = 0,\n    ) -&gt; None:\n        if num_replicas is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            num_replicas = dist.get_world_size()\n        if rank is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            rank = dist.get_rank()\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.epoch = 0\n        self.shuffle = shuffle\n        self.seed = seed\n\n    @property\n    def total_size(self) -&gt; int:\n        \"\"\"Return the total size of the dataset.\"\"\"\n        return len(self.dataset)\n\n    @property\n    def num_samples(self) -&gt; int:\n        \"\"\"Return the number of samples managed by the sampler.\"\"\"\n        indices = list(range(self.total_size))[\n            self.rank : self.total_size : self.num_replicas\n        ]\n        return len(indices)  # true value without extra samples\n\n    def __iter__(self) -&gt; Iterator[int]:\n        \"\"\"Return an iterator that iterates over the indices of the dataset.\"\"\"\n        if self.shuffle:\n            # deterministically shuffle based on epoch and seed\n            g = torch.Generator()\n            g.manual_seed(self.seed + self.epoch)\n            indices = torch.randperm(self.total_size, generator=g).tolist()\n        else:\n            indices = list(range(self.total_size))\n\n        # subsample\n        indices = indices[self.rank : self.total_size : self.num_replicas]\n        assert len(indices) == self.num_samples\n\n        return iter(indices)\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the number of samples.\"\"\"\n        return self.num_samples\n\n    def set_epoch(self, epoch: int) -&gt; None:\n        \"\"\"Set the epoch for this sampler.\n\n        When :attr:`shuffle=True`, this ensures all replicas use a different random\n        ordering for each epoch. Otherwise, the next iteration of this sampler\n        will yield the same ordering.\n\n        Parameters\n        ----------\n        epoch : int\n            Epoch number.\n\n        \"\"\"\n        self.epoch = epoch\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.DistributedEvalSampler.total_size","title":"total_size  <code>property</code>","text":"<pre><code>total_size\n</code></pre> <p>Return the total size of the dataset.</p>"},{"location":"api/#mmlearn.datasets.core.DistributedEvalSampler.num_samples","title":"num_samples  <code>property</code>","text":"<pre><code>num_samples\n</code></pre> <p>Return the number of samples managed by the sampler.</p>"},{"location":"api/#mmlearn.datasets.core.DistributedEvalSampler.__iter__","title":"__iter__","text":"<pre><code>__iter__()\n</code></pre> <p>Return an iterator that iterates over the indices of the dataset.</p> Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>def __iter__(self) -&gt; Iterator[int]:\n    \"\"\"Return an iterator that iterates over the indices of the dataset.\"\"\"\n    if self.shuffle:\n        # deterministically shuffle based on epoch and seed\n        g = torch.Generator()\n        g.manual_seed(self.seed + self.epoch)\n        indices = torch.randperm(self.total_size, generator=g).tolist()\n    else:\n        indices = list(range(self.total_size))\n\n    # subsample\n    indices = indices[self.rank : self.total_size : self.num_replicas]\n    assert len(indices) == self.num_samples\n\n    return iter(indices)\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.DistributedEvalSampler.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the number of samples.</p> Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the number of samples.\"\"\"\n    return self.num_samples\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.DistributedEvalSampler.set_epoch","title":"set_epoch","text":"<pre><code>set_epoch(epoch)\n</code></pre> <p>Set the epoch for this sampler.</p> <p>When :attr:<code>shuffle=True</code>, this ensures all replicas use a different random ordering for each epoch. Otherwise, the next iteration of this sampler will yield the same ordering.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>Epoch number.</p> required Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>def set_epoch(self, epoch: int) -&gt; None:\n    \"\"\"Set the epoch for this sampler.\n\n    When :attr:`shuffle=True`, this ensures all replicas use a different random\n    ordering for each epoch. Otherwise, the next iteration of this sampler\n    will yield the same ordering.\n\n    Parameters\n    ----------\n    epoch : int\n        Epoch number.\n\n    \"\"\"\n    self.epoch = epoch\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.find_matching_indices","title":"find_matching_indices","text":"<pre><code>find_matching_indices(\n    first_example_ids, second_example_ids\n)\n</code></pre> <p>Find the indices of matching examples given two tensors of example ids.</p> <p>Matching examples are defined as examples with the same value in both tensors. This method is useful for finding pairs of examples from different modalities that are related to each other in a batch.</p> <p>Parameters:</p> Name Type Description Default <code>first_example_ids</code> <code>Tensor</code> <p>A tensor of example ids of shape <code>(N, 2)</code>, where <code>N</code> is the number of examples.</p> required <code>second_example_ids</code> <code>Tensor</code> <p>A tensor of example ids of shape <code>(M, 2)</code>, where <code>M</code> is the number of examples.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>A tuple of tensors containing the indices of matching examples in the first and second tensor, respectively.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If either <code>first_example_ids</code> or <code>second_example_ids</code> is not a tensor.</p> <code>ValueError</code> <p>If either <code>first_example_ids</code> or <code>second_example_ids</code> is not a 2D tensor with the second dimension having a size of <code>2</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; img_example_ids = torch.tensor([(0, 0), (0, 1), (1, 0), (1, 1)])\n&gt;&gt;&gt; text_example_ids = torch.tensor([(1, 0), (1, 1), (2, 0), (2, 1), (2, 2)])\n&gt;&gt;&gt; find_matching_indices(img_example_ids, text_example_ids)\n(tensor([2, 3]), tensor([0, 1]))\n</code></pre> Source code in <code>mmlearn/datasets/core/example.py</code> <pre><code>def find_matching_indices(\n    first_example_ids: torch.Tensor, second_example_ids: torch.Tensor\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Find the indices of matching examples given two tensors of example ids.\n\n    Matching examples are defined as examples with the same value in both tensors.\n    This method is useful for finding pairs of examples from different modalities\n    that are related to each other in a batch.\n\n    Parameters\n    ----------\n    first_example_ids : torch.Tensor\n        A tensor of example ids of shape `(N, 2)`, where `N` is the number of examples.\n    second_example_ids : torch.Tensor\n        A tensor of example ids of shape `(M, 2)`, where `M` is the number of examples.\n\n    Returns\n    -------\n    tuple[torch.Tensor, torch.Tensor]\n        A tuple of tensors containing the indices of matching examples in the first and\n        second tensor, respectively.\n\n    Raises\n    ------\n    TypeError\n        If either `first_example_ids` or `second_example_ids` is not a tensor.\n    ValueError\n        If either `first_example_ids` or `second_example_ids` is not a 2D tensor\n        with the second dimension having a size of `2`.\n\n    Examples\n    --------\n    &gt;&gt;&gt; img_example_ids = torch.tensor([(0, 0), (0, 1), (1, 0), (1, 1)])\n    &gt;&gt;&gt; text_example_ids = torch.tensor([(1, 0), (1, 1), (2, 0), (2, 1), (2, 2)])\n    &gt;&gt;&gt; find_matching_indices(img_example_ids, text_example_ids)\n    (tensor([2, 3]), tensor([0, 1]))\n\n\n    \"\"\"\n    if not isinstance(first_example_ids, torch.Tensor) or not isinstance(\n        second_example_ids,\n        torch.Tensor,\n    ):\n        raise TypeError(\n            f\"Expected inputs to be tensors, but got {type(first_example_ids)} \"\n            f\"and {type(second_example_ids)}.\",\n        )\n    val = 2\n    if not (first_example_ids.ndim == val and first_example_ids.shape[1] == val):\n        raise ValueError(\n            \"Expected argument `first_example_ids` to be a tensor of shape (N, 2), \"\n            f\"but got shape {first_example_ids.shape}.\",\n        )\n    if not (second_example_ids.ndim == val and second_example_ids.shape[1] == val):\n        raise ValueError(\n            \"Expected argument `second_example_ids` to be a tensor of shape (N, 2), \"\n            f\"but got shape {second_example_ids.shape}.\",\n        )\n\n    first_example_ids = first_example_ids.unsqueeze(1)  # shape=(N, 1, 2)\n    second_example_ids = second_example_ids.unsqueeze(0)  # shape=(1, M, 2)\n\n    # compare all elements; results in a shape (N, M) tensor\n    matches = torch.all(first_example_ids == second_example_ids, dim=-1)\n    first_indices, second_indices = torch.where(matches)\n    return first_indices, second_indices\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.combined_dataset","title":"combined_dataset","text":"<p>Wrapper for combining multiple datasets into one.</p>"},{"location":"api/#mmlearn.datasets.core.combined_dataset.CombinedDataset","title":"CombinedDataset","text":"<p>               Bases: <code>Dataset[Example]</code></p> <p>Combine multiple datasets into one.</p> <p>This class is similar to class:<code>~torch.utils.data.ConcatDataset</code> but allows for combining iterable-style datasets with map-style datasets. The iterable-style datasets must implement the :meth:<code>__len__</code> method, which is used to determine the total length of the combined dataset. When an index is passed to the combined dataset, the dataset that contains the example at that index is determined and the example is retrieved from that dataset. Since iterable-style datasets do not support random access, the examples are retrieved sequentially from the iterable-style datasets. When the end of an iterable-style dataset is reached, the iterator is reset and the next example is retrieved from the beginning of the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>datasets</code> <code>Iterable[Union[Dataset, IterableDataset]]</code> <p>Iterable of datasets to combine.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the datasets in the input iterable are not instances of class:<code>~torch.utils.data.Dataset</code> or class:<code>~torch.utils.data.IterableDataset</code>.</p> <code>ValueError</code> <p>If the input iterable of datasets is empty.</p> Source code in <code>mmlearn/datasets/core/combined_dataset.py</code> <pre><code>class CombinedDataset(Dataset[Example]):\n    \"\"\"Combine multiple datasets into one.\n\n    This class is similar to :py:class:`~torch.utils.data.ConcatDataset` but allows\n    for combining iterable-style datasets with map-style datasets. The iterable-style\n    datasets must implement the :meth:`__len__` method, which is used to determine the\n    total length of the combined dataset. When an index is passed to the combined\n    dataset, the dataset that contains the example at that index is determined and\n    the example is retrieved from that dataset. Since iterable-style datasets do\n    not support random access, the examples are retrieved sequentially from the\n    iterable-style datasets. When the end of an iterable-style dataset is reached,\n    the iterator is reset and the next example is retrieved from the beginning of\n    the dataset.\n\n\n    Parameters\n    ----------\n    datasets : Iterable[Union[torch.utils.data.Dataset, torch.utils.data.IterableDataset]]\n        Iterable of datasets to combine.\n\n    Raises\n    ------\n    TypeError\n        If any of the datasets in the input iterable are not instances of\n        :py:class:`~torch.utils.data.Dataset` or :py:class:`~torch.utils.data.IterableDataset`.\n    ValueError\n        If the input iterable of datasets is empty.\n\n    \"\"\"  # noqa: W505\n\n    def __init__(\n        self, datasets: Iterable[Union[Dataset[Example], IterableDataset[Example]]]\n    ) -&gt; None:\n        self.datasets, _ = tree_flatten(datasets)\n        if not all(\n            isinstance(dataset, (Dataset, IterableDataset)) for dataset in self.datasets\n        ):\n            raise TypeError(\n                \"Expected argument `datasets` to be an iterable of `Dataset` or \"\n                f\"`IterableDataset` instances, but found: {self.datasets}\",\n            )\n        if len(self.datasets) == 0:\n            raise ValueError(\n                \"Expected a non-empty iterable of datasets but found an empty iterable\",\n            )\n\n        self._cumulative_sizes: list[int] = np.cumsum(\n            [len(dataset) for dataset in self.datasets]\n        ).tolist()\n        self._iterators: list[Iterator[Example]] = []\n        self._iter_dataset_mapping: dict[int, int] = {}\n\n        # create iterators for iterable datasets and map dataset index to iterator index\n        for idx, dataset in enumerate(self.datasets):\n            if isinstance(dataset, IterableDataset):\n                self._iterators.append(iter(dataset))\n                self._iter_dataset_mapping[idx] = len(self._iterators) - 1\n\n    def __getitem__(self, idx: int) -&gt; Example:\n        \"\"\"Return an example from the combined dataset.\"\"\"\n        if idx &lt; 0:  # handle negative indices\n            if -idx &gt; len(self):\n                raise IndexError(\n                    f\"Index {idx} is out of bounds for the combined dataset with \"\n                    f\"length {len(self)}\",\n                )\n            idx = len(self) + idx\n\n        dataset_idx = bisect.bisect_right(self._cumulative_sizes, idx)\n\n        curr_dataset = self.datasets[dataset_idx]\n        if isinstance(curr_dataset, IterableDataset):\n            iter_idx = self._iter_dataset_mapping[dataset_idx]\n            try:\n                example = next(self._iterators[iter_idx])\n            except StopIteration:\n                self._iterators[iter_idx] = iter(curr_dataset)\n                example = next(self._iterators[iter_idx])\n        else:\n            if dataset_idx == 0:\n                example_idx = idx\n            else:\n                example_idx = idx - self._cumulative_sizes[dataset_idx - 1]\n            example = curr_dataset[example_idx]\n\n        if not isinstance(example, Example):\n            raise TypeError(\n                \"Expected dataset examples to be instances of `Example` \"\n                f\"but found {type(example)}\",\n            )\n\n        if not hasattr(example, \"dataset_index\"):\n            example.dataset_index = dataset_idx\n        if not hasattr(example, \"example_ids\"):\n            example.create_ids()\n\n        return example\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the total number of examples in the combined dataset.\"\"\"\n        return self._cumulative_sizes[-1]\n</code></pre> <code></code> __getitem__ \u00b6 <pre><code>__getitem__(idx)\n</code></pre> <p>Return an example from the combined dataset.</p> Source code in <code>mmlearn/datasets/core/combined_dataset.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; Example:\n    \"\"\"Return an example from the combined dataset.\"\"\"\n    if idx &lt; 0:  # handle negative indices\n        if -idx &gt; len(self):\n            raise IndexError(\n                f\"Index {idx} is out of bounds for the combined dataset with \"\n                f\"length {len(self)}\",\n            )\n        idx = len(self) + idx\n\n    dataset_idx = bisect.bisect_right(self._cumulative_sizes, idx)\n\n    curr_dataset = self.datasets[dataset_idx]\n    if isinstance(curr_dataset, IterableDataset):\n        iter_idx = self._iter_dataset_mapping[dataset_idx]\n        try:\n            example = next(self._iterators[iter_idx])\n        except StopIteration:\n            self._iterators[iter_idx] = iter(curr_dataset)\n            example = next(self._iterators[iter_idx])\n    else:\n        if dataset_idx == 0:\n            example_idx = idx\n        else:\n            example_idx = idx - self._cumulative_sizes[dataset_idx - 1]\n        example = curr_dataset[example_idx]\n\n    if not isinstance(example, Example):\n        raise TypeError(\n            \"Expected dataset examples to be instances of `Example` \"\n            f\"but found {type(example)}\",\n        )\n\n    if not hasattr(example, \"dataset_index\"):\n        example.dataset_index = dataset_idx\n    if not hasattr(example, \"example_ids\"):\n        example.create_ids()\n\n    return example\n</code></pre> <code></code> __len__ \u00b6 <pre><code>__len__()\n</code></pre> <p>Return the total number of examples in the combined dataset.</p> Source code in <code>mmlearn/datasets/core/combined_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the total number of examples in the combined dataset.\"\"\"\n    return self._cumulative_sizes[-1]\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.data_collator","title":"data_collator","text":"<p>Data collators for batching examples.</p>"},{"location":"api/#mmlearn.datasets.core.data_collator.DefaultDataCollator","title":"DefaultDataCollator  <code>dataclass</code>","text":"<p>Default data collator for batching examples.</p> <p>This data collator will collate a list of class:<code>~mmlearn.datasets.core.example.Example</code> objects into a batch. It can also apply processing functions to specified keys in the batch before returning it.</p> <p>Parameters:</p> Name Type Description Default <code>batch_processors</code> <code>Optional[dict[str, Callable[[Any], Any]]]</code> <p>Dictionary of callables to apply to the batch before returning it.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the batch processor for a key does not return a dictionary with the key in it.</p> Source code in <code>mmlearn/datasets/core/data_collator.py</code> <pre><code>@dataclass\nclass DefaultDataCollator:\n    \"\"\"Default data collator for batching examples.\n\n    This data collator will collate a list of :py:class:`~mmlearn.datasets.core.example.Example`\n    objects into a batch. It can also apply processing functions to specified keys\n    in the batch before returning it.\n\n    Parameters\n    ----------\n    batch_processors : Optional[dict[str, Callable[[Any], Any]]], optional, default=None\n        Dictionary of callables to apply to the batch before returning it.\n\n    Raises\n    ------\n    ValueError\n        If the batch processor for a key does not return a dictionary with the\n        key in it.\n    \"\"\"  # noqa: W505\n\n    #: Dictionary of callables to apply to the batch before returning it.\n    #: The key is the name of the key in the batch, and the value is the processing\n    #: function to apply to the key. The processing function must take a single\n    #: argument and return a single value. If the processing function returns\n    #: a dictionary, it must contain the key that was processed in it (all the\n    #: other keys will also be included in the batch).\n    batch_processors: Optional[dict[str, Callable[[Any], Any]]] = None\n\n    def __call__(self, examples: list[Example]) -&gt; dict[str, Any]:\n        \"\"\"Collate a list of `Example` objects and apply processing functions.\"\"\"\n        batch = collate_example_list(examples)\n\n        if self.batch_processors is not None:\n            for key, processor in self.batch_processors.items():\n                batch_key: str = key\n                if Modalities.has_modality(key):\n                    batch_key = Modalities.get_modality(key).name\n\n                if batch_key in batch:\n                    batch_processed = processor(batch[batch_key])\n                    if isinstance(batch_processed, Mapping):\n                        if batch_key not in batch_processed:\n                            raise ValueError(\n                                f\"Batch processor for '{key}' key must return a dictionary \"\n                                f\"with '{batch_key}' in it.\"\n                            )\n                        batch.update(batch_processed)\n                    else:\n                        batch[batch_key] = batch_processed\n\n        return batch\n</code></pre> <code></code> __call__ \u00b6 <pre><code>__call__(examples)\n</code></pre> <p>Collate a list of <code>Example</code> objects and apply processing functions.</p> Source code in <code>mmlearn/datasets/core/data_collator.py</code> <pre><code>def __call__(self, examples: list[Example]) -&gt; dict[str, Any]:\n    \"\"\"Collate a list of `Example` objects and apply processing functions.\"\"\"\n    batch = collate_example_list(examples)\n\n    if self.batch_processors is not None:\n        for key, processor in self.batch_processors.items():\n            batch_key: str = key\n            if Modalities.has_modality(key):\n                batch_key = Modalities.get_modality(key).name\n\n            if batch_key in batch:\n                batch_processed = processor(batch[batch_key])\n                if isinstance(batch_processed, Mapping):\n                    if batch_key not in batch_processed:\n                        raise ValueError(\n                            f\"Batch processor for '{key}' key must return a dictionary \"\n                            f\"with '{batch_key}' in it.\"\n                        )\n                    batch.update(batch_processed)\n                else:\n                    batch[batch_key] = batch_processed\n\n    return batch\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.data_collator.collate_example_list","title":"collate_example_list","text":"<pre><code>collate_example_list(examples)\n</code></pre> <p>Collate a list of class:<code>~mmlearn.datasets.core.example.Example</code> objects into a batch.</p> <p>Parameters:</p> Name Type Description Default <code>examples</code> <code>list[Example]</code> <p>list of examples to collate.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary of batched examples.</p> Source code in <code>mmlearn/datasets/core/data_collator.py</code> <pre><code>def collate_example_list(examples: list[Example]) -&gt; dict[str, Any]:\n    \"\"\"Collate a list of :py:class:`~mmlearn.datasets.core.example.Example` objects into a batch.\n\n    Parameters\n    ----------\n    examples : list[Example]\n        list of examples to collate.\n\n    Returns\n    -------\n    dict[str, Any]\n        Dictionary of batched examples.\n\n    \"\"\"  # noqa: W505\n    return _collate_example_dict(_merge_examples(examples))\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.example","title":"example","text":"<p>Module for example-related classes and functions.</p>"},{"location":"api/#mmlearn.datasets.core.example.Example","title":"Example","text":"<p>               Bases: <code>OrderedDict[Any, Any]</code></p> <p>A representation of a single example from a dataset.</p> <p>This class is a subclass of class:<code>~collections.OrderedDict</code> and provides attribute-style access. This means that <code>example[\"text\"]</code> and <code>example.text</code> are equivalent. All datasets in this library return examples as class:<code>~mmlearn.datasets.core.example.Example</code> objects.</p> <p>Parameters:</p> Name Type Description Default <code>init_dict</code> <code>Optional[MutableMapping[Hashable, Any]]</code> <p>Dictionary to init <code>Example</code> class with.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; example = Example({\"text\": torch.tensor(2)})\n&gt;&gt;&gt; example.text.zero_()\ntensor(0)\n&gt;&gt;&gt; example.context = torch.tensor(4)  # set custom attributes after initialization\n</code></pre> Source code in <code>mmlearn/datasets/core/example.py</code> <pre><code>class Example(OrderedDict[Any, Any]):\n    \"\"\"A representation of a single example from a dataset.\n\n    This class is a subclass of :py:class:`~collections.OrderedDict` and provides\n    attribute-style access. This means that `example[\"text\"]` and `example.text`\n    are equivalent. All datasets in this library return examples as\n    :py:class:`~mmlearn.datasets.core.example.Example` objects.\n\n\n    Parameters\n    ----------\n    init_dict : Optional[MutableMapping[Hashable, Any]], optional, default=None\n        Dictionary to init `Example` class with.\n\n    Examples\n    --------\n    &gt;&gt;&gt; example = Example({\"text\": torch.tensor(2)})\n    &gt;&gt;&gt; example.text.zero_()\n    tensor(0)\n    &gt;&gt;&gt; example.context = torch.tensor(4)  # set custom attributes after initialization\n    \"\"\"\n\n    def __init__(\n        self,\n        init_dict: Optional[MutableMapping[Hashable, Any]] = None,\n    ) -&gt; None:\n        if init_dict is None:\n            init_dict = {}\n        super().__init__(init_dict)\n\n    def create_ids(self) -&gt; None:\n        \"\"\"Create a unique id for the example from the dataset and example index.\n\n        This method combines the dataset index and example index to create an\n        attribute called `example_ids`, which is a dictionary of tensors. The\n        dictionary keys are all the keys in the example except for `example_ids`,\n        `example_index`, and `dataset_index`. The values are tensors of shape `(2,)`\n        containing the tuple `(dataset_index, example_index)` for each key.\n        The `example_ids` is used to (re-)identify pairs of examples from different\n        modalities after they have been combined into a batch.\n\n        Warns\n        -----\n        UserWarning\n            If the `example_index` and `dataset_index` attributes are not set.\n\n        Notes\n        -----\n        - The Example must have the following attributes set before calling this\n          this method: `example_index` (usually set/returned by the dataset) and\n          `dataset_index` (usually set by the :py:class:`~mmlearn.datasets.core.combined_dataset.CombinedDataset` object)\n        - The :py:func:`~mmlearn.datasets.core.example.find_matching_indices`\n          function can be used to find matching examples given two tensors of example ids.\n\n        \"\"\"  # noqa: W505\n        if hasattr(self, \"example_index\") and hasattr(self, \"dataset_index\"):\n            self.example_ids = {\n                key: torch.tensor([self.dataset_index, self.example_index])\n                for key in self.keys()\n                if key not in (\"example_ids\", \"example_index\", \"dataset_index\")\n            }\n        else:\n            rank_zero_warn(\n                \"Cannot create `example_ids` without `example_index` and `dataset_index` \"\n                \"attributes. Set these attributes before calling `create_ids`. \"\n                \"No `example_ids` was created.\",\n                stacklevel=2,\n                category=UserWarning,\n            )\n\n    def __getattr__(self, key: str) -&gt; Any:\n        \"\"\"Get attribute by key.\"\"\"\n        try:\n            return self[key]\n        except KeyError:\n            raise AttributeError(key) from None\n\n    def __setattr__(self, key: str, value: Any) -&gt; None:\n        \"\"\"Set attribute by key.\"\"\"\n        if isinstance(value, MutableMapping):\n            value = Example(value)\n        self[key] = value\n\n    def __setitem__(self, key: Hashable, value: Any) -&gt; None:\n        \"\"\"Set item by key.\"\"\"\n        if isinstance(value, MutableMapping):\n            value = Example(value)\n        super().__setitem__(key, value)\n</code></pre> <code></code> create_ids \u00b6 <pre><code>create_ids()\n</code></pre> <p>Create a unique id for the example from the dataset and example index.</p> <p>This method combines the dataset index and example index to create an attribute called <code>example_ids</code>, which is a dictionary of tensors. The dictionary keys are all the keys in the example except for <code>example_ids</code>, <code>example_index</code>, and <code>dataset_index</code>. The values are tensors of shape <code>(2,)</code> containing the tuple <code>(dataset_index, example_index)</code> for each key. The <code>example_ids</code> is used to (re-)identify pairs of examples from different modalities after they have been combined into a batch.</p> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If the <code>example_index</code> and <code>dataset_index</code> attributes are not set.</p> Notes <ul> <li>The Example must have the following attributes set before calling this   this method: <code>example_index</code> (usually set/returned by the dataset) and   <code>dataset_index</code> (usually set by the class:<code>~mmlearn.datasets.core.combined_dataset.CombinedDataset</code> object)</li> <li>The func:<code>~mmlearn.datasets.core.example.find_matching_indices</code>   function can be used to find matching examples given two tensors of example ids.</li> </ul> Source code in <code>mmlearn/datasets/core/example.py</code> <pre><code>def create_ids(self) -&gt; None:\n    \"\"\"Create a unique id for the example from the dataset and example index.\n\n    This method combines the dataset index and example index to create an\n    attribute called `example_ids`, which is a dictionary of tensors. The\n    dictionary keys are all the keys in the example except for `example_ids`,\n    `example_index`, and `dataset_index`. The values are tensors of shape `(2,)`\n    containing the tuple `(dataset_index, example_index)` for each key.\n    The `example_ids` is used to (re-)identify pairs of examples from different\n    modalities after they have been combined into a batch.\n\n    Warns\n    -----\n    UserWarning\n        If the `example_index` and `dataset_index` attributes are not set.\n\n    Notes\n    -----\n    - The Example must have the following attributes set before calling this\n      this method: `example_index` (usually set/returned by the dataset) and\n      `dataset_index` (usually set by the :py:class:`~mmlearn.datasets.core.combined_dataset.CombinedDataset` object)\n    - The :py:func:`~mmlearn.datasets.core.example.find_matching_indices`\n      function can be used to find matching examples given two tensors of example ids.\n\n    \"\"\"  # noqa: W505\n    if hasattr(self, \"example_index\") and hasattr(self, \"dataset_index\"):\n        self.example_ids = {\n            key: torch.tensor([self.dataset_index, self.example_index])\n            for key in self.keys()\n            if key not in (\"example_ids\", \"example_index\", \"dataset_index\")\n        }\n    else:\n        rank_zero_warn(\n            \"Cannot create `example_ids` without `example_index` and `dataset_index` \"\n            \"attributes. Set these attributes before calling `create_ids`. \"\n            \"No `example_ids` was created.\",\n            stacklevel=2,\n            category=UserWarning,\n        )\n</code></pre> <code></code> __getattr__ \u00b6 <pre><code>__getattr__(key)\n</code></pre> <p>Get attribute by key.</p> Source code in <code>mmlearn/datasets/core/example.py</code> <pre><code>def __getattr__(self, key: str) -&gt; Any:\n    \"\"\"Get attribute by key.\"\"\"\n    try:\n        return self[key]\n    except KeyError:\n        raise AttributeError(key) from None\n</code></pre> <code></code> __setattr__ \u00b6 <pre><code>__setattr__(key, value)\n</code></pre> <p>Set attribute by key.</p> Source code in <code>mmlearn/datasets/core/example.py</code> <pre><code>def __setattr__(self, key: str, value: Any) -&gt; None:\n    \"\"\"Set attribute by key.\"\"\"\n    if isinstance(value, MutableMapping):\n        value = Example(value)\n    self[key] = value\n</code></pre> <code></code> __setitem__ \u00b6 <pre><code>__setitem__(key, value)\n</code></pre> <p>Set item by key.</p> Source code in <code>mmlearn/datasets/core/example.py</code> <pre><code>def __setitem__(self, key: Hashable, value: Any) -&gt; None:\n    \"\"\"Set item by key.\"\"\"\n    if isinstance(value, MutableMapping):\n        value = Example(value)\n    super().__setitem__(key, value)\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.example.find_matching_indices","title":"find_matching_indices","text":"<pre><code>find_matching_indices(\n    first_example_ids, second_example_ids\n)\n</code></pre> <p>Find the indices of matching examples given two tensors of example ids.</p> <p>Matching examples are defined as examples with the same value in both tensors. This method is useful for finding pairs of examples from different modalities that are related to each other in a batch.</p> <p>Parameters:</p> Name Type Description Default <code>first_example_ids</code> <code>Tensor</code> <p>A tensor of example ids of shape <code>(N, 2)</code>, where <code>N</code> is the number of examples.</p> required <code>second_example_ids</code> <code>Tensor</code> <p>A tensor of example ids of shape <code>(M, 2)</code>, where <code>M</code> is the number of examples.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>A tuple of tensors containing the indices of matching examples in the first and second tensor, respectively.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If either <code>first_example_ids</code> or <code>second_example_ids</code> is not a tensor.</p> <code>ValueError</code> <p>If either <code>first_example_ids</code> or <code>second_example_ids</code> is not a 2D tensor with the second dimension having a size of <code>2</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; img_example_ids = torch.tensor([(0, 0), (0, 1), (1, 0), (1, 1)])\n&gt;&gt;&gt; text_example_ids = torch.tensor([(1, 0), (1, 1), (2, 0), (2, 1), (2, 2)])\n&gt;&gt;&gt; find_matching_indices(img_example_ids, text_example_ids)\n(tensor([2, 3]), tensor([0, 1]))\n</code></pre> Source code in <code>mmlearn/datasets/core/example.py</code> <pre><code>def find_matching_indices(\n    first_example_ids: torch.Tensor, second_example_ids: torch.Tensor\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Find the indices of matching examples given two tensors of example ids.\n\n    Matching examples are defined as examples with the same value in both tensors.\n    This method is useful for finding pairs of examples from different modalities\n    that are related to each other in a batch.\n\n    Parameters\n    ----------\n    first_example_ids : torch.Tensor\n        A tensor of example ids of shape `(N, 2)`, where `N` is the number of examples.\n    second_example_ids : torch.Tensor\n        A tensor of example ids of shape `(M, 2)`, where `M` is the number of examples.\n\n    Returns\n    -------\n    tuple[torch.Tensor, torch.Tensor]\n        A tuple of tensors containing the indices of matching examples in the first and\n        second tensor, respectively.\n\n    Raises\n    ------\n    TypeError\n        If either `first_example_ids` or `second_example_ids` is not a tensor.\n    ValueError\n        If either `first_example_ids` or `second_example_ids` is not a 2D tensor\n        with the second dimension having a size of `2`.\n\n    Examples\n    --------\n    &gt;&gt;&gt; img_example_ids = torch.tensor([(0, 0), (0, 1), (1, 0), (1, 1)])\n    &gt;&gt;&gt; text_example_ids = torch.tensor([(1, 0), (1, 1), (2, 0), (2, 1), (2, 2)])\n    &gt;&gt;&gt; find_matching_indices(img_example_ids, text_example_ids)\n    (tensor([2, 3]), tensor([0, 1]))\n\n\n    \"\"\"\n    if not isinstance(first_example_ids, torch.Tensor) or not isinstance(\n        second_example_ids,\n        torch.Tensor,\n    ):\n        raise TypeError(\n            f\"Expected inputs to be tensors, but got {type(first_example_ids)} \"\n            f\"and {type(second_example_ids)}.\",\n        )\n    val = 2\n    if not (first_example_ids.ndim == val and first_example_ids.shape[1] == val):\n        raise ValueError(\n            \"Expected argument `first_example_ids` to be a tensor of shape (N, 2), \"\n            f\"but got shape {first_example_ids.shape}.\",\n        )\n    if not (second_example_ids.ndim == val and second_example_ids.shape[1] == val):\n        raise ValueError(\n            \"Expected argument `second_example_ids` to be a tensor of shape (N, 2), \"\n            f\"but got shape {second_example_ids.shape}.\",\n        )\n\n    first_example_ids = first_example_ids.unsqueeze(1)  # shape=(N, 1, 2)\n    second_example_ids = second_example_ids.unsqueeze(0)  # shape=(1, M, 2)\n\n    # compare all elements; results in a shape (N, M) tensor\n    matches = torch.all(first_example_ids == second_example_ids, dim=-1)\n    first_indices, second_indices = torch.where(matches)\n    return first_indices, second_indices\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.modalities","title":"modalities","text":"<p>Module for managing supported modalities in the library.</p>"},{"location":"api/#mmlearn.datasets.core.modalities.Modality","title":"Modality  <code>dataclass</code>","text":"<p>A representation of a modality in the library.</p> <p>This class is used to represent a modality in the library. It contains the name of the modality and the properties that can be associated with it. The properties are dynamically generated based on the name of the modality and can be accessed as attributes of the class.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the modality.</p> required <code>modality_specific_properties</code> <code>Optional[dict[str, str]]</code> <p>Additional properties specific to the modality, by default None</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the property already exists for the modality or if the format string is invalid.</p> Source code in <code>mmlearn/datasets/core/modalities.py</code> <pre><code>@dataclass\nclass Modality:\n    \"\"\"A representation of a modality in the library.\n\n    This class is used to represent a modality in the library. It contains the name of\n    the modality and the properties that can be associated with it. The properties are\n    dynamically generated based on the name of the modality and can be accessed as\n    attributes of the class.\n\n    Parameters\n    ----------\n    name : str\n        The name of the modality.\n    modality_specific_properties : Optional[dict[str, str]], optional, default=None\n        Additional properties specific to the modality, by default None\n\n    Raises\n    ------\n    ValueError\n        If the property already exists for the modality or if the format string is\n        invalid.\n    \"\"\"\n\n    #: The name of the modality.\n    name: str\n\n    #: Target/label associated with the modality. This will return ``name_target``.\n    target: str = field(init=False, repr=False)\n\n    #: Attention mask associated with the modality. This will return\n    # ``name_attention_mask``.\n    attention_mask: str = field(init=False, repr=False)\n\n    #: Input mask associated with the modality. This will return ``name_mask``.\n    mask: str = field(init=False, repr=False)\n\n    #: Embedding associated with the modality. This will return ``name_embedding``.\n    embedding: str = field(init=False, repr=False)\n\n    #: Masked embedding associated with the modality. This will return\n    # ``name_masked_embedding``.\n    masked_embedding: str = field(init=False, repr=False)\n\n    #: Embedding from an Exponential Moving Average (EMA) encoder associated with\n    #: the modality.\n    ema_embedding: str = field(init=False, repr=False)\n\n    #: Other properties specific to the modality.\n    modality_specific_properties: Optional[dict[str, str]] = field(\n        default=None, repr=False\n    )\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Initialize the modality with the name and properties.\"\"\"\n        self.name = self.name.lower()\n        self._properties = {}\n\n        for field_name in self.__dataclass_fields__:\n            if field_name not in (\"name\", \"modality_specific_properties\"):\n                field_value = f\"{self.name}_{field_name}\"\n                self._properties[field_name] = field_value\n                setattr(self, field_name, field_value)\n\n        if self.modality_specific_properties is not None:\n            for (\n                property_name,\n                format_string,\n            ) in self.modality_specific_properties.items():\n                self.add_property(property_name, format_string)\n\n    @property\n    def properties(self) -&gt; dict[str, str]:\n        \"\"\"Return the properties associated with the modality.\"\"\"\n        return self._properties\n\n    def add_property(self, name: str, format_string: str) -&gt; None:\n        \"\"\"Add a new property to the modality.\n\n        Parameters\n        ----------\n        name : str\n            The name of the property.\n        format_string : str\n            The format string for the property. The format string should contain a\n            placeholder that will be replaced with the name of the modality when the\n            property is accessed.\n\n        Warns\n        -----\n        UserWarning\n            If the property already exists for the modality. It will overwrite the\n            existing property.\n\n        Raises\n        ------\n        ValueError\n            If `format_string` is invalid. A valid format string contains at least one\n            placeholder enclosed in curly braces.\n        \"\"\"\n        if name in self._properties:\n            warnings.warn(\n                f\"Property '{name}' already exists for modality '{super().__str__()}'.\"\n                \"Will overwrite the existing property.\",\n                category=UserWarning,\n                stacklevel=2,\n            )\n\n        if not _is_format_string(format_string):\n            raise ValueError(\n                f\"Invalid format string '{format_string}' for property \"\n                f\"'{name}' of modality '{super().__str__()}'.\"\n            )\n\n        self._properties[name] = format_string.format(self.name)\n        setattr(self, name, self._properties[name])\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the object as a string.\"\"\"\n        return self.name.lower()\n</code></pre> <code></code> properties <code>property</code> \u00b6 <pre><code>properties\n</code></pre> <p>Return the properties associated with the modality.</p> <code></code> __post_init__ \u00b6 <pre><code>__post_init__()\n</code></pre> <p>Initialize the modality with the name and properties.</p> Source code in <code>mmlearn/datasets/core/modalities.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Initialize the modality with the name and properties.\"\"\"\n    self.name = self.name.lower()\n    self._properties = {}\n\n    for field_name in self.__dataclass_fields__:\n        if field_name not in (\"name\", \"modality_specific_properties\"):\n            field_value = f\"{self.name}_{field_name}\"\n            self._properties[field_name] = field_value\n            setattr(self, field_name, field_value)\n\n    if self.modality_specific_properties is not None:\n        for (\n            property_name,\n            format_string,\n        ) in self.modality_specific_properties.items():\n            self.add_property(property_name, format_string)\n</code></pre> <code></code> add_property \u00b6 <pre><code>add_property(name, format_string)\n</code></pre> <p>Add a new property to the modality.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the property.</p> required <code>format_string</code> <code>str</code> <p>The format string for the property. The format string should contain a placeholder that will be replaced with the name of the modality when the property is accessed.</p> required <p>Warns:</p> Type Description <code>UserWarning</code> <p>If the property already exists for the modality. It will overwrite the existing property.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>format_string</code> is invalid. A valid format string contains at least one placeholder enclosed in curly braces.</p> Source code in <code>mmlearn/datasets/core/modalities.py</code> <pre><code>def add_property(self, name: str, format_string: str) -&gt; None:\n    \"\"\"Add a new property to the modality.\n\n    Parameters\n    ----------\n    name : str\n        The name of the property.\n    format_string : str\n        The format string for the property. The format string should contain a\n        placeholder that will be replaced with the name of the modality when the\n        property is accessed.\n\n    Warns\n    -----\n    UserWarning\n        If the property already exists for the modality. It will overwrite the\n        existing property.\n\n    Raises\n    ------\n    ValueError\n        If `format_string` is invalid. A valid format string contains at least one\n        placeholder enclosed in curly braces.\n    \"\"\"\n    if name in self._properties:\n        warnings.warn(\n            f\"Property '{name}' already exists for modality '{super().__str__()}'.\"\n            \"Will overwrite the existing property.\",\n            category=UserWarning,\n            stacklevel=2,\n        )\n\n    if not _is_format_string(format_string):\n        raise ValueError(\n            f\"Invalid format string '{format_string}' for property \"\n            f\"'{name}' of modality '{super().__str__()}'.\"\n        )\n\n    self._properties[name] = format_string.format(self.name)\n    setattr(self, name, self._properties[name])\n</code></pre> <code></code> __str__ \u00b6 <pre><code>__str__()\n</code></pre> <p>Return the object as a string.</p> Source code in <code>mmlearn/datasets/core/modalities.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the object as a string.\"\"\"\n    return self.name.lower()\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.modalities.ModalityRegistry","title":"ModalityRegistry","text":"<p>Modality registry.</p> <p>A singleton class that manages the supported modalities (and their properties) in the library. The class provides methods to add new modalities and properties, and to access the existing modalities. The class is implemented as a singleton to ensure that there is only one instance of the registry in the library.</p> Source code in <code>mmlearn/datasets/core/modalities.py</code> <pre><code>class ModalityRegistry:\n    \"\"\"Modality registry.\n\n    A singleton class that manages the supported modalities (and their properties) in\n    the library. The class provides methods to add new modalities and properties, and\n    to access the existing modalities. The class is implemented as a singleton to\n    ensure that there is only one instance of the registry in the library.\n    \"\"\"\n\n    _instance: ClassVar[Any] = None\n    _modality_registry: dict[str, Modality] = {}\n\n    def __new__(cls) -&gt; Self:\n        \"\"\"Create a new instance of the class if it does not exist.\"\"\"\n        if cls._instance is None:\n            cls._instance = super().__new__(cls)\n            cls._instance._modality_registry = {}\n        return cls._instance  # type: ignore[no-any-return]\n\n    def register_modality(\n        self, name: str, modality_specific_properties: Optional[dict[str, str]] = None\n    ) -&gt; None:\n        \"\"\"Add a new modality to the registry.\n\n        Parameters\n        ----------\n        name : str\n            The name of the modality.\n        modality_specific_properties : Optional[dict[str, str]], optional, default=None\n            Additional properties specific to the modality.\n\n        Warns\n        -----\n        UserWarning\n            If the modality already exists in the registry. It will overwrite the\n            existing modality.\n\n        \"\"\"\n        if name.lower() in self._modality_registry:\n            warnings.warn(\n                f\"Modality '{name}' already exists in the registry. Overwriting...\",\n                category=UserWarning,\n                stacklevel=2,\n            )\n\n        name = name.lower()\n        modality = Modality(name, modality_specific_properties)\n        self._modality_registry[name] = modality\n        setattr(self, name, modality)\n\n    def add_default_property(self, name: str, format_string: str) -&gt; None:\n        \"\"\"Add a new property that is applicable to all modalities.\n\n        Parameters\n        ----------\n        name : str\n            The name of the property.\n        format_string : str\n            The format string for the property. The format string should contain a\n            placeholder that will be replaced with the name of the modality when the\n            property is accessed.\n\n        Warns\n        -----\n        UserWarning\n            If the property already exists for the default properties. It will\n            overwrite the existing property.\n\n        Raises\n        ------\n        ValueError\n            If the format string is invalid. A valid format string contains at least one\n            placeholder enclosed in curly braces.\n        \"\"\"\n        for modality in self._modality_registry.values():\n            modality.add_property(name, format_string)\n\n    def has_modality(self, name: str) -&gt; bool:\n        \"\"\"Check if the modality exists in the registry.\n\n        Parameters\n        ----------\n        name : str\n            The name of the modality.\n\n        Returns\n        -------\n        bool\n            True if the modality exists in the registry, False otherwise.\n        \"\"\"\n        return name.lower() in self._modality_registry\n\n    def get_modality(self, name: str) -&gt; Modality:\n        \"\"\"Get the modality name from the registry.\n\n        Parameters\n        ----------\n        name : str\n            The name of the modality.\n\n        Returns\n        -------\n        Modality\n            The modality object from the registry.\n        \"\"\"\n        return self._modality_registry[name.lower()]\n\n    def get_modality_properties(self, name: str) -&gt; dict[str, str]:\n        \"\"\"Get the properties of a modality from the registry.\n\n        Parameters\n        ----------\n        name : str\n            The name of the modality.\n\n        Returns\n        -------\n        dict[str, str]\n            The properties associated with the modality.\n        \"\"\"\n        return self.get_modality(name).properties\n\n    def list_modalities(self) -&gt; list[Modality]:\n        \"\"\"Get the list of supported modalities in the registry.\n\n        Returns\n        -------\n        list[Modality]\n            The list of supported modalities in the registry.\n        \"\"\"\n        return list(self._modality_registry.values())\n\n    def __getattr__(self, name: str) -&gt; Modality:\n        \"\"\"Access a modality as an attribute by its name.\"\"\"\n        if name.lower() in self._modality_registry:\n            return self._modality_registry[name.lower()]\n        raise AttributeError(\n            f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n        )\n</code></pre> <code></code> __new__ \u00b6 <pre><code>__new__()\n</code></pre> <p>Create a new instance of the class if it does not exist.</p> Source code in <code>mmlearn/datasets/core/modalities.py</code> <pre><code>def __new__(cls) -&gt; Self:\n    \"\"\"Create a new instance of the class if it does not exist.\"\"\"\n    if cls._instance is None:\n        cls._instance = super().__new__(cls)\n        cls._instance._modality_registry = {}\n    return cls._instance  # type: ignore[no-any-return]\n</code></pre> <code></code> register_modality \u00b6 <pre><code>register_modality(name, modality_specific_properties=None)\n</code></pre> <p>Add a new modality to the registry.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the modality.</p> required <code>modality_specific_properties</code> <code>Optional[dict[str, str]]</code> <p>Additional properties specific to the modality.</p> <code>None</code> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If the modality already exists in the registry. It will overwrite the existing modality.</p> Source code in <code>mmlearn/datasets/core/modalities.py</code> <pre><code>def register_modality(\n    self, name: str, modality_specific_properties: Optional[dict[str, str]] = None\n) -&gt; None:\n    \"\"\"Add a new modality to the registry.\n\n    Parameters\n    ----------\n    name : str\n        The name of the modality.\n    modality_specific_properties : Optional[dict[str, str]], optional, default=None\n        Additional properties specific to the modality.\n\n    Warns\n    -----\n    UserWarning\n        If the modality already exists in the registry. It will overwrite the\n        existing modality.\n\n    \"\"\"\n    if name.lower() in self._modality_registry:\n        warnings.warn(\n            f\"Modality '{name}' already exists in the registry. Overwriting...\",\n            category=UserWarning,\n            stacklevel=2,\n        )\n\n    name = name.lower()\n    modality = Modality(name, modality_specific_properties)\n    self._modality_registry[name] = modality\n    setattr(self, name, modality)\n</code></pre> <code></code> add_default_property \u00b6 <pre><code>add_default_property(name, format_string)\n</code></pre> <p>Add a new property that is applicable to all modalities.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the property.</p> required <code>format_string</code> <code>str</code> <p>The format string for the property. The format string should contain a placeholder that will be replaced with the name of the modality when the property is accessed.</p> required <p>Warns:</p> Type Description <code>UserWarning</code> <p>If the property already exists for the default properties. It will overwrite the existing property.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the format string is invalid. A valid format string contains at least one placeholder enclosed in curly braces.</p> Source code in <code>mmlearn/datasets/core/modalities.py</code> <pre><code>def add_default_property(self, name: str, format_string: str) -&gt; None:\n    \"\"\"Add a new property that is applicable to all modalities.\n\n    Parameters\n    ----------\n    name : str\n        The name of the property.\n    format_string : str\n        The format string for the property. The format string should contain a\n        placeholder that will be replaced with the name of the modality when the\n        property is accessed.\n\n    Warns\n    -----\n    UserWarning\n        If the property already exists for the default properties. It will\n        overwrite the existing property.\n\n    Raises\n    ------\n    ValueError\n        If the format string is invalid. A valid format string contains at least one\n        placeholder enclosed in curly braces.\n    \"\"\"\n    for modality in self._modality_registry.values():\n        modality.add_property(name, format_string)\n</code></pre> <code></code> has_modality \u00b6 <pre><code>has_modality(name)\n</code></pre> <p>Check if the modality exists in the registry.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the modality.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the modality exists in the registry, False otherwise.</p> Source code in <code>mmlearn/datasets/core/modalities.py</code> <pre><code>def has_modality(self, name: str) -&gt; bool:\n    \"\"\"Check if the modality exists in the registry.\n\n    Parameters\n    ----------\n    name : str\n        The name of the modality.\n\n    Returns\n    -------\n    bool\n        True if the modality exists in the registry, False otherwise.\n    \"\"\"\n    return name.lower() in self._modality_registry\n</code></pre> <code></code> get_modality \u00b6 <pre><code>get_modality(name)\n</code></pre> <p>Get the modality name from the registry.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the modality.</p> required <p>Returns:</p> Type Description <code>Modality</code> <p>The modality object from the registry.</p> Source code in <code>mmlearn/datasets/core/modalities.py</code> <pre><code>def get_modality(self, name: str) -&gt; Modality:\n    \"\"\"Get the modality name from the registry.\n\n    Parameters\n    ----------\n    name : str\n        The name of the modality.\n\n    Returns\n    -------\n    Modality\n        The modality object from the registry.\n    \"\"\"\n    return self._modality_registry[name.lower()]\n</code></pre> <code></code> get_modality_properties \u00b6 <pre><code>get_modality_properties(name)\n</code></pre> <p>Get the properties of a modality from the registry.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the modality.</p> required <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>The properties associated with the modality.</p> Source code in <code>mmlearn/datasets/core/modalities.py</code> <pre><code>def get_modality_properties(self, name: str) -&gt; dict[str, str]:\n    \"\"\"Get the properties of a modality from the registry.\n\n    Parameters\n    ----------\n    name : str\n        The name of the modality.\n\n    Returns\n    -------\n    dict[str, str]\n        The properties associated with the modality.\n    \"\"\"\n    return self.get_modality(name).properties\n</code></pre> <code></code> list_modalities \u00b6 <pre><code>list_modalities()\n</code></pre> <p>Get the list of supported modalities in the registry.</p> <p>Returns:</p> Type Description <code>list[Modality]</code> <p>The list of supported modalities in the registry.</p> Source code in <code>mmlearn/datasets/core/modalities.py</code> <pre><code>def list_modalities(self) -&gt; list[Modality]:\n    \"\"\"Get the list of supported modalities in the registry.\n\n    Returns\n    -------\n    list[Modality]\n        The list of supported modalities in the registry.\n    \"\"\"\n    return list(self._modality_registry.values())\n</code></pre> <code></code> __getattr__ \u00b6 <pre><code>__getattr__(name)\n</code></pre> <p>Access a modality as an attribute by its name.</p> Source code in <code>mmlearn/datasets/core/modalities.py</code> <pre><code>def __getattr__(self, name: str) -&gt; Modality:\n    \"\"\"Access a modality as an attribute by its name.\"\"\"\n    if name.lower() in self._modality_registry:\n        return self._modality_registry[name.lower()]\n    raise AttributeError(\n        f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n    )\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.samplers","title":"samplers","text":"<p>Samplers for data loading.</p>"},{"location":"api/#mmlearn.datasets.core.samplers.CombinedDatasetRatioSampler","title":"CombinedDatasetRatioSampler","text":"<p>               Bases: <code>Sampler[int]</code></p> <p>Sampler for weighted sampling from a class:<code>~mmlearn.datasets.core.combined_dataset.CombinedDataset</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>CombinedDataset</code> <p>An instance of class:<code>~mmlearn.datasets.core.combined_dataset.CombinedDataset</code> to sample from.</p> required <code>ratios</code> <code>Optional[Sequence[float]]</code> <p>A sequence of ratios for sampling from each dataset in the combined dataset. The length of the sequence must be equal to the number of datasets in the combined dataset (<code>dataset</code>). If <code>None</code>, the length of each dataset in the combined dataset is used as the ratio. The ratios are normalized to sum to 1.</p> <code>None</code> <code>num_samples</code> <code>Optional[int]</code> <p>The number of samples to draw from the combined dataset. If <code>None</code>, the sampler will draw as many samples as there are in the combined dataset. This number must yield at least one sample per dataset in the combined dataset, when multiplied by the corresponding ratio.</p> <code>None</code> <code>replacement</code> <code>bool</code> <p>Whether to sample with replacement or not.</p> <code>False</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the sampled indices or not. If <code>False</code>, the indices of each dataset will appear in the order they are stored in the combined dataset. This is similar to sequential sampling from each dataset. The datasets that make up the combined dataset are still sampled randomly.</p> <code>True</code> <code>rank</code> <code>Optional[int]</code> <p>Rank of the current process within :attr:<code>num_replicas</code>. By default, :attr:<code>rank</code> is retrieved from the current distributed group.</p> <code>None</code> <code>num_replicas</code> <code>Optional[int]</code> <p>Number of processes participating in distributed training. By default, :attr:<code>num_replicas</code> is retrieved from the current distributed group.</p> <code>None</code> <code>drop_last</code> <code>bool</code> <p>Whether to drop the last incomplete batch or not. If <code>True</code>, the sampler will drop samples to make the number of samples evenly divisible by the number of replicas in distributed mode.</p> <code>False</code> <code>seed</code> <code>int</code> <p>Random seed used to when sampling from the combined dataset and shuffling the sampled indices.</p> <code>0</code> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>CombinedDataset</code> <p>The dataset to sample from.</p> <code>num_samples</code> <code>int</code> <p>The number of samples to draw from the combined dataset.</p> <code>probs</code> <code>Tensor</code> <p>The probabilities for sampling from each dataset in the combined dataset. This is computed from the <code>ratios</code> argument and is normalized to sum to 1.</p> <code>replacement</code> <code>bool</code> <p>Whether to sample with replacement or not.</p> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the sampled indices or not.</p> <code>rank</code> <code>int</code> <p>Rank of the current process within :attr:<code>num_replicas</code>.</p> <code>num_replicas</code> <code>int</code> <p>Number of processes participating in distributed training.</p> <code>drop_last</code> <code>bool</code> <p>Whether to drop samples to make the number of samples evenly divisible by the number of replicas in distributed mode.</p> <code>seed</code> <code>int</code> <p>Random seed used to when sampling from the combined dataset and shuffling the sampled indices.</p> <code>epoch</code> <code>int</code> <p>Current epoch number. This is used to set the random seed. This is useful in distributed mode to ensure that each process receives a different random ordering of the samples.</p> <code>total_size</code> <code>int</code> <p>The total number of samples across all processes.</p> Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>@store(group=\"dataloader/sampler\", provider=\"mmlearn\")\nclass CombinedDatasetRatioSampler(Sampler[int]):\n    \"\"\"Sampler for weighted sampling from a :py:class:`~mmlearn.datasets.core.combined_dataset.CombinedDataset`.\n\n    Parameters\n    ----------\n    dataset : CombinedDataset\n        An instance of :py:class:`~mmlearn.datasets.core.combined_dataset.CombinedDataset`\n        to sample from.\n    ratios : Optional[Sequence[float]], optional, default=None\n        A sequence of ratios for sampling from each dataset in the combined dataset.\n        The length of the sequence must be equal to the number of datasets in the\n        combined dataset (`dataset`). If `None`, the length of each dataset in the\n        combined dataset is used as the ratio. The ratios are normalized to sum to 1.\n    num_samples : Optional[int], optional, default=None\n        The number of samples to draw from the combined dataset. If `None`, the\n        sampler will draw as many samples as there are in the combined dataset.\n        This number must yield at least one sample per dataset in the combined\n        dataset, when multiplied by the corresponding ratio.\n    replacement : bool, default=False\n        Whether to sample with replacement or not.\n    shuffle : bool, default=True\n        Whether to shuffle the sampled indices or not. If `False`, the indices of\n        each dataset will appear in the order they are stored in the combined dataset.\n        This is similar to sequential sampling from each dataset. The datasets\n        that make up the combined dataset are still sampled randomly.\n    rank : Optional[int], optional, default=None\n        Rank of the current process within :attr:`num_replicas`. By default,\n        :attr:`rank` is retrieved from the current distributed group.\n    num_replicas : Optional[int], optional, default=None\n        Number of processes participating in distributed training. By\n        default, :attr:`num_replicas` is retrieved from the current distributed group.\n    drop_last : bool, default=False\n        Whether to drop the last incomplete batch or not. If `True`, the sampler will\n        drop samples to make the number of samples evenly divisible by the number of\n        replicas in distributed mode.\n    seed : int, default=0\n        Random seed used to when sampling from the combined dataset and shuffling\n        the sampled indices.\n\n    Attributes\n    ----------\n    dataset : CombinedDataset\n        The dataset to sample from.\n    num_samples : int\n        The number of samples to draw from the combined dataset.\n    probs : torch.Tensor\n        The probabilities for sampling from each dataset in the combined dataset.\n        This is computed from the `ratios` argument and is normalized to sum to 1.\n    replacement : bool\n        Whether to sample with replacement or not.\n    shuffle : bool\n        Whether to shuffle the sampled indices or not.\n    rank : int\n        Rank of the current process within :attr:`num_replicas`.\n    num_replicas : int\n        Number of processes participating in distributed training.\n    drop_last : bool\n        Whether to drop samples to make the number of samples evenly divisible by the\n        number of replicas in distributed mode.\n    seed : int\n        Random seed used to when sampling from the combined dataset and shuffling\n        the sampled indices.\n    epoch : int\n        Current epoch number. This is used to set the random seed. This is useful\n        in distributed mode to ensure that each process receives a different random\n        ordering of the samples.\n    total_size : int\n        The total number of samples across all processes.\n    \"\"\"  # noqa: W505\n\n    def __init__(  # noqa: PLR0912\n        self,\n        dataset: CombinedDataset,\n        ratios: Optional[Sequence[float]] = None,\n        num_samples: Optional[int] = None,\n        replacement: bool = False,\n        shuffle: bool = True,\n        rank: Optional[int] = None,\n        num_replicas: Optional[int] = None,\n        drop_last: bool = False,\n        seed: int = 0,\n    ):\n        if not isinstance(dataset, CombinedDataset):\n            raise TypeError(\n                \"Expected argument `dataset` to be of type `CombinedDataset`, \"\n                f\"but got {type(dataset)}.\",\n            )\n        if not isinstance(seed, int):\n            raise TypeError(\n                f\"Expected argument `seed` to be an integer, but got {type(seed)}.\",\n            )\n        if num_replicas is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            num_replicas = dist.get_world_size()\n        if rank is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            rank = dist.get_rank()\n        if rank &gt;= num_replicas or rank &lt; 0:\n            raise ValueError(\n                f\"Invalid rank {rank}, rank should be in the interval [0, {num_replicas - 1}]\"\n            )\n\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.drop_last = drop_last\n        self.replacement = replacement\n        self.shuffle = shuffle\n        self.seed = seed\n        self.epoch = 0\n\n        self._num_samples = num_samples\n        if not isinstance(self.num_samples, int) or self.num_samples &lt;= 0:\n            raise ValueError(\n                \"Expected argument `num_samples` to be a positive integer, but got \"\n                f\"{self.num_samples}.\",\n            )\n\n        if ratios is None:\n            ratios = [len(subset) for subset in self.dataset.datasets]\n\n        num_datasets = len(self.dataset.datasets)\n        if len(ratios) != num_datasets:\n            raise ValueError(\n                f\"Expected argument `ratios` to be of length {num_datasets}, \"\n                f\"but got length {len(ratios)}.\",\n            )\n        prob_sum = sum(ratios)\n        if not all(ratio &gt;= 0 for ratio in ratios) and prob_sum &gt; 0:\n            raise ValueError(\n                \"Expected argument `ratios` to be a sequence of non-negative numbers. \"\n                f\"Got {ratios}.\",\n            )\n        self.probs = torch.tensor(\n            [ratio / prob_sum for ratio in ratios],\n            dtype=torch.double,\n        )\n        if any((prob * self.num_samples) &lt;= 0 for prob in self.probs):\n            raise ValueError(\n                \"Expected dataset ratio to result in at least one sample per dataset. \"\n                f\"Got dataset sizes {self.probs * self.num_samples}.\",\n            )\n\n    @property\n    def num_samples(self) -&gt; int:\n        \"\"\"Return the number of samples managed by the sampler.\"\"\"\n        # dataset size might change at runtime\n        if self._num_samples is None:\n            num_samples = len(self.dataset)\n        else:\n            num_samples = self._num_samples\n\n        if self.drop_last and num_samples % self.num_replicas != 0:\n            # split to nearest available length that is evenly divisible.\n            # This is to ensure each rank receives the same amount of data when\n            # using this Sampler.\n            num_samples = math.ceil(\n                (num_samples - self.num_replicas) / self.num_replicas,\n            )\n        else:\n            num_samples = math.ceil(num_samples / self.num_replicas)\n        return num_samples\n\n    @property\n    def total_size(self) -&gt; int:\n        \"\"\"Return the total size of the dataset.\"\"\"\n        return self.num_samples * self.num_replicas\n\n    def __iter__(self) -&gt; Iterator[int]:\n        \"\"\"Return an iterator that yields sample indices for the combined dataset.\"\"\"\n        generator = torch.Generator()\n        seed = self.seed + self.epoch\n        generator.manual_seed(seed)\n\n        cumulative_sizes = [0] + self.dataset._cumulative_sizes\n        num_samples_per_dataset = [int(prob * self.total_size) for prob in self.probs]\n        indices = []\n        for i in range(len(self.dataset.datasets)):\n            per_dataset_indices: torch.Tensor = torch.multinomial(\n                torch.ones(cumulative_sizes[i + 1] - cumulative_sizes[i]),\n                num_samples_per_dataset[i],\n                replacement=self.replacement,\n                generator=generator,\n            )\n            # adjust indices to reflect position in cumulative dataset\n            per_dataset_indices += cumulative_sizes[i]\n            assert per_dataset_indices.max() &lt; cumulative_sizes[i + 1], (\n                f\"Indices from dataset {i} exceed dataset size. \"\n                f\"Got indices {per_dataset_indices} and dataset size {cumulative_sizes[i + 1]}.\",\n            )\n            indices.append(per_dataset_indices)\n\n        indices = torch.cat(indices)\n        if self.shuffle:\n            rand_indices = torch.randperm(len(indices), generator=generator)\n            indices = indices[rand_indices]\n\n        indices = indices.tolist()  # type: ignore[attr-defined]\n        num_indices = len(indices)\n\n        if num_indices &lt; self.total_size:\n            padding_size = self.total_size - num_indices\n            if padding_size &lt;= num_indices:\n                indices += indices[:padding_size]\n            else:\n                indices += (indices * math.ceil(padding_size / num_indices))[\n                    :padding_size\n                ]\n        elif num_indices &gt; self.total_size:\n            indices = indices[: self.total_size]\n        assert len(indices) == self.total_size\n\n        # subsample\n        indices = indices[self.rank : self.total_size : self.num_replicas]\n        assert len(indices) == self.num_samples, (\n            f\"Expected {self.num_samples} samples, but got {len(indices)}.\",\n        )\n\n        yield from iter(indices)\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the total number of samples in the sampler.\"\"\"\n        return self.num_samples\n\n    def set_epoch(self, epoch: int) -&gt; None:\n        \"\"\"Set the epoch for this sampler.\n\n        When :attr:`shuffle=True`, this ensures all replicas use a different random\n        ordering for each epoch. Otherwise, the next iteration of this sampler\n        will yield the same ordering.\n\n        Parameters\n        ----------\n        epoch : int\n            Epoch number.\n\n        \"\"\"\n        self.epoch = epoch\n\n        # some iterable datasets (especially huggingface iterable datasets) might\n        # require setting the epoch to ensure shuffling works properly\n        for dataset in self.dataset.datasets:\n            if hasattr(dataset, \"set_epoch\"):\n                dataset.set_epoch(epoch)\n</code></pre> <code></code> num_samples <code>property</code> \u00b6 <pre><code>num_samples\n</code></pre> <p>Return the number of samples managed by the sampler.</p> <code></code> total_size <code>property</code> \u00b6 <pre><code>total_size\n</code></pre> <p>Return the total size of the dataset.</p> <code></code> __iter__ \u00b6 <pre><code>__iter__()\n</code></pre> <p>Return an iterator that yields sample indices for the combined dataset.</p> Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>def __iter__(self) -&gt; Iterator[int]:\n    \"\"\"Return an iterator that yields sample indices for the combined dataset.\"\"\"\n    generator = torch.Generator()\n    seed = self.seed + self.epoch\n    generator.manual_seed(seed)\n\n    cumulative_sizes = [0] + self.dataset._cumulative_sizes\n    num_samples_per_dataset = [int(prob * self.total_size) for prob in self.probs]\n    indices = []\n    for i in range(len(self.dataset.datasets)):\n        per_dataset_indices: torch.Tensor = torch.multinomial(\n            torch.ones(cumulative_sizes[i + 1] - cumulative_sizes[i]),\n            num_samples_per_dataset[i],\n            replacement=self.replacement,\n            generator=generator,\n        )\n        # adjust indices to reflect position in cumulative dataset\n        per_dataset_indices += cumulative_sizes[i]\n        assert per_dataset_indices.max() &lt; cumulative_sizes[i + 1], (\n            f\"Indices from dataset {i} exceed dataset size. \"\n            f\"Got indices {per_dataset_indices} and dataset size {cumulative_sizes[i + 1]}.\",\n        )\n        indices.append(per_dataset_indices)\n\n    indices = torch.cat(indices)\n    if self.shuffle:\n        rand_indices = torch.randperm(len(indices), generator=generator)\n        indices = indices[rand_indices]\n\n    indices = indices.tolist()  # type: ignore[attr-defined]\n    num_indices = len(indices)\n\n    if num_indices &lt; self.total_size:\n        padding_size = self.total_size - num_indices\n        if padding_size &lt;= num_indices:\n            indices += indices[:padding_size]\n        else:\n            indices += (indices * math.ceil(padding_size / num_indices))[\n                :padding_size\n            ]\n    elif num_indices &gt; self.total_size:\n        indices = indices[: self.total_size]\n    assert len(indices) == self.total_size\n\n    # subsample\n    indices = indices[self.rank : self.total_size : self.num_replicas]\n    assert len(indices) == self.num_samples, (\n        f\"Expected {self.num_samples} samples, but got {len(indices)}.\",\n    )\n\n    yield from iter(indices)\n</code></pre> <code></code> __len__ \u00b6 <pre><code>__len__()\n</code></pre> <p>Return the total number of samples in the sampler.</p> Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the total number of samples in the sampler.\"\"\"\n    return self.num_samples\n</code></pre> <code></code> set_epoch \u00b6 <pre><code>set_epoch(epoch)\n</code></pre> <p>Set the epoch for this sampler.</p> <p>When :attr:<code>shuffle=True</code>, this ensures all replicas use a different random ordering for each epoch. Otherwise, the next iteration of this sampler will yield the same ordering.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>Epoch number.</p> required Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>def set_epoch(self, epoch: int) -&gt; None:\n    \"\"\"Set the epoch for this sampler.\n\n    When :attr:`shuffle=True`, this ensures all replicas use a different random\n    ordering for each epoch. Otherwise, the next iteration of this sampler\n    will yield the same ordering.\n\n    Parameters\n    ----------\n    epoch : int\n        Epoch number.\n\n    \"\"\"\n    self.epoch = epoch\n\n    # some iterable datasets (especially huggingface iterable datasets) might\n    # require setting the epoch to ensure shuffling works properly\n    for dataset in self.dataset.datasets:\n        if hasattr(dataset, \"set_epoch\"):\n            dataset.set_epoch(epoch)\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.samplers.DistributedEvalSampler","title":"DistributedEvalSampler","text":"<p>               Bases: <code>Sampler[int]</code></p> <p>Sampler for distributed evaluation.</p> <p>The main differences between this and class:<code>torch.utils.data.DistributedSampler</code> are that this sampler does not add extra samples to make it evenly divisible and shuffling is disabled by default.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>Dataset used for sampling.</p> required <code>num_replicas</code> <code>Optional[int]</code> <p>Number of processes participating in distributed training. By default, :attr:<code>rank</code> is retrieved from the current distributed group.</p> <code>None</code> <code>rank</code> <code>Optional[int]</code> <p>Rank of the current process within :attr:<code>num_replicas</code>. By default, :attr:<code>rank</code> is retrieved from the current distributed group.</p> <code>None</code> <code>shuffle</code> <code>bool</code> <p>If <code>True</code> (default), sampler will shuffle the indices.</p> <code>False</code> <code>seed</code> <code>int</code> <p>Random seed used to shuffle the sampler if :attr:<code>shuffle=True</code>. This number should be identical across all processes in the distributed group.</p> <code>0</code> Warnings <p>DistributedEvalSampler should NOT be used for training. The distributed processes could hang forever. See [1]_ for details</p> Notes <ul> <li>This sampler is for evaluation purpose where synchronization does not happen   every epoch. Synchronization should be done outside the dataloader loop.   It is especially useful in conjunction with   class:<code>torch.nn.parallel.DistributedDataParallel</code> [2]_.</li> <li>The input Dataset is assumed to be of constant size.</li> <li>This implementation is adapted from [3]_.</li> </ul> References <p>.. [1] https://github.com/pytorch/pytorch/issues/22584 .. [2] https://discuss.pytorch.org/t/how-to-validate-in-distributeddataparallel-correctly/94267/11 .. [3] https://github.com/SeungjunNah/DeepDeblur-PyTorch/blob/master/src/data/sampler.py</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; def example():\n...     start_epoch, n_epochs = 0, 2\n...     sampler = DistributedEvalSampler(dataset) if is_distributed else None\n...     loader = DataLoader(dataset, shuffle=(sampler is None), sampler=sampler)\n...     for epoch in range(start_epoch, n_epochs):\n...         if is_distributed:\n...             sampler.set_epoch(epoch)\n...         evaluate(loader)\n</code></pre> Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>@store(group=\"dataloader/sampler\", provider=\"mmlearn\")\nclass DistributedEvalSampler(Sampler[int]):\n    \"\"\"Sampler for distributed evaluation.\n\n    The main differences between this and :py:class:`torch.utils.data.DistributedSampler`\n    are that this sampler does not add extra samples to make it evenly divisible and\n    shuffling is disabled by default.\n\n    Parameters\n    ----------\n    dataset : torch.utils.data.Dataset\n        Dataset used for sampling.\n    num_replicas : Optional[int], optional, default=None\n        Number of processes participating in distributed training. By\n        default, :attr:`rank` is retrieved from the current distributed group.\n    rank : Optional[int], optional, default=None\n        Rank of the current process within :attr:`num_replicas`. By default,\n        :attr:`rank` is retrieved from the current distributed group.\n    shuffle : bool, optional, default=False\n        If `True` (default), sampler will shuffle the indices.\n    seed : int, optional, default=0\n        Random seed used to shuffle the sampler if :attr:`shuffle=True`.\n        This number should be identical across all processes in the\n        distributed group.\n\n    Warnings\n    --------\n    DistributedEvalSampler should NOT be used for training. The distributed processes\n    could hang forever. See [1]_ for details\n\n    Notes\n    -----\n    - This sampler is for evaluation purpose where synchronization does not happen\n      every epoch. Synchronization should be done outside the dataloader loop.\n      It is especially useful in conjunction with\n      :py:class:`torch.nn.parallel.DistributedDataParallel` [2]_.\n    - The input Dataset is assumed to be of constant size.\n    - This implementation is adapted from [3]_.\n\n    References\n    ----------\n    .. [1] https://github.com/pytorch/pytorch/issues/22584\n    .. [2] https://discuss.pytorch.org/t/how-to-validate-in-distributeddataparallel-correctly/94267/11\n    .. [3] https://github.com/SeungjunNah/DeepDeblur-PyTorch/blob/master/src/data/sampler.py\n\n\n    Examples\n    --------\n    &gt;&gt;&gt; def example():\n    ...     start_epoch, n_epochs = 0, 2\n    ...     sampler = DistributedEvalSampler(dataset) if is_distributed else None\n    ...     loader = DataLoader(dataset, shuffle=(sampler is None), sampler=sampler)\n    ...     for epoch in range(start_epoch, n_epochs):\n    ...         if is_distributed:\n    ...             sampler.set_epoch(epoch)\n    ...         evaluate(loader)\n\n    \"\"\"  # noqa: W505\n\n    def __init__(\n        self,\n        dataset: Dataset[Sized],\n        num_replicas: Optional[int] = None,\n        rank: Optional[int] = None,\n        shuffle: bool = False,\n        seed: int = 0,\n    ) -&gt; None:\n        if num_replicas is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            num_replicas = dist.get_world_size()\n        if rank is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            rank = dist.get_rank()\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.epoch = 0\n        self.shuffle = shuffle\n        self.seed = seed\n\n    @property\n    def total_size(self) -&gt; int:\n        \"\"\"Return the total size of the dataset.\"\"\"\n        return len(self.dataset)\n\n    @property\n    def num_samples(self) -&gt; int:\n        \"\"\"Return the number of samples managed by the sampler.\"\"\"\n        indices = list(range(self.total_size))[\n            self.rank : self.total_size : self.num_replicas\n        ]\n        return len(indices)  # true value without extra samples\n\n    def __iter__(self) -&gt; Iterator[int]:\n        \"\"\"Return an iterator that iterates over the indices of the dataset.\"\"\"\n        if self.shuffle:\n            # deterministically shuffle based on epoch and seed\n            g = torch.Generator()\n            g.manual_seed(self.seed + self.epoch)\n            indices = torch.randperm(self.total_size, generator=g).tolist()\n        else:\n            indices = list(range(self.total_size))\n\n        # subsample\n        indices = indices[self.rank : self.total_size : self.num_replicas]\n        assert len(indices) == self.num_samples\n\n        return iter(indices)\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the number of samples.\"\"\"\n        return self.num_samples\n\n    def set_epoch(self, epoch: int) -&gt; None:\n        \"\"\"Set the epoch for this sampler.\n\n        When :attr:`shuffle=True`, this ensures all replicas use a different random\n        ordering for each epoch. Otherwise, the next iteration of this sampler\n        will yield the same ordering.\n\n        Parameters\n        ----------\n        epoch : int\n            Epoch number.\n\n        \"\"\"\n        self.epoch = epoch\n</code></pre> <code></code> total_size <code>property</code> \u00b6 <pre><code>total_size\n</code></pre> <p>Return the total size of the dataset.</p> <code></code> num_samples <code>property</code> \u00b6 <pre><code>num_samples\n</code></pre> <p>Return the number of samples managed by the sampler.</p> <code></code> __iter__ \u00b6 <pre><code>__iter__()\n</code></pre> <p>Return an iterator that iterates over the indices of the dataset.</p> Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>def __iter__(self) -&gt; Iterator[int]:\n    \"\"\"Return an iterator that iterates over the indices of the dataset.\"\"\"\n    if self.shuffle:\n        # deterministically shuffle based on epoch and seed\n        g = torch.Generator()\n        g.manual_seed(self.seed + self.epoch)\n        indices = torch.randperm(self.total_size, generator=g).tolist()\n    else:\n        indices = list(range(self.total_size))\n\n    # subsample\n    indices = indices[self.rank : self.total_size : self.num_replicas]\n    assert len(indices) == self.num_samples\n\n    return iter(indices)\n</code></pre> <code></code> __len__ \u00b6 <pre><code>__len__()\n</code></pre> <p>Return the number of samples.</p> Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the number of samples.\"\"\"\n    return self.num_samples\n</code></pre> <code></code> set_epoch \u00b6 <pre><code>set_epoch(epoch)\n</code></pre> <p>Set the epoch for this sampler.</p> <p>When :attr:<code>shuffle=True</code>, this ensures all replicas use a different random ordering for each epoch. Otherwise, the next iteration of this sampler will yield the same ordering.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>Epoch number.</p> required Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>def set_epoch(self, epoch: int) -&gt; None:\n    \"\"\"Set the epoch for this sampler.\n\n    When :attr:`shuffle=True`, this ensures all replicas use a different random\n    ordering for each epoch. Otherwise, the next iteration of this sampler\n    will yield the same ordering.\n\n    Parameters\n    ----------\n    epoch : int\n        Epoch number.\n\n    \"\"\"\n    self.epoch = epoch\n</code></pre>"},{"location":"api/#mmlearn.datasets.imagenet","title":"imagenet","text":"<p>ImageNet dataset.</p>"},{"location":"api/#mmlearn.datasets.imagenet.ImageNet","title":"ImageNet","text":"<p>               Bases: <code>ImageFolder</code></p> <p>ImageNet dataset.</p> <p>This is a wrapper around the class:<code>~torchvision.datasets.ImageFolder</code> class that returns an class:<code>~mmlearn.datasets.core.example.Example</code> object.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>Path to the root directory of the dataset.</p> required <code>split</code> <code>(train, val)</code> <p>The split of the dataset to use.</p> <code>\"train\"</code> <code>transform</code> <code>Optional[Callable]</code> <p>A callable that takes in a PIL image and returns a transformed version of the image as a PyTorch tensor.</p> <code>None</code> <code>target_transform</code> <code>Optional[Callable]</code> <p>A callable that takes in the target and transforms it.</p> <code>None</code> <code>mask_generator</code> <code>Optional[Callable]</code> <p>A callable that generates a mask for the image.</p> <code>None</code> Source code in <code>mmlearn/datasets/imagenet.py</code> <pre><code>@store(\n    group=\"datasets\",\n    provider=\"mmlearn\",\n    root_dir=os.getenv(\"IMAGENET_ROOT_DIR\", MISSING),\n)\nclass ImageNet(ImageFolder):\n    \"\"\"ImageNet dataset.\n\n    This is a wrapper around the :py:class:`~torchvision.datasets.ImageFolder` class\n    that returns an :py:class:`~mmlearn.datasets.core.example.Example` object.\n\n    Parameters\n    ----------\n    root_dir : str\n        Path to the root directory of the dataset.\n    split : {\"train\", \"val\"}, default=\"train\"\n        The split of the dataset to use.\n    transform : Optional[Callable], optional, default=None\n        A callable that takes in a PIL image and returns a transformed version\n        of the image as a PyTorch tensor.\n    target_transform : Optional[Callable], optional, default=None\n        A callable that takes in the target and transforms it.\n    mask_generator : Optional[Callable], optional, default=None\n        A callable that generates a mask for the image.\n    \"\"\"\n\n    def __init__(\n        self,\n        root_dir: str,\n        split: Literal[\"train\", \"val\"] = \"train\",\n        transform: Optional[Callable[..., Any]] = None,\n        target_transform: Optional[Callable[..., Any]] = None,\n        mask_generator: Optional[Callable[..., Any]] = None,\n    ) -&gt; None:\n        split = \"train\" if split == \"train\" else \"val\"\n        root_dir = os.path.join(root_dir, split)\n        super().__init__(\n            root=root_dir, transform=transform, target_transform=target_transform\n        )\n        self.mask_generator = mask_generator\n\n    def __getitem__(self, index: int) -&gt; Example:\n        \"\"\"Get an example at the given index.\"\"\"\n        image, target = super().__getitem__(index)\n        example = Example(\n            {\n                Modalities.RGB.name: image,\n                Modalities.RGB.target: target,\n                EXAMPLE_INDEX_KEY: index,\n            }\n        )\n        mask = self.mask_generator() if self.mask_generator else None\n        if mask is not None:  # error will be raised during collation if `None`\n            example[Modalities.RGB.mask] = mask\n        return example\n\n    @property\n    def zero_shot_prompt_templates(self) -&gt; list[str]:\n        \"\"\"Return the zero-shot prompt templates.\"\"\"\n        return [\n            \"a bad photo of a {}.\",\n            \"a photo of many {}.\",\n            \"a sculpture of a {}.\",\n            \"a photo of the hard to see {}.\",\n            \"a low resolution photo of the {}.\",\n            \"a rendering of a {}.\",\n            \"graffiti of a {}.\",\n            \"a bad photo of the {}.\",\n            \"a cropped photo of the {}.\",\n            \"a tattoo of a {}.\",\n            \"the embroidered {}.\",\n            \"a photo of a hard to see {}.\",\n            \"a bright photo of a {}.\",\n            \"a photo of a clean {}.\",\n            \"a photo of a dirty {}.\",\n            \"a dark photo of the {}.\",\n            \"a drawing of a {}.\",\n            \"a photo of my {}.\",\n            \"the plastic {}.\",\n            \"a photo of the cool {}.\",\n            \"a close-up photo of a {}.\",\n            \"a black and white photo of the {}.\",\n            \"a painting of the {}.\",\n            \"a painting of a {}.\",\n            \"a pixelated photo of the {}.\",\n            \"a sculpture of the {}.\",\n            \"a bright photo of the {}.\",\n            \"a cropped photo of a {}.\",\n            \"a plastic {}.\",\n            \"a photo of the dirty {}.\",\n            \"a jpeg corrupted photo of a {}.\",\n            \"a blurry photo of the {}.\",\n            \"a photo of the {}.\",\n            \"a good photo of the {}.\",\n            \"a rendering of the {}.\",\n            \"a {} in a video game.\",\n            \"a photo of one {}.\",\n            \"a doodle of a {}.\",\n            \"a close-up photo of the {}.\",\n            \"a photo of a {}.\",\n            \"the origami {}.\",\n            \"the {} in a video game.\",\n            \"a sketch of a {}.\",\n            \"a doodle of the {}.\",\n            \"a origami {}.\",\n            \"a low resolution photo of a {}.\",\n            \"the toy {}.\",\n            \"a rendition of the {}.\",\n            \"a photo of the clean {}.\",\n            \"a photo of a large {}.\",\n            \"a rendition of a {}.\",\n            \"a photo of a nice {}.\",\n            \"a photo of a weird {}.\",\n            \"a blurry photo of a {}.\",\n            \"a cartoon {}.\",\n            \"art of a {}.\",\n            \"a sketch of the {}.\",\n            \"a embroidered {}.\",\n            \"a pixelated photo of a {}.\",\n            \"itap of the {}.\",\n            \"a jpeg corrupted photo of the {}.\",\n            \"a good photo of a {}.\",\n            \"a plushie {}.\",\n            \"a photo of the nice {}.\",\n            \"a photo of the small {}.\",\n            \"a photo of the weird {}.\",\n            \"the cartoon {}.\",\n            \"art of the {}.\",\n            \"a drawing of the {}.\",\n            \"a photo of the large {}.\",\n            \"a black and white photo of a {}.\",\n            \"the plushie {}.\",\n            \"a dark photo of a {}.\",\n            \"itap of a {}.\",\n            \"graffiti of the {}.\",\n            \"a toy {}.\",\n            \"itap of my {}.\",\n            \"a photo of a cool {}.\",\n            \"a photo of a small {}.\",\n            \"a tattoo of the {}.\",\n        ]\n\n    @property\n    def id2label(self) -&gt; dict[int, str]:\n        \"\"\"Return the label mapping.\"\"\"\n        return {\n            0: \"tench\",\n            1: \"goldfish\",\n            2: \"great white shark\",\n            3: \"tiger shark\",\n            4: \"hammerhead shark\",\n            5: \"electric ray\",\n            6: \"stingray\",\n            7: \"rooster\",\n            8: \"hen\",\n            9: \"ostrich\",\n            10: \"brambling\",\n            11: \"goldfinch\",\n            12: \"house finch\",\n            13: \"junco\",\n            14: \"indigo bunting\",\n            15: \"American robin\",\n            16: \"bulbul\",\n            17: \"jay\",\n            18: \"magpie\",\n            19: \"chickadee\",\n            20: \"American dipper\",\n            21: \"kite (bird of prey)\",\n            22: \"bald eagle\",\n            23: \"vulture\",\n            24: \"great grey owl\",\n            25: \"fire salamander\",\n            26: \"smooth newt\",\n            27: \"newt\",\n            28: \"spotted salamander\",\n            29: \"axolotl\",\n            30: \"American bullfrog\",\n            31: \"tree frog\",\n            32: \"tailed frog\",\n            33: \"loggerhead sea turtle\",\n            34: \"leatherback sea turtle\",\n            35: \"mud turtle\",\n            36: \"terrapin\",\n            37: \"box turtle\",\n            38: \"banded gecko\",\n            39: \"green iguana\",\n            40: \"Carolina anole\",\n            41: \"desert grassland whiptail lizard\",\n            42: \"agama\",\n            43: \"frilled-necked lizard\",\n            44: \"alligator lizard\",\n            45: \"Gila monster\",\n            46: \"European green lizard\",\n            47: \"chameleon\",\n            48: \"Komodo dragon\",\n            49: \"Nile crocodile\",\n            50: \"American alligator\",\n            51: \"triceratops\",\n            52: \"worm snake\",\n            53: \"ring-necked snake\",\n            54: \"eastern hog-nosed snake\",\n            55: \"smooth green snake\",\n            56: \"kingsnake\",\n            57: \"garter snake\",\n            58: \"water snake\",\n            59: \"vine snake\",\n            60: \"night snake\",\n            61: \"boa constrictor\",\n            62: \"African rock python\",\n            63: \"Indian cobra\",\n            64: \"green mamba\",\n            65: \"sea snake\",\n            66: \"Saharan horned viper\",\n            67: \"eastern diamondback rattlesnake\",\n            68: \"sidewinder rattlesnake\",\n            69: \"trilobite\",\n            70: \"harvestman\",\n            71: \"scorpion\",\n            72: \"yellow garden spider\",\n            73: \"barn spider\",\n            74: \"European garden spider\",\n            75: \"southern black widow\",\n            76: \"tarantula\",\n            77: \"wolf spider\",\n            78: \"tick\",\n            79: \"centipede\",\n            80: \"black grouse\",\n            81: \"ptarmigan\",\n            82: \"ruffed grouse\",\n            83: \"prairie grouse\",\n            84: \"peafowl\",\n            85: \"quail\",\n            86: \"partridge\",\n            87: \"african grey parrot\",\n            88: \"macaw\",\n            89: \"sulphur-crested cockatoo\",\n            90: \"lorikeet\",\n            91: \"coucal\",\n            92: \"bee eater\",\n            93: \"hornbill\",\n            94: \"hummingbird\",\n            95: \"jacamar\",\n            96: \"toucan\",\n            97: \"duck\",\n            98: \"red-breasted merganser\",\n            99: \"goose\",\n            100: \"black swan\",\n            101: \"tusker\",\n            102: \"echidna\",\n            103: \"platypus\",\n            104: \"wallaby\",\n            105: \"koala\",\n            106: \"wombat\",\n            107: \"jellyfish\",\n            108: \"sea anemone\",\n            109: \"brain coral\",\n            110: \"flatworm\",\n            111: \"nematode\",\n            112: \"conch\",\n            113: \"snail\",\n            114: \"slug\",\n            115: \"sea slug\",\n            116: \"chiton\",\n            117: \"chambered nautilus\",\n            118: \"Dungeness crab\",\n            119: \"rock crab\",\n            120: \"fiddler crab\",\n            121: \"red king crab\",\n            122: \"American lobster\",\n            123: \"spiny lobster\",\n            124: \"crayfish\",\n            125: \"hermit crab\",\n            126: \"isopod\",\n            127: \"white stork\",\n            128: \"black stork\",\n            129: \"spoonbill\",\n            130: \"flamingo\",\n            131: \"little blue heron\",\n            132: \"great egret\",\n            133: \"bittern bird\",\n            134: \"crane bird\",\n            135: \"limpkin\",\n            136: \"common gallinule\",\n            137: \"American coot\",\n            138: \"bustard\",\n            139: \"ruddy turnstone\",\n            140: \"dunlin\",\n            141: \"common redshank\",\n            142: \"dowitcher\",\n            143: \"oystercatcher\",\n            144: \"pelican\",\n            145: \"king penguin\",\n            146: \"albatross\",\n            147: \"grey whale\",\n            148: \"killer whale\",\n            149: \"dugong\",\n            150: \"sea lion\",\n            151: \"Chihuahua\",\n            152: \"Japanese Chin\",\n            153: \"Maltese\",\n            154: \"Pekingese\",\n            155: \"Shih Tzu\",\n            156: \"King Charles Spaniel\",\n            157: \"Papillon\",\n            158: \"toy terrier\",\n            159: \"Rhodesian Ridgeback\",\n            160: \"Afghan Hound\",\n            161: \"Basset Hound\",\n            162: \"Beagle\",\n            163: \"Bloodhound\",\n            164: \"Bluetick Coonhound\",\n            165: \"Black and Tan Coonhound\",\n            166: \"Treeing Walker Coonhound\",\n            167: \"English foxhound\",\n            168: \"Redbone Coonhound\",\n            169: \"borzoi\",\n            170: \"Irish Wolfhound\",\n            171: \"Italian Greyhound\",\n            172: \"Whippet\",\n            173: \"Ibizan Hound\",\n            174: \"Norwegian Elkhound\",\n            175: \"Otterhound\",\n            176: \"Saluki\",\n            177: \"Scottish Deerhound\",\n            178: \"Weimaraner\",\n            179: \"Staffordshire Bull Terrier\",\n            180: \"American Staffordshire Terrier\",\n            181: \"Bedlington Terrier\",\n            182: \"Border Terrier\",\n            183: \"Kerry Blue Terrier\",\n            184: \"Irish Terrier\",\n            185: \"Norfolk Terrier\",\n            186: \"Norwich Terrier\",\n            187: \"Yorkshire Terrier\",\n            188: \"Wire Fox Terrier\",\n            189: \"Lakeland Terrier\",\n            190: \"Sealyham Terrier\",\n            191: \"Airedale Terrier\",\n            192: \"Cairn Terrier\",\n            193: \"Australian Terrier\",\n            194: \"Dandie Dinmont Terrier\",\n            195: \"Boston Terrier\",\n            196: \"Miniature Schnauzer\",\n            197: \"Giant Schnauzer\",\n            198: \"Standard Schnauzer\",\n            199: \"Scottish Terrier\",\n            200: \"Tibetan Terrier\",\n            201: \"Australian Silky Terrier\",\n            202: \"Soft-coated Wheaten Terrier\",\n            203: \"West Highland White Terrier\",\n            204: \"Lhasa Apso\",\n            205: \"Flat-Coated Retriever\",\n            206: \"Curly-coated Retriever\",\n            207: \"Golden Retriever\",\n            208: \"Labrador Retriever\",\n            209: \"Chesapeake Bay Retriever\",\n            210: \"German Shorthaired Pointer\",\n            211: \"Vizsla\",\n            212: \"English Setter\",\n            213: \"Irish Setter\",\n            214: \"Gordon Setter\",\n            215: \"Brittany dog\",\n            216: \"Clumber Spaniel\",\n            217: \"English Springer Spaniel\",\n            218: \"Welsh Springer Spaniel\",\n            219: \"Cocker Spaniel\",\n            220: \"Sussex Spaniel\",\n            221: \"Irish Water Spaniel\",\n            222: \"Kuvasz\",\n            223: \"Schipperke\",\n            224: \"Groenendael dog\",\n            225: \"Malinois\",\n            226: \"Briard\",\n            227: \"Australian Kelpie\",\n            228: \"Komondor\",\n            229: \"Old English Sheepdog\",\n            230: \"Shetland Sheepdog\",\n            231: \"collie\",\n            232: \"Border Collie\",\n            233: \"Bouvier des Flandres dog\",\n            234: \"Rottweiler\",\n            235: \"German Shepherd Dog\",\n            236: \"Dobermann\",\n            237: \"Miniature Pinscher\",\n            238: \"Greater Swiss Mountain Dog\",\n            239: \"Bernese Mountain Dog\",\n            240: \"Appenzeller Sennenhund\",\n            241: \"Entlebucher Sennenhund\",\n            242: \"Boxer\",\n            243: \"Bullmastiff\",\n            244: \"Tibetan Mastiff\",\n            245: \"French Bulldog\",\n            246: \"Great Dane\",\n            247: \"St. Bernard\",\n            248: \"husky\",\n            249: \"Alaskan Malamute\",\n            250: \"Siberian Husky\",\n            251: \"Dalmatian\",\n            252: \"Affenpinscher\",\n            253: \"Basenji\",\n            254: \"pug\",\n            255: \"Leonberger\",\n            256: \"Newfoundland dog\",\n            257: \"Great Pyrenees dog\",\n            258: \"Samoyed\",\n            259: \"Pomeranian\",\n            260: \"Chow Chow\",\n            261: \"Keeshond\",\n            262: \"brussels griffon\",\n            263: \"Pembroke Welsh Corgi\",\n            264: \"Cardigan Welsh Corgi\",\n            265: \"Toy Poodle\",\n            266: \"Miniature Poodle\",\n            267: \"Standard Poodle\",\n            268: \"Mexican hairless dog (xoloitzcuintli)\",\n            269: \"grey wolf\",\n            270: \"Alaskan tundra wolf\",\n            271: \"red wolf or maned wolf\",\n            272: \"coyote\",\n            273: \"dingo\",\n            274: \"dhole\",\n            275: \"African wild dog\",\n            276: \"hyena\",\n            277: \"red fox\",\n            278: \"kit fox\",\n            279: \"Arctic fox\",\n            280: \"grey fox\",\n            281: \"tabby cat\",\n            282: \"tiger cat\",\n            283: \"Persian cat\",\n            284: \"Siamese cat\",\n            285: \"Egyptian Mau\",\n            286: \"cougar\",\n            287: \"lynx\",\n            288: \"leopard\",\n            289: \"snow leopard\",\n            290: \"jaguar\",\n            291: \"lion\",\n            292: \"tiger\",\n            293: \"cheetah\",\n            294: \"brown bear\",\n            295: \"American black bear\",\n            296: \"polar bear\",\n            297: \"sloth bear\",\n            298: \"mongoose\",\n            299: \"meerkat\",\n            300: \"tiger beetle\",\n            301: \"ladybug\",\n            302: \"ground beetle\",\n            303: \"longhorn beetle\",\n            304: \"leaf beetle\",\n            305: \"dung beetle\",\n            306: \"rhinoceros beetle\",\n            307: \"weevil\",\n            308: \"fly\",\n            309: \"bee\",\n            310: \"ant\",\n            311: \"grasshopper\",\n            312: \"cricket insect\",\n            313: \"stick insect\",\n            314: \"cockroach\",\n            315: \"praying mantis\",\n            316: \"cicada\",\n            317: \"leafhopper\",\n            318: \"lacewing\",\n            319: \"dragonfly\",\n            320: \"damselfly\",\n            321: \"red admiral butterfly\",\n            322: \"ringlet butterfly\",\n            323: \"monarch butterfly\",\n            324: \"small white butterfly\",\n            325: \"sulphur butterfly\",\n            326: \"gossamer-winged butterfly\",\n            327: \"starfish\",\n            328: \"sea urchin\",\n            329: \"sea cucumber\",\n            330: \"cottontail rabbit\",\n            331: \"hare\",\n            332: \"Angora rabbit\",\n            333: \"hamster\",\n            334: \"porcupine\",\n            335: \"fox squirrel\",\n            336: \"marmot\",\n            337: \"beaver\",\n            338: \"guinea pig\",\n            339: \"common sorrel horse\",\n            340: \"zebra\",\n            341: \"pig\",\n            342: \"wild boar\",\n            343: \"warthog\",\n            344: \"hippopotamus\",\n            345: \"ox\",\n            346: \"water buffalo\",\n            347: \"bison\",\n            348: \"ram (adult male sheep)\",\n            349: \"bighorn sheep\",\n            350: \"Alpine ibex\",\n            351: \"hartebeest\",\n            352: \"impala (antelope)\",\n            353: \"gazelle\",\n            354: \"arabian camel\",\n            355: \"llama\",\n            356: \"weasel\",\n            357: \"mink\",\n            358: \"European polecat\",\n            359: \"black-footed ferret\",\n            360: \"otter\",\n            361: \"skunk\",\n            362: \"badger\",\n            363: \"armadillo\",\n            364: \"three-toed sloth\",\n            365: \"orangutan\",\n            366: \"gorilla\",\n            367: \"chimpanzee\",\n            368: \"gibbon\",\n            369: \"siamang\",\n            370: \"guenon\",\n            371: \"patas monkey\",\n            372: \"baboon\",\n            373: \"macaque\",\n            374: \"langur\",\n            375: \"black-and-white colobus\",\n            376: \"proboscis monkey\",\n            377: \"marmoset\",\n            378: \"white-headed capuchin\",\n            379: \"howler monkey\",\n            380: \"titi monkey\",\n            381: \"Geoffroy's spider monkey\",\n            382: \"common squirrel monkey\",\n            383: \"ring-tailed lemur\",\n            384: \"indri\",\n            385: \"Asian elephant\",\n            386: \"African bush elephant\",\n            387: \"red panda\",\n            388: \"giant panda\",\n            389: \"snoek fish\",\n            390: \"eel\",\n            391: \"silver salmon\",\n            392: \"rock beauty fish\",\n            393: \"clownfish\",\n            394: \"sturgeon\",\n            395: \"gar fish\",\n            396: \"lionfish\",\n            397: \"pufferfish\",\n            398: \"abacus\",\n            399: \"abaya\",\n            400: \"academic gown\",\n            401: \"accordion\",\n            402: \"acoustic guitar\",\n            403: \"aircraft carrier\",\n            404: \"airliner\",\n            405: \"airship\",\n            406: \"altar\",\n            407: \"ambulance\",\n            408: \"amphibious vehicle\",\n            409: \"analog clock\",\n            410: \"apiary\",\n            411: \"apron\",\n            412: \"trash can\",\n            413: \"assault rifle\",\n            414: \"backpack\",\n            415: \"bakery\",\n            416: \"balance beam\",\n            417: \"balloon\",\n            418: \"ballpoint pen\",\n            419: \"Band-Aid\",\n            420: \"banjo\",\n            421: \"baluster / handrail\",\n            422: \"barbell\",\n            423: \"barber chair\",\n            424: \"barbershop\",\n            425: \"barn\",\n            426: \"barometer\",\n            427: \"barrel\",\n            428: \"wheelbarrow\",\n            429: \"baseball\",\n            430: \"basketball\",\n            431: \"bassinet\",\n            432: \"bassoon\",\n            433: \"swimming cap\",\n            434: \"bath towel\",\n            435: \"bathtub\",\n            436: \"station wagon\",\n            437: \"lighthouse\",\n            438: \"beaker\",\n            439: \"military hat (bearskin or shako)\",\n            440: \"beer bottle\",\n            441: \"beer glass\",\n            442: \"bell tower\",\n            443: \"baby bib\",\n            444: \"tandem bicycle\",\n            445: \"bikini\",\n            446: \"ring binder\",\n            447: \"binoculars\",\n            448: \"birdhouse\",\n            449: \"boathouse\",\n            450: \"bobsleigh\",\n            451: \"bolo tie\",\n            452: \"poke bonnet\",\n            453: \"bookcase\",\n            454: \"bookstore\",\n            455: \"bottle cap\",\n            456: \"hunting bow\",\n            457: \"bow tie\",\n            458: \"brass memorial plaque\",\n            459: \"bra\",\n            460: \"breakwater\",\n            461: \"breastplate\",\n            462: \"broom\",\n            463: \"bucket\",\n            464: \"buckle\",\n            465: \"bulletproof vest\",\n            466: \"high-speed train\",\n            467: \"butcher shop\",\n            468: \"taxicab\",\n            469: \"cauldron\",\n            470: \"candle\",\n            471: \"cannon\",\n            472: \"canoe\",\n            473: \"can opener\",\n            474: \"cardigan\",\n            475: \"car mirror\",\n            476: \"carousel\",\n            477: \"tool kit\",\n            478: \"cardboard box / carton\",\n            479: \"car wheel\",\n            480: \"automated teller machine\",\n            481: \"cassette\",\n            482: \"cassette player\",\n            483: \"castle\",\n            484: \"catamaran\",\n            485: \"CD player\",\n            486: \"cello\",\n            487: \"mobile phone\",\n            488: \"chain\",\n            489: \"chain-link fence\",\n            490: \"chain mail\",\n            491: \"chainsaw\",\n            492: \"storage chest\",\n            493: \"chiffonier\",\n            494: \"bell or wind chime\",\n            495: \"china cabinet\",\n            496: \"Christmas stocking\",\n            497: \"church\",\n            498: \"movie theater\",\n            499: \"cleaver\",\n            500: \"cliff dwelling\",\n            501: \"cloak\",\n            502: \"clogs\",\n            503: \"cocktail shaker\",\n            504: \"coffee mug\",\n            505: \"coffeemaker\",\n            506: \"spiral or coil\",\n            507: \"combination lock\",\n            508: \"computer keyboard\",\n            509: \"candy store\",\n            510: \"container ship\",\n            511: \"convertible\",\n            512: \"corkscrew\",\n            513: \"cornet\",\n            514: \"cowboy boot\",\n            515: \"cowboy hat\",\n            516: \"cradle\",\n            517: \"construction crane\",\n            518: \"crash helmet\",\n            519: \"crate\",\n            520: \"infant bed\",\n            521: \"Crock Pot\",\n            522: \"croquet ball\",\n            523: \"crutch\",\n            524: \"cuirass\",\n            525: \"dam\",\n            526: \"desk\",\n            527: \"desktop computer\",\n            528: \"rotary dial telephone\",\n            529: \"diaper\",\n            530: \"digital clock\",\n            531: \"digital watch\",\n            532: \"dining table\",\n            533: \"dishcloth\",\n            534: \"dishwasher\",\n            535: \"disc brake\",\n            536: \"dock\",\n            537: \"dog sled\",\n            538: \"dome\",\n            539: \"doormat\",\n            540: \"drilling rig\",\n            541: \"drum\",\n            542: \"drumstick\",\n            543: \"dumbbell\",\n            544: \"Dutch oven\",\n            545: \"electric fan\",\n            546: \"electric guitar\",\n            547: \"electric locomotive\",\n            548: \"entertainment center\",\n            549: \"envelope\",\n            550: \"espresso machine\",\n            551: \"face powder\",\n            552: \"feather boa\",\n            553: \"filing cabinet\",\n            554: \"fireboat\",\n            555: \"fire truck\",\n            556: \"fire screen\",\n            557: \"flagpole\",\n            558: \"flute\",\n            559: \"folding chair\",\n            560: \"football helmet\",\n            561: \"forklift\",\n            562: \"fountain\",\n            563: \"fountain pen\",\n            564: \"four-poster bed\",\n            565: \"freight car\",\n            566: \"French horn\",\n            567: \"frying pan\",\n            568: \"fur coat\",\n            569: \"garbage truck\",\n            570: \"gas mask or respirator\",\n            571: \"gas pump\",\n            572: \"goblet\",\n            573: \"go-kart\",\n            574: \"golf ball\",\n            575: \"golf cart\",\n            576: \"gondola\",\n            577: \"gong\",\n            578: \"gown\",\n            579: \"grand piano\",\n            580: \"greenhouse\",\n            581: \"radiator grille\",\n            582: \"grocery store\",\n            583: \"guillotine\",\n            584: \"hair clip\",\n            585: \"hair spray\",\n            586: \"half-track\",\n            587: \"hammer\",\n            588: \"hamper\",\n            589: \"hair dryer\",\n            590: \"hand-held computer\",\n            591: \"handkerchief\",\n            592: \"hard disk drive\",\n            593: \"harmonica\",\n            594: \"harp\",\n            595: \"combine harvester\",\n            596: \"hatchet\",\n            597: \"holster\",\n            598: \"home theater\",\n            599: \"honeycomb\",\n            600: \"hook\",\n            601: \"hoop skirt\",\n            602: \"gymnastic horizontal bar\",\n            603: \"horse-drawn vehicle\",\n            604: \"hourglass\",\n            605: \"iPod\",\n            606: \"clothes iron\",\n            607: \"carved pumpkin\",\n            608: \"jeans\",\n            609: \"jeep\",\n            610: \"T-shirt\",\n            611: \"jigsaw puzzle\",\n            612: \"rickshaw\",\n            613: \"joystick\",\n            614: \"kimono\",\n            615: \"knee pad\",\n            616: \"knot\",\n            617: \"lab coat\",\n            618: \"ladle\",\n            619: \"lampshade\",\n            620: \"laptop computer\",\n            621: \"lawn mower\",\n            622: \"lens cap\",\n            623: \"letter opener\",\n            624: \"library\",\n            625: \"lifeboat\",\n            626: \"lighter\",\n            627: \"limousine\",\n            628: \"ocean liner\",\n            629: \"lipstick\",\n            630: \"slip-on shoe\",\n            631: \"lotion\",\n            632: \"music speaker\",\n            633: \"loupe magnifying glass\",\n            634: \"sawmill\",\n            635: \"magnetic compass\",\n            636: \"messenger bag\",\n            637: \"mailbox\",\n            638: \"tights\",\n            639: \"one-piece bathing suit\",\n            640: \"manhole cover\",\n            641: \"maraca\",\n            642: \"marimba\",\n            643: \"mask\",\n            644: \"matchstick\",\n            645: \"maypole\",\n            646: \"maze\",\n            647: \"measuring cup\",\n            648: \"medicine cabinet\",\n            649: \"megalith\",\n            650: \"microphone\",\n            651: \"microwave oven\",\n            652: \"military uniform\",\n            653: \"milk can\",\n            654: \"minibus\",\n            655: \"miniskirt\",\n            656: \"minivan\",\n            657: \"missile\",\n            658: \"mitten\",\n            659: \"mixing bowl\",\n            660: \"mobile home\",\n            661: \"ford model t\",\n            662: \"modem\",\n            663: \"monastery\",\n            664: \"monitor\",\n            665: \"moped\",\n            666: \"mortar and pestle\",\n            667: \"graduation cap\",\n            668: \"mosque\",\n            669: \"mosquito net\",\n            670: \"vespa\",\n            671: \"mountain bike\",\n            672: \"tent\",\n            673: \"computer mouse\",\n            674: \"mousetrap\",\n            675: \"moving van\",\n            676: \"muzzle\",\n            677: \"metal nail\",\n            678: \"neck brace\",\n            679: \"necklace\",\n            680: \"baby pacifier\",\n            681: \"notebook computer\",\n            682: \"obelisk\",\n            683: \"oboe\",\n            684: \"ocarina\",\n            685: \"odometer\",\n            686: \"oil filter\",\n            687: \"pipe organ\",\n            688: \"oscilloscope\",\n            689: \"overskirt\",\n            690: \"bullock cart\",\n            691: \"oxygen mask\",\n            692: \"product packet / packaging\",\n            693: \"paddle\",\n            694: \"paddle wheel\",\n            695: \"padlock\",\n            696: \"paintbrush\",\n            697: \"pajamas\",\n            698: \"palace\",\n            699: \"pan flute\",\n            700: \"paper towel\",\n            701: \"parachute\",\n            702: \"parallel bars\",\n            703: \"park bench\",\n            704: \"parking meter\",\n            705: \"railroad car\",\n            706: \"patio\",\n            707: \"payphone\",\n            708: \"pedestal\",\n            709: \"pencil case\",\n            710: \"pencil sharpener\",\n            711: \"perfume\",\n            712: \"Petri dish\",\n            713: \"photocopier\",\n            714: \"plectrum\",\n            715: \"Pickelhaube\",\n            716: \"picket fence\",\n            717: \"pickup truck\",\n            718: \"pier\",\n            719: \"piggy bank\",\n            720: \"pill bottle\",\n            721: \"pillow\",\n            722: \"ping-pong ball\",\n            723: \"pinwheel\",\n            724: \"pirate ship\",\n            725: \"drink pitcher\",\n            726: \"block plane\",\n            727: \"planetarium\",\n            728: \"plastic bag\",\n            729: \"plate rack\",\n            730: \"farm plow\",\n            731: \"plunger\",\n            732: \"Polaroid camera\",\n            733: \"pole\",\n            734: \"police van\",\n            735: \"poncho\",\n            736: \"pool table\",\n            737: \"soda bottle\",\n            738: \"plant pot\",\n            739: \"potter's wheel\",\n            740: \"power drill\",\n            741: \"prayer rug\",\n            742: \"printer\",\n            743: \"prison\",\n            744: \"missile\",\n            745: \"projector\",\n            746: \"hockey puck\",\n            747: \"punching bag\",\n            748: \"purse\",\n            749: \"quill\",\n            750: \"quilt\",\n            751: \"race car\",\n            752: \"racket\",\n            753: \"radiator\",\n            754: \"radio\",\n            755: \"radio telescope\",\n            756: \"rain barrel\",\n            757: \"recreational vehicle\",\n            758: \"fishing casting reel\",\n            759: \"reflex camera\",\n            760: \"refrigerator\",\n            761: \"remote control\",\n            762: \"restaurant\",\n            763: \"revolver\",\n            764: \"rifle\",\n            765: \"rocking chair\",\n            766: \"rotisserie\",\n            767: \"eraser\",\n            768: \"rugby ball\",\n            769: \"ruler measuring stick\",\n            770: \"sneaker\",\n            771: \"safe\",\n            772: \"safety pin\",\n            773: \"salt shaker\",\n            774: \"sandal\",\n            775: \"sarong\",\n            776: \"saxophone\",\n            777: \"scabbard\",\n            778: \"weighing scale\",\n            779: \"school bus\",\n            780: \"schooner\",\n            781: \"scoreboard\",\n            782: \"CRT monitor\",\n            783: \"screw\",\n            784: \"screwdriver\",\n            785: \"seat belt\",\n            786: \"sewing machine\",\n            787: \"shield\",\n            788: \"shoe store\",\n            789: \"shoji screen / room divider\",\n            790: \"shopping basket\",\n            791: \"shopping cart\",\n            792: \"shovel\",\n            793: \"shower cap\",\n            794: \"shower curtain\",\n            795: \"ski\",\n            796: \"balaclava ski mask\",\n            797: \"sleeping bag\",\n            798: \"slide rule\",\n            799: \"sliding door\",\n            800: \"slot machine\",\n            801: \"snorkel\",\n            802: \"snowmobile\",\n            803: \"snowplow\",\n            804: \"soap dispenser\",\n            805: \"soccer ball\",\n            806: \"sock\",\n            807: \"solar thermal collector\",\n            808: \"sombrero\",\n            809: \"soup bowl\",\n            810: \"keyboard space bar\",\n            811: \"space heater\",\n            812: \"space shuttle\",\n            813: \"spatula\",\n            814: \"motorboat\",\n            815: \"spider web\",\n            816: \"spindle\",\n            817: \"sports car\",\n            818: \"spotlight\",\n            819: \"stage\",\n            820: \"steam locomotive\",\n            821: \"through arch bridge\",\n            822: \"steel drum\",\n            823: \"stethoscope\",\n            824: \"scarf\",\n            825: \"stone wall\",\n            826: \"stopwatch\",\n            827: \"stove\",\n            828: \"strainer\",\n            829: \"tram\",\n            830: \"stretcher\",\n            831: \"couch\",\n            832: \"stupa\",\n            833: \"submarine\",\n            834: \"suit\",\n            835: \"sundial\",\n            836: \"sunglasses\",\n            837: \"sunglasses\",\n            838: \"sunscreen\",\n            839: \"suspension bridge\",\n            840: \"mop\",\n            841: \"sweatshirt\",\n            842: \"swim trunks / shorts\",\n            843: \"swing\",\n            844: \"electrical switch\",\n            845: \"syringe\",\n            846: \"table lamp\",\n            847: \"tank\",\n            848: \"tape player\",\n            849: \"teapot\",\n            850: \"teddy bear\",\n            851: \"television\",\n            852: \"tennis ball\",\n            853: \"thatched roof\",\n            854: \"front curtain\",\n            855: \"thimble\",\n            856: \"threshing machine\",\n            857: \"throne\",\n            858: \"tile roof\",\n            859: \"toaster\",\n            860: \"tobacco shop\",\n            861: \"toilet seat\",\n            862: \"torch\",\n            863: \"totem pole\",\n            864: \"tow truck\",\n            865: \"toy store\",\n            866: \"tractor\",\n            867: \"semi-trailer truck\",\n            868: \"tray\",\n            869: \"trench coat\",\n            870: \"tricycle\",\n            871: \"trimaran\",\n            872: \"tripod\",\n            873: \"triumphal arch\",\n            874: \"trolleybus\",\n            875: \"trombone\",\n            876: \"hot tub\",\n            877: \"turnstile\",\n            878: \"typewriter keyboard\",\n            879: \"umbrella\",\n            880: \"unicycle\",\n            881: \"upright piano\",\n            882: \"vacuum cleaner\",\n            883: \"vase\",\n            884: \"vaulted or arched ceiling\",\n            885: \"velvet fabric\",\n            886: \"vending machine\",\n            887: \"vestment\",\n            888: \"viaduct\",\n            889: \"violin\",\n            890: \"volleyball\",\n            891: \"waffle iron\",\n            892: \"wall clock\",\n            893: \"wallet\",\n            894: \"wardrobe\",\n            895: \"military aircraft\",\n            896: \"sink\",\n            897: \"washing machine\",\n            898: \"water bottle\",\n            899: \"water jug\",\n            900: \"water tower\",\n            901: \"whiskey jug\",\n            902: \"whistle\",\n            903: \"hair wig\",\n            904: \"window screen\",\n            905: \"window shade\",\n            906: \"Windsor tie\",\n            907: \"wine bottle\",\n            908: \"airplane wing\",\n            909: \"wok\",\n            910: \"wooden spoon\",\n            911: \"wool\",\n            912: \"split-rail fence\",\n            913: \"shipwreck\",\n            914: \"sailboat\",\n            915: \"yurt\",\n            916: \"website\",\n            917: \"comic book\",\n            918: \"crossword\",\n            919: \"traffic or street sign\",\n            920: \"traffic light\",\n            921: \"dust jacket\",\n            922: \"menu\",\n            923: \"plate\",\n            924: \"guacamole\",\n            925: \"consomme\",\n            926: \"hot pot\",\n            927: \"trifle\",\n            928: \"ice cream\",\n            929: \"popsicle\",\n            930: \"baguette\",\n            931: \"bagel\",\n            932: \"pretzel\",\n            933: \"cheeseburger\",\n            934: \"hot dog\",\n            935: \"mashed potatoes\",\n            936: \"cabbage\",\n            937: \"broccoli\",\n            938: \"cauliflower\",\n            939: \"zucchini\",\n            940: \"spaghetti squash\",\n            941: \"acorn squash\",\n            942: \"butternut squash\",\n            943: \"cucumber\",\n            944: \"artichoke\",\n            945: \"bell pepper\",\n            946: \"cardoon\",\n            947: \"mushroom\",\n            948: \"Granny Smith apple\",\n            949: \"strawberry\",\n            950: \"orange\",\n            951: \"lemon\",\n            952: \"fig\",\n            953: \"pineapple\",\n            954: \"banana\",\n            955: \"jackfruit\",\n            956: \"cherimoya (custard apple)\",\n            957: \"pomegranate\",\n            958: \"hay\",\n            959: \"carbonara\",\n            960: \"chocolate syrup\",\n            961: \"dough\",\n            962: \"meatloaf\",\n            963: \"pizza\",\n            964: \"pot pie\",\n            965: \"burrito\",\n            966: \"red wine\",\n            967: \"espresso\",\n            968: \"tea cup\",\n            969: \"eggnog\",\n            970: \"mountain\",\n            971: \"bubble\",\n            972: \"cliff\",\n            973: \"coral reef\",\n            974: \"geyser\",\n            975: \"lakeshore\",\n            976: \"promontory\",\n            977: \"sandbar\",\n            978: \"beach\",\n            979: \"valley\",\n            980: \"volcano\",\n            981: \"baseball player\",\n            982: \"bridegroom\",\n            983: \"scuba diver\",\n            984: \"rapeseed\",\n            985: \"daisy\",\n            986: \"yellow lady's slipper\",\n            987: \"corn\",\n            988: \"acorn\",\n            989: \"rose hip\",\n            990: \"horse chestnut seed\",\n            991: \"coral fungus\",\n            992: \"agaric\",\n            993: \"gyromitra\",\n            994: \"stinkhorn mushroom\",\n            995: \"earth star fungus\",\n            996: \"hen of the woods mushroom\",\n            997: \"bolete\",\n            998: \"corn cob\",\n            999: \"toilet paper\",\n        }\n</code></pre>"},{"location":"api/#mmlearn.datasets.imagenet.ImageNet.zero_shot_prompt_templates","title":"zero_shot_prompt_templates  <code>property</code>","text":"<pre><code>zero_shot_prompt_templates\n</code></pre> <p>Return the zero-shot prompt templates.</p>"},{"location":"api/#mmlearn.datasets.imagenet.ImageNet.id2label","title":"id2label  <code>property</code>","text":"<pre><code>id2label\n</code></pre> <p>Return the label mapping.</p>"},{"location":"api/#mmlearn.datasets.imagenet.ImageNet.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(index)\n</code></pre> <p>Get an example at the given index.</p> Source code in <code>mmlearn/datasets/imagenet.py</code> <pre><code>def __getitem__(self, index: int) -&gt; Example:\n    \"\"\"Get an example at the given index.\"\"\"\n    image, target = super().__getitem__(index)\n    example = Example(\n        {\n            Modalities.RGB.name: image,\n            Modalities.RGB.target: target,\n            EXAMPLE_INDEX_KEY: index,\n        }\n    )\n    mask = self.mask_generator() if self.mask_generator else None\n    if mask is not None:  # error will be raised during collation if `None`\n        example[Modalities.RGB.mask] = mask\n    return example\n</code></pre>"},{"location":"api/#mmlearn.datasets.librispeech","title":"librispeech","text":"<p>LibriSpeech dataset.</p>"},{"location":"api/#mmlearn.datasets.librispeech.LibriSpeech","title":"LibriSpeech","text":"<p>               Bases: <code>Dataset[Example]</code></p> <p>LibriSpeech dataset.</p> <p>This is a wrapper around class:<code>torchaudio.datasets.LIBRISPEECH</code> that assumes that the dataset is already downloaded and the top-level directory of the dataset in the root directory is <code>librispeech</code>.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>Root directory of dataset.</p> required <code>split</code> <code>(train - clean - 100, train - clean - 360, train - other - 500, dev - clean, dev - other, test - clean, test - other)</code> <p>Split of the dataset to use.</p> <code>\"train-clean-100\"</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If <code>torchaudio</code> is not installed.</p> Notes <p>This dataset only returns the audio and transcript from the dataset.</p> Source code in <code>mmlearn/datasets/librispeech.py</code> <pre><code>@store(\n    group=\"datasets\",\n    provider=\"mmlearn\",\n    root_dir=os.getenv(\"LIBRISPEECH_ROOT_DIR\", MISSING),\n)\nclass LibriSpeech(Dataset[Example]):\n    \"\"\"LibriSpeech dataset.\n\n    This is a wrapper around :py:class:`torchaudio.datasets.LIBRISPEECH` that assumes\n    that the dataset is already downloaded and the top-level directory of the dataset\n    in the root directory is `librispeech`.\n\n    Parameters\n    ----------\n    root_dir : str\n        Root directory of dataset.\n    split : {\"train-clean-100\", \"train-clean-360\", \"train-other-500\", \"dev-clean\", \"dev-other\", \"test-clean\", \"test-other\"}, default=\"train-clean-100\"\n        Split of the dataset to use.\n\n    Raises\n    ------\n    ImportError\n        If ``torchaudio`` is not installed.\n\n    Notes\n    -----\n    This dataset only returns the audio and transcript from the dataset.\n\n    \"\"\"  # noqa: W505\n\n    def __init__(self, root_dir: str, split: str = \"train-clean-100\") -&gt; None:\n        super().__init__()\n        if not _TORCHAUDIO_AVAILABLE:\n            raise ImportError(\n                \"LibriSpeech dataset requires `torchaudio`, which is not installed.\"\n            )\n        from torchaudio.datasets import LIBRISPEECH\n\n        self.dataset = LIBRISPEECH(\n            root=root_dir,\n            url=split,\n            download=False,\n            folder_in_archive=\"librispeech\",\n        )\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the length of the dataset.\"\"\"\n        return len(self.dataset)\n\n    def __getitem__(self, idx: int) -&gt; Example:\n        \"\"\"Return an example from the dataset.\"\"\"\n        waveform, sample_rate, transcript, _, _, _ = self.dataset[idx]\n        assert sample_rate == SAMPLE_RATE, (\n            f\"Expected sample rate to be `16000`, got {sample_rate}.\"\n        )\n        waveform = pad_or_trim(waveform.flatten())\n\n        return Example(\n            {\n                Modalities.AUDIO.name: waveform,\n                Modalities.TEXT.name: transcript,\n                EXAMPLE_INDEX_KEY: idx,\n            },\n        )\n</code></pre>"},{"location":"api/#mmlearn.datasets.librispeech.LibriSpeech.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the length of the dataset.</p> Source code in <code>mmlearn/datasets/librispeech.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the length of the dataset.\"\"\"\n    return len(self.dataset)\n</code></pre>"},{"location":"api/#mmlearn.datasets.librispeech.LibriSpeech.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> <p>Return an example from the dataset.</p> Source code in <code>mmlearn/datasets/librispeech.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; Example:\n    \"\"\"Return an example from the dataset.\"\"\"\n    waveform, sample_rate, transcript, _, _, _ = self.dataset[idx]\n    assert sample_rate == SAMPLE_RATE, (\n        f\"Expected sample rate to be `16000`, got {sample_rate}.\"\n    )\n    waveform = pad_or_trim(waveform.flatten())\n\n    return Example(\n        {\n            Modalities.AUDIO.name: waveform,\n            Modalities.TEXT.name: transcript,\n            EXAMPLE_INDEX_KEY: idx,\n        },\n    )\n</code></pre>"},{"location":"api/#mmlearn.datasets.librispeech.pad_or_trim","title":"pad_or_trim","text":"<pre><code>pad_or_trim(array, length=30 * SAMPLE_RATE, *, axis=-1)\n</code></pre> <p>Pad or trim the audio array to <code>length</code> along the given axis.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>Tensor</code> <p>Audio array.</p> required <code>length</code> <code>int</code> <p>Length to pad or trim to. Defaults to 30 seconds at 16 kHz.</p> <code>480000</code> <code>axis</code> <code>int</code> <p>Axis along which to pad or trim.</p> <code>-1</code> <p>Returns:</p> Name Type Description <code>array</code> <code>Tensor</code> <p>Padded or trimmed audio array.</p> References <p>.. [1] https://github.com/openai/whisper/blob/main/whisper/audio.py#L65C1-L88C17</p> Source code in <code>mmlearn/datasets/librispeech.py</code> <pre><code>def pad_or_trim(\n    array: torch.Tensor, length: int = 30 * SAMPLE_RATE, *, axis: int = -1\n) -&gt; torch.Tensor:\n    \"\"\"Pad or trim the audio array to `length` along the given axis.\n\n    Parameters\n    ----------\n    array : torch.Tensor\n        Audio array.\n    length : int, default=480000\n        Length to pad or trim to. Defaults to 30 seconds at 16 kHz.\n    axis : int, default=-1\n        Axis along which to pad or trim.\n\n    Returns\n    -------\n    array : torch.Tensor\n        Padded or trimmed audio array.\n\n    References\n    ----------\n    .. [1] https://github.com/openai/whisper/blob/main/whisper/audio.py#L65C1-L88C17\n\n    \"\"\"\n    if array.shape[axis] &gt; length:\n        array = array.index_select(\n            dim=axis,\n            index=torch.arange(length, device=array.device),\n        )\n\n    if array.shape[axis] &lt; length:\n        pad_widths = [(0, 0)] * array.ndim\n        pad_widths[axis] = (0, length - array.shape[axis])\n        array = F.pad(array, [pad for sizes in pad_widths[::-1] for pad in sizes])\n\n    return array\n</code></pre>"},{"location":"api/#mmlearn.datasets.llvip","title":"llvip","text":"<p>LLVIP dataset.</p>"},{"location":"api/#mmlearn.datasets.llvip.LLVIPDataset","title":"LLVIPDataset","text":"<p>               Bases: <code>Dataset[Example]</code></p> <p>Low-Light Visible-Infrared Pair (LLVIP) dataset.</p> <p>Loads pairs of <code>RGB</code> and <code>THERMAL</code> images from the LLVIP dataset.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>Path to the root directory of the dataset. The directory should contain 'visible' and 'infrared' subdirectories.</p> required <code>train</code> <code>bool</code> <p>Flag to indicate whether to load the training or test set.</p> <code>True</code> <code>transform</code> <code>Optional[Callable[[Image], Tensor]]</code> <p>A callable that takes in a PIL image and returns a transformed version of the image as a PyTorch tensor. This is applied to both RGB and thermal images.</p> <code>None</code> Source code in <code>mmlearn/datasets/llvip.py</code> <pre><code>@store(\n    name=\"LLVIP\",\n    group=\"datasets\",\n    provider=\"mmlearn\",\n    root_dir=os.getenv(\"LLVIP_ROOT_DIR\", MISSING),\n)\nclass LLVIPDataset(Dataset[Example]):\n    \"\"\"Low-Light Visible-Infrared Pair (LLVIP) dataset.\n\n    Loads pairs of `RGB` and `THERMAL` images from the LLVIP dataset.\n\n    Parameters\n    ----------\n    root_dir : str\n        Path to the root directory of the dataset. The directory should contain\n        'visible' and 'infrared' subdirectories.\n    train : bool, default=True\n        Flag to indicate whether to load the training or test set.\n    transform : Optional[Callable[[PIL.Image], torch.Tensor]], optional, default=None\n        A callable that takes in a PIL image and returns a transformed version\n        of the image as a PyTorch tensor. This is applied to both RGB and thermal\n        images.\n    \"\"\"\n\n    def __init__(\n        self,\n        root_dir: str,\n        train: bool = True,\n        transform: Optional[Callable[[PILImage], torch.Tensor]] = None,\n    ):\n        self.path_images_rgb = os.path.join(\n            root_dir,\n            \"visible\",\n            \"train\" if train else \"test\",\n        )\n        self.path_images_ir = os.path.join(\n            root_dir, \"infrared\", \"train\" if train else \"test\"\n        )\n        self.train = train\n        self.transform = transform or transforms.ToTensor()\n\n        self.rgb_images = sorted(glob.glob(os.path.join(self.path_images_rgb, \"*.jpg\")))\n        self.ir_images = sorted(glob.glob(os.path.join(self.path_images_ir, \"*.jpg\")))\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the length of the dataset.\"\"\"\n        return len(self.rgb_images)\n\n    def __getitem__(self, idx: int) -&gt; Example:\n        \"\"\"Return an example from the dataset.\"\"\"\n        rgb_image_path = self.rgb_images[idx]\n        ir_image_path = self.ir_images[idx]\n\n        rgb_image = PILImage.open(rgb_image_path).convert(\"RGB\")\n        ir_image = PILImage.open(ir_image_path).convert(\"L\")\n\n        example = Example(\n            {\n                Modalities.RGB.name: self.transform(rgb_image),\n                Modalities.THERMAL.name: self.transform(ir_image),\n                EXAMPLE_INDEX_KEY: idx,\n            },\n        )\n\n        if self.train:\n            annot_path = (\n                rgb_image_path.replace(\"visible\", \"Annotations\")\n                .replace(\".jpg\", \".xml\")\n                .replace(\"train\", \"\")\n            )\n            annot = self._get_bbox(annot_path)\n            example[\"annotation\"] = {\n                \"bboxes\": torch.from_numpy(annot[\"bboxes\"]),\n                \"labels\": torch.from_numpy(annot[\"labels\"]),\n            }\n        return example\n\n    def _get_bbox(self, filename: str) -&gt; dict[str, np.ndarray]:\n        \"\"\"Parse the XML file to get bounding boxes and labels.\n\n        Parameters\n        ----------\n        filename : str\n            Path to the annotation XML file.\n\n        Returns\n        -------\n        dict\n            A dictionary containing bounding boxes and labels.\n        \"\"\"\n        try:\n            root = ET.parse(filename).getroot()\n\n            bboxes, labels = [], []\n            for obj in root.findall(\"object\"):\n                bbox_obj = obj.find(\"bndbox\")\n                bbox = [\n                    int(bbox_obj.find(dim).text)  # type: ignore[union-attr,arg-type]\n                    for dim in [\"xmin\", \"ymin\", \"xmax\", \"ymax\"]\n                ]\n                bboxes.append(bbox)\n                labels.append(1)  # Assuming 'person' is the only label\n            return {\n                \"bboxes\": np.array(bboxes).astype(\"float\"),\n                \"labels\": np.array(labels).astype(\"int\"),\n            }\n        except ET.ParseError as e:\n            raise ValueError(f\"Error parsing XML: {e}\") from None\n        except Exception as e:\n            raise RuntimeError(\n                f\"Error processing annotation file {filename}: {e}\",\n            ) from None\n</code></pre>"},{"location":"api/#mmlearn.datasets.llvip.LLVIPDataset.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the length of the dataset.</p> Source code in <code>mmlearn/datasets/llvip.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the length of the dataset.\"\"\"\n    return len(self.rgb_images)\n</code></pre>"},{"location":"api/#mmlearn.datasets.llvip.LLVIPDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> <p>Return an example from the dataset.</p> Source code in <code>mmlearn/datasets/llvip.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; Example:\n    \"\"\"Return an example from the dataset.\"\"\"\n    rgb_image_path = self.rgb_images[idx]\n    ir_image_path = self.ir_images[idx]\n\n    rgb_image = PILImage.open(rgb_image_path).convert(\"RGB\")\n    ir_image = PILImage.open(ir_image_path).convert(\"L\")\n\n    example = Example(\n        {\n            Modalities.RGB.name: self.transform(rgb_image),\n            Modalities.THERMAL.name: self.transform(ir_image),\n            EXAMPLE_INDEX_KEY: idx,\n        },\n    )\n\n    if self.train:\n        annot_path = (\n            rgb_image_path.replace(\"visible\", \"Annotations\")\n            .replace(\".jpg\", \".xml\")\n            .replace(\"train\", \"\")\n        )\n        annot = self._get_bbox(annot_path)\n        example[\"annotation\"] = {\n            \"bboxes\": torch.from_numpy(annot[\"bboxes\"]),\n            \"labels\": torch.from_numpy(annot[\"labels\"]),\n        }\n    return example\n</code></pre>"},{"location":"api/#mmlearn.datasets.nihcxr","title":"nihcxr","text":"<p>NIH Chest X-ray Dataset.</p>"},{"location":"api/#mmlearn.datasets.nihcxr.NIHCXR","title":"NIHCXR","text":"<p>               Bases: <code>Dataset[Example]</code></p> <p>NIH Chest X-ray dataset.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>Directory which contains <code>.json</code> files stating all dataset entries.</p> required <code>split</code> <code>(train, test, bbox)</code> <p>Dataset split. \"bbox\" is a subset of \"test\" which contains bounding box info.</p> <code>\"train\"</code> <code>transform</code> <code>Optional[Callable[[Image], Tensor]]</code> <p>A callable that takes in a PIL image and returns a transformed version of the image as a PyTorch tensor.</p> <code>None</code> Source code in <code>mmlearn/datasets/nihcxr.py</code> <pre><code>@store(\n    group=\"datasets\",\n    provider=\"mmlearn\",\n    root_dir=os.getenv(\"NIH_CXR_DIR\", MISSING),\n    split=\"train\",\n)\nclass NIHCXR(Dataset[Example]):\n    \"\"\"NIH Chest X-ray dataset.\n\n    Parameters\n    ----------\n    root_dir : str\n        Directory which contains `.json` files stating all dataset entries.\n    split : {\"train\", \"test\", \"bbox\"}\n        Dataset split. \"bbox\" is a subset of \"test\" which contains bounding box info.\n    transform : Optional[Callable[[PIL.Image], torch.Tensor]], optional, default=None\n        A callable that takes in a PIL image and returns a transformed version\n        of the image as a PyTorch tensor.\n    \"\"\"\n\n    def __init__(\n        self,\n        root_dir: str,\n        split: Literal[\"train\", \"test\", \"bbox\"],\n        transform: Optional[Callable[[Image.Image], torch.Tensor]] = None,\n    ) -&gt; None:\n        assert split in [\"train\", \"test\", \"bbox\"], f\"split {split} is not available.\"\n        assert callable(transform) or transform is None, (\n            \"transform is not callable or None.\"\n        )\n\n        data_path = os.path.join(root_dir, split + \"_data.json\")\n\n        assert os.path.isfile(data_path), f\"entries file does not exist: {data_path}.\"\n\n        with open(data_path, \"rb\") as file:\n            entries = json.load(file)\n        self.entries = entries\n\n        if transform is not None:\n            self.transform = transform\n        else:\n            self.transform = Compose([Resize(224), CenterCrop(224), ToTensor()])\n\n        self.bbox = split == \"bbox\"\n\n    def __getitem__(self, idx: int) -&gt; Example:\n        \"\"\"Return image-label or image-label-tabular(bbox).\"\"\"\n        entry = self.entries[idx]\n        image = Image.open(entry[\"image_path\"]).convert(\"RGB\")\n        image = self.transform(image)\n        label = torch.tensor(entry[\"label\"])\n\n        example = Example(\n            {\n                Modalities.RGB.name: image,\n                Modalities.RGB.target: label,\n                \"qid\": entry[\"qid\"],\n                EXAMPLE_INDEX_KEY: idx,\n            }\n        )\n\n        if self.bbox:\n            example[\"bbox\"] = entry[\"bbox\"]\n\n        return example\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the length of the dataset.\"\"\"\n        return len(self.entries)\n</code></pre>"},{"location":"api/#mmlearn.datasets.nihcxr.NIHCXR.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> <p>Return image-label or image-label-tabular(bbox).</p> Source code in <code>mmlearn/datasets/nihcxr.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; Example:\n    \"\"\"Return image-label or image-label-tabular(bbox).\"\"\"\n    entry = self.entries[idx]\n    image = Image.open(entry[\"image_path\"]).convert(\"RGB\")\n    image = self.transform(image)\n    label = torch.tensor(entry[\"label\"])\n\n    example = Example(\n        {\n            Modalities.RGB.name: image,\n            Modalities.RGB.target: label,\n            \"qid\": entry[\"qid\"],\n            EXAMPLE_INDEX_KEY: idx,\n        }\n    )\n\n    if self.bbox:\n        example[\"bbox\"] = entry[\"bbox\"]\n\n    return example\n</code></pre>"},{"location":"api/#mmlearn.datasets.nihcxr.NIHCXR.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the length of the dataset.</p> Source code in <code>mmlearn/datasets/nihcxr.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the length of the dataset.\"\"\"\n    return len(self.entries)\n</code></pre>"},{"location":"api/#mmlearn.datasets.nyuv2","title":"nyuv2","text":"<p>SUN RGB-D dataset.</p>"},{"location":"api/#mmlearn.datasets.nyuv2.NYUv2Dataset","title":"NYUv2Dataset","text":"<p>               Bases: <code>Dataset[Example]</code></p> <p>NYUv2 dataset.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>Path to the root directory of the dataset.</p> required <code>split</code> <code>(train, test)</code> <p>Split of the dataset to use.</p> <code>\"train\"</code> <code>return_type</code> <code>(disparity, image)</code> <p>Return type of the depth images.</p> <ul> <li><code>\"disparity\"</code>: Return the depth image as disparity map.</li> <li><code>\"image\"</code>: Return the depth image as a 3-channel image.</li> </ul> <code>\"disparity\"</code> <code>rgb_transform</code> <code>Optional[Callable[[Image], Tensor]]</code> <p>A callable that takes in an RGB PIL image and returns a transformed version of the image as a PyTorch tensor.</p> <code>None</code> <code>depth_transform</code> <code>Optional[Callable[[Image], Tensor]]</code> <p>A callable that takes in a depth PIL image and returns a transformed version of the image as a PyTorch tensor.</p> <code>None</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If <code>opencv-python</code> is not installed.</p> Source code in <code>mmlearn/datasets/nyuv2.py</code> <pre><code>@store(\n    name=\"NYUv2\",\n    group=\"datasets\",\n    provider=\"mmlearn\",\n    root_dir=os.getenv(\"NYUV2_ROOT_DIR\", MISSING),\n)\nclass NYUv2Dataset(Dataset[Example]):\n    \"\"\"NYUv2 dataset.\n\n    Parameters\n    ----------\n    root_dir : str\n        Path to the root directory of the dataset.\n    split : {\"train\", \"test\"}, default=\"train\"\n        Split of the dataset to use.\n    return_type : {\"disparity\", \"image\"}, default=\"disparity\"\n        Return type of the depth images.\n\n        - `\"disparity\"`: Return the depth image as disparity map.\n        - `\"image\"`: Return the depth image as a 3-channel image.\n    rgb_transform: Callable[[PIL.Image], torch.Tensor], default=None\n        A callable that takes in an RGB PIL image and returns a transformed version\n        of the image as a PyTorch tensor.\n    depth_transform: Callable[[PIL.Image], torch.Tensor], default=None\n        A callable that takes in a depth PIL image and returns a transformed version\n        of the image as a PyTorch tensor.\n\n    Raises\n    ------\n    ImportError\n        If `opencv-python` is not installed.\n    \"\"\"\n\n    def __init__(\n        self,\n        root_dir: str,\n        split: Literal[\"train\", \"test\"] = \"train\",\n        return_type: Literal[\"disparity\", \"image\"] = \"disparity\",\n        rgb_transform: Optional[Callable[[PILImage], torch.Tensor]] = None,\n        depth_transform: Optional[Callable[[PILImage], torch.Tensor]] = None,\n    ) -&gt; None:\n        super().__init__()\n        if not _OPENCV_AVAILABLE:\n            raise ImportError(\n                \"NYUv2 dataset requires `opencv-python` which is not installed.\",\n            )\n        self._validate_args(root_dir, split, rgb_transform, depth_transform)\n        self.return_type = return_type\n\n        self.root_dir = root_dir\n        with open(os.path.join(root_dir, f\"{split}.txt\"), \"r\") as f:\n            file_ids = f.readlines()\n        file_ids = [f.strip() for f in file_ids]\n\n        root_dir = os.path.join(root_dir, split)\n        depth_files = [os.path.join(root_dir, \"depth\", f\"{f}.png\") for f in file_ids]\n        rgb_files = [os.path.join(root_dir, \"rgb\", f\"{f}.png\") for f in file_ids]\n\n        label_files = [\n            os.path.join(root_dir, \"scene_class\", f\"{f}.txt\") for f in file_ids\n        ]\n        labels = [str(open(f).read().strip()) for f in label_files]  # noqa: SIM115\n        labels = [label.replace(\"_\", \" \") for label in labels]\n        labels = [\n            _LABELS.index(label) if label in _LABELS else len(_LABELS)  # type: ignore\n            for label in labels\n        ]\n\n        # remove the samples with classes not in _LABELS\n        # this is to follow the same classes used in ImageBind\n        if split == \"test\":\n            valid_indices = [\n                i\n                for i, label in enumerate(labels)\n                if label &lt; len(_LABELS)  # type: ignore\n            ]\n            rgb_files = [rgb_files[i] for i in valid_indices]\n            depth_files = [depth_files[i] for i in valid_indices]\n            labels = [labels[i] for i in valid_indices]\n\n        self.samples = list(zip(rgb_files, depth_files, labels, strict=False))\n\n        self.rgb_transform = rgb_transform\n        self.depth_transform = depth_transform\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the length of the dataset.\"\"\"\n        return len(self.samples)\n\n    def _validate_args(\n        self,\n        root_dir: str,\n        split: str,\n        rgb_transform: Optional[Callable[[PILImage], torch.Tensor]],\n        depth_transform: Optional[Callable[[PILImage], torch.Tensor]],\n    ) -&gt; None:\n        \"\"\"Validate arguments.\"\"\"\n        if not os.path.isdir(root_dir):\n            raise NotADirectoryError(\n                f\"The given `root_dir` {root_dir} is not a directory\",\n            )\n        if split not in [\"train\", \"test\"]:\n            raise ValueError(\n                f\"Expected `split` to be one of `'train'` or `'test'`, but got {split}\",\n            )\n        if rgb_transform is not None and not callable(rgb_transform):\n            raise TypeError(\n                f\"Expected argument `rgb_transform` to be callable, but got {type(rgb_transform)}\",\n            )\n        if depth_transform is not None and not callable(depth_transform):\n            raise TypeError(\n                f\"Expected `depth_transform` to be callable, but got {type(depth_transform)}\",\n            )\n\n    def __getitem__(self, idx: int) -&gt; Example:\n        \"\"\"Return RGB and depth images at index `idx`.\"\"\"\n        # Read images\n        rgb_image = cv2.imread(self.samples[idx][0], cv2.IMREAD_UNCHANGED)\n        if self.rgb_transform is not None:\n            rgb_image = self.rgb_transform(to_pil_image(rgb_image))\n\n        if self.return_type == \"disparity\":\n            depth_image = depth_normalize(\n                self.samples[idx][1],\n            )\n        else:\n            # Using cv2 instead of PIL Image since we use PNG grayscale images.\n            depth_image = cv2.imread(\n                self.samples[idx][1],\n                cv2.IMREAD_GRAYSCALE,\n            )\n            # Make a 3-channel depth image to enable passing to a pretrained ViT.\n            depth_image = np.repeat(depth_image[:, :, np.newaxis], 3, axis=-1)\n\n        if self.depth_transform is not None:\n            depth_image = self.depth_transform(to_pil_image(depth_image))\n\n        return Example(\n            {\n                Modalities.RGB.name: rgb_image,\n                Modalities.DEPTH.name: depth_image,\n                EXAMPLE_INDEX_KEY: idx,\n                Modalities.DEPTH.target: self.samples[idx][2],\n            }\n        )\n</code></pre>"},{"location":"api/#mmlearn.datasets.nyuv2.NYUv2Dataset.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the length of the dataset.</p> Source code in <code>mmlearn/datasets/nyuv2.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the length of the dataset.\"\"\"\n    return len(self.samples)\n</code></pre>"},{"location":"api/#mmlearn.datasets.nyuv2.NYUv2Dataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> <p>Return RGB and depth images at index <code>idx</code>.</p> Source code in <code>mmlearn/datasets/nyuv2.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; Example:\n    \"\"\"Return RGB and depth images at index `idx`.\"\"\"\n    # Read images\n    rgb_image = cv2.imread(self.samples[idx][0], cv2.IMREAD_UNCHANGED)\n    if self.rgb_transform is not None:\n        rgb_image = self.rgb_transform(to_pil_image(rgb_image))\n\n    if self.return_type == \"disparity\":\n        depth_image = depth_normalize(\n            self.samples[idx][1],\n        )\n    else:\n        # Using cv2 instead of PIL Image since we use PNG grayscale images.\n        depth_image = cv2.imread(\n            self.samples[idx][1],\n            cv2.IMREAD_GRAYSCALE,\n        )\n        # Make a 3-channel depth image to enable passing to a pretrained ViT.\n        depth_image = np.repeat(depth_image[:, :, np.newaxis], 3, axis=-1)\n\n    if self.depth_transform is not None:\n        depth_image = self.depth_transform(to_pil_image(depth_image))\n\n    return Example(\n        {\n            Modalities.RGB.name: rgb_image,\n            Modalities.DEPTH.name: depth_image,\n            EXAMPLE_INDEX_KEY: idx,\n            Modalities.DEPTH.target: self.samples[idx][2],\n        }\n    )\n</code></pre>"},{"location":"api/#mmlearn.datasets.nyuv2.depth_normalize","title":"depth_normalize","text":"<pre><code>depth_normalize(depth_file, min_depth=0.01, max_depth=50)\n</code></pre> <p>Load depth file and convert to disparity image.</p> <p>Parameters:</p> Name Type Description Default <code>depth_file</code> <code>str</code> <p>Path to the depth file.</p> required <code>min_depth</code> <code>float</code> <p>Minimum depth value to clip the depth image.</p> <code>0.01</code> <code>max_depth</code> <code>int</code> <p>Maximum depth value to clip the depth image.</p> <code>50</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The normalized depth image.</p> Source code in <code>mmlearn/datasets/nyuv2.py</code> <pre><code>def depth_normalize(\n    depth_file: str, min_depth: float = 0.01, max_depth: int = 50\n) -&gt; torch.Tensor:\n    \"\"\"Load depth file and convert to disparity image.\n\n    Parameters\n    ----------\n    depth_file : str\n        Path to the depth file.\n    min_depth : float, default=0.01\n        Minimum depth value to clip the depth image.\n    max_depth : int, default=50\n        Maximum depth value to clip the depth image.\n\n    Returns\n    -------\n    torch.Tensor\n        The normalized depth image.\n    \"\"\"\n    depth_image = np.array(PILImage.open(depth_file))\n    depth = np.array(depth_image).astype(np.float32)\n    depth_in_meters = depth / 1000.0\n\n    if min_depth is not None:\n        depth_in_meters = depth_in_meters.clip(min=min_depth, max=max_depth)\n\n    return torch.from_numpy(depth_in_meters).float()\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors","title":"processors","text":"<p>Data processors.</p>"},{"location":"api/#mmlearn.datasets.processors.BlockwiseImagePatchMaskGenerator","title":"BlockwiseImagePatchMaskGenerator","text":"<p>Blockwise image patch mask generator.</p> <p>This is primarily intended for the data2vec method.</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>Union[int, tuple[int, int]]</code> <p>The size of the input image. If an integer is provided, the image is assumed to be square.</p> required <code>num_masking_patches</code> <code>int</code> <p>The number of patches to mask.</p> required <code>min_num_patches</code> <code>int</code> <p>The minimum number of patches to mask.</p> <code>4</code> <code>max_num_patches</code> <code>int</code> <p>The maximum number of patches to mask.</p> <code>None</code> <code>min_aspect_ratio</code> <code>float</code> <p>The minimum aspect ratio of the patch.</p> <code>0.3</code> <code>max_aspect_ratio</code> <code>float</code> <p>The maximum aspect ratio of the patch.</p> <code>None</code> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>@store(group=\"datasets/masking\", provider=\"mmlearn\")\nclass BlockwiseImagePatchMaskGenerator:\n    \"\"\"Blockwise image patch mask generator.\n\n    This is primarily intended for the data2vec method.\n\n    Parameters\n    ----------\n    input_size : Union[int, tuple[int, int]]\n        The size of the input image. If an integer is provided, the image is assumed\n        to be square.\n    num_masking_patches : int\n        The number of patches to mask.\n    min_num_patches : int, default=4\n        The minimum number of patches to mask.\n    max_num_patches : int, default=None\n        The maximum number of patches to mask.\n    min_aspect_ratio : float, default=0.3\n        The minimum aspect ratio of the patch.\n    max_aspect_ratio : float, default=None\n        The maximum aspect ratio of the patch.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_size: Union[int, tuple[int, int]],\n        num_masking_patches: int,\n        min_num_patches: int = 4,\n        max_num_patches: Any = None,\n        min_aspect_ratio: float = 0.3,\n        max_aspect_ratio: Any = None,\n    ):\n        if not isinstance(input_size, tuple):\n            input_size = (input_size,) * 2\n        self.height, self.width = input_size\n\n        self.num_masking_patches = num_masking_patches\n\n        self.min_num_patches = min_num_patches\n        self.max_num_patches = (\n            num_masking_patches if max_num_patches is None else max_num_patches\n        )\n\n        max_aspect_ratio = max_aspect_ratio or 1 / min_aspect_ratio\n        self.log_aspect_ratio = (math.log(min_aspect_ratio), math.log(max_aspect_ratio))\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Generate a printable representation.\n\n        Returns\n        -------\n        str\n            A printable representation of the object.\n\n        \"\"\"\n        return \"Generator(%d, %d -&gt; [%d ~ %d], max = %d, %.3f ~ %.3f)\" % (\n            self.height,\n            self.width,\n            self.min_num_patches,\n            self.max_num_patches,\n            self.num_masking_patches,\n            self.log_aspect_ratio[0],\n            self.log_aspect_ratio[1],\n        )\n\n    def get_shape(self) -&gt; tuple[int, int]:\n        \"\"\"Get the shape of the input.\n\n        Returns\n        -------\n        tuple[int, int]\n            The shape of the input as a tuple `(height, width)`.\n        \"\"\"\n        return self.height, self.width\n\n    def _mask(self, mask: torch.Tensor, max_mask_patches: int) -&gt; int:\n        \"\"\"Masking function.\n\n        This function mask adjacent patches by first selecting a target area and aspect\n        ratio. Since, there might be overlap between selected areas  or the selected\n        area might already be masked, it runs for a  maximum of 10 attempts or until the\n        specified number of patches (max_mask_patches) is achieved.\n\n\n        Parameters\n        ----------\n        mask: torch.Tensor\n            Current mask. The mask to be updated.\n        max_mask_patches: int\n            The maximum number of patches to be masked.\n\n        Returns\n        -------\n        delta: int\n            The number of patches that were successfully masked.\n\n        Notes\n        -----\n        - `target_area`: Randomly chosen target area for the patch.\n        - `aspect_ratio`: Randomly chosen aspect ratio for the patch.\n        - `h`: Height of the patch based on the target area and aspect ratio.\n        - `w`: Width of the patch based on the target area and aspect ratio.\n        - `top`: Randomly chosen top position for the patch.\n        - `left`: Randomly chosen left position for the patch.\n        - `num_masked`: Number of masked pixels within the proposed patch area.\n        - `delta`: Accumulated count of modified pixels.\n        \"\"\"\n        delta = 0\n        for _ in range(10):\n            target_area = random.uniform(self.min_num_patches, max_mask_patches)\n            aspect_ratio = math.exp(random.uniform(*self.log_aspect_ratio))\n            h = int(round(math.sqrt(target_area * aspect_ratio)))\n            w = int(round(math.sqrt(target_area / aspect_ratio)))\n            if w &lt; self.width and h &lt; self.height:\n                top = random.randint(0, self.height - h)\n                left = random.randint(0, self.width - w)\n\n                num_masked = mask[top : top + h, left : left + w].sum()\n                # Overlap\n                if 0 &lt; h * w - num_masked &lt;= max_mask_patches:\n                    for i in range(top, top + h):\n                        for j in range(left, left + w):\n                            if mask[i, j] == 0:\n                                mask[i, j] = 1\n                                delta += 1\n\n                if delta &gt; 0:\n                    break\n        return delta\n\n    def __call__(self) -&gt; torch.Tensor:\n        \"\"\"Generate a random mask.\n\n        Returns a random mask of shape (nb_patches, nb_patches) based on the\n        configuration where the number of patches to be masked is num_masking_patches.\n\n        Returns\n        -------\n        mask: torch.Tensor\n            A mask of shape (nb_patches, nb_patches)\n\n        \"\"\"\n        mask = torch.zeros(self.get_shape(), dtype=torch.int)\n        mask_count = 0\n        while mask_count &lt; self.num_masking_patches:\n            max_mask_patches = self.num_masking_patches - mask_count\n            max_mask_patches = min(max_mask_patches, self.max_num_patches)\n\n            delta = self._mask(mask, max_mask_patches)\n            if delta == 0:\n                break\n            mask_count += delta\n\n        return mask\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.BlockwiseImagePatchMaskGenerator.__repr__","title":"__repr__","text":"<pre><code>__repr__()\n</code></pre> <p>Generate a printable representation.</p> <p>Returns:</p> Type Description <code>str</code> <p>A printable representation of the object.</p> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Generate a printable representation.\n\n    Returns\n    -------\n    str\n        A printable representation of the object.\n\n    \"\"\"\n    return \"Generator(%d, %d -&gt; [%d ~ %d], max = %d, %.3f ~ %.3f)\" % (\n        self.height,\n        self.width,\n        self.min_num_patches,\n        self.max_num_patches,\n        self.num_masking_patches,\n        self.log_aspect_ratio[0],\n        self.log_aspect_ratio[1],\n    )\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.BlockwiseImagePatchMaskGenerator.get_shape","title":"get_shape","text":"<pre><code>get_shape()\n</code></pre> <p>Get the shape of the input.</p> <p>Returns:</p> Type Description <code>tuple[int, int]</code> <p>The shape of the input as a tuple <code>(height, width)</code>.</p> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>def get_shape(self) -&gt; tuple[int, int]:\n    \"\"\"Get the shape of the input.\n\n    Returns\n    -------\n    tuple[int, int]\n        The shape of the input as a tuple `(height, width)`.\n    \"\"\"\n    return self.height, self.width\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.BlockwiseImagePatchMaskGenerator.__call__","title":"__call__","text":"<pre><code>__call__()\n</code></pre> <p>Generate a random mask.</p> <p>Returns a random mask of shape (nb_patches, nb_patches) based on the configuration where the number of patches to be masked is num_masking_patches.</p> <p>Returns:</p> Name Type Description <code>mask</code> <code>Tensor</code> <p>A mask of shape (nb_patches, nb_patches)</p> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>def __call__(self) -&gt; torch.Tensor:\n    \"\"\"Generate a random mask.\n\n    Returns a random mask of shape (nb_patches, nb_patches) based on the\n    configuration where the number of patches to be masked is num_masking_patches.\n\n    Returns\n    -------\n    mask: torch.Tensor\n        A mask of shape (nb_patches, nb_patches)\n\n    \"\"\"\n    mask = torch.zeros(self.get_shape(), dtype=torch.int)\n    mask_count = 0\n    while mask_count &lt; self.num_masking_patches:\n        max_mask_patches = self.num_masking_patches - mask_count\n        max_mask_patches = min(max_mask_patches, self.max_num_patches)\n\n        delta = self._mask(mask, max_mask_patches)\n        if delta == 0:\n            break\n        mask_count += delta\n\n    return mask\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.RandomMaskGenerator","title":"RandomMaskGenerator","text":"<p>Random mask generator.</p> <p>Returns a random mask of shape <code>(nb_patches, nb_patches)</code> based on the configuration where the number of patches to be masked is num_masking_patches. This is intended to be used for tasks like masked language modeling.</p> <p>Parameters:</p> Name Type Description Default <code>probability</code> <code>float</code> <p>Probability of masking a token.</p> required Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>@store(group=\"datasets/masking\", provider=\"mmlearn\", probability=0.15)\nclass RandomMaskGenerator:\n    \"\"\"Random mask generator.\n\n    Returns a random mask of shape `(nb_patches, nb_patches)` based on the\n    configuration where the number of patches to be masked is num_masking_patches.\n    **This is intended to be used for tasks like masked language modeling.**\n\n    Parameters\n    ----------\n    probability : float\n        Probability of masking a token.\n    \"\"\"\n\n    def __init__(self, probability: float):\n        self.probability = probability\n\n    def __call__(\n        self,\n        inputs: torch.Tensor,\n        tokenizer: PreTrainedTokenizerBase,\n        special_tokens_mask: Optional[torch.Tensor] = None,\n    ) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Generate a random mask.\n\n        Returns a random mask of shape (nb_patches, nb_patches) based on the\n        configuration where the number of patches to be masked is num_masking_patches.\n\n        Returns\n        -------\n        inputs : torch.Tensor\n            The encoded inputs.\n        tokenizer : PreTrainedTokenizer\n            The tokenizer.\n        special_tokens_mask : Optional[torch.Tensor], default=None\n            Mask for special tokens.\n        \"\"\"\n        inputs = tokenizer.pad(inputs, return_tensors=\"pt\")[\"input_ids\"]\n        labels = inputs.clone()\n        # We sample a few tokens in each sequence for MLM training\n        # (with probability `self.probability`)\n        probability_matrix = torch.full(labels.shape, self.probability)\n        if special_tokens_mask is None:\n            special_tokens_mask = tokenizer.get_special_tokens_mask(\n                labels, already_has_special_tokens=True\n            )\n            special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n        else:\n            special_tokens_mask = special_tokens_mask.bool()\n\n        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n        masked_indices = torch.bernoulli(probability_matrix).bool()\n        labels[~masked_indices] = tokenizer.pad_token_id\n        # 80% of the time, replace masked input tokens with tokenizer.mask_token([MASK])\n        indices_replaced = (\n            torch.bernoulli(torch.full(labels.shape, 0.8)).bool() &amp; masked_indices\n        )\n        inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n\n        # 10% of the time, we replace masked input tokens with random word\n        indices_random = (\n            torch.bernoulli(torch.full(labels.shape, 0.5)).bool()\n            &amp; masked_indices\n            &amp; ~indices_replaced\n        )\n        random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n        inputs[indices_random] = random_words[indices_random]\n\n        # Rest of the time (10% of the time) we keep the masked input tokens unchanged\n        return inputs, labels, masked_indices\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.RandomMaskGenerator.__call__","title":"__call__","text":"<pre><code>__call__(inputs, tokenizer, special_tokens_mask=None)\n</code></pre> <p>Generate a random mask.</p> <p>Returns a random mask of shape (nb_patches, nb_patches) based on the configuration where the number of patches to be masked is num_masking_patches.</p> <p>Returns:</p> Name Type Description <code>inputs</code> <code>Tensor</code> <p>The encoded inputs.</p> <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>The tokenizer.</p> <code>special_tokens_mask</code> <code>Optional[torch.Tensor], default=None</code> <p>Mask for special tokens.</p> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>def __call__(\n    self,\n    inputs: torch.Tensor,\n    tokenizer: PreTrainedTokenizerBase,\n    special_tokens_mask: Optional[torch.Tensor] = None,\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Generate a random mask.\n\n    Returns a random mask of shape (nb_patches, nb_patches) based on the\n    configuration where the number of patches to be masked is num_masking_patches.\n\n    Returns\n    -------\n    inputs : torch.Tensor\n        The encoded inputs.\n    tokenizer : PreTrainedTokenizer\n        The tokenizer.\n    special_tokens_mask : Optional[torch.Tensor], default=None\n        Mask for special tokens.\n    \"\"\"\n    inputs = tokenizer.pad(inputs, return_tensors=\"pt\")[\"input_ids\"]\n    labels = inputs.clone()\n    # We sample a few tokens in each sequence for MLM training\n    # (with probability `self.probability`)\n    probability_matrix = torch.full(labels.shape, self.probability)\n    if special_tokens_mask is None:\n        special_tokens_mask = tokenizer.get_special_tokens_mask(\n            labels, already_has_special_tokens=True\n        )\n        special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n    else:\n        special_tokens_mask = special_tokens_mask.bool()\n\n    probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n    masked_indices = torch.bernoulli(probability_matrix).bool()\n    labels[~masked_indices] = tokenizer.pad_token_id\n    # 80% of the time, replace masked input tokens with tokenizer.mask_token([MASK])\n    indices_replaced = (\n        torch.bernoulli(torch.full(labels.shape, 0.8)).bool() &amp; masked_indices\n    )\n    inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n\n    # 10% of the time, we replace masked input tokens with random word\n    indices_random = (\n        torch.bernoulli(torch.full(labels.shape, 0.5)).bool()\n        &amp; masked_indices\n        &amp; ~indices_replaced\n    )\n    random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n    inputs[indices_random] = random_words[indices_random]\n\n    # Rest of the time (10% of the time) we keep the masked input tokens unchanged\n    return inputs, labels, masked_indices\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.HFTokenizer","title":"HFTokenizer","text":"<p>A wrapper for loading HuggingFace tokenizers.</p> <p>This class wraps any huggingface tokenizer that can be initialized with meth:<code>transformers.AutoTokenizer.from_pretrained</code>. It preprocesses the input text and returns a dictionary with the tokenized text and other relevant information like attention masks.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>Pretrained model name or path - same as in meth:<code>transformers.AutoTokenizer.from_pretrained</code>.</p> required <code>max_length</code> <code>Optional[int]</code> <p>Maximum length of the tokenized sequence. This is passed to the tokenizer :meth:<code>__call__</code> method.</p> <code>None</code> <code>padding</code> <code>bool or str</code> <p>Padding strategy. Same as in meth:<code>transformers.AutoTokenizer.from_pretrained</code>; passed to the tokenizer :meth:<code>__call__</code> method.</p> <code>False</code> <code>truncation</code> <code>Optional[Union[bool, str]]</code> <p>Truncation strategy. Same as in meth:<code>transformers.AutoTokenizer.from_pretrained</code>; passed to the tokenizer :meth:<code>__call__</code> method.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to meth:<code>transformers.AutoTokenizer.from_pretrained</code>.</p> <code>{}</code> Source code in <code>mmlearn/datasets/processors/tokenizers.py</code> <pre><code>@store(group=\"datasets/tokenizers\", provider=\"mmlearn\")\nclass HFTokenizer:\n    \"\"\"A wrapper for loading HuggingFace tokenizers.\n\n    This class wraps any huggingface tokenizer that can be initialized with\n    :py:meth:`transformers.AutoTokenizer.from_pretrained`. It preprocesses the\n    input text and returns a dictionary with the tokenized text and other\n    relevant information like attention masks.\n\n    Parameters\n    ----------\n    model_name_or_path : str\n        Pretrained model name or path - same as in :py:meth:`transformers.AutoTokenizer.from_pretrained`.\n    max_length : Optional[int], optional, default=None\n        Maximum length of the tokenized sequence. This is passed to the tokenizer\n        :meth:`__call__` method.\n    padding : bool or str, default=False\n        Padding strategy. Same as in :py:meth:`transformers.AutoTokenizer.from_pretrained`;\n        passed to the tokenizer :meth:`__call__` method.\n    truncation : Optional[Union[bool, str]], optional, default=None\n        Truncation strategy. Same as in :py:meth:`transformers.AutoTokenizer.from_pretrained`;\n        passed to the tokenizer :meth:`__call__` method.\n    **kwargs : Any\n        Additional arguments passed to :py:meth:`transformers.AutoTokenizer.from_pretrained`.\n    \"\"\"  # noqa: W505\n\n    def __init__(\n        self,\n        model_name_or_path: str,\n        max_length: Optional[int] = None,\n        padding: Union[bool, str] = False,\n        truncation: Optional[Union[bool, str]] = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, **kwargs)\n        self.max_length = max_length\n        self.padding = padding\n        self.truncation = truncation\n\n    def __call__(\n        self, sentence: Union[str, list[str]], **kwargs: Any\n    ) -&gt; dict[str, torch.Tensor]:\n        \"\"\"Tokenize a text or a list of texts using the HuggingFace tokenizer.\n\n        Parameters\n        ----------\n        sentence : Union[str, list[str]]\n            Sentence(s) to be tokenized.\n        **kwargs : Any\n            Additional arguments passed to the tokenizer :meth:`__call__` method.\n\n        Returns\n        -------\n        dict[str, torch.Tensor]\n            Tokenized sentence(s).\n\n        Notes\n        -----\n        The ``input_ids`` key is replaced with ``Modalities.TEXT`` for consistency.\n        \"\"\"\n        batch_encoding = self.tokenizer(\n            sentence,\n            max_length=self.max_length,\n            padding=self.padding,\n            truncation=self.truncation,\n            return_tensors=\"pt\",\n            **kwargs,\n        )\n\n        if isinstance(\n            sentence, str\n        ):  # remove batch dimension if input is a single sentence\n            for key, value in batch_encoding.items():\n                if isinstance(value, torch.Tensor):\n                    batch_encoding[key] = torch.squeeze(value, 0)\n\n        # use 'Modalities.TEXT' key for input_ids for consistency\n        batch_encoding[Modalities.TEXT.name] = batch_encoding[\"input_ids\"]\n        return dict(batch_encoding)\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.HFTokenizer.__call__","title":"__call__","text":"<pre><code>__call__(sentence, **kwargs)\n</code></pre> <p>Tokenize a text or a list of texts using the HuggingFace tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>sentence</code> <code>Union[str, list[str]]</code> <p>Sentence(s) to be tokenized.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to the tokenizer :meth:<code>__call__</code> method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>Tokenized sentence(s).</p> Notes <p>The <code>input_ids</code> key is replaced with <code>Modalities.TEXT</code> for consistency.</p> Source code in <code>mmlearn/datasets/processors/tokenizers.py</code> <pre><code>def __call__(\n    self, sentence: Union[str, list[str]], **kwargs: Any\n) -&gt; dict[str, torch.Tensor]:\n    \"\"\"Tokenize a text or a list of texts using the HuggingFace tokenizer.\n\n    Parameters\n    ----------\n    sentence : Union[str, list[str]]\n        Sentence(s) to be tokenized.\n    **kwargs : Any\n        Additional arguments passed to the tokenizer :meth:`__call__` method.\n\n    Returns\n    -------\n    dict[str, torch.Tensor]\n        Tokenized sentence(s).\n\n    Notes\n    -----\n    The ``input_ids`` key is replaced with ``Modalities.TEXT`` for consistency.\n    \"\"\"\n    batch_encoding = self.tokenizer(\n        sentence,\n        max_length=self.max_length,\n        padding=self.padding,\n        truncation=self.truncation,\n        return_tensors=\"pt\",\n        **kwargs,\n    )\n\n    if isinstance(\n        sentence, str\n    ):  # remove batch dimension if input is a single sentence\n        for key, value in batch_encoding.items():\n            if isinstance(value, torch.Tensor):\n                batch_encoding[key] = torch.squeeze(value, 0)\n\n    # use 'Modalities.TEXT' key for input_ids for consistency\n    batch_encoding[Modalities.TEXT.name] = batch_encoding[\"input_ids\"]\n    return dict(batch_encoding)\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.TrimText","title":"TrimText","text":"<p>Trim text strings as a preprocessing step before tokenization.</p> <p>Parameters:</p> Name Type Description Default <code>trim_size</code> <code>int</code> <p>The maximum length of the trimmed text.</p> required Source code in <code>mmlearn/datasets/processors/transforms.py</code> <pre><code>@store(group=\"datasets/transforms\", provider=\"mmlearn\")\nclass TrimText:\n    \"\"\"Trim text strings as a preprocessing step before tokenization.\n\n    Parameters\n    ----------\n    trim_size : int\n        The maximum length of the trimmed text.\n    \"\"\"\n\n    def __init__(self, trim_size: int) -&gt; None:\n        self.trim_size = trim_size\n\n    def __call__(self, sentence: Union[str, list[str]]) -&gt; Union[str, list[str]]:\n        \"\"\"Trim the given sentence(s).\n\n        Parameters\n        ----------\n        sentence : Union[str, list[str]]\n            Sentence(s) to be trimmed.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            Trimmed sentence(s).\n\n        Raises\n        ------\n        TypeError\n            If the input sentence is not a string or list of strings.\n        \"\"\"\n        if not isinstance(sentence, (list, str)):\n            raise TypeError(\n                \"Expected argument `sentence` to be a string or list of strings, \"\n                f\"but got {type(sentence)}\"\n            )\n\n        if isinstance(sentence, str):\n            return sentence[: self.trim_size]\n\n        for i, s in enumerate(sentence):\n            sentence[i] = s[: self.trim_size]\n\n        return sentence\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.TrimText.__call__","title":"__call__","text":"<pre><code>__call__(sentence)\n</code></pre> <p>Trim the given sentence(s).</p> <p>Parameters:</p> Name Type Description Default <code>sentence</code> <code>Union[str, list[str]]</code> <p>Sentence(s) to be trimmed.</p> required <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>Trimmed sentence(s).</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input sentence is not a string or list of strings.</p> Source code in <code>mmlearn/datasets/processors/transforms.py</code> <pre><code>def __call__(self, sentence: Union[str, list[str]]) -&gt; Union[str, list[str]]:\n    \"\"\"Trim the given sentence(s).\n\n    Parameters\n    ----------\n    sentence : Union[str, list[str]]\n        Sentence(s) to be trimmed.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        Trimmed sentence(s).\n\n    Raises\n    ------\n    TypeError\n        If the input sentence is not a string or list of strings.\n    \"\"\"\n    if not isinstance(sentence, (list, str)):\n        raise TypeError(\n            \"Expected argument `sentence` to be a string or list of strings, \"\n            f\"but got {type(sentence)}\"\n        )\n\n    if isinstance(sentence, str):\n        return sentence[: self.trim_size]\n\n    for i, s in enumerate(sentence):\n        sentence[i] = s[: self.trim_size]\n\n    return sentence\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.masking","title":"masking","text":"<p>Token mask generators.</p>"},{"location":"api/#mmlearn.datasets.processors.masking.RandomMaskGenerator","title":"RandomMaskGenerator","text":"<p>Random mask generator.</p> <p>Returns a random mask of shape <code>(nb_patches, nb_patches)</code> based on the configuration where the number of patches to be masked is num_masking_patches. This is intended to be used for tasks like masked language modeling.</p> <p>Parameters:</p> Name Type Description Default <code>probability</code> <code>float</code> <p>Probability of masking a token.</p> required Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>@store(group=\"datasets/masking\", provider=\"mmlearn\", probability=0.15)\nclass RandomMaskGenerator:\n    \"\"\"Random mask generator.\n\n    Returns a random mask of shape `(nb_patches, nb_patches)` based on the\n    configuration where the number of patches to be masked is num_masking_patches.\n    **This is intended to be used for tasks like masked language modeling.**\n\n    Parameters\n    ----------\n    probability : float\n        Probability of masking a token.\n    \"\"\"\n\n    def __init__(self, probability: float):\n        self.probability = probability\n\n    def __call__(\n        self,\n        inputs: torch.Tensor,\n        tokenizer: PreTrainedTokenizerBase,\n        special_tokens_mask: Optional[torch.Tensor] = None,\n    ) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Generate a random mask.\n\n        Returns a random mask of shape (nb_patches, nb_patches) based on the\n        configuration where the number of patches to be masked is num_masking_patches.\n\n        Returns\n        -------\n        inputs : torch.Tensor\n            The encoded inputs.\n        tokenizer : PreTrainedTokenizer\n            The tokenizer.\n        special_tokens_mask : Optional[torch.Tensor], default=None\n            Mask for special tokens.\n        \"\"\"\n        inputs = tokenizer.pad(inputs, return_tensors=\"pt\")[\"input_ids\"]\n        labels = inputs.clone()\n        # We sample a few tokens in each sequence for MLM training\n        # (with probability `self.probability`)\n        probability_matrix = torch.full(labels.shape, self.probability)\n        if special_tokens_mask is None:\n            special_tokens_mask = tokenizer.get_special_tokens_mask(\n                labels, already_has_special_tokens=True\n            )\n            special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n        else:\n            special_tokens_mask = special_tokens_mask.bool()\n\n        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n        masked_indices = torch.bernoulli(probability_matrix).bool()\n        labels[~masked_indices] = tokenizer.pad_token_id\n        # 80% of the time, replace masked input tokens with tokenizer.mask_token([MASK])\n        indices_replaced = (\n            torch.bernoulli(torch.full(labels.shape, 0.8)).bool() &amp; masked_indices\n        )\n        inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n\n        # 10% of the time, we replace masked input tokens with random word\n        indices_random = (\n            torch.bernoulli(torch.full(labels.shape, 0.5)).bool()\n            &amp; masked_indices\n            &amp; ~indices_replaced\n        )\n        random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n        inputs[indices_random] = random_words[indices_random]\n\n        # Rest of the time (10% of the time) we keep the masked input tokens unchanged\n        return inputs, labels, masked_indices\n</code></pre> <code></code> __call__ \u00b6 <pre><code>__call__(inputs, tokenizer, special_tokens_mask=None)\n</code></pre> <p>Generate a random mask.</p> <p>Returns a random mask of shape (nb_patches, nb_patches) based on the configuration where the number of patches to be masked is num_masking_patches.</p> <p>Returns:</p> Name Type Description <code>inputs</code> <code>Tensor</code> <p>The encoded inputs.</p> <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>The tokenizer.</p> <code>special_tokens_mask</code> <code>Optional[torch.Tensor], default=None</code> <p>Mask for special tokens.</p> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>def __call__(\n    self,\n    inputs: torch.Tensor,\n    tokenizer: PreTrainedTokenizerBase,\n    special_tokens_mask: Optional[torch.Tensor] = None,\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Generate a random mask.\n\n    Returns a random mask of shape (nb_patches, nb_patches) based on the\n    configuration where the number of patches to be masked is num_masking_patches.\n\n    Returns\n    -------\n    inputs : torch.Tensor\n        The encoded inputs.\n    tokenizer : PreTrainedTokenizer\n        The tokenizer.\n    special_tokens_mask : Optional[torch.Tensor], default=None\n        Mask for special tokens.\n    \"\"\"\n    inputs = tokenizer.pad(inputs, return_tensors=\"pt\")[\"input_ids\"]\n    labels = inputs.clone()\n    # We sample a few tokens in each sequence for MLM training\n    # (with probability `self.probability`)\n    probability_matrix = torch.full(labels.shape, self.probability)\n    if special_tokens_mask is None:\n        special_tokens_mask = tokenizer.get_special_tokens_mask(\n            labels, already_has_special_tokens=True\n        )\n        special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n    else:\n        special_tokens_mask = special_tokens_mask.bool()\n\n    probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n    masked_indices = torch.bernoulli(probability_matrix).bool()\n    labels[~masked_indices] = tokenizer.pad_token_id\n    # 80% of the time, replace masked input tokens with tokenizer.mask_token([MASK])\n    indices_replaced = (\n        torch.bernoulli(torch.full(labels.shape, 0.8)).bool() &amp; masked_indices\n    )\n    inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n\n    # 10% of the time, we replace masked input tokens with random word\n    indices_random = (\n        torch.bernoulli(torch.full(labels.shape, 0.5)).bool()\n        &amp; masked_indices\n        &amp; ~indices_replaced\n    )\n    random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n    inputs[indices_random] = random_words[indices_random]\n\n    # Rest of the time (10% of the time) we keep the masked input tokens unchanged\n    return inputs, labels, masked_indices\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.masking.BlockwiseImagePatchMaskGenerator","title":"BlockwiseImagePatchMaskGenerator","text":"<p>Blockwise image patch mask generator.</p> <p>This is primarily intended for the data2vec method.</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>Union[int, tuple[int, int]]</code> <p>The size of the input image. If an integer is provided, the image is assumed to be square.</p> required <code>num_masking_patches</code> <code>int</code> <p>The number of patches to mask.</p> required <code>min_num_patches</code> <code>int</code> <p>The minimum number of patches to mask.</p> <code>4</code> <code>max_num_patches</code> <code>int</code> <p>The maximum number of patches to mask.</p> <code>None</code> <code>min_aspect_ratio</code> <code>float</code> <p>The minimum aspect ratio of the patch.</p> <code>0.3</code> <code>max_aspect_ratio</code> <code>float</code> <p>The maximum aspect ratio of the patch.</p> <code>None</code> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>@store(group=\"datasets/masking\", provider=\"mmlearn\")\nclass BlockwiseImagePatchMaskGenerator:\n    \"\"\"Blockwise image patch mask generator.\n\n    This is primarily intended for the data2vec method.\n\n    Parameters\n    ----------\n    input_size : Union[int, tuple[int, int]]\n        The size of the input image. If an integer is provided, the image is assumed\n        to be square.\n    num_masking_patches : int\n        The number of patches to mask.\n    min_num_patches : int, default=4\n        The minimum number of patches to mask.\n    max_num_patches : int, default=None\n        The maximum number of patches to mask.\n    min_aspect_ratio : float, default=0.3\n        The minimum aspect ratio of the patch.\n    max_aspect_ratio : float, default=None\n        The maximum aspect ratio of the patch.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_size: Union[int, tuple[int, int]],\n        num_masking_patches: int,\n        min_num_patches: int = 4,\n        max_num_patches: Any = None,\n        min_aspect_ratio: float = 0.3,\n        max_aspect_ratio: Any = None,\n    ):\n        if not isinstance(input_size, tuple):\n            input_size = (input_size,) * 2\n        self.height, self.width = input_size\n\n        self.num_masking_patches = num_masking_patches\n\n        self.min_num_patches = min_num_patches\n        self.max_num_patches = (\n            num_masking_patches if max_num_patches is None else max_num_patches\n        )\n\n        max_aspect_ratio = max_aspect_ratio or 1 / min_aspect_ratio\n        self.log_aspect_ratio = (math.log(min_aspect_ratio), math.log(max_aspect_ratio))\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Generate a printable representation.\n\n        Returns\n        -------\n        str\n            A printable representation of the object.\n\n        \"\"\"\n        return \"Generator(%d, %d -&gt; [%d ~ %d], max = %d, %.3f ~ %.3f)\" % (\n            self.height,\n            self.width,\n            self.min_num_patches,\n            self.max_num_patches,\n            self.num_masking_patches,\n            self.log_aspect_ratio[0],\n            self.log_aspect_ratio[1],\n        )\n\n    def get_shape(self) -&gt; tuple[int, int]:\n        \"\"\"Get the shape of the input.\n\n        Returns\n        -------\n        tuple[int, int]\n            The shape of the input as a tuple `(height, width)`.\n        \"\"\"\n        return self.height, self.width\n\n    def _mask(self, mask: torch.Tensor, max_mask_patches: int) -&gt; int:\n        \"\"\"Masking function.\n\n        This function mask adjacent patches by first selecting a target area and aspect\n        ratio. Since, there might be overlap between selected areas  or the selected\n        area might already be masked, it runs for a  maximum of 10 attempts or until the\n        specified number of patches (max_mask_patches) is achieved.\n\n\n        Parameters\n        ----------\n        mask: torch.Tensor\n            Current mask. The mask to be updated.\n        max_mask_patches: int\n            The maximum number of patches to be masked.\n\n        Returns\n        -------\n        delta: int\n            The number of patches that were successfully masked.\n\n        Notes\n        -----\n        - `target_area`: Randomly chosen target area for the patch.\n        - `aspect_ratio`: Randomly chosen aspect ratio for the patch.\n        - `h`: Height of the patch based on the target area and aspect ratio.\n        - `w`: Width of the patch based on the target area and aspect ratio.\n        - `top`: Randomly chosen top position for the patch.\n        - `left`: Randomly chosen left position for the patch.\n        - `num_masked`: Number of masked pixels within the proposed patch area.\n        - `delta`: Accumulated count of modified pixels.\n        \"\"\"\n        delta = 0\n        for _ in range(10):\n            target_area = random.uniform(self.min_num_patches, max_mask_patches)\n            aspect_ratio = math.exp(random.uniform(*self.log_aspect_ratio))\n            h = int(round(math.sqrt(target_area * aspect_ratio)))\n            w = int(round(math.sqrt(target_area / aspect_ratio)))\n            if w &lt; self.width and h &lt; self.height:\n                top = random.randint(0, self.height - h)\n                left = random.randint(0, self.width - w)\n\n                num_masked = mask[top : top + h, left : left + w].sum()\n                # Overlap\n                if 0 &lt; h * w - num_masked &lt;= max_mask_patches:\n                    for i in range(top, top + h):\n                        for j in range(left, left + w):\n                            if mask[i, j] == 0:\n                                mask[i, j] = 1\n                                delta += 1\n\n                if delta &gt; 0:\n                    break\n        return delta\n\n    def __call__(self) -&gt; torch.Tensor:\n        \"\"\"Generate a random mask.\n\n        Returns a random mask of shape (nb_patches, nb_patches) based on the\n        configuration where the number of patches to be masked is num_masking_patches.\n\n        Returns\n        -------\n        mask: torch.Tensor\n            A mask of shape (nb_patches, nb_patches)\n\n        \"\"\"\n        mask = torch.zeros(self.get_shape(), dtype=torch.int)\n        mask_count = 0\n        while mask_count &lt; self.num_masking_patches:\n            max_mask_patches = self.num_masking_patches - mask_count\n            max_mask_patches = min(max_mask_patches, self.max_num_patches)\n\n            delta = self._mask(mask, max_mask_patches)\n            if delta == 0:\n                break\n            mask_count += delta\n\n        return mask\n</code></pre> <code></code> __repr__ \u00b6 <pre><code>__repr__()\n</code></pre> <p>Generate a printable representation.</p> <p>Returns:</p> Type Description <code>str</code> <p>A printable representation of the object.</p> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Generate a printable representation.\n\n    Returns\n    -------\n    str\n        A printable representation of the object.\n\n    \"\"\"\n    return \"Generator(%d, %d -&gt; [%d ~ %d], max = %d, %.3f ~ %.3f)\" % (\n        self.height,\n        self.width,\n        self.min_num_patches,\n        self.max_num_patches,\n        self.num_masking_patches,\n        self.log_aspect_ratio[0],\n        self.log_aspect_ratio[1],\n    )\n</code></pre> <code></code> get_shape \u00b6 <pre><code>get_shape()\n</code></pre> <p>Get the shape of the input.</p> <p>Returns:</p> Type Description <code>tuple[int, int]</code> <p>The shape of the input as a tuple <code>(height, width)</code>.</p> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>def get_shape(self) -&gt; tuple[int, int]:\n    \"\"\"Get the shape of the input.\n\n    Returns\n    -------\n    tuple[int, int]\n        The shape of the input as a tuple `(height, width)`.\n    \"\"\"\n    return self.height, self.width\n</code></pre> <code></code> __call__ \u00b6 <pre><code>__call__()\n</code></pre> <p>Generate a random mask.</p> <p>Returns a random mask of shape (nb_patches, nb_patches) based on the configuration where the number of patches to be masked is num_masking_patches.</p> <p>Returns:</p> Name Type Description <code>mask</code> <code>Tensor</code> <p>A mask of shape (nb_patches, nb_patches)</p> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>def __call__(self) -&gt; torch.Tensor:\n    \"\"\"Generate a random mask.\n\n    Returns a random mask of shape (nb_patches, nb_patches) based on the\n    configuration where the number of patches to be masked is num_masking_patches.\n\n    Returns\n    -------\n    mask: torch.Tensor\n        A mask of shape (nb_patches, nb_patches)\n\n    \"\"\"\n    mask = torch.zeros(self.get_shape(), dtype=torch.int)\n    mask_count = 0\n    while mask_count &lt; self.num_masking_patches:\n        max_mask_patches = self.num_masking_patches - mask_count\n        max_mask_patches = min(max_mask_patches, self.max_num_patches)\n\n        delta = self._mask(mask, max_mask_patches)\n        if delta == 0:\n            break\n        mask_count += delta\n\n    return mask\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.masking.IJEPAMaskGenerator","title":"IJEPAMaskGenerator  <code>dataclass</code>","text":"<p>Generates encoder and predictor masks for preprocessing.</p> <p>This class generates masks dynamically for batches of examples.</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>tuple[int, int]</code> <p>Input image size.</p> <code>(224, 224)</code> <code>patch_size</code> <code>int</code> <p>Size of each patch.</p> <code>16</code> <code>min_keep</code> <code>int</code> <p>Minimum number of patches to keep.</p> <code>10</code> <code>allow_overlap</code> <code>bool</code> <p>Whether to allow overlap between encoder and predictor masks.</p> <code>False</code> <code>enc_mask_scale</code> <code>tuple[float, float]</code> <p>Scale range for encoder mask.</p> <code>(0.85, 1.0)</code> <code>pred_mask_scale</code> <code>tuple[float, float]</code> <p>Scale range for predictor mask.</p> <code>(0.15, 0.2)</code> <code>aspect_ratio</code> <code>tuple[float, float]</code> <p>Aspect ratio range for mask blocks.</p> <code>(0.75, 1.0)</code> <code>nenc</code> <code>int</code> <p>Number of encoder masks to generate.</p> <code>1</code> <code>npred</code> <code>int</code> <p>Number of predictor masks to generate.</p> <code>4</code> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>@dataclass\nclass IJEPAMaskGenerator:\n    \"\"\"Generates encoder and predictor masks for preprocessing.\n\n    This class generates masks dynamically for batches of examples.\n\n    Parameters\n    ----------\n    input_size : tuple[int, int], default=(224, 224)\n        Input image size.\n    patch_size : int, default=16\n        Size of each patch.\n    min_keep : int, default=10\n        Minimum number of patches to keep.\n    allow_overlap : bool, default=False\n        Whether to allow overlap between encoder and predictor masks.\n    enc_mask_scale : tuple[float, float], default=(0.85, 1.0)\n        Scale range for encoder mask.\n    pred_mask_scale : tuple[float, float], default=(0.15, 0.2)\n        Scale range for predictor mask.\n    aspect_ratio : tuple[float, float], default=(0.75, 1.0)\n        Aspect ratio range for mask blocks.\n    nenc : int, default=1\n        Number of encoder masks to generate.\n    npred : int, default=4\n        Number of predictor masks to generate.\n    \"\"\"\n\n    input_size: tuple[int, int] = (224, 224)\n    patch_size: int = 16\n    min_keep: int = 10\n    allow_overlap: bool = False\n    enc_mask_scale: tuple[float, float] = (0.85, 1.0)\n    pred_mask_scale: tuple[float, float] = (0.15, 0.2)\n    aspect_ratio: tuple[float, float] = (0.75, 1.5)\n    nenc: int = 1\n    npred: int = 4\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Initialize the mask generator.\"\"\"\n        self.height = self.input_size[0] // self.patch_size\n        self.width = self.input_size[1] // self.patch_size\n\n    def _sample_block_size(\n        self,\n        generator: torch.Generator,\n        scale: tuple[float, float],\n        aspect_ratio: tuple[float, float],\n    ) -&gt; tuple[int, int]:\n        \"\"\"Sample the size of the mask block based on scale and aspect ratio.\"\"\"\n        _rand = torch.rand(1, generator=generator).item()\n        min_s, max_s = scale\n        mask_scale = min_s + _rand * (max_s - min_s)\n        max_keep = int(self.height * self.width * mask_scale)\n\n        min_ar, max_ar = aspect_ratio\n        aspect_ratio_val = min_ar + _rand * (max_ar - min_ar)\n\n        h = int(round(math.sqrt(max_keep * aspect_ratio_val)))\n        w = int(round(math.sqrt(max_keep / aspect_ratio_val)))\n\n        h = min(h, self.height - 1)\n        w = min(w, self.width - 1)\n\n        return h, w\n\n    def _sample_block_mask(\n        self, b_size: tuple[int, int]\n    ) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Sample a mask block.\"\"\"\n        h, w = b_size\n        top = torch.randint(0, self.height - h, (1,)).item()\n        left = torch.randint(0, self.width - w, (1,)).item()\n        mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n        mask[top : top + h, left : left + w] = 1\n\n        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n        mask_complement[top : top + h, left : left + w] = 0\n\n        return mask.flatten(), mask_complement.flatten()\n\n    def __call__(self, batch_size: int = 1) -&gt; dict[str, Any]:\n        \"\"\"Generate encoder and predictor masks for a batch of examples.\n\n        Parameters\n        ----------\n        batch_size : int, default=1\n            The batch size for which to generate masks.\n\n        Returns\n        -------\n        dict[str, Any]\n            A dictionary of encoder masks and predictor masks.\n        \"\"\"\n        seed = torch.randint(\n            0, 2**32, (1,)\n        ).item()  # Sample random seed for reproducibility\n        g = torch.Generator().manual_seed(seed)\n\n        # Sample block sizes\n        p_size = self._sample_block_size(\n            generator=g, scale=self.pred_mask_scale, aspect_ratio=self.aspect_ratio\n        )\n        e_size = self._sample_block_size(\n            generator=g, scale=self.enc_mask_scale, aspect_ratio=(1.0, 1.0)\n        )\n\n        # Generate predictor masks\n        masks_pred, masks_enc = [], []\n        for _ in range(self.npred):\n            mask_p, _ = self._sample_block_mask(p_size)\n            # Expand mask to match batch size\n            mask_p = mask_p.unsqueeze(0).expand(batch_size, -1)\n            masks_pred.append(mask_p)\n\n        # Generate encoder masks\n        for _ in range(self.nenc):\n            mask_e, _ = self._sample_block_mask(e_size)\n            # Expand mask to match batch size\n            mask_e = mask_e.unsqueeze(0).expand(batch_size, -1)\n            masks_enc.append(mask_e)\n\n        return {\n            \"encoder_masks\": masks_enc,  # list of tensors of shape (batch_size, N)\n            \"predictor_masks\": masks_pred,  # list of tensors of shape (batch_size, N)\n        }\n</code></pre> <code></code> __post_init__ \u00b6 <pre><code>__post_init__()\n</code></pre> <p>Initialize the mask generator.</p> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Initialize the mask generator.\"\"\"\n    self.height = self.input_size[0] // self.patch_size\n    self.width = self.input_size[1] // self.patch_size\n</code></pre> <code></code> __call__ \u00b6 <pre><code>__call__(batch_size=1)\n</code></pre> <p>Generate encoder and predictor masks for a batch of examples.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>The batch size for which to generate masks.</p> <code>1</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary of encoder masks and predictor masks.</p> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>def __call__(self, batch_size: int = 1) -&gt; dict[str, Any]:\n    \"\"\"Generate encoder and predictor masks for a batch of examples.\n\n    Parameters\n    ----------\n    batch_size : int, default=1\n        The batch size for which to generate masks.\n\n    Returns\n    -------\n    dict[str, Any]\n        A dictionary of encoder masks and predictor masks.\n    \"\"\"\n    seed = torch.randint(\n        0, 2**32, (1,)\n    ).item()  # Sample random seed for reproducibility\n    g = torch.Generator().manual_seed(seed)\n\n    # Sample block sizes\n    p_size = self._sample_block_size(\n        generator=g, scale=self.pred_mask_scale, aspect_ratio=self.aspect_ratio\n    )\n    e_size = self._sample_block_size(\n        generator=g, scale=self.enc_mask_scale, aspect_ratio=(1.0, 1.0)\n    )\n\n    # Generate predictor masks\n    masks_pred, masks_enc = [], []\n    for _ in range(self.npred):\n        mask_p, _ = self._sample_block_mask(p_size)\n        # Expand mask to match batch size\n        mask_p = mask_p.unsqueeze(0).expand(batch_size, -1)\n        masks_pred.append(mask_p)\n\n    # Generate encoder masks\n    for _ in range(self.nenc):\n        mask_e, _ = self._sample_block_mask(e_size)\n        # Expand mask to match batch size\n        mask_e = mask_e.unsqueeze(0).expand(batch_size, -1)\n        masks_enc.append(mask_e)\n\n    return {\n        \"encoder_masks\": masks_enc,  # list of tensors of shape (batch_size, N)\n        \"predictor_masks\": masks_pred,  # list of tensors of shape (batch_size, N)\n    }\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.masking.apply_masks","title":"apply_masks","text":"<pre><code>apply_masks(x, masks)\n</code></pre> <p>Apply masks to the input tensor by selecting the patches to keep based on the masks.</p> <p>This function is primarily intended to be used for the class:<code>i-JEPA &lt;mmlearn.tasks.ijepa.IJEPA&gt;</code>.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape <code>(B, N, D)</code>.</p> required <code>masks</code> <code>Union[Tensor, list[Tensor]]</code> <p>A list of mask tensors of shape <code>(N,)</code>, <code>(1, N)</code>, or <code>(B, N)</code>.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The masked tensor where only the patches indicated by the masks are kept. The output tensor has shape <code>(B * num_masks, N', D)</code>, where <code>N'</code> is the number of patches kept.</p> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>def apply_masks(\n    x: torch.Tensor, masks: Union[torch.Tensor, list[torch.Tensor]]\n) -&gt; torch.Tensor:\n    \"\"\"\n    Apply masks to the input tensor by selecting the patches to keep based on the masks.\n\n    This function is primarily intended to be used for the\n    :py:class:`i-JEPA &lt;mmlearn.tasks.ijepa.IJEPA&gt;`.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape ``(B, N, D)``.\n    masks : Union[torch.Tensor, list[torch.Tensor]]\n        A list of mask tensors of shape ``(N,)``, ``(1, N)``, or ``(B, N)``.\n\n    Returns\n    -------\n    torch.Tensor\n        The masked tensor where only the patches indicated by the masks are kept.\n        The output tensor has shape ``(B * num_masks, N', D)``, where ``N'`` is\n        the number of patches kept.\n    \"\"\"\n    all_x = []\n    batch_size = x.size(0)\n    for m_ in masks:\n        m = m_.to(x.device)\n\n        # Ensure mask is at least 2D\n        if m.dim() == 1:\n            m = m.unsqueeze(0)  # Shape: (1, N)\n\n        # Expand mask to match the batch size if needed\n        if m.size(0) == 1 and batch_size &gt; 1:\n            m = m.expand(batch_size, -1)  # Shape: (B, N)\n\n        # Expand mask to match x's dimensions\n        m_expanded = (\n            m.unsqueeze(-1).expand(-1, -1, x.size(-1)).bool()\n        )  # Shape: (B, N, D)\n\n        # Use boolean indexing\n        selected_patches = x[m_expanded].view(batch_size, -1, x.size(-1))\n        all_x.append(selected_patches)\n\n    # Concatenate along the batch dimension\n    return torch.cat(all_x, dim=0)\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.tokenizers","title":"tokenizers","text":"<p>Tokenizers - modules that convert raw input to sequences of tokens.</p>"},{"location":"api/#mmlearn.datasets.processors.tokenizers.HFTokenizer","title":"HFTokenizer","text":"<p>A wrapper for loading HuggingFace tokenizers.</p> <p>This class wraps any huggingface tokenizer that can be initialized with meth:<code>transformers.AutoTokenizer.from_pretrained</code>. It preprocesses the input text and returns a dictionary with the tokenized text and other relevant information like attention masks.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>Pretrained model name or path - same as in meth:<code>transformers.AutoTokenizer.from_pretrained</code>.</p> required <code>max_length</code> <code>Optional[int]</code> <p>Maximum length of the tokenized sequence. This is passed to the tokenizer :meth:<code>__call__</code> method.</p> <code>None</code> <code>padding</code> <code>bool or str</code> <p>Padding strategy. Same as in meth:<code>transformers.AutoTokenizer.from_pretrained</code>; passed to the tokenizer :meth:<code>__call__</code> method.</p> <code>False</code> <code>truncation</code> <code>Optional[Union[bool, str]]</code> <p>Truncation strategy. Same as in meth:<code>transformers.AutoTokenizer.from_pretrained</code>; passed to the tokenizer :meth:<code>__call__</code> method.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to meth:<code>transformers.AutoTokenizer.from_pretrained</code>.</p> <code>{}</code> Source code in <code>mmlearn/datasets/processors/tokenizers.py</code> <pre><code>@store(group=\"datasets/tokenizers\", provider=\"mmlearn\")\nclass HFTokenizer:\n    \"\"\"A wrapper for loading HuggingFace tokenizers.\n\n    This class wraps any huggingface tokenizer that can be initialized with\n    :py:meth:`transformers.AutoTokenizer.from_pretrained`. It preprocesses the\n    input text and returns a dictionary with the tokenized text and other\n    relevant information like attention masks.\n\n    Parameters\n    ----------\n    model_name_or_path : str\n        Pretrained model name or path - same as in :py:meth:`transformers.AutoTokenizer.from_pretrained`.\n    max_length : Optional[int], optional, default=None\n        Maximum length of the tokenized sequence. This is passed to the tokenizer\n        :meth:`__call__` method.\n    padding : bool or str, default=False\n        Padding strategy. Same as in :py:meth:`transformers.AutoTokenizer.from_pretrained`;\n        passed to the tokenizer :meth:`__call__` method.\n    truncation : Optional[Union[bool, str]], optional, default=None\n        Truncation strategy. Same as in :py:meth:`transformers.AutoTokenizer.from_pretrained`;\n        passed to the tokenizer :meth:`__call__` method.\n    **kwargs : Any\n        Additional arguments passed to :py:meth:`transformers.AutoTokenizer.from_pretrained`.\n    \"\"\"  # noqa: W505\n\n    def __init__(\n        self,\n        model_name_or_path: str,\n        max_length: Optional[int] = None,\n        padding: Union[bool, str] = False,\n        truncation: Optional[Union[bool, str]] = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, **kwargs)\n        self.max_length = max_length\n        self.padding = padding\n        self.truncation = truncation\n\n    def __call__(\n        self, sentence: Union[str, list[str]], **kwargs: Any\n    ) -&gt; dict[str, torch.Tensor]:\n        \"\"\"Tokenize a text or a list of texts using the HuggingFace tokenizer.\n\n        Parameters\n        ----------\n        sentence : Union[str, list[str]]\n            Sentence(s) to be tokenized.\n        **kwargs : Any\n            Additional arguments passed to the tokenizer :meth:`__call__` method.\n\n        Returns\n        -------\n        dict[str, torch.Tensor]\n            Tokenized sentence(s).\n\n        Notes\n        -----\n        The ``input_ids`` key is replaced with ``Modalities.TEXT`` for consistency.\n        \"\"\"\n        batch_encoding = self.tokenizer(\n            sentence,\n            max_length=self.max_length,\n            padding=self.padding,\n            truncation=self.truncation,\n            return_tensors=\"pt\",\n            **kwargs,\n        )\n\n        if isinstance(\n            sentence, str\n        ):  # remove batch dimension if input is a single sentence\n            for key, value in batch_encoding.items():\n                if isinstance(value, torch.Tensor):\n                    batch_encoding[key] = torch.squeeze(value, 0)\n\n        # use 'Modalities.TEXT' key for input_ids for consistency\n        batch_encoding[Modalities.TEXT.name] = batch_encoding[\"input_ids\"]\n        return dict(batch_encoding)\n</code></pre> <code></code> __call__ \u00b6 <pre><code>__call__(sentence, **kwargs)\n</code></pre> <p>Tokenize a text or a list of texts using the HuggingFace tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>sentence</code> <code>Union[str, list[str]]</code> <p>Sentence(s) to be tokenized.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to the tokenizer :meth:<code>__call__</code> method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>Tokenized sentence(s).</p> Notes <p>The <code>input_ids</code> key is replaced with <code>Modalities.TEXT</code> for consistency.</p> Source code in <code>mmlearn/datasets/processors/tokenizers.py</code> <pre><code>def __call__(\n    self, sentence: Union[str, list[str]], **kwargs: Any\n) -&gt; dict[str, torch.Tensor]:\n    \"\"\"Tokenize a text or a list of texts using the HuggingFace tokenizer.\n\n    Parameters\n    ----------\n    sentence : Union[str, list[str]]\n        Sentence(s) to be tokenized.\n    **kwargs : Any\n        Additional arguments passed to the tokenizer :meth:`__call__` method.\n\n    Returns\n    -------\n    dict[str, torch.Tensor]\n        Tokenized sentence(s).\n\n    Notes\n    -----\n    The ``input_ids`` key is replaced with ``Modalities.TEXT`` for consistency.\n    \"\"\"\n    batch_encoding = self.tokenizer(\n        sentence,\n        max_length=self.max_length,\n        padding=self.padding,\n        truncation=self.truncation,\n        return_tensors=\"pt\",\n        **kwargs,\n    )\n\n    if isinstance(\n        sentence, str\n    ):  # remove batch dimension if input is a single sentence\n        for key, value in batch_encoding.items():\n            if isinstance(value, torch.Tensor):\n                batch_encoding[key] = torch.squeeze(value, 0)\n\n    # use 'Modalities.TEXT' key for input_ids for consistency\n    batch_encoding[Modalities.TEXT.name] = batch_encoding[\"input_ids\"]\n    return dict(batch_encoding)\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.tokenizers.Img2Seq","title":"Img2Seq","text":"<p>               Bases: <code>Module</code></p> <p>Convert a batch of images to a batch of sequences.</p> <p>Parameters:</p> Name Type Description Default <code>img_size</code> <code>tuple of int</code> <p>The size of the input image.</p> required <code>patch_size</code> <code>tuple of int</code> <p>The size of the patch.</p> required <code>n_channels</code> <code>int</code> <p>The number of channels in the input image.</p> required <code>d_model</code> <code>int</code> <p>The dimension of the output sequence.</p> required Source code in <code>mmlearn/datasets/processors/tokenizers.py</code> <pre><code>class Img2Seq(nn.Module):\n    \"\"\"Convert a batch of images to a batch of sequences.\n\n    Parameters\n    ----------\n    img_size : tuple of int\n        The size of the input image.\n    patch_size : tuple of int\n        The size of the patch.\n    n_channels : int\n        The number of channels in the input image.\n    d_model : int\n        The dimension of the output sequence.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        img_size: tuple[int, int],\n        patch_size: tuple[int, int],\n        n_channels: int,\n        d_model: int,\n    ) -&gt; None:\n        super().__init__()\n        self.patch_size = patch_size\n        self.img_size = img_size\n\n        nh, nw = img_size[0] // patch_size[0], img_size[1] // patch_size[1]\n        n_tokens = nh * nw\n\n        token_dim = patch_size[0] * patch_size[1] * n_channels\n        self.linear = nn.Linear(token_dim, d_model)\n        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n        self.pos_emb = nn.Parameter(torch.randn(n_tokens, d_model))\n\n    def __call__(self, batch: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Convert a batch of images to a batch of sequences.\n\n        Parameters\n        ----------\n        batch : torch.Tensor\n            Batch of images of shape ``(b, h, w, c)`` where ``b`` is the batch size,\n            ``h`` is the height, ``w`` is the width, and ``c`` is the number of\n            channels.\n\n        Returns\n        -------\n        torch.Tensor\n            Batch of sequences of shape ``(b, s, d)`` where ``b`` is the batch size,\n            ``s`` is the sequence length, and ``d`` is the dimension of the output\n            sequence.\n        \"\"\"\n        batch = _patchify(batch, self.patch_size)\n\n        b, c, nh, nw, ph, pw = batch.shape\n\n        # Flattening the patches\n        batch = torch.permute(batch, [0, 2, 3, 4, 5, 1])\n        batch = torch.reshape(batch, [b, nh * nw, ph * pw * c])\n\n        batch = self.linear(batch)\n        cls: torch.Tensor = self.cls_token.expand([b, -1, -1])\n        emb: torch.Tensor = batch + self.pos_emb\n\n        return torch.cat([cls, emb], axis=1)\n</code></pre> <code></code> __call__ \u00b6 <pre><code>__call__(batch)\n</code></pre> <p>Convert a batch of images to a batch of sequences.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Tensor</code> <p>Batch of images of shape <code>(b, h, w, c)</code> where <code>b</code> is the batch size, <code>h</code> is the height, <code>w</code> is the width, and <code>c</code> is the number of channels.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Batch of sequences of shape <code>(b, s, d)</code> where <code>b</code> is the batch size, <code>s</code> is the sequence length, and <code>d</code> is the dimension of the output sequence.</p> Source code in <code>mmlearn/datasets/processors/tokenizers.py</code> <pre><code>def __call__(self, batch: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Convert a batch of images to a batch of sequences.\n\n    Parameters\n    ----------\n    batch : torch.Tensor\n        Batch of images of shape ``(b, h, w, c)`` where ``b`` is the batch size,\n        ``h`` is the height, ``w`` is the width, and ``c`` is the number of\n        channels.\n\n    Returns\n    -------\n    torch.Tensor\n        Batch of sequences of shape ``(b, s, d)`` where ``b`` is the batch size,\n        ``s`` is the sequence length, and ``d`` is the dimension of the output\n        sequence.\n    \"\"\"\n    batch = _patchify(batch, self.patch_size)\n\n    b, c, nh, nw, ph, pw = batch.shape\n\n    # Flattening the patches\n    batch = torch.permute(batch, [0, 2, 3, 4, 5, 1])\n    batch = torch.reshape(batch, [b, nh * nw, ph * pw * c])\n\n    batch = self.linear(batch)\n    cls: torch.Tensor = self.cls_token.expand([b, -1, -1])\n    emb: torch.Tensor = batch + self.pos_emb\n\n    return torch.cat([cls, emb], axis=1)\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.transforms","title":"transforms","text":"<p>Custom transforms for datasets/inputs.</p>"},{"location":"api/#mmlearn.datasets.processors.transforms.TrimText","title":"TrimText","text":"<p>Trim text strings as a preprocessing step before tokenization.</p> <p>Parameters:</p> Name Type Description Default <code>trim_size</code> <code>int</code> <p>The maximum length of the trimmed text.</p> required Source code in <code>mmlearn/datasets/processors/transforms.py</code> <pre><code>@store(group=\"datasets/transforms\", provider=\"mmlearn\")\nclass TrimText:\n    \"\"\"Trim text strings as a preprocessing step before tokenization.\n\n    Parameters\n    ----------\n    trim_size : int\n        The maximum length of the trimmed text.\n    \"\"\"\n\n    def __init__(self, trim_size: int) -&gt; None:\n        self.trim_size = trim_size\n\n    def __call__(self, sentence: Union[str, list[str]]) -&gt; Union[str, list[str]]:\n        \"\"\"Trim the given sentence(s).\n\n        Parameters\n        ----------\n        sentence : Union[str, list[str]]\n            Sentence(s) to be trimmed.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            Trimmed sentence(s).\n\n        Raises\n        ------\n        TypeError\n            If the input sentence is not a string or list of strings.\n        \"\"\"\n        if not isinstance(sentence, (list, str)):\n            raise TypeError(\n                \"Expected argument `sentence` to be a string or list of strings, \"\n                f\"but got {type(sentence)}\"\n            )\n\n        if isinstance(sentence, str):\n            return sentence[: self.trim_size]\n\n        for i, s in enumerate(sentence):\n            sentence[i] = s[: self.trim_size]\n\n        return sentence\n</code></pre> <code></code> __call__ \u00b6 <pre><code>__call__(sentence)\n</code></pre> <p>Trim the given sentence(s).</p> <p>Parameters:</p> Name Type Description Default <code>sentence</code> <code>Union[str, list[str]]</code> <p>Sentence(s) to be trimmed.</p> required <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>Trimmed sentence(s).</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input sentence is not a string or list of strings.</p> Source code in <code>mmlearn/datasets/processors/transforms.py</code> <pre><code>def __call__(self, sentence: Union[str, list[str]]) -&gt; Union[str, list[str]]:\n    \"\"\"Trim the given sentence(s).\n\n    Parameters\n    ----------\n    sentence : Union[str, list[str]]\n        Sentence(s) to be trimmed.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        Trimmed sentence(s).\n\n    Raises\n    ------\n    TypeError\n        If the input sentence is not a string or list of strings.\n    \"\"\"\n    if not isinstance(sentence, (list, str)):\n        raise TypeError(\n            \"Expected argument `sentence` to be a string or list of strings, \"\n            f\"but got {type(sentence)}\"\n        )\n\n    if isinstance(sentence, str):\n        return sentence[: self.trim_size]\n\n    for i, s in enumerate(sentence):\n        sentence[i] = s[: self.trim_size]\n\n    return sentence\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.transforms.repeat_interleave_batch","title":"repeat_interleave_batch","text":"<pre><code>repeat_interleave_batch(x, b, repeat)\n</code></pre> <p>Repeat and interleave a tensor across the batch dimension.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor to be repeated.</p> required <code>b</code> <code>int</code> <p>Size of the batch to be repeated.</p> required <code>repeat</code> <code>int</code> <p>Number of times to repeat each batch.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The repeated tensor with shape adjusted for the batch.</p> Source code in <code>mmlearn/datasets/processors/transforms.py</code> <pre><code>def repeat_interleave_batch(x: torch.Tensor, b: int, repeat: int) -&gt; torch.Tensor:\n    \"\"\"Repeat and interleave a tensor across the batch dimension.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor to be repeated.\n    b : int\n        Size of the batch to be repeated.\n    repeat : int\n        Number of times to repeat each batch.\n\n    Returns\n    -------\n    torch.Tensor\n        The repeated tensor with shape adjusted for the batch.\n    \"\"\"\n    n = len(x) // b\n    return torch.cat(\n        [\n            torch.cat([x[i * b : (i + 1) * b] for _ in range(repeat)], dim=0)\n            for i in range(n)\n        ],\n        dim=0,\n    )\n</code></pre>"},{"location":"api/#mmlearn.datasets.sunrgbd","title":"sunrgbd","text":"<p>SUN RGB-D dataset.</p>"},{"location":"api/#mmlearn.datasets.sunrgbd.SUNRGBDDataset","title":"SUNRGBDDataset","text":"<p>               Bases: <code>Dataset[Example]</code></p> <p>SUN RGB-D dataset.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>Path to the root directory of the dataset.</p> required <code>split</code> <code>(train, test)</code> <p>Split of the dataset to use.</p> <code>\"train\"</code> <code>return_type</code> <code>(disparity, image)</code> <p>Return type of the depth images. If \"disparity\", the depth images are converted to disparity similar to the ImageBind implementation. Otherwise, return the depth image as a 3-channel image.</p> <code>\"disparity\"</code> <code>rgb_transform</code> <code>Optional[Callable[[Image], Tensor]]</code> <p>A callable that takes in an RGB PIL image and returns a transformed version of the image as a PyTorch tensor.</p> <code>None</code> <code>depth_transform</code> <code>Optional[Callable[[Image], Tensor]]</code> <p>A callable that takes in a depth PIL image and returns a transformed version of the image as a PyTorch tensor.</p> <code>None</code> References <p>.. [1] Repo followed to extract the dataset: https://github.com/TUI-NICR/nicr-scene-analysis-datasets</p> Source code in <code>mmlearn/datasets/sunrgbd.py</code> <pre><code>@store(\n    name=\"SUNRGBD\",\n    group=\"datasets\",\n    provider=\"mmlearn\",\n    root_dir=os.getenv(\"SUNRGBD_ROOT_DIR\", MISSING),\n)\nclass SUNRGBDDataset(Dataset[Example]):\n    \"\"\"SUN RGB-D dataset.\n\n    Parameters\n    ----------\n    root_dir : str\n        Path to the root directory of the dataset.\n    split : {\"train\", \"test\"}, default=\"train\"\n        Split of the dataset to use.\n    return_type : {\"disparity\", \"image\"}, default=\"disparity\"\n        Return type of the depth images. If \"disparity\", the depth images are\n        converted to disparity similar to the ImageBind implementation.\n        Otherwise, return the depth image as a 3-channel image.\n    rgb_transform: Callable[[PIL.Image], torch.Tensor], default=None\n        A callable that takes in an RGB PIL image and returns a transformed version\n        of the image as a PyTorch tensor.\n    depth_transform: Callable[[PIL.Image], torch.Tensor], default=None\n        A callable that takes in a depth PIL image and returns a transformed version\n        of the image as a PyTorch tensor.\n\n    References\n    ----------\n    .. [1] Repo followed to extract the dataset: https://github.com/TUI-NICR/nicr-scene-analysis-datasets\n    \"\"\"\n\n    def __init__(\n        self,\n        root_dir: str,\n        split: Literal[\"train\", \"test\"] = \"train\",\n        return_type: Literal[\"disparity\", \"image\"] = \"disparity\",\n        rgb_transform: Optional[Callable[[PILImage], torch.Tensor]] = None,\n        depth_transform: Optional[Callable[[PILImage], torch.Tensor]] = None,\n    ) -&gt; None:\n        super().__init__()\n        if not _OPENCV_AVAILABLE:\n            raise ImportError(\n                \"SUN RGB-D dataset requires `opencv-python` which is not installed.\",\n            )\n\n        self._validate_args(root_dir, split, rgb_transform, depth_transform)\n        self.return_type = return_type\n\n        self.root_dir = root_dir\n        with open(os.path.join(root_dir, f\"{split}.txt\"), \"r\") as f:\n            file_ids = f.readlines()\n        file_ids = [f.strip() for f in file_ids]\n\n        root_dir = os.path.join(root_dir, split)\n        depth_files = [os.path.join(root_dir, \"depth\", f\"{f}.png\") for f in file_ids]\n        rgb_files = [os.path.join(root_dir, \"rgb\", f\"{f}.jpg\") for f in file_ids]\n        intrinsic_files = [\n            os.path.join(root_dir, \"intrinsics\", f\"{f}.txt\") for f in file_ids\n        ]\n\n        sensor_types = [\n            file.removeprefix(os.path.join(root_dir, \"depth\")).split(os.sep)[1]\n            for file in depth_files\n        ]\n\n        label_files = [\n            os.path.join(root_dir, \"scene_class\", f\"{f}.txt\") for f in file_ids\n        ]\n        labels = []\n        for label_file in label_files:\n            with open(label_file, \"r\") as file:  # noqa: SIM115\n                labels.append(file.read().strip())\n        labels = [label.replace(\"_\", \" \") for label in labels]\n        labels = [\n            _LABELS.index(label) if label in _LABELS else len(_LABELS)  # type: ignore\n            for label in labels\n        ]\n\n        # remove the samples with classes not in _LABELS\n        # this is to follow the same classes used in ImageBind\n        if split == \"test\":\n            valid_indices = [\n                i\n                for i, label in enumerate(labels)\n                if label &lt; len(_LABELS)  # type: ignore\n            ]\n            rgb_files = [rgb_files[i] for i in valid_indices]\n            depth_files = [depth_files[i] for i in valid_indices]\n            labels = [labels[i] for i in valid_indices]\n            intrinsic_files = [intrinsic_files[i] for i in valid_indices]\n            sensor_types = [sensor_types[i] for i in valid_indices]\n\n        self.samples = list(\n            zip(\n                rgb_files,\n                depth_files,\n                labels,\n                intrinsic_files,\n                sensor_types,\n                strict=False,\n            )\n        )\n\n        self.rgb_transform = rgb_transform\n        self.depth_transform = depth_transform\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the length of the dataset.\"\"\"\n        return len(self.samples)\n\n    def _validate_args(\n        self,\n        root_dir: str,\n        split: str,\n        rgb_transform: Optional[Callable[[PILImage], torch.Tensor]],\n        depth_transform: Optional[Callable[[PILImage], torch.Tensor]],\n    ) -&gt; None:\n        \"\"\"Validate arguments.\"\"\"\n        if not os.path.isdir(root_dir):\n            raise NotADirectoryError(\n                f\"The given `root_dir` {root_dir} is not a directory\",\n            )\n        if split not in [\"train\", \"test\"]:\n            raise ValueError(\n                f\"Expected `split` to be one of `'train'` or `'test'`, but got {split}\",\n            )\n        if rgb_transform is not None and not callable(rgb_transform):\n            raise TypeError(\n                f\"Expected argument `rgb_transform` to be callable, but got {type(rgb_transform)}\",\n            )\n        if depth_transform is not None and not callable(depth_transform):\n            raise TypeError(\n                f\"Expected `depth_transform` to be callable, but got {type(depth_transform)}\",\n            )\n\n    def __getitem__(self, idx: int) -&gt; Example:\n        \"\"\"Return RGB and depth images at index `idx`.\"\"\"\n        # Read images\n        rgb_image = cv2.imread(self.samples[idx][0], cv2.IMREAD_UNCHANGED)\n        if self.rgb_transform is not None:\n            rgb_image = self.rgb_transform(to_pil_image(rgb_image))\n\n        if self.return_type == \"disparity\":\n            depth_image = convert_depth_to_disparity(\n                self.samples[idx][1],\n                self.samples[idx][3],\n                self.samples[idx][4],\n            )\n        else:\n            # Using cv2 instead of PIL Image since we use PNG grayscale images.\n            depth_image = cv2.imread(\n                self.samples[idx][1],\n                cv2.IMREAD_GRAYSCALE,\n            )\n            # Make a 3-channel depth image to enable passing to a pretrained ViT.\n            depth_image = np.repeat(depth_image[:, :, np.newaxis], 3, axis=-1)\n\n        if self.depth_transform is not None:\n            depth_image = self.depth_transform(to_pil_image(depth_image))\n\n        return Example(\n            {\n                Modalities.RGB.name: rgb_image,\n                Modalities.DEPTH.name: depth_image,\n                EXAMPLE_INDEX_KEY: idx,\n                Modalities.DEPTH.target: self.samples[idx][2],\n            }\n        )\n</code></pre>"},{"location":"api/#mmlearn.datasets.sunrgbd.SUNRGBDDataset.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the length of the dataset.</p> Source code in <code>mmlearn/datasets/sunrgbd.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the length of the dataset.\"\"\"\n    return len(self.samples)\n</code></pre>"},{"location":"api/#mmlearn.datasets.sunrgbd.SUNRGBDDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> <p>Return RGB and depth images at index <code>idx</code>.</p> Source code in <code>mmlearn/datasets/sunrgbd.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; Example:\n    \"\"\"Return RGB and depth images at index `idx`.\"\"\"\n    # Read images\n    rgb_image = cv2.imread(self.samples[idx][0], cv2.IMREAD_UNCHANGED)\n    if self.rgb_transform is not None:\n        rgb_image = self.rgb_transform(to_pil_image(rgb_image))\n\n    if self.return_type == \"disparity\":\n        depth_image = convert_depth_to_disparity(\n            self.samples[idx][1],\n            self.samples[idx][3],\n            self.samples[idx][4],\n        )\n    else:\n        # Using cv2 instead of PIL Image since we use PNG grayscale images.\n        depth_image = cv2.imread(\n            self.samples[idx][1],\n            cv2.IMREAD_GRAYSCALE,\n        )\n        # Make a 3-channel depth image to enable passing to a pretrained ViT.\n        depth_image = np.repeat(depth_image[:, :, np.newaxis], 3, axis=-1)\n\n    if self.depth_transform is not None:\n        depth_image = self.depth_transform(to_pil_image(depth_image))\n\n    return Example(\n        {\n            Modalities.RGB.name: rgb_image,\n            Modalities.DEPTH.name: depth_image,\n            EXAMPLE_INDEX_KEY: idx,\n            Modalities.DEPTH.target: self.samples[idx][2],\n        }\n    )\n</code></pre>"},{"location":"api/#mmlearn.datasets.sunrgbd.convert_depth_to_disparity","title":"convert_depth_to_disparity","text":"<pre><code>convert_depth_to_disparity(\n    depth_file,\n    intrinsics_file,\n    sensor_type,\n    min_depth=0.01,\n    max_depth=50,\n)\n</code></pre> <p>Load depth file and convert to disparity.</p> <p>Parameters:</p> Name Type Description Default <code>depth_file</code> <code>str</code> <p>Path to the depth file.</p> required <code>intrinsics_file</code> <code>str</code> <p>Intrinsics_file is a txt file supplied in SUNRGBD with sensor information Can be found at the path: os.path.join(root_dir, room_name, \"intrinsics.txt\")</p> required <code>sensor_type</code> <code>str</code> <p>Sensor type of the depth file.</p> required <code>min_depth</code> <code>float</code> <p>Minimum depth value to clip the depth image.</p> <code>0.01</code> <code>max_depth</code> <code>int</code> <p>Maximum depth value to clip the depth image.</p> <code>50</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Disparity image from the depth image following the ImageBind implementation.</p> Source code in <code>mmlearn/datasets/sunrgbd.py</code> <pre><code>def convert_depth_to_disparity(\n    depth_file: str,\n    intrinsics_file: str,\n    sensor_type: str,\n    min_depth: float = 0.01,\n    max_depth: int = 50,\n) -&gt; torch.Tensor:\n    \"\"\"Load depth file and convert to disparity.\n\n    Parameters\n    ----------\n    depth_file : str\n        Path to the depth file.\n    intrinsics_file : str\n        Intrinsics_file is a txt file supplied in SUNRGBD with sensor information\n        Can be found at the path: os.path.join(root_dir, room_name, \"intrinsics.txt\")\n    sensor_type : str\n        Sensor type of the depth file.\n    min_depth : float, default=0.01\n        Minimum depth value to clip the depth image.\n    max_depth : int, default=50\n        Maximum depth value to clip the depth image.\n\n    Returns\n    -------\n    torch.Tensor\n        Disparity image from the depth image following the ImageBind implementation.\n    \"\"\"\n    with open(intrinsics_file, \"r\") as fh:\n        lines = fh.readlines()\n        focal_length = float(lines[0].strip().split()[0])\n    baseline = sensor_to_params[sensor_type][\"baseline\"]\n    depth_image = np.array(PILImage.open(depth_file))\n    depth = np.array(depth_image).astype(np.float32)\n    depth_in_meters = depth / 1000.0\n    if min_depth is not None:\n        depth_in_meters = depth_in_meters.clip(min=min_depth, max=max_depth)\n    disparity = baseline * focal_length / depth_in_meters\n    return torch.from_numpy(disparity).float()\n</code></pre>"},{"location":"api/#mmlearn.hf_utils","title":"hf_utils","text":"<p>Utilities for loading components from the HuggingFace <code>transformers</code> library.</p>"},{"location":"api/#mmlearn.hf_utils.load_huggingface_model","title":"load_huggingface_model","text":"<pre><code>load_huggingface_model(\n    model_type,\n    model_name_or_path,\n    load_pretrained_weights=True,\n    get_model_attr=None,\n    model_config_kwargs=None,\n    config_type=None,\n)\n</code></pre> <p>Load a model from the HuggingFace <code>transformers</code> library.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>Type[_BaseAutoModelClass]</code> <p>The model class to instantiate e.g. <code>transformers.AutoModel</code>.</p> required <code>model_name_or_path</code> <code>str</code> <p>The model name or path to load the model from.</p> required <code>load_pretrained_weights</code> <code>bool</code> <p>Whether to load the pretrained weights or not. If false, the argument <code>pretrained_model_name_or_path</code> will be used to get the model configuration and the model will be initialized with random weights.</p> <code>True</code> <code>get_model_attr</code> <code>Optional[str]</code> <p>If not None, the attribute of the model to return. For example, if the model is an <code>transformers.AutoModel</code> and <code>get_model_attr='encoder'</code>, the encoder part of the model will be returned. If <code>None</code>, the full model will be returned.</p> <code>None</code> <code>model_config_kwargs</code> <code>Optional[dict[str, Any]]</code> <p>Additional keyword arguments to pass to the model configuration. The values in kwargs of any keys which are configuration attributes will be used to override the loaded values. Behavior concerning key/value pairs whose keys are not configuration attributes is controlled by the <code>return_unused_kwargs</code> keyword parameter.</p> <code>None</code> <code>config_type</code> <code>Optional[Type[PretrainedConfig]]</code> <p>The class of the configuration to use. If None, <code>transformers.AutoConfig</code> will be used.</p> <code>None</code> <p>Returns:</p> Type Description <code>Module</code> <p>The instantiated model.</p> Source code in <code>mmlearn/hf_utils.py</code> <pre><code>def load_huggingface_model(\n    model_type: Type[_BaseAutoModelClass],\n    model_name_or_path: str,\n    load_pretrained_weights: bool = True,\n    get_model_attr: Optional[str] = None,\n    model_config_kwargs: Optional[dict[str, Any]] = None,\n    config_type: Optional[Type[PretrainedConfig]] = None,\n) -&gt; nn.Module:\n    \"\"\"Load a model from the HuggingFace ``transformers`` library.\n\n    Parameters\n    ----------\n    model_type : Type[_BaseAutoModelClass]\n        The model class to instantiate e.g. ``transformers.AutoModel``.\n    model_name_or_path : str\n        The model name or path to load the model from.\n    load_pretrained_weights : bool, optional, default=True\n        Whether to load the pretrained weights or not. If false, the argument\n        ``pretrained_model_name_or_path`` will be used to get the model configuration\n        and the model will be initialized with random weights.\n    get_model_attr : Optional[str], optional, default=None\n        If not None, the attribute of the model to return. For example, if the model\n        is an ``transformers.AutoModel`` and ``get_model_attr='encoder'``, the\n        encoder part of the model will be returned. If ``None``, the full model\n        will be returned.\n    model_config_kwargs : Optional[dict[str, Any]], optional, default=None\n        Additional keyword arguments to pass to the model configuration.\n        The values in kwargs of any keys which are configuration attributes will\n        be used to override the loaded values. Behavior concerning key/value pairs\n        whose keys are *not* configuration attributes is controlled by the\n        ``return_unused_kwargs`` keyword parameter.\n    config_type : Optional[Type[PretrainedConfig]], optional, default=None\n        The class of the configuration to use. If None, ``transformers.AutoConfig``\n        will be used.\n\n    Returns\n    -------\n    torch.nn.Module\n        The instantiated model.\n    \"\"\"\n    model_config_kwargs = model_config_kwargs or {}\n    if load_pretrained_weights:\n        model = model_type.from_pretrained(model_name_or_path, **model_config_kwargs)\n    else:\n        if config_type is None:\n            config_type = AutoConfig\n        config, kwargs = config_type.from_pretrained(\n            pretrained_model_name_or_path=model_name_or_path,\n            return_unused_kwargs=True,\n            **model_config_kwargs,\n        )\n        model = model_type.from_config(config, **kwargs)\n\n    if get_model_attr is not None and hasattr(model, get_model_attr):\n        model = getattr(model, get_model_attr)\n\n    return model\n</code></pre>"},{"location":"api/#mmlearn.modules","title":"modules","text":"<p>Reusable components for building tasks.</p>"},{"location":"api/#mmlearn.modules.ema","title":"ema","text":"<p>Exponential Moving Average (EMA) module.</p>"},{"location":"api/#mmlearn.modules.ema.ExponentialMovingAverage","title":"ExponentialMovingAverage","text":"<p>Exponential Moving Average (EMA) for the input model.</p> <p>At each step the parameter of the EMA model is updates as the weighted average of the model's parameters.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to apply EMA to.</p> required <code>ema_decay</code> <code>float</code> <p>The initial decay value for EMA.</p> required <code>ema_end_decay</code> <code>float</code> <p>The final decay value for EMA.</p> required <code>ema_anneal_end_step</code> <code>int</code> <p>The number of steps to anneal the decay from <code>ema_decay</code> to <code>ema_end_decay</code>.</p> required <code>skip_keys</code> <code>Optional[Union[list[str], Set[str]]]</code> <p>The keys to skip in the EMA update. These parameters will be copied directly from the model to the EMA model.</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If a deep copy of the model cannot be created.</p> Source code in <code>mmlearn/modules/ema.py</code> <pre><code>class ExponentialMovingAverage:\n    \"\"\"Exponential Moving Average (EMA) for the input model.\n\n    At each step the parameter of the EMA model is updates as the weighted average\n    of the model's parameters.\n\n    Parameters\n    ----------\n    model : torch.nn.Module\n        The model to apply EMA to.\n    ema_decay : float\n        The initial decay value for EMA.\n    ema_end_decay : float\n        The final decay value for EMA.\n    ema_anneal_end_step : int\n        The number of steps to anneal the decay from ``ema_decay`` to ``ema_end_decay``.\n    skip_keys : Optional[Union[list[str], Set[str]]], optional, default=None\n        The keys to skip in the EMA update. These parameters will be copied directly\n        from the model to the EMA model.\n\n    Raises\n    ------\n    RuntimeError\n        If a deep copy of the model cannot be created.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: torch.nn.Module,\n        ema_decay: float,\n        ema_end_decay: float,\n        ema_anneal_end_step: int,\n        skip_keys: Optional[Union[list[str], Set[str]]] = None,\n    ) -&gt; None:\n        self.model = self.deepcopy_model(model)\n\n        self.skip_keys: Union[list[str], set[str]] = skip_keys or set()\n        self.num_updates = 0\n        self.decay = ema_decay  # stores the current decay value\n        self.ema_decay = ema_decay\n        self.ema_end_decay = ema_end_decay\n        self.ema_anneal_end_step = ema_anneal_end_step\n\n        self._model_configured = False\n\n    @staticmethod\n    def deepcopy_model(model: torch.nn.Module) -&gt; torch.nn.Module:\n        \"\"\"Deep copy the model.\n\n        Parameters\n        ----------\n        model : torch.nn.Module\n            The model to copy.\n\n        Returns\n        -------\n        torch.nn.Module\n            The copied model.\n\n        Raises\n        ------\n        RuntimeError\n            If the model cannot be copied.\n        \"\"\"\n        try:\n            return copy.deepcopy(model)\n        except RuntimeError as e:\n            raise RuntimeError(\"Unable to copy the model \", e) from e\n\n    @staticmethod\n    def get_annealed_rate(\n        start: float,\n        end: float,\n        curr_step: int,\n        total_steps: int,\n    ) -&gt; float:\n        \"\"\"Calculate EMA annealing rate.\"\"\"\n        r = end - start\n        pct_remaining = 1 - curr_step / total_steps\n        return end - r * pct_remaining\n\n    def configure_model(self, device_id: Union[int, torch.device]) -&gt; None:\n        \"\"\"Configure the model for EMA.\"\"\"\n        if self._model_configured:\n            return\n\n        self.model.requires_grad_(False)\n        self.model.to(device_id)\n\n        self._model_configured = True\n\n    def step(self, new_model: torch.nn.Module) -&gt; None:\n        \"\"\"Perform single EMA update step.\"\"\"\n        if not self._model_configured:\n            raise RuntimeError(\n                \"Model is not configured for EMA. Call `configure_model` first.\"\n            )\n\n        self._update_weights(new_model)\n        self._update_ema_decay()\n\n    def restore(self, model: torch.nn.Module) -&gt; torch.nn.Module:\n        \"\"\"Reassign weights from another model.\n\n        Parameters\n        ----------\n        model : torch.nn.Module\n            Model to load weights from.\n\n        Returns\n        -------\n        torch.nn.Module\n            model with new weights\n        \"\"\"\n        d = self.model.state_dict()\n        model.load_state_dict(d, strict=False)\n        return model\n\n    def state_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Return the state dict of the model.\"\"\"\n        return self.model.state_dict()  # type: ignore[no-any-return]\n\n    @torch.no_grad()  # type: ignore[misc]\n    def _update_weights(self, new_model: torch.nn.Module) -&gt; None:\n        if self.decay &lt; 1:\n            ema_state_dict = {}\n            ema_params = self.model.state_dict()\n\n            for key, param in new_model.state_dict().items():\n                ema_param = ema_params[key].float()\n\n                if param.shape != ema_param.shape:\n                    raise ValueError(\n                        \"Incompatible tensor shapes between student param and teacher param\"\n                        + \"{} vs. {}\".format(param.shape, ema_param.shape)\n                    )\n\n                if key in self.skip_keys or not param.requires_grad:\n                    ema_param = param.to(dtype=ema_param.dtype).clone()\n                else:\n                    ema_param.mul_(self.decay)\n                    ema_param.add_(\n                        param.to(dtype=ema_param.dtype),\n                        alpha=1 - self.decay,\n                    )\n                ema_state_dict[key] = ema_param\n\n            self.model.load_state_dict(ema_state_dict, strict=False)\n            self.num_updates += 1\n        else:\n            rank_zero_warn(\n                \"Exponential Moving Average decay is 1.0, no update is applied to the model.\",\n                stacklevel=1,\n                category=UserWarning,\n            )\n\n    def _update_ema_decay(self) -&gt; None:\n        if self.ema_decay != self.ema_end_decay:\n            if self.num_updates &gt;= self.ema_anneal_end_step:\n                decay = self.ema_end_decay\n            else:\n                decay = self.get_annealed_rate(\n                    self.ema_decay,\n                    self.ema_end_decay,\n                    self.num_updates,\n                    self.ema_anneal_end_step,\n                )\n            self.decay = decay\n</code></pre>"},{"location":"api/#mmlearn.modules.ema.ExponentialMovingAverage.deepcopy_model","title":"deepcopy_model  <code>staticmethod</code>","text":"<pre><code>deepcopy_model(model)\n</code></pre> <p>Deep copy the model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to copy.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>The copied model.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the model cannot be copied.</p> Source code in <code>mmlearn/modules/ema.py</code> <pre><code>@staticmethod\ndef deepcopy_model(model: torch.nn.Module) -&gt; torch.nn.Module:\n    \"\"\"Deep copy the model.\n\n    Parameters\n    ----------\n    model : torch.nn.Module\n        The model to copy.\n\n    Returns\n    -------\n    torch.nn.Module\n        The copied model.\n\n    Raises\n    ------\n    RuntimeError\n        If the model cannot be copied.\n    \"\"\"\n    try:\n        return copy.deepcopy(model)\n    except RuntimeError as e:\n        raise RuntimeError(\"Unable to copy the model \", e) from e\n</code></pre>"},{"location":"api/#mmlearn.modules.ema.ExponentialMovingAverage.get_annealed_rate","title":"get_annealed_rate  <code>staticmethod</code>","text":"<pre><code>get_annealed_rate(start, end, curr_step, total_steps)\n</code></pre> <p>Calculate EMA annealing rate.</p> Source code in <code>mmlearn/modules/ema.py</code> <pre><code>@staticmethod\ndef get_annealed_rate(\n    start: float,\n    end: float,\n    curr_step: int,\n    total_steps: int,\n) -&gt; float:\n    \"\"\"Calculate EMA annealing rate.\"\"\"\n    r = end - start\n    pct_remaining = 1 - curr_step / total_steps\n    return end - r * pct_remaining\n</code></pre>"},{"location":"api/#mmlearn.modules.ema.ExponentialMovingAverage.configure_model","title":"configure_model","text":"<pre><code>configure_model(device_id)\n</code></pre> <p>Configure the model for EMA.</p> Source code in <code>mmlearn/modules/ema.py</code> <pre><code>def configure_model(self, device_id: Union[int, torch.device]) -&gt; None:\n    \"\"\"Configure the model for EMA.\"\"\"\n    if self._model_configured:\n        return\n\n    self.model.requires_grad_(False)\n    self.model.to(device_id)\n\n    self._model_configured = True\n</code></pre>"},{"location":"api/#mmlearn.modules.ema.ExponentialMovingAverage.step","title":"step","text":"<pre><code>step(new_model)\n</code></pre> <p>Perform single EMA update step.</p> Source code in <code>mmlearn/modules/ema.py</code> <pre><code>def step(self, new_model: torch.nn.Module) -&gt; None:\n    \"\"\"Perform single EMA update step.\"\"\"\n    if not self._model_configured:\n        raise RuntimeError(\n            \"Model is not configured for EMA. Call `configure_model` first.\"\n        )\n\n    self._update_weights(new_model)\n    self._update_ema_decay()\n</code></pre>"},{"location":"api/#mmlearn.modules.ema.ExponentialMovingAverage.restore","title":"restore","text":"<pre><code>restore(model)\n</code></pre> <p>Reassign weights from another model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Model to load weights from.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>model with new weights</p> Source code in <code>mmlearn/modules/ema.py</code> <pre><code>def restore(self, model: torch.nn.Module) -&gt; torch.nn.Module:\n    \"\"\"Reassign weights from another model.\n\n    Parameters\n    ----------\n    model : torch.nn.Module\n        Model to load weights from.\n\n    Returns\n    -------\n    torch.nn.Module\n        model with new weights\n    \"\"\"\n    d = self.model.state_dict()\n    model.load_state_dict(d, strict=False)\n    return model\n</code></pre>"},{"location":"api/#mmlearn.modules.ema.ExponentialMovingAverage.state_dict","title":"state_dict","text":"<pre><code>state_dict()\n</code></pre> <p>Return the state dict of the model.</p> Source code in <code>mmlearn/modules/ema.py</code> <pre><code>def state_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Return the state dict of the model.\"\"\"\n    return self.model.state_dict()  # type: ignore[no-any-return]\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders","title":"encoders","text":"<p>Encoders.</p>"},{"location":"api/#mmlearn.modules.encoders.HFCLIPTextEncoder","title":"HFCLIPTextEncoder","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper around the <code>CLIPTextModel</code> from HuggingFace.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>The huggingface model name or a local path from which to load the model.</p> required <code>pretrained</code> <code>bool</code> <p>Whether to load the pretrained weights or not.</p> <code>True</code> <code>pooling_layer</code> <code>Optional[Module]</code> <p>Pooling layer to apply to the last hidden state of the model.</p> <code>None</code> <code>freeze_layers</code> <code>Union[int, float, list[int], bool]</code> <p>Whether to freeze layers of the model and which layers to freeze. If <code>True</code>, all model layers are frozen. If it is an integer, the first <code>N</code> layers of the model are frozen. If it is a float, the first <code>N</code> percent of the layers are frozen. If it is a list of integers, the layers at the indices in the list are frozen.</p> <code>False</code> <code>freeze_layer_norm</code> <code>bool</code> <p>Whether to freeze the layer normalization layers of the model.</p> <code>True</code> <code>peft_config</code> <code>Optional[PeftConfig]</code> <p>The configuration from the <code>peft &lt;https://huggingface.co/docs/peft/index&gt;</code>_ library to use to wrap the model for parameter-efficient finetuning.</p> <code>None</code> <code>model_config_kwargs</code> <code>Optional[dict[str, Any]]</code> <p>Additional keyword arguments to pass to the model configuration.</p> <code>None</code> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If both <code>peft_config</code> and <code>freeze_layers</code> are set. The <code>peft_config</code> will override the <code>freeze_layers</code> setting.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>@store(\n    group=\"modules/encoders\",\n    provider=\"mmlearn\",\n    model_name_or_path=\"openai/clip-vit-base-patch16\",\n    hydra_convert=\"object\",  # required for `peft_config` to be converted to a `PeftConfig` object\n)\nclass HFCLIPTextEncoder(nn.Module):\n    \"\"\"Wrapper around the ``CLIPTextModel`` from HuggingFace.\n\n    Parameters\n    ----------\n    model_name_or_path : str\n        The huggingface model name or a local path from which to load the model.\n    pretrained : bool, default=True\n        Whether to load the pretrained weights or not.\n    pooling_layer : Optional[torch.nn.Module], optional, default=None\n        Pooling layer to apply to the last hidden state of the model.\n    freeze_layers : Union[int, float, list[int], bool], default=False\n        Whether to freeze layers of the model and which layers to freeze. If ``True``,\n        all model layers are frozen. If it is an integer, the first ``N`` layers of\n        the model are frozen. If it is a float, the first ``N`` percent of the layers\n        are frozen. If it is a list of integers, the layers at the indices in the\n        list are frozen.\n    freeze_layer_norm : bool, default=True\n        Whether to freeze the layer normalization layers of the model.\n    peft_config : Optional[PeftConfig], optional, default=None\n        The configuration from the `peft &lt;https://huggingface.co/docs/peft/index&gt;`_\n        library to use to wrap the model for parameter-efficient finetuning.\n    model_config_kwargs : Optional[dict[str, Any]], optional, default=None\n        Additional keyword arguments to pass to the model configuration.\n\n    Warns\n    -----\n    UserWarning\n        If both ``peft_config`` and ``freeze_layers`` are set. The ``peft_config``\n        will override the ``freeze_layers`` setting.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name_or_path: str,\n        pretrained: bool = True,\n        pooling_layer: Optional[nn.Module] = None,\n        freeze_layers: Union[int, float, list[int], bool] = False,\n        freeze_layer_norm: bool = True,\n        peft_config: Optional[\"PeftConfig\"] = None,\n        model_config_kwargs: Optional[dict[str, Any]] = None,\n    ) -&gt; None:\n        super().__init__()\n        _warn_freeze_with_peft(peft_config, freeze_layers)\n\n        model = hf_utils.load_huggingface_model(\n            transformers.CLIPTextModel,\n            model_name_or_path=model_name_or_path,\n            load_pretrained_weights=pretrained,\n            model_config_kwargs=model_config_kwargs,\n        )\n        model = _freeze_text_model(model, freeze_layers, freeze_layer_norm)\n        if peft_config is not None:\n            model = hf_utils._wrap_peft_model(model, peft_config)\n\n        self.model = model\n        self.pooling_layer = pooling_layer\n\n    def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The ``input_ids`` will be expected under the\n            ``Modalities.TEXT`` key.\n\n        Returns\n        -------\n        BaseModelOutput\n            The output of the model, including the last hidden state, all hidden\n            states, and the attention weights, if ``output_attentions`` is set\n            to ``True``.\n        \"\"\"\n        outputs = self.model(\n            input_ids=inputs[Modalities.TEXT.name],\n            attention_mask=inputs.get(\"attention_mask\")\n            or inputs.get(Modalities.TEXT.attention_mask),\n            position_ids=inputs.get(\"position_ids\"),\n            output_attentions=inputs.get(\"output_attentions\"),\n            return_dict=True,\n        )\n        last_hidden_state = outputs.hidden_states[-1]  # NOTE: no layer norm applied\n        if self.pooling_layer:\n            last_hidden_state = self.pooling_layer(last_hidden_state)\n\n        return BaseModelOutput(\n            last_hidden_state=last_hidden_state,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.HFCLIPTextEncoder.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The <code>input_ids</code> will be expected under the <code>Modalities.TEXT</code> key.</p> required <p>Returns:</p> Type Description <code>BaseModelOutput</code> <p>The output of the model, including the last hidden state, all hidden states, and the attention weights, if <code>output_attentions</code> is set to <code>True</code>.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The ``input_ids`` will be expected under the\n        ``Modalities.TEXT`` key.\n\n    Returns\n    -------\n    BaseModelOutput\n        The output of the model, including the last hidden state, all hidden\n        states, and the attention weights, if ``output_attentions`` is set\n        to ``True``.\n    \"\"\"\n    outputs = self.model(\n        input_ids=inputs[Modalities.TEXT.name],\n        attention_mask=inputs.get(\"attention_mask\")\n        or inputs.get(Modalities.TEXT.attention_mask),\n        position_ids=inputs.get(\"position_ids\"),\n        output_attentions=inputs.get(\"output_attentions\"),\n        return_dict=True,\n    )\n    last_hidden_state = outputs.hidden_states[-1]  # NOTE: no layer norm applied\n    if self.pooling_layer:\n        last_hidden_state = self.pooling_layer(last_hidden_state)\n\n    return BaseModelOutput(\n        last_hidden_state=last_hidden_state,\n        hidden_states=outputs.hidden_states,\n        attentions=outputs.attentions,\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.HFCLIPTextEncoderWithProjection","title":"HFCLIPTextEncoderWithProjection","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper around the <code>CLIPTextModelWithProjection</code> from HuggingFace.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>The huggingface model name or a local path from which to load the model.</p> required <code>pretrained</code> <code>bool</code> <p>Whether to load the pretrained weights or not.</p> <code>True</code> <code>use_all_token_embeddings</code> <code>bool</code> <p>Whether to use all token embeddings for the text. If <code>False</code> the first token embedding will be used.</p> <code>False</code> <code>freeze_layers</code> <code>Union[int, float, list[int], bool]</code> <p>Whether to freeze layers of the model and which layers to freeze. If <code>True</code>, all model layers are frozen. If it is an integer, the first <code>N</code> layers of the model are frozen. If it is a float, the first <code>N</code> percent of the layers are frozen. If it is a list of integers, the layers at the indices in the list are frozen.</p> <code>False</code> <code>freeze_layer_norm</code> <code>bool</code> <p>Whether to freeze the layer normalization layers of the model.</p> <code>True</code> <code>peft_config</code> <code>Optional[PeftConfig]</code> <p>The configuration from the <code>peft &lt;https://huggingface.co/docs/peft/index&gt;</code>_ library to use to wrap the model for parameter-efficient finetuning.</p> <code>None</code> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If both <code>peft_config</code> and <code>freeze_layers</code> are set. The <code>peft_config</code> will override the <code>freeze_layers</code> setting.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>@store(\n    group=\"modules/encoders\",\n    provider=\"mmlearn\",\n    model_name_or_path=\"openai/clip-vit-base-patch16\",\n    hydra_convert=\"object\",\n)\nclass HFCLIPTextEncoderWithProjection(nn.Module):\n    \"\"\"Wrapper around the ``CLIPTextModelWithProjection`` from HuggingFace.\n\n    Parameters\n    ----------\n    model_name_or_path : str\n        The huggingface model name or a local path from which to load the model.\n    pretrained : bool, default=True\n        Whether to load the pretrained weights or not.\n    use_all_token_embeddings : bool, default=False\n        Whether to use all token embeddings for the text. If ``False`` the first token\n        embedding will be used.\n    freeze_layers : Union[int, float, list[int], bool], default=False\n        Whether to freeze layers of the model and which layers to freeze. If ``True``,\n        all model layers are frozen. If it is an integer, the first ``N`` layers of\n        the model are frozen. If it is a float, the first ``N`` percent of the layers\n        are frozen. If it is a list of integers, the layers at the indices in the\n        list are frozen.\n    freeze_layer_norm : bool, default=True\n        Whether to freeze the layer normalization layers of the model.\n    peft_config : Optional[PeftConfig], optional, default=None\n        The configuration from the `peft &lt;https://huggingface.co/docs/peft/index&gt;`_\n        library to use to wrap the model for parameter-efficient finetuning.\n\n    Warns\n    -----\n    UserWarning\n        If both ``peft_config`` and ``freeze_layers`` are set. The ``peft_config``\n        will override the ``freeze_layers`` setting.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name_or_path: str,\n        pretrained: bool = True,\n        use_all_token_embeddings: bool = False,\n        freeze_layers: Union[int, float, list[int], bool] = False,\n        freeze_layer_norm: bool = True,\n        peft_config: Optional[\"PeftConfig\"] = None,\n        model_config_kwargs: Optional[dict[str, Any]] = None,\n    ) -&gt; None:\n        super().__init__()\n        _warn_freeze_with_peft(peft_config, freeze_layers)\n\n        self.use_all_token_embeddings = use_all_token_embeddings\n\n        model = hf_utils.load_huggingface_model(\n            transformers.CLIPTextModelWithProjection,\n            model_name_or_path,\n            load_pretrained_weights=pretrained,\n            model_config_kwargs=model_config_kwargs,\n            config_type=CLIPTextConfig,\n        )\n\n        model = _freeze_text_model(model, freeze_layers, freeze_layer_norm)\n        if peft_config is not None:\n            model = hf_utils._wrap_peft_model(model, peft_config)\n\n        self.model = model\n\n    def forward(self, inputs: dict[str, Any]) -&gt; tuple[torch.Tensor]:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The ``input_ids`` will be expected under the\n            ``Modalities.TEXT`` key.\n\n        Returns\n        -------\n        tuple[torch.Tensor]\n            The text embeddings. Will be a tuple with a single element.\n        \"\"\"\n        input_ids = inputs[Modalities.TEXT.name]\n        attention_mask: Optional[torch.Tensor] = inputs.get(\n            \"attention_mask\", inputs.get(Modalities.TEXT.attention_mask, None)\n        )\n        position_ids = inputs.get(\"position_ids\")\n\n        if self.use_all_token_embeddings:\n            text_outputs = self.model.text_model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                return_dict=True,\n            )\n            # TODO: add more options for pooling before projection\n            text_embeds = self.model.text_projection(text_outputs.last_hidden_state)\n        else:\n            text_embeds = self.model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                return_dict=True,\n            ).text_embeds\n\n        return (text_embeds,)\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.HFCLIPTextEncoderWithProjection.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The <code>input_ids</code> will be expected under the <code>Modalities.TEXT</code> key.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor]</code> <p>The text embeddings. Will be a tuple with a single element.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; tuple[torch.Tensor]:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The ``input_ids`` will be expected under the\n        ``Modalities.TEXT`` key.\n\n    Returns\n    -------\n    tuple[torch.Tensor]\n        The text embeddings. Will be a tuple with a single element.\n    \"\"\"\n    input_ids = inputs[Modalities.TEXT.name]\n    attention_mask: Optional[torch.Tensor] = inputs.get(\n        \"attention_mask\", inputs.get(Modalities.TEXT.attention_mask, None)\n    )\n    position_ids = inputs.get(\"position_ids\")\n\n    if self.use_all_token_embeddings:\n        text_outputs = self.model.text_model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            return_dict=True,\n        )\n        # TODO: add more options for pooling before projection\n        text_embeds = self.model.text_projection(text_outputs.last_hidden_state)\n    else:\n        text_embeds = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            return_dict=True,\n        ).text_embeds\n\n    return (text_embeds,)\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.HFCLIPVisionEncoder","title":"HFCLIPVisionEncoder","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper around the <code>CLIPVisionModel</code> from HuggingFace.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>The huggingface model name or a local path from which to load the model.</p> required <code>pretrained</code> <code>bool</code> <p>Whether to load the pretrained weights or not.</p> <code>True</code> <code>pooling_layer</code> <code>Optional[Module]</code> <p>Pooling layer to apply to the last hidden state of the model.</p> <code>None</code> <code>freeze_layers</code> <code>Union[int, float, list[int], bool]</code> <p>Whether to freeze layers of the model and which layers to freeze. If <code>True</code>, all model layers are frozen. If it is an integer, the first <code>N</code> layers of the model are frozen. If it is a float, the first <code>N</code> percent of the layers are frozen. If it is a list of integers, the layers at the indices in the list are frozen.</p> <code>False</code> <code>freeze_layer_norm</code> <code>bool</code> <p>Whether to freeze the layer normalization layers of the model.</p> <code>True</code> <code>patch_dropout_rate</code> <code>float</code> <p>The proportion of patch embeddings to drop out.</p> <code>0.0</code> <code>patch_dropout_shuffle</code> <code>bool</code> <p>Whether to shuffle the patches while applying patch dropout.</p> <code>False</code> <code>patch_dropout_bias</code> <code>Optional[float]</code> <p>The bias to apply to the patch dropout mask.</p> <code>None</code> <code>peft_config</code> <code>Optional[PeftConfig]</code> <p>The configuration from the <code>peft &lt;https://huggingface.co/docs/peft/index&gt;</code>_ library to use to wrap the model for parameter-efficient finetuning.</p> <code>None</code> <code>model_config_kwargs</code> <code>Optional[dict[str, Any]]</code> <p>Additional keyword arguments to pass to the model configuration.</p> <code>None</code> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If both <code>peft_config</code> and <code>freeze_layers</code> are set. The <code>peft_config</code> will override the <code>freeze_layers</code> setting.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>@store(\n    group=\"modules/encoders\",\n    provider=\"mmlearn\",\n    model_name_or_path=\"openai/clip-vit-base-patch16\",\n    hydra_convert=\"object\",\n)\nclass HFCLIPVisionEncoder(nn.Module):\n    \"\"\"Wrapper around the ``CLIPVisionModel`` from HuggingFace.\n\n    Parameters\n    ----------\n    model_name_or_path : str\n        The huggingface model name or a local path from which to load the model.\n    pretrained : bool, default=True\n        Whether to load the pretrained weights or not.\n    pooling_layer : Optional[torch.nn.Module], optional, default=None\n        Pooling layer to apply to the last hidden state of the model.\n    freeze_layers : Union[int, float, list[int], bool], default=False\n        Whether to freeze layers of the model and which layers to freeze. If ``True``,\n        all model layers are frozen. If it is an integer, the first ``N`` layers of\n        the model are frozen. If it is a float, the first ``N`` percent of the layers\n        are frozen. If it is a list of integers, the layers at the indices in the\n        list are frozen.\n    freeze_layer_norm : bool, default=True\n        Whether to freeze the layer normalization layers of the model.\n    patch_dropout_rate : float, default=0.0\n        The proportion of patch embeddings to drop out.\n    patch_dropout_shuffle : bool, default=False\n        Whether to shuffle the patches while applying patch dropout.\n    patch_dropout_bias : Optional[float], optional, default=None\n        The bias to apply to the patch dropout mask.\n    peft_config : Optional[PeftConfig], optional, default=None\n        The configuration from the `peft &lt;https://huggingface.co/docs/peft/index&gt;`_\n        library to use to wrap the model for parameter-efficient finetuning.\n    model_config_kwargs : Optional[dict[str, Any]], optional, default=None\n        Additional keyword arguments to pass to the model configuration.\n\n    Warns\n    -----\n    UserWarning\n        If both ``peft_config`` and ``freeze_layers`` are set. The ``peft_config``\n        will override the ``freeze_layers`` setting.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name_or_path: str,\n        pretrained: bool = True,\n        pooling_layer: Optional[nn.Module] = None,\n        freeze_layers: Union[int, float, list[int], bool] = False,\n        freeze_layer_norm: bool = True,\n        patch_dropout_rate: float = 0.0,\n        patch_dropout_shuffle: bool = False,\n        patch_dropout_bias: Optional[float] = None,\n        peft_config: Optional[\"PeftConfig\"] = None,\n        model_config_kwargs: Optional[dict[str, Any]] = None,\n    ) -&gt; None:\n        super().__init__()\n        _warn_freeze_with_peft(peft_config, freeze_layers)\n\n        model = hf_utils.load_huggingface_model(\n            transformers.CLIPVisionModel,\n            model_name_or_path=model_name_or_path,\n            load_pretrained_weights=pretrained,\n            model_config_kwargs=model_config_kwargs,\n        )\n        model = _freeze_vision_model(model, freeze_layers, freeze_layer_norm)\n        if peft_config is not None:\n            model = hf_utils._wrap_peft_model(model, peft_config)\n\n        self.model = model.vision_model\n        self.pooling_layer = pooling_layer\n        self.patch_dropout = None\n        if patch_dropout_rate &gt; 0:\n            self.patch_dropout = PatchDropout(\n                keep_rate=1 - patch_dropout_rate,\n                token_shuffling=patch_dropout_shuffle,\n                bias=patch_dropout_bias,\n            )\n\n    def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The image tensor will be expected under the\n            ``Modalities.RGB`` key.\n\n        Returns\n        -------\n        BaseModelOutput\n            The output of the model, including the last hidden state, all hidden states,\n            and the attention weights, if ``output_attentions`` is set to ``True``.\n\n        \"\"\"\n        # FIXME: handle other vision modalities\n        pixel_values = inputs[Modalities.RGB.name]\n        hidden_states = self.model.embeddings(pixel_values)\n        if self.patch_dropout is not None:\n            hidden_states = self.patch_dropout(hidden_states)\n        hidden_states = self.model.pre_layrnorm(hidden_states)\n\n        encoder_outputs = self.model.encoder(\n            inputs_embeds=hidden_states,\n            output_attentions=inputs.get(\n                \"output_attentions\", self.model.config.output_attentions\n            ),\n            output_hidden_states=True,\n            return_dict=True,\n        )\n\n        last_hidden_state = encoder_outputs[0]\n        if self.pooling_layer is not None:\n            last_hidden_state = self.pooling_layer(last_hidden_state)\n\n        return BaseModelOutput(\n            last_hidden_state=last_hidden_state,\n            hidden_states=encoder_outputs.hidden_states,\n            attentions=encoder_outputs.attentions,\n        )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.HFCLIPVisionEncoder.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The image tensor will be expected under the <code>Modalities.RGB</code> key.</p> required <p>Returns:</p> Type Description <code>BaseModelOutput</code> <p>The output of the model, including the last hidden state, all hidden states, and the attention weights, if <code>output_attentions</code> is set to <code>True</code>.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The image tensor will be expected under the\n        ``Modalities.RGB`` key.\n\n    Returns\n    -------\n    BaseModelOutput\n        The output of the model, including the last hidden state, all hidden states,\n        and the attention weights, if ``output_attentions`` is set to ``True``.\n\n    \"\"\"\n    # FIXME: handle other vision modalities\n    pixel_values = inputs[Modalities.RGB.name]\n    hidden_states = self.model.embeddings(pixel_values)\n    if self.patch_dropout is not None:\n        hidden_states = self.patch_dropout(hidden_states)\n    hidden_states = self.model.pre_layrnorm(hidden_states)\n\n    encoder_outputs = self.model.encoder(\n        inputs_embeds=hidden_states,\n        output_attentions=inputs.get(\n            \"output_attentions\", self.model.config.output_attentions\n        ),\n        output_hidden_states=True,\n        return_dict=True,\n    )\n\n    last_hidden_state = encoder_outputs[0]\n    if self.pooling_layer is not None:\n        last_hidden_state = self.pooling_layer(last_hidden_state)\n\n    return BaseModelOutput(\n        last_hidden_state=last_hidden_state,\n        hidden_states=encoder_outputs.hidden_states,\n        attentions=encoder_outputs.attentions,\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.HFCLIPVisionEncoderWithProjection","title":"HFCLIPVisionEncoderWithProjection","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper around the <code>CLIPVisionModelWithProjection</code> class from HuggingFace.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>The huggingface model name or a local path from which to load the model.</p> required <code>pretrained</code> <code>bool</code> <p>Whether to load the pretrained weights or not.</p> <code>True</code> <code>use_all_token_embeddings</code> <code>bool</code> <p>Whether to use all token embeddings for the text. If <code>False</code> the first token embedding will be used.</p> <code>False</code> <code>freeze_layers</code> <code>Union[int, float, list[int], bool]</code> <p>Whether to freeze layers of the model and which layers to freeze. If <code>True</code>, all model layers are frozen. If it is an integer, the first <code>N` layers of the model are frozen. If it is a float, the first</code>N`` percent of the layers are frozen. If it is a list of integers, the layers at the indices in the list are frozen.</p> <code>False</code> <code>freeze_layer_norm</code> <code>bool</code> <p>Whether to freeze the layer normalization layers of the model.</p> <code>True</code> <code>patch_dropout_rate</code> <code>float</code> <p>The proportion of patch embeddings to drop out.</p> <code>0.0</code> <code>patch_dropout_shuffle</code> <code>bool</code> <p>Whether to shuffle the patches while applying patch dropout.</p> <code>False</code> <code>patch_dropout_bias</code> <code>float</code> <p>The bias to apply to the patch dropout mask.</p> <code>None</code> <code>peft_config</code> <code>Optional[PeftConfig]</code> <p>The configuration from the <code>peft &lt;https://huggingface.co/docs/peft/index&gt;</code>_ library to use to wrap the model for parameter-efficient finetuning.</p> <code>None</code> <code>model_config_kwargs</code> <code>dict[str, Any]</code> <p>Additional keyword arguments to pass to the model configuration.</p> <code>None</code> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If both <code>peft_config</code> and <code>freeze_layers</code> are set. The <code>peft_config</code> will override the <code>freeze_layers</code> setting.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>@store(\n    group=\"modules/encoders\",\n    provider=\"mmlearn\",\n    model_name_or_path=\"openai/clip-vit-base-patch16\",\n    hydra_convert=\"object\",\n)\nclass HFCLIPVisionEncoderWithProjection(nn.Module):\n    \"\"\"Wrapper around the ``CLIPVisionModelWithProjection`` class from HuggingFace.\n\n    Parameters\n    ----------\n    model_name_or_path : str\n        The huggingface model name or a local path from which to load the model.\n    pretrained : bool, default=True\n        Whether to load the pretrained weights or not.\n    use_all_token_embeddings : bool, default=False\n        Whether to use all token embeddings for the text. If ``False`` the first token\n        embedding will be used.\n    freeze_layers : Union[int, float, list[int], bool], default=False\n        Whether to freeze layers of the model and which layers to freeze. If ``True``,\n        all model layers are frozen. If it is an integer, the first ``N` layers of\n        the model are frozen. If it is a float, the first ``N`` percent of the layers\n        are frozen. If it is a list of integers, the layers at the indices in the\n        list are frozen.\n    freeze_layer_norm : bool, default=True\n        Whether to freeze the layer normalization layers of the model.\n    patch_dropout_rate : float, default=0.0\n        The proportion of patch embeddings to drop out.\n    patch_dropout_shuffle : bool, default=False\n        Whether to shuffle the patches while applying patch dropout.\n    patch_dropout_bias : float, optional, default=None\n        The bias to apply to the patch dropout mask.\n    peft_config : Optional[PeftConfig], optional, default=None\n        The configuration from the `peft &lt;https://huggingface.co/docs/peft/index&gt;`_\n        library to use to wrap the model for parameter-efficient finetuning.\n    model_config_kwargs : dict[str, Any], optional, default=None\n        Additional keyword arguments to pass to the model configuration.\n\n    Warns\n    -----\n    UserWarning\n        If both ``peft_config`` and ``freeze_layers`` are set. The ``peft_config``\n        will override the ``freeze_layers`` setting.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name_or_path: str,\n        pretrained: bool = True,\n        use_all_token_embeddings: bool = False,\n        patch_dropout_rate: float = 0.0,\n        patch_dropout_shuffle: bool = False,\n        patch_dropout_bias: Optional[float] = None,\n        freeze_layers: Union[int, float, list[int], bool] = False,\n        freeze_layer_norm: bool = True,\n        peft_config: Optional[\"PeftConfig\"] = None,\n        model_config_kwargs: Optional[dict[str, Any]] = None,\n    ) -&gt; None:\n        super().__init__()\n        _warn_freeze_with_peft(peft_config, freeze_layers)\n\n        self.use_all_token_embeddings = use_all_token_embeddings\n\n        model = hf_utils.load_huggingface_model(\n            transformers.CLIPVisionModelWithProjection,\n            model_name_or_path,\n            load_pretrained_weights=pretrained,\n            model_config_kwargs=model_config_kwargs,\n            config_type=CLIPVisionConfig,\n        )\n\n        model = _freeze_vision_model(model, freeze_layers, freeze_layer_norm)\n        if peft_config is not None:\n            model = hf_utils._wrap_peft_model(model, peft_config)\n\n        self.model = model\n        self.patch_dropout = None\n        if patch_dropout_rate &gt; 0:\n            self.patch_dropout = PatchDropout(\n                keep_rate=1 - patch_dropout_rate,\n                token_shuffling=patch_dropout_shuffle,\n                bias=patch_dropout_bias,\n            )\n\n    def forward(self, inputs: dict[str, Any]) -&gt; tuple[torch.Tensor]:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The image tensor will be expected under the\n            ``Modalities.RGB`` key.\n\n        Returns\n        -------\n        tuple[torch.Tensor]\n            The image embeddings. Will be a tuple with a single element.\n        \"\"\"\n        pixel_values = inputs[Modalities.RGB.name]\n        hidden_states = self.model.vision_model.embeddings(pixel_values)\n        if self.patch_dropout is not None:\n            hidden_states = self.patch_dropout(hidden_states)\n        hidden_states = self.model.vision_model.pre_layrnorm(hidden_states)\n\n        encoder_outputs = self.model.vision_model.encoder(\n            inputs_embeds=hidden_states, return_dict=True\n        )\n\n        last_hidden_state = encoder_outputs.last_hidden_state\n        if self.use_all_token_embeddings:\n            pooled_output = last_hidden_state\n        else:\n            pooled_output = last_hidden_state[:, 0, :]\n        pooled_output = self.model.vision_model.post_layernorm(pooled_output)\n\n        return (self.model.visual_projection(pooled_output),)\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.HFCLIPVisionEncoderWithProjection.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The image tensor will be expected under the <code>Modalities.RGB</code> key.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor]</code> <p>The image embeddings. Will be a tuple with a single element.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; tuple[torch.Tensor]:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The image tensor will be expected under the\n        ``Modalities.RGB`` key.\n\n    Returns\n    -------\n    tuple[torch.Tensor]\n        The image embeddings. Will be a tuple with a single element.\n    \"\"\"\n    pixel_values = inputs[Modalities.RGB.name]\n    hidden_states = self.model.vision_model.embeddings(pixel_values)\n    if self.patch_dropout is not None:\n        hidden_states = self.patch_dropout(hidden_states)\n    hidden_states = self.model.vision_model.pre_layrnorm(hidden_states)\n\n    encoder_outputs = self.model.vision_model.encoder(\n        inputs_embeds=hidden_states, return_dict=True\n    )\n\n    last_hidden_state = encoder_outputs.last_hidden_state\n    if self.use_all_token_embeddings:\n        pooled_output = last_hidden_state\n    else:\n        pooled_output = last_hidden_state[:, 0, :]\n    pooled_output = self.model.vision_model.post_layernorm(pooled_output)\n\n    return (self.model.visual_projection(pooled_output),)\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.HFTextEncoder","title":"HFTextEncoder","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper around huggingface models in the <code>AutoModelForTextEncoding</code> class.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>The huggingface model name or a local path from which to load the model.</p> required <code>pretrained</code> <code>bool</code> <p>Whether to load the pretrained weights or not.</p> <code>True</code> <code>pooling_layer</code> <code>Optional[Module]</code> <p>Pooling layer to apply to the last hidden state of the model.</p> <code>None</code> <code>freeze_layers</code> <code>Union[int, float, list[int], bool]</code> <p>Whether to freeze layers of the model and which layers to freeze. If <code>True</code>, all model layers are frozen. If it is an integer, the first <code>N</code> layers of the model are frozen. If it is a float, the first <code>N</code> percent of the layers are frozen. If it is a list of integers, the layers at the indices in the list are frozen.</p> <code>False</code> <code>freeze_layer_norm</code> <code>bool</code> <p>Whether to freeze the layer normalization layers of the model.</p> <code>True</code> <code>peft_config</code> <code>Optional[PeftConfig]</code> <p>The configuration from the <code>peft &lt;https://huggingface.co/docs/peft/index&gt;</code>_ library to use to wrap the model for parameter-efficient finetuning.</p> <code>None</code> <code>model_config_kwargs</code> <code>Optional[dict[str, Any]]</code> <p>Additional keyword arguments to pass to the model configuration.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model is a decoder model or if freezing individual layers is not supported for the model type.</p> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If both <code>peft_config</code> and <code>freeze_layers</code> are set. The <code>peft_config</code> will override the <code>freeze_layers</code> setting.</p> Source code in <code>mmlearn/modules/encoders/text.py</code> <pre><code>@store(group=\"modules/encoders\", provider=\"mmlearn\", hydra_convert=\"object\")\nclass HFTextEncoder(nn.Module):\n    \"\"\"Wrapper around huggingface models in the ``AutoModelForTextEncoding`` class.\n\n    Parameters\n    ----------\n    model_name_or_path : str\n        The huggingface model name or a local path from which to load the model.\n    pretrained : bool, default=True\n        Whether to load the pretrained weights or not.\n    pooling_layer : Optional[torch.nn.Module], optional, default=None\n        Pooling layer to apply to the last hidden state of the model.\n    freeze_layers : Union[int, float, list[int], bool], default=False\n        Whether to freeze layers of the model and which layers to freeze. If ``True``,\n        all model layers are frozen. If it is an integer, the first ``N`` layers of\n        the model are frozen. If it is a float, the first ``N`` percent of the layers\n        are frozen. If it is a list of integers, the layers at the indices in the\n        list are frozen.\n    freeze_layer_norm : bool, default=True\n        Whether to freeze the layer normalization layers of the model.\n    peft_config : Optional[PeftConfig], optional, default=None\n        The configuration from the `peft &lt;https://huggingface.co/docs/peft/index&gt;`_\n        library to use to wrap the model for parameter-efficient finetuning.\n    model_config_kwargs : Optional[dict[str, Any]], optional, default=None\n        Additional keyword arguments to pass to the model configuration.\n\n    Raises\n    ------\n    ValueError\n        If the model is a decoder model or if freezing individual layers is not\n        supported for the model type.\n\n    Warns\n    -----\n    UserWarning\n        If both ``peft_config`` and ``freeze_layers`` are set. The ``peft_config``\n        will override the ``freeze_layers`` setting.\n\n\n    \"\"\"\n\n    def __init__(  # noqa: PLR0912\n        self,\n        model_name_or_path: str,\n        pretrained: bool = True,\n        pooling_layer: Optional[nn.Module] = None,\n        freeze_layers: Union[int, float, list[int], bool] = False,\n        freeze_layer_norm: bool = True,\n        peft_config: Optional[\"PeftConfig\"] = None,\n        model_config_kwargs: Optional[dict[str, Any]] = None,\n    ):\n        super().__init__()\n        if model_config_kwargs is None:\n            model_config_kwargs = {}\n        model_config_kwargs[\"output_hidden_states\"] = True\n        model_config_kwargs[\"add_pooling_layer\"] = False\n        model = hf_utils.load_huggingface_model(\n            AutoModelForTextEncoding,\n            model_name_or_path,\n            load_pretrained_weights=pretrained,\n            model_config_kwargs=model_config_kwargs,\n        )\n        if hasattr(model.config, \"is_decoder\") and model.config.is_decoder:\n            raise ValueError(\"Model is a decoder. Only encoder models are supported.\")\n\n        if not pretrained and freeze_layers:\n            rank_zero_warn(\n                \"Freezing layers when loading a model with random weights may lead to \"\n                \"unexpected behavior. Consider setting `freeze_layers=False` if \"\n                \"`pretrained=False`.\",\n            )\n\n        if isinstance(freeze_layers, bool) and freeze_layers:\n            for name, param in model.named_parameters():\n                param.requires_grad = (\n                    (not freeze_layer_norm) if \"LayerNorm\" in name else False\n                )\n\n        if isinstance(\n            freeze_layers, (float, int, list)\n        ) and model.config.model_type in [\"flaubert\", \"xlm\"]:\n            # flaubert and xlm models have a different architecture that does not\n            # support freezing individual layers in the same way as other models\n            raise ValueError(\n                f\"Freezing individual layers is not supported for {model.config.model_type} \"\n                \"models. Please use `freeze_layers=False` or `freeze_layers=True`.\"\n            )\n\n        # get list of layers\n        embeddings = model.embeddings\n        encoder = getattr(model, \"encoder\", None) or getattr(\n            model, \"transformer\", model\n        )\n        encoder_layers = (\n            getattr(encoder, \"layer\", None)\n            or getattr(encoder, \"layers\", None)\n            or getattr(encoder, \"block\", None)\n        )\n        if encoder_layers is None and hasattr(encoder, \"albert_layer_groups\"):\n            encoder_layers = [\n                layer\n                for group in encoder.albert_layer_groups\n                for layer in group.albert_layers\n            ]\n        modules = [embeddings]\n        if encoder_layers is not None and isinstance(encoder_layers, list):\n            modules.extend(encoder_layers)\n\n        if isinstance(freeze_layers, float):\n            freeze_layers = int(freeze_layers * len(modules))\n        if isinstance(freeze_layers, int):\n            freeze_layers = list(range(freeze_layers))\n\n        if isinstance(freeze_layers, list):\n            for idx, module in enumerate(modules):\n                if idx in freeze_layers:\n                    for name, param in module.named_parameters():\n                        param.requires_grad = (\n                            (not freeze_layer_norm) if \"LayerNorm\" in name else False\n                        )\n\n        if peft_config is not None:\n            model = hf_utils._wrap_peft_model(model, peft_config)\n\n        self.model = model\n        self.pooling_layer = pooling_layer\n\n    def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The ``input_ids`` will be expected under the\n            ``Modalities.TEXT`` key.\n\n        Returns\n        -------\n        BaseModelOutput\n            The output of the model, including the last hidden state, all hidden states,\n            and the attention weights, if ``output_attentions`` is set to ``True``.\n        \"\"\"\n        outputs = self.model(\n            input_ids=inputs[Modalities.TEXT.name],\n            attention_mask=inputs.get(\n                \"attention_mask\", inputs.get(Modalities.TEXT.attention_mask, None)\n            ),\n            position_ids=inputs.get(\"position_ids\"),\n            output_attentions=inputs.get(\"output_attentions\"),\n            return_dict=True,\n        )\n        last_hidden_state = outputs.hidden_states[-1]  # NOTE: no layer norm applied\n        if self.pooling_layer:\n            last_hidden_state = self.pooling_layer(last_hidden_state)\n\n        return BaseModelOutput(\n            last_hidden_state=last_hidden_state,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.HFTextEncoder.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The <code>input_ids</code> will be expected under the <code>Modalities.TEXT</code> key.</p> required <p>Returns:</p> Type Description <code>BaseModelOutput</code> <p>The output of the model, including the last hidden state, all hidden states, and the attention weights, if <code>output_attentions</code> is set to <code>True</code>.</p> Source code in <code>mmlearn/modules/encoders/text.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The ``input_ids`` will be expected under the\n        ``Modalities.TEXT`` key.\n\n    Returns\n    -------\n    BaseModelOutput\n        The output of the model, including the last hidden state, all hidden states,\n        and the attention weights, if ``output_attentions`` is set to ``True``.\n    \"\"\"\n    outputs = self.model(\n        input_ids=inputs[Modalities.TEXT.name],\n        attention_mask=inputs.get(\n            \"attention_mask\", inputs.get(Modalities.TEXT.attention_mask, None)\n        ),\n        position_ids=inputs.get(\"position_ids\"),\n        output_attentions=inputs.get(\"output_attentions\"),\n        return_dict=True,\n    )\n    last_hidden_state = outputs.hidden_states[-1]  # NOTE: no layer norm applied\n    if self.pooling_layer:\n        last_hidden_state = self.pooling_layer(last_hidden_state)\n\n    return BaseModelOutput(\n        last_hidden_state=last_hidden_state,\n        hidden_states=outputs.hidden_states,\n        attentions=outputs.attentions,\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.TimmViT","title":"TimmViT","text":"<p>               Bases: <code>Module</code></p> <p>Vision Transformer model from timm.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the model to use.</p> required <code>modality</code> <code>str</code> <p>The modality of the input data. This allows this model to be used with different image modalities e.g. RGB, Depth, etc.</p> <code>\"RGB\"</code> <code>projection_dim</code> <code>int</code> <p>The dimension of the projection head.</p> <code>768</code> <code>pretrained</code> <code>bool</code> <p>Whether to use the pretrained weights.</p> <code>True</code> <code>freeze_layers</code> <code>Union[int, float, list[int], bool]</code> <p>Whether to freeze the layers.</p> <code>False</code> <code>freeze_layer_norm</code> <code>bool</code> <p>Whether to freeze the layer norm.</p> <code>True</code> <code>peft_config</code> <code>Optional[PeftConfig]</code> <p>The configuration from the <code>peft &lt;https://huggingface.co/docs/peft/index&gt;</code>_ library to use to wrap the model for parameter-efficient finetuning.</p> <code>None</code> <code>model_kwargs</code> <code>Optional[dict[str, Any]]</code> <p>Additional keyword arguments for the model.</p> <code>None</code> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>@store(\n    group=\"modules/encoders\",\n    provider=\"mmlearn\",\n    model_name=\"vit_base_patch16_224\",\n    hydra_convert=\"object\",\n)\nclass TimmViT(nn.Module):\n    \"\"\"Vision Transformer model from timm.\n\n    Parameters\n    ----------\n    model_name : str\n        The name of the model to use.\n    modality : str, default=\"RGB\"\n        The modality of the input data. This allows this model to be used with different\n        image modalities e.g. RGB, Depth, etc.\n    projection_dim : int, default=768\n        The dimension of the projection head.\n    pretrained : bool, default=True\n        Whether to use the pretrained weights.\n    freeze_layers : Union[int, float, list[int], bool], default=False\n        Whether to freeze the layers.\n    freeze_layer_norm : bool, default=True\n        Whether to freeze the layer norm.\n    peft_config : Optional[PeftConfig], optional, default=None\n        The configuration from the `peft &lt;https://huggingface.co/docs/peft/index&gt;`_\n        library to use to wrap the model for parameter-efficient finetuning.\n    model_kwargs : Optional[dict[str, Any]], default=None\n        Additional keyword arguments for the model.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str,\n        modality: str = \"RGB\",\n        projection_dim: int = 768,\n        pretrained: bool = True,\n        freeze_layers: Union[int, float, list[int], bool] = False,\n        freeze_layer_norm: bool = True,\n        peft_config: Optional[\"PeftConfig\"] = None,\n        model_kwargs: Optional[dict[str, Any]] = None,\n    ) -&gt; None:\n        super().__init__()\n        self.modality = Modalities.get_modality(modality)\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        self.model: TimmVisionTransformer = timm.create_model(\n            model_name,\n            pretrained=pretrained,\n            num_classes=projection_dim,\n            **model_kwargs,\n        )\n        assert isinstance(self.model, TimmVisionTransformer), (\n            f\"Model {model_name} is not a Vision Transformer. \"\n            \"Please provide a model name that corresponds to a Vision Transformer.\"\n        )\n\n        self._freeze_layers(freeze_layers, freeze_layer_norm)\n\n        if peft_config is not None:\n            self.model = hf_utils._wrap_peft_model(self.model, peft_config)\n\n    def _freeze_layers(\n        self, freeze_layers: Union[int, float, list[int], bool], freeze_layer_norm: bool\n    ) -&gt; None:\n        \"\"\"Freeze the layers of the model.\n\n        Parameters\n        ----------\n        freeze_layers : Union[int, float, list[int], bool]\n            Whether to freeze the layers.\n        freeze_layer_norm : bool\n            Whether to freeze the layer norm.\n        \"\"\"\n        if isinstance(freeze_layers, bool) and freeze_layers:\n            for name, param in self.model.named_parameters():\n                param.requires_grad = (\n                    (not freeze_layer_norm) if \"norm\" in name else False\n                )\n\n        modules = [self.model.patch_embed, *self.model.blocks, self.model.norm]\n        if isinstance(freeze_layers, float):\n            freeze_layers = int(freeze_layers * len(modules))\n        if isinstance(freeze_layers, int):\n            freeze_layers = list(range(freeze_layers))\n\n        if isinstance(freeze_layers, list):\n            for idx, module in enumerate(modules):\n                if idx in freeze_layers:\n                    for name, param in module.named_parameters():\n                        param.requires_grad = (\n                            (not freeze_layer_norm) if \"norm\" in name else False\n                        )\n\n    def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The ``image`` will be expected under the\n            ``Modalities.RGB`` key.\n\n        Returns\n        -------\n        BaseModelOutput\n            The output of the model.\n        \"\"\"\n        x = inputs[self.modality.name]\n        last_hidden_state, hidden_states = self.model.forward_intermediates(\n            x, output_fmt=\"NLC\"\n        )\n        last_hidden_state = self.model.forward_head(last_hidden_state)\n\n        return BaseModelOutput(\n            last_hidden_state=last_hidden_state, hidden_states=hidden_states\n        )\n\n    def get_intermediate_layers(\n        self, inputs: dict[str, Any], n: int = 1\n    ) -&gt; list[torch.Tensor]:\n        \"\"\"Get the output of the intermediate layers.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The ``image`` will be expected under the ``Modalities.RGB``\n            key.\n        n : int, default=1\n            The number of intermediate layers to return.\n\n        Returns\n        -------\n        list[torch.Tensor]\n            The outputs of the last n intermediate layers.\n        \"\"\"\n        return self.model.get_intermediate_layers(inputs[Modalities.RGB], n)  # type: ignore\n\n    def get_patch_info(self) -&gt; tuple[int, int]:\n        \"\"\"Get patch size and number of patches.\n\n        Returns\n        -------\n        tuple[int, int]\n            Patch size and number of patches.\n        \"\"\"\n        patch_size = self.model.patch_embed.patch_size[0]\n        num_patches = self.model.patch_embed.num_patches\n        return patch_size, num_patches\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.TimmViT.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The <code>image</code> will be expected under the <code>Modalities.RGB</code> key.</p> required <p>Returns:</p> Type Description <code>BaseModelOutput</code> <p>The output of the model.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The ``image`` will be expected under the\n        ``Modalities.RGB`` key.\n\n    Returns\n    -------\n    BaseModelOutput\n        The output of the model.\n    \"\"\"\n    x = inputs[self.modality.name]\n    last_hidden_state, hidden_states = self.model.forward_intermediates(\n        x, output_fmt=\"NLC\"\n    )\n    last_hidden_state = self.model.forward_head(last_hidden_state)\n\n    return BaseModelOutput(\n        last_hidden_state=last_hidden_state, hidden_states=hidden_states\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.TimmViT.get_intermediate_layers","title":"get_intermediate_layers","text":"<pre><code>get_intermediate_layers(inputs, n=1)\n</code></pre> <p>Get the output of the intermediate layers.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The <code>image</code> will be expected under the <code>Modalities.RGB</code> key.</p> required <code>n</code> <code>int</code> <p>The number of intermediate layers to return.</p> <code>1</code> <p>Returns:</p> Type Description <code>list[Tensor]</code> <p>The outputs of the last n intermediate layers.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>def get_intermediate_layers(\n    self, inputs: dict[str, Any], n: int = 1\n) -&gt; list[torch.Tensor]:\n    \"\"\"Get the output of the intermediate layers.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The ``image`` will be expected under the ``Modalities.RGB``\n        key.\n    n : int, default=1\n        The number of intermediate layers to return.\n\n    Returns\n    -------\n    list[torch.Tensor]\n        The outputs of the last n intermediate layers.\n    \"\"\"\n    return self.model.get_intermediate_layers(inputs[Modalities.RGB], n)  # type: ignore\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.TimmViT.get_patch_info","title":"get_patch_info","text":"<pre><code>get_patch_info()\n</code></pre> <p>Get patch size and number of patches.</p> <p>Returns:</p> Type Description <code>tuple[int, int]</code> <p>Patch size and number of patches.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>def get_patch_info(self) -&gt; tuple[int, int]:\n    \"\"\"Get patch size and number of patches.\n\n    Returns\n    -------\n    tuple[int, int]\n        Patch size and number of patches.\n    \"\"\"\n    patch_size = self.model.patch_embed.patch_size[0]\n    num_patches = self.model.patch_embed.num_patches\n    return patch_size, num_patches\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.clip","title":"clip","text":"<p>Wrappers and interfaces for CLIP models.</p>"},{"location":"api/#mmlearn.modules.encoders.clip.HFCLIPTextEncoder","title":"HFCLIPTextEncoder","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper around the <code>CLIPTextModel</code> from HuggingFace.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>The huggingface model name or a local path from which to load the model.</p> required <code>pretrained</code> <code>bool</code> <p>Whether to load the pretrained weights or not.</p> <code>True</code> <code>pooling_layer</code> <code>Optional[Module]</code> <p>Pooling layer to apply to the last hidden state of the model.</p> <code>None</code> <code>freeze_layers</code> <code>Union[int, float, list[int], bool]</code> <p>Whether to freeze layers of the model and which layers to freeze. If <code>True</code>, all model layers are frozen. If it is an integer, the first <code>N</code> layers of the model are frozen. If it is a float, the first <code>N</code> percent of the layers are frozen. If it is a list of integers, the layers at the indices in the list are frozen.</p> <code>False</code> <code>freeze_layer_norm</code> <code>bool</code> <p>Whether to freeze the layer normalization layers of the model.</p> <code>True</code> <code>peft_config</code> <code>Optional[PeftConfig]</code> <p>The configuration from the <code>peft &lt;https://huggingface.co/docs/peft/index&gt;</code>_ library to use to wrap the model for parameter-efficient finetuning.</p> <code>None</code> <code>model_config_kwargs</code> <code>Optional[dict[str, Any]]</code> <p>Additional keyword arguments to pass to the model configuration.</p> <code>None</code> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If both <code>peft_config</code> and <code>freeze_layers</code> are set. The <code>peft_config</code> will override the <code>freeze_layers</code> setting.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>@store(\n    group=\"modules/encoders\",\n    provider=\"mmlearn\",\n    model_name_or_path=\"openai/clip-vit-base-patch16\",\n    hydra_convert=\"object\",  # required for `peft_config` to be converted to a `PeftConfig` object\n)\nclass HFCLIPTextEncoder(nn.Module):\n    \"\"\"Wrapper around the ``CLIPTextModel`` from HuggingFace.\n\n    Parameters\n    ----------\n    model_name_or_path : str\n        The huggingface model name or a local path from which to load the model.\n    pretrained : bool, default=True\n        Whether to load the pretrained weights or not.\n    pooling_layer : Optional[torch.nn.Module], optional, default=None\n        Pooling layer to apply to the last hidden state of the model.\n    freeze_layers : Union[int, float, list[int], bool], default=False\n        Whether to freeze layers of the model and which layers to freeze. If ``True``,\n        all model layers are frozen. If it is an integer, the first ``N`` layers of\n        the model are frozen. If it is a float, the first ``N`` percent of the layers\n        are frozen. If it is a list of integers, the layers at the indices in the\n        list are frozen.\n    freeze_layer_norm : bool, default=True\n        Whether to freeze the layer normalization layers of the model.\n    peft_config : Optional[PeftConfig], optional, default=None\n        The configuration from the `peft &lt;https://huggingface.co/docs/peft/index&gt;`_\n        library to use to wrap the model for parameter-efficient finetuning.\n    model_config_kwargs : Optional[dict[str, Any]], optional, default=None\n        Additional keyword arguments to pass to the model configuration.\n\n    Warns\n    -----\n    UserWarning\n        If both ``peft_config`` and ``freeze_layers`` are set. The ``peft_config``\n        will override the ``freeze_layers`` setting.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name_or_path: str,\n        pretrained: bool = True,\n        pooling_layer: Optional[nn.Module] = None,\n        freeze_layers: Union[int, float, list[int], bool] = False,\n        freeze_layer_norm: bool = True,\n        peft_config: Optional[\"PeftConfig\"] = None,\n        model_config_kwargs: Optional[dict[str, Any]] = None,\n    ) -&gt; None:\n        super().__init__()\n        _warn_freeze_with_peft(peft_config, freeze_layers)\n\n        model = hf_utils.load_huggingface_model(\n            transformers.CLIPTextModel,\n            model_name_or_path=model_name_or_path,\n            load_pretrained_weights=pretrained,\n            model_config_kwargs=model_config_kwargs,\n        )\n        model = _freeze_text_model(model, freeze_layers, freeze_layer_norm)\n        if peft_config is not None:\n            model = hf_utils._wrap_peft_model(model, peft_config)\n\n        self.model = model\n        self.pooling_layer = pooling_layer\n\n    def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The ``input_ids`` will be expected under the\n            ``Modalities.TEXT`` key.\n\n        Returns\n        -------\n        BaseModelOutput\n            The output of the model, including the last hidden state, all hidden\n            states, and the attention weights, if ``output_attentions`` is set\n            to ``True``.\n        \"\"\"\n        outputs = self.model(\n            input_ids=inputs[Modalities.TEXT.name],\n            attention_mask=inputs.get(\"attention_mask\")\n            or inputs.get(Modalities.TEXT.attention_mask),\n            position_ids=inputs.get(\"position_ids\"),\n            output_attentions=inputs.get(\"output_attentions\"),\n            return_dict=True,\n        )\n        last_hidden_state = outputs.hidden_states[-1]  # NOTE: no layer norm applied\n        if self.pooling_layer:\n            last_hidden_state = self.pooling_layer(last_hidden_state)\n\n        return BaseModelOutput(\n            last_hidden_state=last_hidden_state,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n</code></pre> <code></code> forward \u00b6 <pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The <code>input_ids</code> will be expected under the <code>Modalities.TEXT</code> key.</p> required <p>Returns:</p> Type Description <code>BaseModelOutput</code> <p>The output of the model, including the last hidden state, all hidden states, and the attention weights, if <code>output_attentions</code> is set to <code>True</code>.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The ``input_ids`` will be expected under the\n        ``Modalities.TEXT`` key.\n\n    Returns\n    -------\n    BaseModelOutput\n        The output of the model, including the last hidden state, all hidden\n        states, and the attention weights, if ``output_attentions`` is set\n        to ``True``.\n    \"\"\"\n    outputs = self.model(\n        input_ids=inputs[Modalities.TEXT.name],\n        attention_mask=inputs.get(\"attention_mask\")\n        or inputs.get(Modalities.TEXT.attention_mask),\n        position_ids=inputs.get(\"position_ids\"),\n        output_attentions=inputs.get(\"output_attentions\"),\n        return_dict=True,\n    )\n    last_hidden_state = outputs.hidden_states[-1]  # NOTE: no layer norm applied\n    if self.pooling_layer:\n        last_hidden_state = self.pooling_layer(last_hidden_state)\n\n    return BaseModelOutput(\n        last_hidden_state=last_hidden_state,\n        hidden_states=outputs.hidden_states,\n        attentions=outputs.attentions,\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.clip.HFCLIPVisionEncoder","title":"HFCLIPVisionEncoder","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper around the <code>CLIPVisionModel</code> from HuggingFace.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>The huggingface model name or a local path from which to load the model.</p> required <code>pretrained</code> <code>bool</code> <p>Whether to load the pretrained weights or not.</p> <code>True</code> <code>pooling_layer</code> <code>Optional[Module]</code> <p>Pooling layer to apply to the last hidden state of the model.</p> <code>None</code> <code>freeze_layers</code> <code>Union[int, float, list[int], bool]</code> <p>Whether to freeze layers of the model and which layers to freeze. If <code>True</code>, all model layers are frozen. If it is an integer, the first <code>N</code> layers of the model are frozen. If it is a float, the first <code>N</code> percent of the layers are frozen. If it is a list of integers, the layers at the indices in the list are frozen.</p> <code>False</code> <code>freeze_layer_norm</code> <code>bool</code> <p>Whether to freeze the layer normalization layers of the model.</p> <code>True</code> <code>patch_dropout_rate</code> <code>float</code> <p>The proportion of patch embeddings to drop out.</p> <code>0.0</code> <code>patch_dropout_shuffle</code> <code>bool</code> <p>Whether to shuffle the patches while applying patch dropout.</p> <code>False</code> <code>patch_dropout_bias</code> <code>Optional[float]</code> <p>The bias to apply to the patch dropout mask.</p> <code>None</code> <code>peft_config</code> <code>Optional[PeftConfig]</code> <p>The configuration from the <code>peft &lt;https://huggingface.co/docs/peft/index&gt;</code>_ library to use to wrap the model for parameter-efficient finetuning.</p> <code>None</code> <code>model_config_kwargs</code> <code>Optional[dict[str, Any]]</code> <p>Additional keyword arguments to pass to the model configuration.</p> <code>None</code> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If both <code>peft_config</code> and <code>freeze_layers</code> are set. The <code>peft_config</code> will override the <code>freeze_layers</code> setting.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>@store(\n    group=\"modules/encoders\",\n    provider=\"mmlearn\",\n    model_name_or_path=\"openai/clip-vit-base-patch16\",\n    hydra_convert=\"object\",\n)\nclass HFCLIPVisionEncoder(nn.Module):\n    \"\"\"Wrapper around the ``CLIPVisionModel`` from HuggingFace.\n\n    Parameters\n    ----------\n    model_name_or_path : str\n        The huggingface model name or a local path from which to load the model.\n    pretrained : bool, default=True\n        Whether to load the pretrained weights or not.\n    pooling_layer : Optional[torch.nn.Module], optional, default=None\n        Pooling layer to apply to the last hidden state of the model.\n    freeze_layers : Union[int, float, list[int], bool], default=False\n        Whether to freeze layers of the model and which layers to freeze. If ``True``,\n        all model layers are frozen. If it is an integer, the first ``N`` layers of\n        the model are frozen. If it is a float, the first ``N`` percent of the layers\n        are frozen. If it is a list of integers, the layers at the indices in the\n        list are frozen.\n    freeze_layer_norm : bool, default=True\n        Whether to freeze the layer normalization layers of the model.\n    patch_dropout_rate : float, default=0.0\n        The proportion of patch embeddings to drop out.\n    patch_dropout_shuffle : bool, default=False\n        Whether to shuffle the patches while applying patch dropout.\n    patch_dropout_bias : Optional[float], optional, default=None\n        The bias to apply to the patch dropout mask.\n    peft_config : Optional[PeftConfig], optional, default=None\n        The configuration from the `peft &lt;https://huggingface.co/docs/peft/index&gt;`_\n        library to use to wrap the model for parameter-efficient finetuning.\n    model_config_kwargs : Optional[dict[str, Any]], optional, default=None\n        Additional keyword arguments to pass to the model configuration.\n\n    Warns\n    -----\n    UserWarning\n        If both ``peft_config`` and ``freeze_layers`` are set. The ``peft_config``\n        will override the ``freeze_layers`` setting.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name_or_path: str,\n        pretrained: bool = True,\n        pooling_layer: Optional[nn.Module] = None,\n        freeze_layers: Union[int, float, list[int], bool] = False,\n        freeze_layer_norm: bool = True,\n        patch_dropout_rate: float = 0.0,\n        patch_dropout_shuffle: bool = False,\n        patch_dropout_bias: Optional[float] = None,\n        peft_config: Optional[\"PeftConfig\"] = None,\n        model_config_kwargs: Optional[dict[str, Any]] = None,\n    ) -&gt; None:\n        super().__init__()\n        _warn_freeze_with_peft(peft_config, freeze_layers)\n\n        model = hf_utils.load_huggingface_model(\n            transformers.CLIPVisionModel,\n            model_name_or_path=model_name_or_path,\n            load_pretrained_weights=pretrained,\n            model_config_kwargs=model_config_kwargs,\n        )\n        model = _freeze_vision_model(model, freeze_layers, freeze_layer_norm)\n        if peft_config is not None:\n            model = hf_utils._wrap_peft_model(model, peft_config)\n\n        self.model = model.vision_model\n        self.pooling_layer = pooling_layer\n        self.patch_dropout = None\n        if patch_dropout_rate &gt; 0:\n            self.patch_dropout = PatchDropout(\n                keep_rate=1 - patch_dropout_rate,\n                token_shuffling=patch_dropout_shuffle,\n                bias=patch_dropout_bias,\n            )\n\n    def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The image tensor will be expected under the\n            ``Modalities.RGB`` key.\n\n        Returns\n        -------\n        BaseModelOutput\n            The output of the model, including the last hidden state, all hidden states,\n            and the attention weights, if ``output_attentions`` is set to ``True``.\n\n        \"\"\"\n        # FIXME: handle other vision modalities\n        pixel_values = inputs[Modalities.RGB.name]\n        hidden_states = self.model.embeddings(pixel_values)\n        if self.patch_dropout is not None:\n            hidden_states = self.patch_dropout(hidden_states)\n        hidden_states = self.model.pre_layrnorm(hidden_states)\n\n        encoder_outputs = self.model.encoder(\n            inputs_embeds=hidden_states,\n            output_attentions=inputs.get(\n                \"output_attentions\", self.model.config.output_attentions\n            ),\n            output_hidden_states=True,\n            return_dict=True,\n        )\n\n        last_hidden_state = encoder_outputs[0]\n        if self.pooling_layer is not None:\n            last_hidden_state = self.pooling_layer(last_hidden_state)\n\n        return BaseModelOutput(\n            last_hidden_state=last_hidden_state,\n            hidden_states=encoder_outputs.hidden_states,\n            attentions=encoder_outputs.attentions,\n        )\n</code></pre> <code></code> forward \u00b6 <pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The image tensor will be expected under the <code>Modalities.RGB</code> key.</p> required <p>Returns:</p> Type Description <code>BaseModelOutput</code> <p>The output of the model, including the last hidden state, all hidden states, and the attention weights, if <code>output_attentions</code> is set to <code>True</code>.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The image tensor will be expected under the\n        ``Modalities.RGB`` key.\n\n    Returns\n    -------\n    BaseModelOutput\n        The output of the model, including the last hidden state, all hidden states,\n        and the attention weights, if ``output_attentions`` is set to ``True``.\n\n    \"\"\"\n    # FIXME: handle other vision modalities\n    pixel_values = inputs[Modalities.RGB.name]\n    hidden_states = self.model.embeddings(pixel_values)\n    if self.patch_dropout is not None:\n        hidden_states = self.patch_dropout(hidden_states)\n    hidden_states = self.model.pre_layrnorm(hidden_states)\n\n    encoder_outputs = self.model.encoder(\n        inputs_embeds=hidden_states,\n        output_attentions=inputs.get(\n            \"output_attentions\", self.model.config.output_attentions\n        ),\n        output_hidden_states=True,\n        return_dict=True,\n    )\n\n    last_hidden_state = encoder_outputs[0]\n    if self.pooling_layer is not None:\n        last_hidden_state = self.pooling_layer(last_hidden_state)\n\n    return BaseModelOutput(\n        last_hidden_state=last_hidden_state,\n        hidden_states=encoder_outputs.hidden_states,\n        attentions=encoder_outputs.attentions,\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.clip.HFCLIPTextEncoderWithProjection","title":"HFCLIPTextEncoderWithProjection","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper around the <code>CLIPTextModelWithProjection</code> from HuggingFace.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>The huggingface model name or a local path from which to load the model.</p> required <code>pretrained</code> <code>bool</code> <p>Whether to load the pretrained weights or not.</p> <code>True</code> <code>use_all_token_embeddings</code> <code>bool</code> <p>Whether to use all token embeddings for the text. If <code>False</code> the first token embedding will be used.</p> <code>False</code> <code>freeze_layers</code> <code>Union[int, float, list[int], bool]</code> <p>Whether to freeze layers of the model and which layers to freeze. If <code>True</code>, all model layers are frozen. If it is an integer, the first <code>N</code> layers of the model are frozen. If it is a float, the first <code>N</code> percent of the layers are frozen. If it is a list of integers, the layers at the indices in the list are frozen.</p> <code>False</code> <code>freeze_layer_norm</code> <code>bool</code> <p>Whether to freeze the layer normalization layers of the model.</p> <code>True</code> <code>peft_config</code> <code>Optional[PeftConfig]</code> <p>The configuration from the <code>peft &lt;https://huggingface.co/docs/peft/index&gt;</code>_ library to use to wrap the model for parameter-efficient finetuning.</p> <code>None</code> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If both <code>peft_config</code> and <code>freeze_layers</code> are set. The <code>peft_config</code> will override the <code>freeze_layers</code> setting.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>@store(\n    group=\"modules/encoders\",\n    provider=\"mmlearn\",\n    model_name_or_path=\"openai/clip-vit-base-patch16\",\n    hydra_convert=\"object\",\n)\nclass HFCLIPTextEncoderWithProjection(nn.Module):\n    \"\"\"Wrapper around the ``CLIPTextModelWithProjection`` from HuggingFace.\n\n    Parameters\n    ----------\n    model_name_or_path : str\n        The huggingface model name or a local path from which to load the model.\n    pretrained : bool, default=True\n        Whether to load the pretrained weights or not.\n    use_all_token_embeddings : bool, default=False\n        Whether to use all token embeddings for the text. If ``False`` the first token\n        embedding will be used.\n    freeze_layers : Union[int, float, list[int], bool], default=False\n        Whether to freeze layers of the model and which layers to freeze. If ``True``,\n        all model layers are frozen. If it is an integer, the first ``N`` layers of\n        the model are frozen. If it is a float, the first ``N`` percent of the layers\n        are frozen. If it is a list of integers, the layers at the indices in the\n        list are frozen.\n    freeze_layer_norm : bool, default=True\n        Whether to freeze the layer normalization layers of the model.\n    peft_config : Optional[PeftConfig], optional, default=None\n        The configuration from the `peft &lt;https://huggingface.co/docs/peft/index&gt;`_\n        library to use to wrap the model for parameter-efficient finetuning.\n\n    Warns\n    -----\n    UserWarning\n        If both ``peft_config`` and ``freeze_layers`` are set. The ``peft_config``\n        will override the ``freeze_layers`` setting.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name_or_path: str,\n        pretrained: bool = True,\n        use_all_token_embeddings: bool = False,\n        freeze_layers: Union[int, float, list[int], bool] = False,\n        freeze_layer_norm: bool = True,\n        peft_config: Optional[\"PeftConfig\"] = None,\n        model_config_kwargs: Optional[dict[str, Any]] = None,\n    ) -&gt; None:\n        super().__init__()\n        _warn_freeze_with_peft(peft_config, freeze_layers)\n\n        self.use_all_token_embeddings = use_all_token_embeddings\n\n        model = hf_utils.load_huggingface_model(\n            transformers.CLIPTextModelWithProjection,\n            model_name_or_path,\n            load_pretrained_weights=pretrained,\n            model_config_kwargs=model_config_kwargs,\n            config_type=CLIPTextConfig,\n        )\n\n        model = _freeze_text_model(model, freeze_layers, freeze_layer_norm)\n        if peft_config is not None:\n            model = hf_utils._wrap_peft_model(model, peft_config)\n\n        self.model = model\n\n    def forward(self, inputs: dict[str, Any]) -&gt; tuple[torch.Tensor]:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The ``input_ids`` will be expected under the\n            ``Modalities.TEXT`` key.\n\n        Returns\n        -------\n        tuple[torch.Tensor]\n            The text embeddings. Will be a tuple with a single element.\n        \"\"\"\n        input_ids = inputs[Modalities.TEXT.name]\n        attention_mask: Optional[torch.Tensor] = inputs.get(\n            \"attention_mask\", inputs.get(Modalities.TEXT.attention_mask, None)\n        )\n        position_ids = inputs.get(\"position_ids\")\n\n        if self.use_all_token_embeddings:\n            text_outputs = self.model.text_model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                return_dict=True,\n            )\n            # TODO: add more options for pooling before projection\n            text_embeds = self.model.text_projection(text_outputs.last_hidden_state)\n        else:\n            text_embeds = self.model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                return_dict=True,\n            ).text_embeds\n\n        return (text_embeds,)\n</code></pre> <code></code> forward \u00b6 <pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The <code>input_ids</code> will be expected under the <code>Modalities.TEXT</code> key.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor]</code> <p>The text embeddings. Will be a tuple with a single element.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; tuple[torch.Tensor]:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The ``input_ids`` will be expected under the\n        ``Modalities.TEXT`` key.\n\n    Returns\n    -------\n    tuple[torch.Tensor]\n        The text embeddings. Will be a tuple with a single element.\n    \"\"\"\n    input_ids = inputs[Modalities.TEXT.name]\n    attention_mask: Optional[torch.Tensor] = inputs.get(\n        \"attention_mask\", inputs.get(Modalities.TEXT.attention_mask, None)\n    )\n    position_ids = inputs.get(\"position_ids\")\n\n    if self.use_all_token_embeddings:\n        text_outputs = self.model.text_model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            return_dict=True,\n        )\n        # TODO: add more options for pooling before projection\n        text_embeds = self.model.text_projection(text_outputs.last_hidden_state)\n    else:\n        text_embeds = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            return_dict=True,\n        ).text_embeds\n\n    return (text_embeds,)\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.clip.HFCLIPVisionEncoderWithProjection","title":"HFCLIPVisionEncoderWithProjection","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper around the <code>CLIPVisionModelWithProjection</code> class from HuggingFace.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>The huggingface model name or a local path from which to load the model.</p> required <code>pretrained</code> <code>bool</code> <p>Whether to load the pretrained weights or not.</p> <code>True</code> <code>use_all_token_embeddings</code> <code>bool</code> <p>Whether to use all token embeddings for the text. If <code>False</code> the first token embedding will be used.</p> <code>False</code> <code>freeze_layers</code> <code>Union[int, float, list[int], bool]</code> <p>Whether to freeze layers of the model and which layers to freeze. If <code>True</code>, all model layers are frozen. If it is an integer, the first <code>N` layers of the model are frozen. If it is a float, the first</code>N`` percent of the layers are frozen. If it is a list of integers, the layers at the indices in the list are frozen.</p> <code>False</code> <code>freeze_layer_norm</code> <code>bool</code> <p>Whether to freeze the layer normalization layers of the model.</p> <code>True</code> <code>patch_dropout_rate</code> <code>float</code> <p>The proportion of patch embeddings to drop out.</p> <code>0.0</code> <code>patch_dropout_shuffle</code> <code>bool</code> <p>Whether to shuffle the patches while applying patch dropout.</p> <code>False</code> <code>patch_dropout_bias</code> <code>float</code> <p>The bias to apply to the patch dropout mask.</p> <code>None</code> <code>peft_config</code> <code>Optional[PeftConfig]</code> <p>The configuration from the <code>peft &lt;https://huggingface.co/docs/peft/index&gt;</code>_ library to use to wrap the model for parameter-efficient finetuning.</p> <code>None</code> <code>model_config_kwargs</code> <code>dict[str, Any]</code> <p>Additional keyword arguments to pass to the model configuration.</p> <code>None</code> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If both <code>peft_config</code> and <code>freeze_layers</code> are set. The <code>peft_config</code> will override the <code>freeze_layers</code> setting.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>@store(\n    group=\"modules/encoders\",\n    provider=\"mmlearn\",\n    model_name_or_path=\"openai/clip-vit-base-patch16\",\n    hydra_convert=\"object\",\n)\nclass HFCLIPVisionEncoderWithProjection(nn.Module):\n    \"\"\"Wrapper around the ``CLIPVisionModelWithProjection`` class from HuggingFace.\n\n    Parameters\n    ----------\n    model_name_or_path : str\n        The huggingface model name or a local path from which to load the model.\n    pretrained : bool, default=True\n        Whether to load the pretrained weights or not.\n    use_all_token_embeddings : bool, default=False\n        Whether to use all token embeddings for the text. If ``False`` the first token\n        embedding will be used.\n    freeze_layers : Union[int, float, list[int], bool], default=False\n        Whether to freeze layers of the model and which layers to freeze. If ``True``,\n        all model layers are frozen. If it is an integer, the first ``N` layers of\n        the model are frozen. If it is a float, the first ``N`` percent of the layers\n        are frozen. If it is a list of integers, the layers at the indices in the\n        list are frozen.\n    freeze_layer_norm : bool, default=True\n        Whether to freeze the layer normalization layers of the model.\n    patch_dropout_rate : float, default=0.0\n        The proportion of patch embeddings to drop out.\n    patch_dropout_shuffle : bool, default=False\n        Whether to shuffle the patches while applying patch dropout.\n    patch_dropout_bias : float, optional, default=None\n        The bias to apply to the patch dropout mask.\n    peft_config : Optional[PeftConfig], optional, default=None\n        The configuration from the `peft &lt;https://huggingface.co/docs/peft/index&gt;`_\n        library to use to wrap the model for parameter-efficient finetuning.\n    model_config_kwargs : dict[str, Any], optional, default=None\n        Additional keyword arguments to pass to the model configuration.\n\n    Warns\n    -----\n    UserWarning\n        If both ``peft_config`` and ``freeze_layers`` are set. The ``peft_config``\n        will override the ``freeze_layers`` setting.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name_or_path: str,\n        pretrained: bool = True,\n        use_all_token_embeddings: bool = False,\n        patch_dropout_rate: float = 0.0,\n        patch_dropout_shuffle: bool = False,\n        patch_dropout_bias: Optional[float] = None,\n        freeze_layers: Union[int, float, list[int], bool] = False,\n        freeze_layer_norm: bool = True,\n        peft_config: Optional[\"PeftConfig\"] = None,\n        model_config_kwargs: Optional[dict[str, Any]] = None,\n    ) -&gt; None:\n        super().__init__()\n        _warn_freeze_with_peft(peft_config, freeze_layers)\n\n        self.use_all_token_embeddings = use_all_token_embeddings\n\n        model = hf_utils.load_huggingface_model(\n            transformers.CLIPVisionModelWithProjection,\n            model_name_or_path,\n            load_pretrained_weights=pretrained,\n            model_config_kwargs=model_config_kwargs,\n            config_type=CLIPVisionConfig,\n        )\n\n        model = _freeze_vision_model(model, freeze_layers, freeze_layer_norm)\n        if peft_config is not None:\n            model = hf_utils._wrap_peft_model(model, peft_config)\n\n        self.model = model\n        self.patch_dropout = None\n        if patch_dropout_rate &gt; 0:\n            self.patch_dropout = PatchDropout(\n                keep_rate=1 - patch_dropout_rate,\n                token_shuffling=patch_dropout_shuffle,\n                bias=patch_dropout_bias,\n            )\n\n    def forward(self, inputs: dict[str, Any]) -&gt; tuple[torch.Tensor]:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The image tensor will be expected under the\n            ``Modalities.RGB`` key.\n\n        Returns\n        -------\n        tuple[torch.Tensor]\n            The image embeddings. Will be a tuple with a single element.\n        \"\"\"\n        pixel_values = inputs[Modalities.RGB.name]\n        hidden_states = self.model.vision_model.embeddings(pixel_values)\n        if self.patch_dropout is not None:\n            hidden_states = self.patch_dropout(hidden_states)\n        hidden_states = self.model.vision_model.pre_layrnorm(hidden_states)\n\n        encoder_outputs = self.model.vision_model.encoder(\n            inputs_embeds=hidden_states, return_dict=True\n        )\n\n        last_hidden_state = encoder_outputs.last_hidden_state\n        if self.use_all_token_embeddings:\n            pooled_output = last_hidden_state\n        else:\n            pooled_output = last_hidden_state[:, 0, :]\n        pooled_output = self.model.vision_model.post_layernorm(pooled_output)\n\n        return (self.model.visual_projection(pooled_output),)\n</code></pre> <code></code> forward \u00b6 <pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The image tensor will be expected under the <code>Modalities.RGB</code> key.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor]</code> <p>The image embeddings. Will be a tuple with a single element.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; tuple[torch.Tensor]:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The image tensor will be expected under the\n        ``Modalities.RGB`` key.\n\n    Returns\n    -------\n    tuple[torch.Tensor]\n        The image embeddings. Will be a tuple with a single element.\n    \"\"\"\n    pixel_values = inputs[Modalities.RGB.name]\n    hidden_states = self.model.vision_model.embeddings(pixel_values)\n    if self.patch_dropout is not None:\n        hidden_states = self.patch_dropout(hidden_states)\n    hidden_states = self.model.vision_model.pre_layrnorm(hidden_states)\n\n    encoder_outputs = self.model.vision_model.encoder(\n        inputs_embeds=hidden_states, return_dict=True\n    )\n\n    last_hidden_state = encoder_outputs.last_hidden_state\n    if self.use_all_token_embeddings:\n        pooled_output = last_hidden_state\n    else:\n        pooled_output = last_hidden_state[:, 0, :]\n    pooled_output = self.model.vision_model.post_layernorm(pooled_output)\n\n    return (self.model.visual_projection(pooled_output),)\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.text","title":"text","text":"<p>Huggingface text encoder model.</p>"},{"location":"api/#mmlearn.modules.encoders.text.HFTextEncoder","title":"HFTextEncoder","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper around huggingface models in the <code>AutoModelForTextEncoding</code> class.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>The huggingface model name or a local path from which to load the model.</p> required <code>pretrained</code> <code>bool</code> <p>Whether to load the pretrained weights or not.</p> <code>True</code> <code>pooling_layer</code> <code>Optional[Module]</code> <p>Pooling layer to apply to the last hidden state of the model.</p> <code>None</code> <code>freeze_layers</code> <code>Union[int, float, list[int], bool]</code> <p>Whether to freeze layers of the model and which layers to freeze. If <code>True</code>, all model layers are frozen. If it is an integer, the first <code>N</code> layers of the model are frozen. If it is a float, the first <code>N</code> percent of the layers are frozen. If it is a list of integers, the layers at the indices in the list are frozen.</p> <code>False</code> <code>freeze_layer_norm</code> <code>bool</code> <p>Whether to freeze the layer normalization layers of the model.</p> <code>True</code> <code>peft_config</code> <code>Optional[PeftConfig]</code> <p>The configuration from the <code>peft &lt;https://huggingface.co/docs/peft/index&gt;</code>_ library to use to wrap the model for parameter-efficient finetuning.</p> <code>None</code> <code>model_config_kwargs</code> <code>Optional[dict[str, Any]]</code> <p>Additional keyword arguments to pass to the model configuration.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model is a decoder model or if freezing individual layers is not supported for the model type.</p> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If both <code>peft_config</code> and <code>freeze_layers</code> are set. The <code>peft_config</code> will override the <code>freeze_layers</code> setting.</p> Source code in <code>mmlearn/modules/encoders/text.py</code> <pre><code>@store(group=\"modules/encoders\", provider=\"mmlearn\", hydra_convert=\"object\")\nclass HFTextEncoder(nn.Module):\n    \"\"\"Wrapper around huggingface models in the ``AutoModelForTextEncoding`` class.\n\n    Parameters\n    ----------\n    model_name_or_path : str\n        The huggingface model name or a local path from which to load the model.\n    pretrained : bool, default=True\n        Whether to load the pretrained weights or not.\n    pooling_layer : Optional[torch.nn.Module], optional, default=None\n        Pooling layer to apply to the last hidden state of the model.\n    freeze_layers : Union[int, float, list[int], bool], default=False\n        Whether to freeze layers of the model and which layers to freeze. If ``True``,\n        all model layers are frozen. If it is an integer, the first ``N`` layers of\n        the model are frozen. If it is a float, the first ``N`` percent of the layers\n        are frozen. If it is a list of integers, the layers at the indices in the\n        list are frozen.\n    freeze_layer_norm : bool, default=True\n        Whether to freeze the layer normalization layers of the model.\n    peft_config : Optional[PeftConfig], optional, default=None\n        The configuration from the `peft &lt;https://huggingface.co/docs/peft/index&gt;`_\n        library to use to wrap the model for parameter-efficient finetuning.\n    model_config_kwargs : Optional[dict[str, Any]], optional, default=None\n        Additional keyword arguments to pass to the model configuration.\n\n    Raises\n    ------\n    ValueError\n        If the model is a decoder model or if freezing individual layers is not\n        supported for the model type.\n\n    Warns\n    -----\n    UserWarning\n        If both ``peft_config`` and ``freeze_layers`` are set. The ``peft_config``\n        will override the ``freeze_layers`` setting.\n\n\n    \"\"\"\n\n    def __init__(  # noqa: PLR0912\n        self,\n        model_name_or_path: str,\n        pretrained: bool = True,\n        pooling_layer: Optional[nn.Module] = None,\n        freeze_layers: Union[int, float, list[int], bool] = False,\n        freeze_layer_norm: bool = True,\n        peft_config: Optional[\"PeftConfig\"] = None,\n        model_config_kwargs: Optional[dict[str, Any]] = None,\n    ):\n        super().__init__()\n        if model_config_kwargs is None:\n            model_config_kwargs = {}\n        model_config_kwargs[\"output_hidden_states\"] = True\n        model_config_kwargs[\"add_pooling_layer\"] = False\n        model = hf_utils.load_huggingface_model(\n            AutoModelForTextEncoding,\n            model_name_or_path,\n            load_pretrained_weights=pretrained,\n            model_config_kwargs=model_config_kwargs,\n        )\n        if hasattr(model.config, \"is_decoder\") and model.config.is_decoder:\n            raise ValueError(\"Model is a decoder. Only encoder models are supported.\")\n\n        if not pretrained and freeze_layers:\n            rank_zero_warn(\n                \"Freezing layers when loading a model with random weights may lead to \"\n                \"unexpected behavior. Consider setting `freeze_layers=False` if \"\n                \"`pretrained=False`.\",\n            )\n\n        if isinstance(freeze_layers, bool) and freeze_layers:\n            for name, param in model.named_parameters():\n                param.requires_grad = (\n                    (not freeze_layer_norm) if \"LayerNorm\" in name else False\n                )\n\n        if isinstance(\n            freeze_layers, (float, int, list)\n        ) and model.config.model_type in [\"flaubert\", \"xlm\"]:\n            # flaubert and xlm models have a different architecture that does not\n            # support freezing individual layers in the same way as other models\n            raise ValueError(\n                f\"Freezing individual layers is not supported for {model.config.model_type} \"\n                \"models. Please use `freeze_layers=False` or `freeze_layers=True`.\"\n            )\n\n        # get list of layers\n        embeddings = model.embeddings\n        encoder = getattr(model, \"encoder\", None) or getattr(\n            model, \"transformer\", model\n        )\n        encoder_layers = (\n            getattr(encoder, \"layer\", None)\n            or getattr(encoder, \"layers\", None)\n            or getattr(encoder, \"block\", None)\n        )\n        if encoder_layers is None and hasattr(encoder, \"albert_layer_groups\"):\n            encoder_layers = [\n                layer\n                for group in encoder.albert_layer_groups\n                for layer in group.albert_layers\n            ]\n        modules = [embeddings]\n        if encoder_layers is not None and isinstance(encoder_layers, list):\n            modules.extend(encoder_layers)\n\n        if isinstance(freeze_layers, float):\n            freeze_layers = int(freeze_layers * len(modules))\n        if isinstance(freeze_layers, int):\n            freeze_layers = list(range(freeze_layers))\n\n        if isinstance(freeze_layers, list):\n            for idx, module in enumerate(modules):\n                if idx in freeze_layers:\n                    for name, param in module.named_parameters():\n                        param.requires_grad = (\n                            (not freeze_layer_norm) if \"LayerNorm\" in name else False\n                        )\n\n        if peft_config is not None:\n            model = hf_utils._wrap_peft_model(model, peft_config)\n\n        self.model = model\n        self.pooling_layer = pooling_layer\n\n    def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The ``input_ids`` will be expected under the\n            ``Modalities.TEXT`` key.\n\n        Returns\n        -------\n        BaseModelOutput\n            The output of the model, including the last hidden state, all hidden states,\n            and the attention weights, if ``output_attentions`` is set to ``True``.\n        \"\"\"\n        outputs = self.model(\n            input_ids=inputs[Modalities.TEXT.name],\n            attention_mask=inputs.get(\n                \"attention_mask\", inputs.get(Modalities.TEXT.attention_mask, None)\n            ),\n            position_ids=inputs.get(\"position_ids\"),\n            output_attentions=inputs.get(\"output_attentions\"),\n            return_dict=True,\n        )\n        last_hidden_state = outputs.hidden_states[-1]  # NOTE: no layer norm applied\n        if self.pooling_layer:\n            last_hidden_state = self.pooling_layer(last_hidden_state)\n\n        return BaseModelOutput(\n            last_hidden_state=last_hidden_state,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n</code></pre> <code></code> forward \u00b6 <pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The <code>input_ids</code> will be expected under the <code>Modalities.TEXT</code> key.</p> required <p>Returns:</p> Type Description <code>BaseModelOutput</code> <p>The output of the model, including the last hidden state, all hidden states, and the attention weights, if <code>output_attentions</code> is set to <code>True</code>.</p> Source code in <code>mmlearn/modules/encoders/text.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The ``input_ids`` will be expected under the\n        ``Modalities.TEXT`` key.\n\n    Returns\n    -------\n    BaseModelOutput\n        The output of the model, including the last hidden state, all hidden states,\n        and the attention weights, if ``output_attentions`` is set to ``True``.\n    \"\"\"\n    outputs = self.model(\n        input_ids=inputs[Modalities.TEXT.name],\n        attention_mask=inputs.get(\n            \"attention_mask\", inputs.get(Modalities.TEXT.attention_mask, None)\n        ),\n        position_ids=inputs.get(\"position_ids\"),\n        output_attentions=inputs.get(\"output_attentions\"),\n        return_dict=True,\n    )\n    last_hidden_state = outputs.hidden_states[-1]  # NOTE: no layer norm applied\n    if self.pooling_layer:\n        last_hidden_state = self.pooling_layer(last_hidden_state)\n\n    return BaseModelOutput(\n        last_hidden_state=last_hidden_state,\n        hidden_states=outputs.hidden_states,\n        attentions=outputs.attentions,\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.vision","title":"vision","text":"<p>Vision encoder implementations.</p>"},{"location":"api/#mmlearn.modules.encoders.vision.TimmViT","title":"TimmViT","text":"<p>               Bases: <code>Module</code></p> <p>Vision Transformer model from timm.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the model to use.</p> required <code>modality</code> <code>str</code> <p>The modality of the input data. This allows this model to be used with different image modalities e.g. RGB, Depth, etc.</p> <code>\"RGB\"</code> <code>projection_dim</code> <code>int</code> <p>The dimension of the projection head.</p> <code>768</code> <code>pretrained</code> <code>bool</code> <p>Whether to use the pretrained weights.</p> <code>True</code> <code>freeze_layers</code> <code>Union[int, float, list[int], bool]</code> <p>Whether to freeze the layers.</p> <code>False</code> <code>freeze_layer_norm</code> <code>bool</code> <p>Whether to freeze the layer norm.</p> <code>True</code> <code>peft_config</code> <code>Optional[PeftConfig]</code> <p>The configuration from the <code>peft &lt;https://huggingface.co/docs/peft/index&gt;</code>_ library to use to wrap the model for parameter-efficient finetuning.</p> <code>None</code> <code>model_kwargs</code> <code>Optional[dict[str, Any]]</code> <p>Additional keyword arguments for the model.</p> <code>None</code> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>@store(\n    group=\"modules/encoders\",\n    provider=\"mmlearn\",\n    model_name=\"vit_base_patch16_224\",\n    hydra_convert=\"object\",\n)\nclass TimmViT(nn.Module):\n    \"\"\"Vision Transformer model from timm.\n\n    Parameters\n    ----------\n    model_name : str\n        The name of the model to use.\n    modality : str, default=\"RGB\"\n        The modality of the input data. This allows this model to be used with different\n        image modalities e.g. RGB, Depth, etc.\n    projection_dim : int, default=768\n        The dimension of the projection head.\n    pretrained : bool, default=True\n        Whether to use the pretrained weights.\n    freeze_layers : Union[int, float, list[int], bool], default=False\n        Whether to freeze the layers.\n    freeze_layer_norm : bool, default=True\n        Whether to freeze the layer norm.\n    peft_config : Optional[PeftConfig], optional, default=None\n        The configuration from the `peft &lt;https://huggingface.co/docs/peft/index&gt;`_\n        library to use to wrap the model for parameter-efficient finetuning.\n    model_kwargs : Optional[dict[str, Any]], default=None\n        Additional keyword arguments for the model.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str,\n        modality: str = \"RGB\",\n        projection_dim: int = 768,\n        pretrained: bool = True,\n        freeze_layers: Union[int, float, list[int], bool] = False,\n        freeze_layer_norm: bool = True,\n        peft_config: Optional[\"PeftConfig\"] = None,\n        model_kwargs: Optional[dict[str, Any]] = None,\n    ) -&gt; None:\n        super().__init__()\n        self.modality = Modalities.get_modality(modality)\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        self.model: TimmVisionTransformer = timm.create_model(\n            model_name,\n            pretrained=pretrained,\n            num_classes=projection_dim,\n            **model_kwargs,\n        )\n        assert isinstance(self.model, TimmVisionTransformer), (\n            f\"Model {model_name} is not a Vision Transformer. \"\n            \"Please provide a model name that corresponds to a Vision Transformer.\"\n        )\n\n        self._freeze_layers(freeze_layers, freeze_layer_norm)\n\n        if peft_config is not None:\n            self.model = hf_utils._wrap_peft_model(self.model, peft_config)\n\n    def _freeze_layers(\n        self, freeze_layers: Union[int, float, list[int], bool], freeze_layer_norm: bool\n    ) -&gt; None:\n        \"\"\"Freeze the layers of the model.\n\n        Parameters\n        ----------\n        freeze_layers : Union[int, float, list[int], bool]\n            Whether to freeze the layers.\n        freeze_layer_norm : bool\n            Whether to freeze the layer norm.\n        \"\"\"\n        if isinstance(freeze_layers, bool) and freeze_layers:\n            for name, param in self.model.named_parameters():\n                param.requires_grad = (\n                    (not freeze_layer_norm) if \"norm\" in name else False\n                )\n\n        modules = [self.model.patch_embed, *self.model.blocks, self.model.norm]\n        if isinstance(freeze_layers, float):\n            freeze_layers = int(freeze_layers * len(modules))\n        if isinstance(freeze_layers, int):\n            freeze_layers = list(range(freeze_layers))\n\n        if isinstance(freeze_layers, list):\n            for idx, module in enumerate(modules):\n                if idx in freeze_layers:\n                    for name, param in module.named_parameters():\n                        param.requires_grad = (\n                            (not freeze_layer_norm) if \"norm\" in name else False\n                        )\n\n    def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The ``image`` will be expected under the\n            ``Modalities.RGB`` key.\n\n        Returns\n        -------\n        BaseModelOutput\n            The output of the model.\n        \"\"\"\n        x = inputs[self.modality.name]\n        last_hidden_state, hidden_states = self.model.forward_intermediates(\n            x, output_fmt=\"NLC\"\n        )\n        last_hidden_state = self.model.forward_head(last_hidden_state)\n\n        return BaseModelOutput(\n            last_hidden_state=last_hidden_state, hidden_states=hidden_states\n        )\n\n    def get_intermediate_layers(\n        self, inputs: dict[str, Any], n: int = 1\n    ) -&gt; list[torch.Tensor]:\n        \"\"\"Get the output of the intermediate layers.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The ``image`` will be expected under the ``Modalities.RGB``\n            key.\n        n : int, default=1\n            The number of intermediate layers to return.\n\n        Returns\n        -------\n        list[torch.Tensor]\n            The outputs of the last n intermediate layers.\n        \"\"\"\n        return self.model.get_intermediate_layers(inputs[Modalities.RGB], n)  # type: ignore\n\n    def get_patch_info(self) -&gt; tuple[int, int]:\n        \"\"\"Get patch size and number of patches.\n\n        Returns\n        -------\n        tuple[int, int]\n            Patch size and number of patches.\n        \"\"\"\n        patch_size = self.model.patch_embed.patch_size[0]\n        num_patches = self.model.patch_embed.num_patches\n        return patch_size, num_patches\n</code></pre> <code></code> forward \u00b6 <pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The <code>image</code> will be expected under the <code>Modalities.RGB</code> key.</p> required <p>Returns:</p> Type Description <code>BaseModelOutput</code> <p>The output of the model.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The ``image`` will be expected under the\n        ``Modalities.RGB`` key.\n\n    Returns\n    -------\n    BaseModelOutput\n        The output of the model.\n    \"\"\"\n    x = inputs[self.modality.name]\n    last_hidden_state, hidden_states = self.model.forward_intermediates(\n        x, output_fmt=\"NLC\"\n    )\n    last_hidden_state = self.model.forward_head(last_hidden_state)\n\n    return BaseModelOutput(\n        last_hidden_state=last_hidden_state, hidden_states=hidden_states\n    )\n</code></pre> <code></code> get_intermediate_layers \u00b6 <pre><code>get_intermediate_layers(inputs, n=1)\n</code></pre> <p>Get the output of the intermediate layers.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The <code>image</code> will be expected under the <code>Modalities.RGB</code> key.</p> required <code>n</code> <code>int</code> <p>The number of intermediate layers to return.</p> <code>1</code> <p>Returns:</p> Type Description <code>list[Tensor]</code> <p>The outputs of the last n intermediate layers.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>def get_intermediate_layers(\n    self, inputs: dict[str, Any], n: int = 1\n) -&gt; list[torch.Tensor]:\n    \"\"\"Get the output of the intermediate layers.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The ``image`` will be expected under the ``Modalities.RGB``\n        key.\n    n : int, default=1\n        The number of intermediate layers to return.\n\n    Returns\n    -------\n    list[torch.Tensor]\n        The outputs of the last n intermediate layers.\n    \"\"\"\n    return self.model.get_intermediate_layers(inputs[Modalities.RGB], n)  # type: ignore\n</code></pre> <code></code> get_patch_info \u00b6 <pre><code>get_patch_info()\n</code></pre> <p>Get patch size and number of patches.</p> <p>Returns:</p> Type Description <code>tuple[int, int]</code> <p>Patch size and number of patches.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>def get_patch_info(self) -&gt; tuple[int, int]:\n    \"\"\"Get patch size and number of patches.\n\n    Returns\n    -------\n    tuple[int, int]\n        Patch size and number of patches.\n    \"\"\"\n    patch_size = self.model.patch_embed.patch_size[0]\n    num_patches = self.model.patch_embed.num_patches\n    return patch_size, num_patches\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.vision.VisionTransformer","title":"VisionTransformer","text":"<p>               Bases: <code>Module</code></p> <p>Vision Transformer.</p> <p>This module implements a Vision Transformer that processes images using a series of transformer blocks and patch embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>modality</code> <code>str</code> <p>The modality of the input data. This allows this model to be used with different image modalities e.g. RGB, Depth, etc.</p> <code>\"RGB\"</code> <code>img_size</code> <code>List[int]</code> <p>List of input image sizes.</p> <code>None</code> <code>patch_size</code> <code>int</code> <p>Size of each patch.</p> <code>16</code> <code>in_chans</code> <code>int</code> <p>Number of input channels.</p> <code>3</code> <code>embed_dim</code> <code>int</code> <p>Embedding dimension.</p> <code>768</code> <code>depth</code> <code>int</code> <p>Number of transformer blocks.</p> <code>12</code> <code>num_heads</code> <code>int</code> <p>Number of attention heads.</p> <code>12</code> <code>mlp_ratio</code> <code>float</code> <p>Ratio of hidden dimension in the MLP.</p> <code>4.0</code> <code>qkv_bias</code> <code>bool</code> <p>If True, add a learnable bias to the query, key, and value projections.</p> <code>True</code> <code>qk_scale</code> <code>Optional[float]</code> <p>Override the default qk scale factor.</p> <code>None</code> <code>drop_rate</code> <code>float</code> <p>Dropout rate for the transformer blocks.</p> <code>0.0</code> <code>attn_drop_rate</code> <code>float</code> <p>Dropout rate for the attention mechanism.</p> <code>0.0</code> <code>drop_path_rate</code> <code>float</code> <p>Dropout rate for stochastic depth.</p> <code>0.0</code> <code>norm_layer</code> <code>Callable[..., Module]</code> <p>Normalization layer to use.</p> <code>torch.nn.LayerNorm</code> <code>init_std</code> <code>float</code> <p>Standard deviation for weight initialization.</p> <code>0.02</code> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>class VisionTransformer(nn.Module):\n    \"\"\"Vision Transformer.\n\n    This module implements a Vision Transformer that processes images using a\n    series of transformer blocks and patch embeddings.\n\n    Parameters\n    ----------\n    modality : str, optional, default=\"RGB\"\n        The modality of the input data. This allows this model to be used with different\n        image modalities e.g. RGB, Depth, etc.\n    img_size : List[int], optional, default=None\n        List of input image sizes.\n    patch_size : int, optional, default=16\n        Size of each patch.\n    in_chans : int, optional, default=3\n        Number of input channels.\n    embed_dim : int, optional, default=768\n        Embedding dimension.\n    depth : int, optional, default=12\n        Number of transformer blocks.\n    num_heads : int, optional, default=12\n        Number of attention heads.\n    mlp_ratio : float, optional, default=4.0\n        Ratio of hidden dimension in the MLP.\n    qkv_bias : bool, optional, default=True\n        If True, add a learnable bias to the query, key, and value projections.\n    qk_scale : Optional[float], optional\n        Override the default qk scale factor.\n    drop_rate : float, optional, default=0.0\n        Dropout rate for the transformer blocks.\n    attn_drop_rate : float, optional, default=0.0\n        Dropout rate for the attention mechanism.\n    drop_path_rate : float, optional, default=0.0\n        Dropout rate for stochastic depth.\n    norm_layer : Callable[..., torch.nn.Module], optional, default=torch.nn.LayerNorm\n        Normalization layer to use.\n    init_std : float, optional, default=0.02\n        Standard deviation for weight initialization.\n    \"\"\"\n\n    def __init__(\n        self,\n        modality: str = \"RGB\",\n        img_size: Optional[list[int]] = None,\n        patch_size: int = 16,\n        in_chans: int = 3,\n        embed_dim: int = 768,\n        depth: int = 12,\n        num_heads: int = 12,\n        mlp_ratio: float = 4.0,\n        qkv_bias: bool = True,\n        qk_scale: Optional[float] = None,\n        global_pool: Literal[\"\", \"avg\", \"avgmax\", \"max\", \"token\"] = \"\",\n        drop_rate: float = 0.0,\n        attn_drop_rate: float = 0.0,\n        drop_path_rate: float = 0.0,\n        norm_layer: Callable[..., nn.Module] = nn.LayerNorm,\n        init_std: float = 0.02,\n    ) -&gt; None:\n        super().__init__()\n        assert global_pool in (\"\", \"avg\", \"avgmax\", \"max\", \"token\")\n\n        self.modality = Modalities.get_modality(modality)\n        self.num_features = self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        img_size = [224, 224] if img_size is None else img_size\n\n        # Patch Embedding\n        self.patch_embed = PatchEmbed(\n            img_size=img_size[0],\n            patch_size=patch_size,\n            in_chans=in_chans,\n            embed_dim=embed_dim,\n        )\n        num_patches = self.patch_embed.num_patches\n\n        # Positional Embedding\n        self.pos_embed = nn.Parameter(\n            torch.zeros(1, num_patches, embed_dim), requires_grad=False\n        )\n        pos_embed = get_2d_sincos_pos_embed(\n            self.pos_embed.shape[-1],\n            int(self.patch_embed.num_patches**0.5),\n            cls_token=False,\n        )\n        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n\n        # Transformer Blocks\n        dpr = [\n            x.item() for x in torch.linspace(0, drop_path_rate, depth)\n        ]  # stochastic depth decay rule\n        self.blocks = nn.ModuleList(\n            [\n                Block(\n                    dim=embed_dim,\n                    num_heads=num_heads,\n                    mlp_ratio=mlp_ratio,\n                    qkv_bias=qkv_bias,\n                    qk_scale=qk_scale,\n                    drop=drop_rate,\n                    attn_drop=attn_drop_rate,\n                    drop_path=dpr[i],\n                    norm_layer=norm_layer,\n                )\n                for i in range(depth)\n            ]\n        )\n        self.norm = norm_layer(embed_dim)\n\n        self.global_pool = global_pool\n\n        # Weight Initialization\n        self.init_std = init_std\n        self.apply(self._init_weights)\n\n    def fix_init_weight(self) -&gt; None:\n        \"\"\"Fix initialization of weights by rescaling them according to layer depth.\"\"\"\n\n        def rescale(param: torch.Tensor, layer_id: int) -&gt; None:\n            param.div_(math.sqrt(2.0 * layer_id))\n\n        for layer_id, layer in enumerate(self.blocks):\n            rescale(layer.attn.proj.weight.data, layer_id + 1)\n            rescale(layer.mlp[-1].weight.data, layer_id + 1)\n\n    def _init_weights(self, m: nn.Module) -&gt; None:\n        \"\"\"Initialize weights for the layers.\"\"\"\n        if isinstance(m, nn.Linear):\n            _trunc_normal(m.weight, std=self.init_std)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n        elif isinstance(m, nn.Conv2d):\n            _trunc_normal(m.weight, std=self.init_std)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n\n    def forward(\n        self, inputs: dict[str, Any], return_hidden_states: bool = False\n    ) -&gt; tuple[torch.Tensor, Optional[list[torch.Tensor]]]:\n        \"\"\"Forward pass through the Vision Transformer.\"\"\"\n        masks = inputs.get(self.modality.mask)\n        if masks is not None and not isinstance(masks, list):\n            masks = [masks]\n\n        x = inputs[self.modality.name]\n        # -- Patchify x\n        x = self.patch_embed(x)\n\n        # -- Add positional embedding to x\n        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n        x = x + pos_embed\n\n        # -- Mask x\n        if masks is not None:\n            x = apply_masks(x, masks)\n\n        # -- Initialize a list to store hidden states\n        hidden_states: Optional[list[torch.Tensor]] = (\n            [] if return_hidden_states else None\n        )\n\n        # -- Forward propagation through blocks\n        for _i, blk in enumerate(self.blocks):\n            x = blk(x)\n            if return_hidden_states and hidden_states is not None:\n                hidden_states.append(x)\n\n        # -- Apply normalization if present\n        if self.norm is not None:\n            x = self.norm(x)\n\n        # -- Apply global pooling\n        x = global_pool_nlc(x, pool_type=self.global_pool)\n\n        # -- Return both final output and hidden states if requested\n        if return_hidden_states:\n            return x, hidden_states\n        return (x, None)\n\n    def interpolate_pos_encoding(\n        self, x: torch.Tensor, pos_embed: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"Interpolate positional encoding to match the size of the input tensor.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor.\n        pos_embed : torch.Tensor\n            Positional embedding tensor.\n\n        Returns\n        -------\n        torch.Tensor\n            Interpolated positional encoding.\n        \"\"\"\n        npatch = x.shape[1] - 1\n        n = pos_embed.shape[1] - 1\n        if npatch == n:\n            return pos_embed\n        class_emb = pos_embed[:, 0]\n        pos_embed = pos_embed[:, 1:]\n        dim = x.shape[-1]\n        pos_embed = nn.functional.interpolate(\n            pos_embed.reshape(1, int(math.sqrt(n)), int(math.sqrt(n)), dim).permute(\n                0, 3, 1, 2\n            ),\n            scale_factor=math.sqrt(npatch / n),\n            mode=\"bicubic\",\n        )\n        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n</code></pre> <code></code> fix_init_weight \u00b6 <pre><code>fix_init_weight()\n</code></pre> <p>Fix initialization of weights by rescaling them according to layer depth.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>def fix_init_weight(self) -&gt; None:\n    \"\"\"Fix initialization of weights by rescaling them according to layer depth.\"\"\"\n\n    def rescale(param: torch.Tensor, layer_id: int) -&gt; None:\n        param.div_(math.sqrt(2.0 * layer_id))\n\n    for layer_id, layer in enumerate(self.blocks):\n        rescale(layer.attn.proj.weight.data, layer_id + 1)\n        rescale(layer.mlp[-1].weight.data, layer_id + 1)\n</code></pre> <code></code> forward \u00b6 <pre><code>forward(inputs, return_hidden_states=False)\n</code></pre> <p>Forward pass through the Vision Transformer.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>def forward(\n    self, inputs: dict[str, Any], return_hidden_states: bool = False\n) -&gt; tuple[torch.Tensor, Optional[list[torch.Tensor]]]:\n    \"\"\"Forward pass through the Vision Transformer.\"\"\"\n    masks = inputs.get(self.modality.mask)\n    if masks is not None and not isinstance(masks, list):\n        masks = [masks]\n\n    x = inputs[self.modality.name]\n    # -- Patchify x\n    x = self.patch_embed(x)\n\n    # -- Add positional embedding to x\n    pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n    x = x + pos_embed\n\n    # -- Mask x\n    if masks is not None:\n        x = apply_masks(x, masks)\n\n    # -- Initialize a list to store hidden states\n    hidden_states: Optional[list[torch.Tensor]] = (\n        [] if return_hidden_states else None\n    )\n\n    # -- Forward propagation through blocks\n    for _i, blk in enumerate(self.blocks):\n        x = blk(x)\n        if return_hidden_states and hidden_states is not None:\n            hidden_states.append(x)\n\n    # -- Apply normalization if present\n    if self.norm is not None:\n        x = self.norm(x)\n\n    # -- Apply global pooling\n    x = global_pool_nlc(x, pool_type=self.global_pool)\n\n    # -- Return both final output and hidden states if requested\n    if return_hidden_states:\n        return x, hidden_states\n    return (x, None)\n</code></pre> <code></code> interpolate_pos_encoding \u00b6 <pre><code>interpolate_pos_encoding(x, pos_embed)\n</code></pre> <p>Interpolate positional encoding to match the size of the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <code>pos_embed</code> <code>Tensor</code> <p>Positional embedding tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Interpolated positional encoding.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>def interpolate_pos_encoding(\n    self, x: torch.Tensor, pos_embed: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"Interpolate positional encoding to match the size of the input tensor.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor.\n    pos_embed : torch.Tensor\n        Positional embedding tensor.\n\n    Returns\n    -------\n    torch.Tensor\n        Interpolated positional encoding.\n    \"\"\"\n    npatch = x.shape[1] - 1\n    n = pos_embed.shape[1] - 1\n    if npatch == n:\n        return pos_embed\n    class_emb = pos_embed[:, 0]\n    pos_embed = pos_embed[:, 1:]\n    dim = x.shape[-1]\n    pos_embed = nn.functional.interpolate(\n        pos_embed.reshape(1, int(math.sqrt(n)), int(math.sqrt(n)), dim).permute(\n            0, 3, 1, 2\n        ),\n        scale_factor=math.sqrt(npatch / n),\n        mode=\"bicubic\",\n    )\n    pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n    return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.vision.VisionTransformerPredictor","title":"VisionTransformerPredictor","text":"<p>               Bases: <code>Module</code></p> <p>Vision Transformer Predictor.</p> <p>This module implements a Vision Transformer that predicts masked tokens using a series of transformer blocks.</p> <p>Parameters:</p> Name Type Description Default <code>num_patches</code> <code>int</code> <p>The number of patches in the input image.</p> <code>196</code> <code>embed_dim</code> <code>int</code> <p>The embedding dimension.</p> <code>768</code> <code>predictor_embed_dim</code> <code>int</code> <p>The embedding dimension for the predictor.</p> <code>384</code> <code>depth</code> <code>int</code> <p>The number of transformer blocks.</p> <code>6</code> <code>num_heads</code> <code>int</code> <p>The number of attention heads.</p> <code>12</code> <code>mlp_ratio</code> <code>float</code> <p>Ratio of the hidden dimension in the MLP.</p> <code>4.0</code> <code>qkv_bias</code> <code>bool</code> <p>If True, add a learnable bias to the query, key, and value projections.</p> <code>True</code> <code>qk_scale</code> <code>Optional[float]</code> <p>Override the default qk scale factor.</p> <code>None</code> <code>drop_rate</code> <code>float</code> <p>Dropout rate for the transformer blocks.</p> <code>0.0</code> <code>attn_drop_rate</code> <code>float</code> <p>Dropout rate for the attention mechanism.</p> <code>0.0</code> <code>drop_path_rate</code> <code>float</code> <p>Dropout rate for stochastic depth.</p> <code>0.0</code> <code>norm_layer</code> <code>Callable[..., Module]</code> <p>Normalization layer to use.</p> <code>torch.nn.LayerNorm</code> <code>init_std</code> <code>float</code> <p>Standard deviation for weight initialization.</p> <code>0.02</code> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>class VisionTransformerPredictor(nn.Module):\n    \"\"\"Vision Transformer Predictor.\n\n    This module implements a Vision Transformer that predicts masked tokens\n    using a series of transformer blocks.\n\n    Parameters\n    ----------\n    num_patches : int\n        The number of patches in the input image.\n    embed_dim : int, optional, default=768\n        The embedding dimension.\n    predictor_embed_dim : int, optional, default=384\n        The embedding dimension for the predictor.\n    depth : int, optional, default=6\n        The number of transformer blocks.\n    num_heads : int, optional, default=12\n        The number of attention heads.\n    mlp_ratio : float, optional, default=4.0\n        Ratio of the hidden dimension in the MLP.\n    qkv_bias : bool, optional, default=True\n        If True, add a learnable bias to the query, key, and value projections.\n    qk_scale : Optional[float], optional, default=None\n        Override the default qk scale factor.\n    drop_rate : float, optional, default=0.0\n        Dropout rate for the transformer blocks.\n    attn_drop_rate : float, optional, default=0.0\n        Dropout rate for the attention mechanism.\n    drop_path_rate : float, optional, default=0.0\n        Dropout rate for stochastic depth.\n    norm_layer : Callable[..., torch.nn.Module], optional, default=torch.nn.LayerNorm\n        Normalization layer to use.\n    init_std : float, optional, default=0.02\n        Standard deviation for weight initialization.\n    \"\"\"\n\n    def __init__(\n        self,\n        num_patches: int = 196,\n        embed_dim: int = 768,\n        predictor_embed_dim: int = 384,\n        depth: int = 6,\n        num_heads: int = 12,\n        mlp_ratio: float = 4.0,\n        qkv_bias: bool = True,\n        qk_scale: Optional[float] = None,\n        drop_rate: float = 0.0,\n        attn_drop_rate: float = 0.0,\n        drop_path_rate: float = 0.0,\n        norm_layer: Callable[..., nn.Module] = nn.LayerNorm,\n        init_std: float = 0.02,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__()\n        self.num_patches = num_patches\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n\n        self.predictor_embed = nn.Linear(self.embed_dim, predictor_embed_dim, bias=True)\n        self.mask_token = nn.Parameter(torch.zeros(1, 1, predictor_embed_dim))\n        dpr = [\n            x.item() for x in torch.linspace(0, drop_path_rate, depth)\n        ]  # stochastic depth decay rule\n\n        # Positional Embedding\n        self.predictor_pos_embed = nn.Parameter(\n            torch.zeros(1, self.num_patches, predictor_embed_dim), requires_grad=False\n        )\n        predictor_pos_embed = get_2d_sincos_pos_embed(\n            self.predictor_pos_embed.shape[-1],\n            int(self.num_patches**0.5),\n            cls_token=False,\n        )\n        self.predictor_pos_embed.data.copy_(\n            torch.from_numpy(predictor_pos_embed).float().unsqueeze(0)\n        )\n\n        # Transformer Blocks\n        self.predictor_blocks = nn.ModuleList(\n            [\n                Block(\n                    dim=predictor_embed_dim,\n                    num_heads=self.num_heads,\n                    mlp_ratio=mlp_ratio,\n                    qkv_bias=qkv_bias,\n                    qk_scale=qk_scale,\n                    drop=drop_rate,\n                    attn_drop=attn_drop_rate,\n                    drop_path=dpr[i],\n                    norm_layer=norm_layer,\n                )\n                for i in range(depth)\n            ]\n        )\n\n        self.predictor_norm = norm_layer(predictor_embed_dim)\n        self.predictor_proj = nn.Linear(predictor_embed_dim, embed_dim, bias=True)\n\n        # Weight Initialization\n        self.init_std = init_std\n        _trunc_normal(self.mask_token, std=self.init_std)\n        self.apply(self._init_weights)\n\n    def fix_init_weight(self) -&gt; None:\n        \"\"\"Fix initialization of weights by rescaling them according to layer depth.\"\"\"\n\n        def rescale(param: torch.Tensor, layer_id: int) -&gt; None:\n            param.div_(math.sqrt(2.0 * layer_id))\n\n        for layer_id, layer in enumerate(self.predictor_blocks):\n            rescale(layer.attn.proj.weight.data, layer_id + 1)\n            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n\n    def _init_weights(self, m: nn.Module) -&gt; None:\n        \"\"\"Initialize weights for the layers.\"\"\"\n        if isinstance(m, nn.Linear):\n            _trunc_normal(m.weight, std=self.init_std)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n        elif isinstance(m, nn.Conv2d):\n            _trunc_normal(m.weight, std=self.init_std)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        masks_x: Union[torch.Tensor, list[torch.Tensor]],\n        masks: Union[torch.Tensor, list[torch.Tensor]],\n    ) -&gt; torch.Tensor:\n        \"\"\"Forward pass through the Vision Transformer Predictor.\"\"\"\n        assert (masks is not None) and (masks_x is not None), (\n            \"Cannot run predictor without mask indices\"\n        )\n\n        if not isinstance(masks_x, list):\n            masks_x = [masks_x]\n\n        if not isinstance(masks, list):\n            masks = [masks]\n\n        # -- Batch Size\n        b = len(x) // len(masks_x)\n\n        # -- Map from encoder-dim to predictor-dim\n        x = self.predictor_embed(x)\n\n        # -- Add positional embedding to x tokens\n        x_pos_embed = self.predictor_pos_embed.repeat(b, 1, 1)\n        x += apply_masks(x_pos_embed, masks_x)\n\n        _, n_ctxt, d = x.shape\n\n        # -- Concatenate mask tokens to x\n        pos_embs = self.predictor_pos_embed.repeat(b, 1, 1)\n        pos_embs = apply_masks(pos_embs, masks)\n        pos_embs = repeat_interleave_batch(pos_embs, b, repeat=len(masks_x))\n        pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1)\n        pred_tokens += pos_embs\n        x = x.repeat(len(masks), 1, 1)\n        x = torch.cat([x, pred_tokens], dim=1)\n\n        # -- Forward propagation\n        for blk in self.predictor_blocks:\n            x = blk(x)\n        x = self.predictor_norm(x)\n\n        # -- Return predictions for mask tokens\n        x = x[:, n_ctxt:]\n        return self.predictor_proj(x)\n</code></pre> <code></code> fix_init_weight \u00b6 <pre><code>fix_init_weight()\n</code></pre> <p>Fix initialization of weights by rescaling them according to layer depth.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>def fix_init_weight(self) -&gt; None:\n    \"\"\"Fix initialization of weights by rescaling them according to layer depth.\"\"\"\n\n    def rescale(param: torch.Tensor, layer_id: int) -&gt; None:\n        param.div_(math.sqrt(2.0 * layer_id))\n\n    for layer_id, layer in enumerate(self.predictor_blocks):\n        rescale(layer.attn.proj.weight.data, layer_id + 1)\n        rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n</code></pre> <code></code> forward \u00b6 <pre><code>forward(x, masks_x, masks)\n</code></pre> <p>Forward pass through the Vision Transformer Predictor.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>def forward(\n    self,\n    x: torch.Tensor,\n    masks_x: Union[torch.Tensor, list[torch.Tensor]],\n    masks: Union[torch.Tensor, list[torch.Tensor]],\n) -&gt; torch.Tensor:\n    \"\"\"Forward pass through the Vision Transformer Predictor.\"\"\"\n    assert (masks is not None) and (masks_x is not None), (\n        \"Cannot run predictor without mask indices\"\n    )\n\n    if not isinstance(masks_x, list):\n        masks_x = [masks_x]\n\n    if not isinstance(masks, list):\n        masks = [masks]\n\n    # -- Batch Size\n    b = len(x) // len(masks_x)\n\n    # -- Map from encoder-dim to predictor-dim\n    x = self.predictor_embed(x)\n\n    # -- Add positional embedding to x tokens\n    x_pos_embed = self.predictor_pos_embed.repeat(b, 1, 1)\n    x += apply_masks(x_pos_embed, masks_x)\n\n    _, n_ctxt, d = x.shape\n\n    # -- Concatenate mask tokens to x\n    pos_embs = self.predictor_pos_embed.repeat(b, 1, 1)\n    pos_embs = apply_masks(pos_embs, masks)\n    pos_embs = repeat_interleave_batch(pos_embs, b, repeat=len(masks_x))\n    pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1)\n    pred_tokens += pos_embs\n    x = x.repeat(len(masks), 1, 1)\n    x = torch.cat([x, pred_tokens], dim=1)\n\n    # -- Forward propagation\n    for blk in self.predictor_blocks:\n        x = blk(x)\n    x = self.predictor_norm(x)\n\n    # -- Return predictions for mask tokens\n    x = x[:, n_ctxt:]\n    return self.predictor_proj(x)\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.vision.vit_predictor","title":"vit_predictor","text":"<pre><code>vit_predictor(kwargs=None)\n</code></pre> <p>Create a VisionTransformerPredictor model.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>dict[str, Any]</code> <p>Keyword arguments for the predictor.</p> <code>None</code> <p>Returns:</p> Type Description <code>VisionTransformerPredictor</code> <p>An instance of VisionTransformerPredictor.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>@cast(\n    VisionTransformerPredictor,\n    store(\n        group=\"modules/encoders\",\n        provider=\"mmlearn\",\n    ),\n)\ndef vit_predictor(\n    kwargs: Optional[dict[str, Any]] = None,\n) -&gt; VisionTransformerPredictor:\n    \"\"\"Create a VisionTransformerPredictor model.\n\n    Parameters\n    ----------\n    kwargs : dict[str, Any], optional, default=None\n        Keyword arguments for the predictor.\n\n    Returns\n    -------\n    VisionTransformerPredictor\n        An instance of VisionTransformerPredictor.\n    \"\"\"\n    if kwargs is None:\n        kwargs = {}\n    return VisionTransformerPredictor(\n        mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.vision.vit_tiny","title":"vit_tiny","text":"<pre><code>vit_tiny(patch_size=16, kwargs=None)\n</code></pre> <p>Create a VisionTransformer model with tiny configuration.</p> <p>Parameters:</p> Name Type Description Default <code>patch_size</code> <code>int</code> <p>Size of each patch.</p> <code>16</code> <code>kwargs</code> <code>dict[str, Any]</code> <p>Keyword arguments for the model variant.</p> <code>None</code> <p>Returns:</p> Type Description <code>VisionTransformer</code> <p>An instance of VisionTransformer.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>@cast(\n    VisionTransformer,\n    store(\n        group=\"modules/encoders\",\n        provider=\"mmlearn\",\n    ),\n)\ndef vit_tiny(\n    patch_size: int = 16, kwargs: Optional[dict[str, Any]] = None\n) -&gt; VisionTransformer:\n    \"\"\"Create a VisionTransformer model with tiny configuration.\n\n    Parameters\n    ----------\n    patch_size : int, default=16\n        Size of each patch.\n    kwargs : dict[str, Any], optional, default=None\n        Keyword arguments for the model variant.\n\n    Returns\n    -------\n    VisionTransformer\n        An instance of VisionTransformer.\n    \"\"\"\n    if kwargs is None:\n        kwargs = {}\n    return VisionTransformer(\n        patch_size=patch_size,\n        embed_dim=192,\n        depth=12,\n        num_heads=3,\n        mlp_ratio=4,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.vision.vit_small","title":"vit_small","text":"<pre><code>vit_small(patch_size=16, kwargs=None)\n</code></pre> <p>Create a VisionTransformer model with small configuration.</p> <p>Parameters:</p> Name Type Description Default <code>patch_size</code> <code>int</code> <p>Size of each patch.</p> <code>16</code> <code>kwargs</code> <code>dict[str, Any]</code> <p>Keyword arguments for the model variant.</p> <code>None</code> <p>Returns:</p> Type Description <code>VisionTransformer</code> <p>An instance of VisionTransformer.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>@cast(\n    VisionTransformer,\n    store(\n        group=\"modules/encoders\",\n        provider=\"mmlearn\",\n    ),\n)\ndef vit_small(\n    patch_size: int = 16, kwargs: Optional[dict[str, Any]] = None\n) -&gt; VisionTransformer:\n    \"\"\"Create a VisionTransformer model with small configuration.\n\n    Parameters\n    ----------\n    patch_size : int, default=16\n        Size of each patch.\n    kwargs : dict[str, Any], optional, default=None\n        Keyword arguments for the model variant.\n\n    Returns\n    -------\n    VisionTransformer\n        An instance of VisionTransformer.\n    \"\"\"\n    if kwargs is None:\n        kwargs = {}\n    return VisionTransformer(\n        patch_size=patch_size,\n        embed_dim=384,\n        depth=12,\n        num_heads=6,\n        mlp_ratio=4,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.vision.vit_base","title":"vit_base","text":"<pre><code>vit_base(patch_size=16, kwargs=None)\n</code></pre> <p>Create a VisionTransformer model with base configuration.</p> <p>Parameters:</p> Name Type Description Default <code>patch_size</code> <code>int</code> <p>Size of each patch.</p> <code>16</code> <code>kwargs</code> <code>dict[str, Any]</code> <p>Keyword arguments for the model variant.</p> <code>None</code> <p>Returns:</p> Type Description <code>VisionTransformer</code> <p>An instance of VisionTransformer.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>@cast(\n    VisionTransformer,\n    store(\n        group=\"modules/encoders\",\n        provider=\"mmlearn\",\n    ),\n)\ndef vit_base(\n    patch_size: int = 16, kwargs: Optional[dict[str, Any]] = None\n) -&gt; VisionTransformer:\n    \"\"\"Create a VisionTransformer model with base configuration.\n\n    Parameters\n    ----------\n    patch_size : int, default=16\n        Size of each patch.\n    kwargs : dict[str, Any], optional, default=None\n        Keyword arguments for the model variant.\n\n    Returns\n    -------\n    VisionTransformer\n        An instance of VisionTransformer.\n    \"\"\"\n    if kwargs is None:\n        kwargs = {}\n    return VisionTransformer(\n        patch_size=patch_size,\n        embed_dim=768,\n        depth=12,\n        num_heads=12,\n        mlp_ratio=4,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.vision.vit_large","title":"vit_large","text":"<pre><code>vit_large(patch_size=16, kwargs=None)\n</code></pre> <p>Create a VisionTransformer model with large configuration.</p> <p>Parameters:</p> Name Type Description Default <code>patch_size</code> <code>int</code> <p>Size of each patch.</p> <code>16</code> <code>kwargs</code> <code>dict[str, Any]</code> <p>Keyword arguments for the model variant.</p> <code>None</code> <p>Returns:</p> Type Description <code>VisionTransformer</code> <p>An instance of VisionTransformer.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>@cast(\n    VisionTransformer,\n    store(\n        group=\"modules/encoders\",\n        provider=\"mmlearn\",\n    ),\n)\ndef vit_large(\n    patch_size: int = 16, kwargs: Optional[dict[str, Any]] = None\n) -&gt; VisionTransformer:\n    \"\"\"Create a VisionTransformer model with large configuration.\n\n    Parameters\n    ----------\n    patch_size : int, default=16\n        Size of each patch.\n    kwargs : dict[str, Any], optional, default=None\n        Keyword arguments for the model variant.\n\n    Returns\n    -------\n    VisionTransformer\n        An instance of VisionTransformer.\n    \"\"\"\n    if kwargs is None:\n        kwargs = {}\n    return VisionTransformer(\n        patch_size=patch_size,\n        embed_dim=1024,\n        depth=24,\n        num_heads=16,\n        mlp_ratio=4,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.vision.vit_huge","title":"vit_huge","text":"<pre><code>vit_huge(patch_size=16, kwargs=None)\n</code></pre> <p>Create a VisionTransformer model with huge configuration.</p> <p>Parameters:</p> Name Type Description Default <code>patch_size</code> <code>int</code> <p>Size of each patch.</p> <code>16</code> <code>kwargs</code> <code>dict[str, Any]</code> <p>Keyword arguments for the model variant.</p> <code>None</code> <p>Returns:</p> Type Description <code>VisionTransformer</code> <p>An instance of VisionTransformer.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>@cast(\n    VisionTransformer,\n    store(\n        group=\"modules/encoders\",\n        provider=\"mmlearn\",\n    ),\n)\ndef vit_huge(\n    patch_size: int = 16, kwargs: Optional[dict[str, Any]] = None\n) -&gt; VisionTransformer:\n    \"\"\"Create a VisionTransformer model with huge configuration.\n\n    Parameters\n    ----------\n    patch_size : int, default=16\n        Size of each patch.\n    kwargs : dict[str, Any], optional, default=None\n        Keyword arguments for the model variant.\n\n    Returns\n    -------\n    VisionTransformer\n        An instance of VisionTransformer.\n    \"\"\"\n    if kwargs is None:\n        kwargs = {}\n    return VisionTransformer(\n        patch_size=patch_size,\n        embed_dim=1280,\n        depth=32,\n        num_heads=16,\n        mlp_ratio=4,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.vision.vit_giant","title":"vit_giant","text":"<pre><code>vit_giant(patch_size=16, kwargs=None)\n</code></pre> <p>Create a VisionTransformer model with giant configuration.</p> <p>Parameters:</p> Name Type Description Default <code>patch_size</code> <code>int</code> <p>Size of each patch.</p> <code>16</code> <code>kwargs</code> <code>dict[str, Any]</code> <p>Keyword arguments for the model variant.</p> <code>None</code> <p>Returns:</p> Type Description <code>VisionTransformer</code> <p>An instance of VisionTransformer.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>@cast(\n    VisionTransformer,\n    store(\n        group=\"modules/encoders\",\n        provider=\"mmlearn\",\n    ),\n)\ndef vit_giant(\n    patch_size: int = 16, kwargs: Optional[dict[str, Any]] = None\n) -&gt; VisionTransformer:\n    \"\"\"Create a VisionTransformer model with giant configuration.\n\n    Parameters\n    ----------\n    patch_size : int, default=16\n        Size of each patch.\n    kwargs : dict[str, Any], optional, default=None\n        Keyword arguments for the model variant.\n\n    Returns\n    -------\n    VisionTransformer\n        An instance of VisionTransformer.\n    \"\"\"\n    if kwargs is None:\n        kwargs = {}\n    return VisionTransformer(\n        patch_size=patch_size,\n        embed_dim=1408,\n        depth=40,\n        num_heads=16,\n        mlp_ratio=48 / 11,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.layers","title":"layers","text":"<p>Custom, reusable layers for models and tasks.</p>"},{"location":"api/#mmlearn.modules.layers.LearnableLogitScaling","title":"LearnableLogitScaling","text":"<p>               Bases: <code>Module</code></p> <p>Logit scaling layer.</p> <p>Parameters:</p> Name Type Description Default <code>init_logit_scale</code> <code>float</code> <p>Initial value of the logit scale.</p> <code>1/0.07</code> <code>learnable</code> <code>bool</code> <p>If True, the logit scale is learnable. Otherwise, it is fixed.</p> <code>True</code> <code>max_logit_scale</code> <code>float</code> <p>Maximum value of the logit scale.</p> <code>100</code> Source code in <code>mmlearn/modules/layers/logit_scaling.py</code> <pre><code>@store(group=\"modules/layers\", provider=\"mmlearn\")\nclass LearnableLogitScaling(torch.nn.Module):\n    \"\"\"Logit scaling layer.\n\n    Parameters\n    ----------\n    init_logit_scale : float, optional, default=1/0.07\n        Initial value of the logit scale.\n    learnable : bool, optional, default=True\n        If True, the logit scale is learnable. Otherwise, it is fixed.\n    max_logit_scale : float, optional, default=100\n        Maximum value of the logit scale.\n    \"\"\"\n\n    def __init__(\n        self,\n        init_logit_scale: float = 1 / 0.07,\n        max_logit_scale: float = 100,\n        learnable: bool = True,\n    ) -&gt; None:\n        super().__init__()\n        self.max_logit_scale = max_logit_scale\n        self.init_logit_scale = init_logit_scale\n        self.learnable = learnable\n        log_logit_scale = torch.ones([]) * np.log(self.init_logit_scale)\n        if learnable:\n            self.log_logit_scale = torch.nn.Parameter(log_logit_scale)\n        else:\n            self.register_buffer(\"log_logit_scale\", log_logit_scale)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Apply the logit scaling to the input tensor.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor of shape ``(batch_sz, seq_len, dim)``.\n        \"\"\"\n        return torch.clip(self.log_logit_scale.exp(), max=self.max_logit_scale) * x\n\n    def extra_repr(self) -&gt; str:\n        \"\"\"Return the string representation of the layer.\"\"\"\n        return (\n            f\"logit_scale_init={self.init_logit_scale},learnable={self.learnable},\"\n            f\" max_logit_scale={self.max_logit_scale}\"\n        )\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.LearnableLogitScaling.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Apply the logit scaling to the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape <code>(batch_sz, seq_len, dim)</code>.</p> required Source code in <code>mmlearn/modules/layers/logit_scaling.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Apply the logit scaling to the input tensor.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape ``(batch_sz, seq_len, dim)``.\n    \"\"\"\n    return torch.clip(self.log_logit_scale.exp(), max=self.max_logit_scale) * x\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.LearnableLogitScaling.extra_repr","title":"extra_repr","text":"<pre><code>extra_repr()\n</code></pre> <p>Return the string representation of the layer.</p> Source code in <code>mmlearn/modules/layers/logit_scaling.py</code> <pre><code>def extra_repr(self) -&gt; str:\n    \"\"\"Return the string representation of the layer.\"\"\"\n    return (\n        f\"logit_scale_init={self.init_logit_scale},learnable={self.learnable},\"\n        f\" max_logit_scale={self.max_logit_scale}\"\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.MLP","title":"MLP","text":"<p>               Bases: <code>Sequential</code></p> <p>Multi-layer perceptron (MLP).</p> <p>This module will create a block of <code>Linear -&gt; Normalization -&gt; Activation -&gt; Dropout</code> layers.</p> <p>Parameters:</p> Name Type Description Default <code>in_dim</code> <code>int</code> <p>The input dimension.</p> required <code>out_dim</code> <code>Optional[int]</code> <p>The output dimension. If not specified, it is set to :attr:<code>in_dim</code>.</p> <code>None</code> <code>hidden_dims</code> <code>Optional[list]</code> <p>The dimensions of the hidden layers. The length of the list determines the number of hidden layers. This parameter is mutually exclusive with :attr:<code>hidden_dims_multiplier</code>.</p> <code>None</code> <code>hidden_dims_multiplier</code> <code>Optional[list]</code> <p>The multipliers to apply to the input dimension to get the dimensions of the hidden layers. The length of the list determines the number of hidden layers. The multipliers will be used to get the dimensions of the hidden layers. This parameter is mutually exclusive with <code>hidden_dims</code>.</p> <code>None</code> <code>apply_multiplier_to_in_dim</code> <code>bool</code> <p>Whether to apply the :attr:<code>hidden_dims_multiplier</code> to :attr:<code>in_dim</code> to get the dimensions of the hidden layers. If <code>False</code>, the multipliers will be applied to the dimensions of the previous hidden layer, starting from :attr:<code>in_dim</code>. This parameter is only relevant when :attr:<code>hidden_dims_multiplier</code> is specified.</p> <code>False</code> <code>norm_layer</code> <code>Optional[Callable[..., Module]]</code> <p>The normalization layer to use. If not specified, no normalization is used. Partial functions can be used to specify the normalization layer with specific parameters.</p> <code>None</code> <code>activation_layer</code> <code>Optional[Callable[..., Module]]</code> <p>The activation layer to use. If not specified, ReLU is used. Partial functions can be used to specify the activation layer with specific parameters.</p> <code>torch.nn.ReLU</code> <code>bias</code> <code>Union[bool, list[bool]]</code> <p>Whether to use bias in the linear layers.</p> <code>True</code> <code>dropout</code> <code>Union[float, list[float]]</code> <p>The dropout probability to use.</p> <code>0.0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If both :attr:<code>hidden_dims</code> and :attr:<code>hidden_dims_multiplier</code> are specified or if the lengths of :attr:<code>bias</code> and :attr:<code>hidden_dims</code> do not match or if the lengths of :attr:<code>dropout</code> and :attr:<code>hidden_dims</code> do not match.</p> Source code in <code>mmlearn/modules/layers/mlp.py</code> <pre><code>@store(group=\"modules/layers\", provider=\"mmlearn\")\nclass MLP(torch.nn.Sequential):\n    \"\"\"Multi-layer perceptron (MLP).\n\n    This module will create a block of ``Linear -&gt; Normalization -&gt; Activation -&gt; Dropout``\n    layers.\n\n    Parameters\n    ----------\n    in_dim : int\n        The input dimension.\n    out_dim : Optional[int], optional, default=None\n        The output dimension. If not specified, it is set to :attr:`in_dim`.\n    hidden_dims : Optional[list], optional, default=None\n        The dimensions of the hidden layers. The length of the list determines the\n        number of hidden layers. This parameter is mutually exclusive with\n        :attr:`hidden_dims_multiplier`.\n    hidden_dims_multiplier : Optional[list], optional, default=None\n        The multipliers to apply to the input dimension to get the dimensions of\n        the hidden layers. The length of the list determines the number of hidden\n        layers. The multipliers will be used to get the dimensions of the hidden\n        layers. This parameter is mutually exclusive with `hidden_dims`.\n    apply_multiplier_to_in_dim : bool, optional, default=False\n        Whether to apply the :attr:`hidden_dims_multiplier` to :attr:`in_dim` to get the\n        dimensions of the hidden layers. If ``False``, the multipliers will be applied\n        to the dimensions of the previous hidden layer, starting from :attr:`in_dim`.\n        This parameter is only relevant when :attr:`hidden_dims_multiplier` is\n        specified.\n    norm_layer : Optional[Callable[..., torch.nn.Module]], optional, default=None\n        The normalization layer to use. If not specified, no normalization is used.\n        Partial functions can be used to specify the normalization layer with specific\n        parameters.\n    activation_layer : Optional[Callable[..., torch.nn.Module]], optional, default=torch.nn.ReLU\n        The activation layer to use. If not specified, ReLU is used. Partial functions\n        can be used to specify the activation layer with specific parameters.\n    bias : Union[bool, list[bool]], optional, default=True\n        Whether to use bias in the linear layers.\n    dropout : Union[float, list[float]], optional, default=0.0\n        The dropout probability to use.\n\n    Raises\n    ------\n    ValueError\n        If both :attr:`hidden_dims` and :attr:`hidden_dims_multiplier` are specified\n        or if the lengths of :attr:`bias` and :attr:`hidden_dims` do not match or if\n        the lengths of :attr:`dropout` and :attr:`hidden_dims` do not match.\n\n    \"\"\"  # noqa: W505\n\n    def __init__(  # noqa: PLR0912\n        self,\n        in_dim: int,\n        out_dim: Optional[int] = None,\n        hidden_dims: Optional[list[int]] = None,\n        hidden_dims_multiplier: Optional[list[float]] = None,\n        apply_multiplier_to_in_dim: bool = False,\n        norm_layer: Optional[Callable[..., torch.nn.Module]] = None,\n        activation_layer: Optional[Callable[..., torch.nn.Module]] = torch.nn.ReLU,\n        bias: Union[bool, list[bool]] = True,\n        dropout: Union[float, list[float]] = 0.0,\n    ) -&gt; None:\n        if hidden_dims is None and hidden_dims_multiplier is None:\n            hidden_dims = []\n        if hidden_dims is not None and hidden_dims_multiplier is not None:\n            raise ValueError(\n                \"Only one of `hidden_dims` or `hidden_dims_multiplier` must be specified.\"\n            )\n\n        if hidden_dims is None and hidden_dims_multiplier is not None:\n            if apply_multiplier_to_in_dim:\n                hidden_dims = [\n                    int(in_dim * multiplier) for multiplier in hidden_dims_multiplier\n                ]\n            else:\n                hidden_dims = [int(in_dim * hidden_dims_multiplier[0])]\n                for multiplier in hidden_dims_multiplier[1:]:\n                    hidden_dims.append(int(hidden_dims[-1] * multiplier))\n\n        if isinstance(bias, bool):\n            bias_list: list[bool] = [bias] * (len(hidden_dims) + 1)  # type: ignore[arg-type]\n        else:\n            bias_list = bias\n        if len(bias_list) != len(hidden_dims) + 1:  # type: ignore[arg-type]\n            raise ValueError(\n                \"Expected `bias` to be a boolean or a list of booleans with length \"\n                \"equal to the number of linear layers in the MLP.\"\n            )\n\n        if isinstance(dropout, float):\n            dropout_list: list[float] = [dropout] * (len(hidden_dims) + 1)  # type: ignore[arg-type]\n        else:\n            dropout_list = dropout\n        if len(dropout_list) != len(hidden_dims) + 1:  # type: ignore[arg-type]\n            raise ValueError(\n                \"Expected `dropout` to be a float or a list of floats with length \"\n                \"equal to the number of linear layers in the MLP.\"\n            )\n\n        # construct list of dimensions for the layers\n        dims = [in_dim] + hidden_dims  # type: ignore[operator]\n        layers = []\n        for layer_idx, (in_features, hidden_features) in enumerate(\n            zip(dims[:-1], dims[1:], strict=False)\n        ):\n            layers.append(\n                torch.nn.Linear(in_features, hidden_features, bias=bias_list[layer_idx])\n            )\n            if norm_layer is not None:\n                layers.append(norm_layer(hidden_features))\n            if activation_layer is not None:\n                layers.append(activation_layer())\n            layers.append(torch.nn.Dropout(dropout_list[layer_idx]))\n\n        out_dim = out_dim or in_dim\n\n        layers.append(torch.nn.Linear(dims[-1], out_dim, bias=bias_list[-1]))\n        layers.append(torch.nn.Dropout(dropout_list[-1]))\n\n        super().__init__(*layers)\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.L2Norm","title":"L2Norm","text":"<p>               Bases: <code>Module</code></p> <p>L2 normalization.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>The dimension along which to normalize.</p> required Source code in <code>mmlearn/modules/layers/normalization.py</code> <pre><code>@store(group=\"modules/layers\", provider=\"mmlearn\")\nclass L2Norm(torch.nn.Module):\n    \"\"\"L2 normalization.\n\n    Parameters\n    ----------\n    dim : int\n        The dimension along which to normalize.\n    \"\"\"\n\n    def __init__(self, dim: int) -&gt; None:\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Apply L2 normalization to the input tensor.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor of shape ``(batch_sz, seq_len, dim)``.\n\n        Returns\n        -------\n        torch.Tensor\n            Normalized tensor of shape ``(batch_sz, seq_len, dim)``.\n        \"\"\"\n        return torch.nn.functional.normalize(x, dim=self.dim, p=2)\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.L2Norm.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Apply L2 normalization to the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape <code>(batch_sz, seq_len, dim)</code>.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized tensor of shape <code>(batch_sz, seq_len, dim)</code>.</p> Source code in <code>mmlearn/modules/layers/normalization.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Apply L2 normalization to the input tensor.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape ``(batch_sz, seq_len, dim)``.\n\n    Returns\n    -------\n    torch.Tensor\n        Normalized tensor of shape ``(batch_sz, seq_len, dim)``.\n    \"\"\"\n    return torch.nn.functional.normalize(x, dim=self.dim, p=2)\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.PatchDropout","title":"PatchDropout","text":"<p>               Bases: <code>Module</code></p> <p>Patch dropout layer.</p> <p>Drops patch tokens (after embedding and adding CLS token) from the input tensor. Usually used in vision transformers to reduce the number of tokens. [1]_</p> <p>Parameters:</p> Name Type Description Default <code>keep_rate</code> <code>float</code> <p>The proportion of tokens to keep.</p> <code>0.5</code> <code>bias</code> <code>Optional[float]</code> <p>The bias to add to the random noise before sorting.</p> <code>None</code> <code>token_shuffling</code> <code>bool</code> <p>If True, the tokens are shuffled.</p> <code>False</code> References <p>.. [1] Liu, Y., Matsoukas, C., Strand, F., Azizpour, H., &amp; Smith, K. (2023).    Patchdropout: Economizing vision transformers using patch dropout. In Proceedings    of the IEEE/CVF Winter Conference on Applications of Computer Vision    (pp. 3953-3962).</p> Source code in <code>mmlearn/modules/layers/patch_dropout.py</code> <pre><code>class PatchDropout(torch.nn.Module):\n    \"\"\"Patch dropout layer.\n\n    Drops patch tokens (after embedding and adding CLS token) from the input tensor.\n    Usually used in vision transformers to reduce the number of tokens. [1]_\n\n    Parameters\n    ----------\n    keep_rate : float, optional, default=0.5\n        The proportion of tokens to keep.\n    bias : Optional[float], optional, default=None\n        The bias to add to the random noise before sorting.\n    token_shuffling : bool, optional, default=False\n        If True, the tokens are shuffled.\n\n    References\n    ----------\n    .. [1] Liu, Y., Matsoukas, C., Strand, F., Azizpour, H., &amp; Smith, K. (2023).\n       Patchdropout: Economizing vision transformers using patch dropout. In Proceedings\n       of the IEEE/CVF Winter Conference on Applications of Computer Vision\n       (pp. 3953-3962).\n    \"\"\"\n\n    def __init__(\n        self,\n        keep_rate: float = 0.5,\n        bias: Optional[float] = None,\n        token_shuffling: bool = False,\n    ):\n        super().__init__()\n        assert 0 &lt; keep_rate &lt;= 1, \"The keep_rate must be in (0,1]\"\n\n        self.bias = bias\n        self.keep_rate = keep_rate\n        self.token_shuffling = token_shuffling\n\n    def forward(self, x: torch.Tensor, force_drop: bool = False) -&gt; torch.Tensor:\n        \"\"\"Drop tokens from the input tensor.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor of shape ``(batch_sz, seq_len, dim)``.\n        force_drop : bool, optional, default=False\n            If True, the tokens are always dropped, even when the model is in\n            evaluation mode.\n\n        Returns\n        -------\n        torch.Tensor\n            Tensor of shape ``(batch_sz, keep_len, dim)`` containing the kept tokens.\n        \"\"\"\n        if (not self.training and not force_drop) or self.keep_rate == 1:\n            return x\n\n        batch_sz, _, dim = x.shape\n\n        cls_mask = torch.zeros(  # assumes that CLS is always the 1st element\n            batch_sz, 1, dtype=torch.int64, device=x.device\n        )\n        patch_mask = self.uniform_mask(x)\n        patch_mask = torch.hstack([cls_mask, patch_mask])\n\n        return torch.gather(x, dim=1, index=patch_mask.unsqueeze(-1).repeat(1, 1, dim))\n\n    def uniform_mask(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Generate token ids to keep from uniform random distribution.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor of shape ``(batch_sz, seq_len, dim)``.\n\n        Returns\n        -------\n        torch.Tensor\n            Tensor of shape ``(batch_sz, keep_len)`` containing the token ids to keep.\n\n        \"\"\"\n        batch_sz, seq_len, _ = x.shape\n        seq_len = seq_len - 1  # patch length (without CLS)\n\n        keep_len = int(seq_len * self.keep_rate)\n        noise = torch.rand(batch_sz, seq_len, device=x.device)\n        if self.bias is not None:\n            noise += self.bias\n        ids = torch.argsort(noise, dim=1)\n        keep_ids = ids[:, :keep_len]\n        if not self.token_shuffling:\n            keep_ids = keep_ids.sort(1)[0]\n        return keep_ids\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.PatchDropout.forward","title":"forward","text":"<pre><code>forward(x, force_drop=False)\n</code></pre> <p>Drop tokens from the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape <code>(batch_sz, seq_len, dim)</code>.</p> required <code>force_drop</code> <code>bool</code> <p>If True, the tokens are always dropped, even when the model is in evaluation mode.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of shape <code>(batch_sz, keep_len, dim)</code> containing the kept tokens.</p> Source code in <code>mmlearn/modules/layers/patch_dropout.py</code> <pre><code>def forward(self, x: torch.Tensor, force_drop: bool = False) -&gt; torch.Tensor:\n    \"\"\"Drop tokens from the input tensor.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape ``(batch_sz, seq_len, dim)``.\n    force_drop : bool, optional, default=False\n        If True, the tokens are always dropped, even when the model is in\n        evaluation mode.\n\n    Returns\n    -------\n    torch.Tensor\n        Tensor of shape ``(batch_sz, keep_len, dim)`` containing the kept tokens.\n    \"\"\"\n    if (not self.training and not force_drop) or self.keep_rate == 1:\n        return x\n\n    batch_sz, _, dim = x.shape\n\n    cls_mask = torch.zeros(  # assumes that CLS is always the 1st element\n        batch_sz, 1, dtype=torch.int64, device=x.device\n    )\n    patch_mask = self.uniform_mask(x)\n    patch_mask = torch.hstack([cls_mask, patch_mask])\n\n    return torch.gather(x, dim=1, index=patch_mask.unsqueeze(-1).repeat(1, 1, dim))\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.PatchDropout.uniform_mask","title":"uniform_mask","text":"<pre><code>uniform_mask(x)\n</code></pre> <p>Generate token ids to keep from uniform random distribution.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape <code>(batch_sz, seq_len, dim)</code>.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of shape <code>(batch_sz, keep_len)</code> containing the token ids to keep.</p> Source code in <code>mmlearn/modules/layers/patch_dropout.py</code> <pre><code>def uniform_mask(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Generate token ids to keep from uniform random distribution.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape ``(batch_sz, seq_len, dim)``.\n\n    Returns\n    -------\n    torch.Tensor\n        Tensor of shape ``(batch_sz, keep_len)`` containing the token ids to keep.\n\n    \"\"\"\n    batch_sz, seq_len, _ = x.shape\n    seq_len = seq_len - 1  # patch length (without CLS)\n\n    keep_len = int(seq_len * self.keep_rate)\n    noise = torch.rand(batch_sz, seq_len, device=x.device)\n    if self.bias is not None:\n        noise += self.bias\n    ids = torch.argsort(noise, dim=1)\n    keep_ids = ids[:, :keep_len]\n    if not self.token_shuffling:\n        keep_ids = keep_ids.sort(1)[0]\n    return keep_ids\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.attention","title":"attention","text":"<p>Attention modules for Vision Transformer (ViT) and related models.</p>"},{"location":"api/#mmlearn.modules.layers.attention.Attention","title":"Attention","text":"<p>               Bases: <code>Module</code></p> <p>Multi-head Self-Attention Mechanism.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>Number of input dimensions.</p> required <code>num_heads</code> <code>int</code> <p>Number of attention heads.</p> <code>8</code> <code>qkv_bias</code> <code>bool</code> <p>If True, adds a learnable bias to the query, key, value projections.</p> <code>False</code> <code>qk_scale</code> <code>Optional[float]</code> <p>Override the default scale factor for the dot-product attention.</p> <code>None</code> <code>attn_drop</code> <code>float</code> <p>Dropout probability for the attention weights.</p> <code>0.0</code> <code>proj_drop</code> <code>float</code> <p>Dropout probability for the output of the attention layer.</p> <code>0.0</code> Source code in <code>mmlearn/modules/layers/attention.py</code> <pre><code>class Attention(nn.Module):\n    \"\"\"Multi-head Self-Attention Mechanism.\n\n    Parameters\n    ----------\n    dim : int\n        Number of input dimensions.\n    num_heads : int, optional, default=8\n        Number of attention heads.\n    qkv_bias : bool, optional, default=False\n        If True, adds a learnable bias to the query, key, value projections.\n    qk_scale : Optional[float], optional, default=None\n        Override the default scale factor for the dot-product attention.\n    attn_drop : float, optional, default=0.0\n        Dropout probability for the attention weights.\n    proj_drop : float, optional, default=0.0\n        Dropout probability for the output of the attention layer.\n    \"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int = 8,\n        qkv_bias: bool = False,\n        qk_scale: Optional[float] = None,\n        attn_drop: float = 0.0,\n        proj_drop: float = 0.0,\n    ) -&gt; None:\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim**-0.5\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x: torch.Tensor) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Forward pass through the multi-head self-attention module.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor of shape ``(batch_sz, seq_len, dim)``.\n\n        Returns\n        -------\n        tuple[torch.Tensor, torch.Tensor]\n            The output tensor and the attention weights.\n        \"\"\"\n        b, n, c = x.shape\n        qkv = (\n            self.qkv(x)\n            .reshape(b, n, 3, self.num_heads, c // self.num_heads)\n            .permute(2, 0, 3, 1, 4)\n        )\n        q, k, v = qkv[0], qkv[1], qkv[2]\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(b, n, c)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x, attn\n</code></pre> <code></code> forward \u00b6 <pre><code>forward(x)\n</code></pre> <p>Forward pass through the multi-head self-attention module.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape <code>(batch_sz, seq_len, dim)</code>.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>The output tensor and the attention weights.</p> Source code in <code>mmlearn/modules/layers/attention.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Forward pass through the multi-head self-attention module.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape ``(batch_sz, seq_len, dim)``.\n\n    Returns\n    -------\n    tuple[torch.Tensor, torch.Tensor]\n        The output tensor and the attention weights.\n    \"\"\"\n    b, n, c = x.shape\n    qkv = (\n        self.qkv(x)\n        .reshape(b, n, 3, self.num_heads, c // self.num_heads)\n        .permute(2, 0, 3, 1, 4)\n    )\n    q, k, v = qkv[0], qkv[1], qkv[2]\n\n    attn = (q @ k.transpose(-2, -1)) * self.scale\n    attn = attn.softmax(dim=-1)\n    attn = self.attn_drop(attn)\n\n    x = (attn @ v).transpose(1, 2).reshape(b, n, c)\n    x = self.proj(x)\n    x = self.proj_drop(x)\n    return x, attn\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.embedding","title":"embedding","text":"<p>Embedding layers.</p>"},{"location":"api/#mmlearn.modules.layers.embedding.PatchEmbed","title":"PatchEmbed","text":"<p>               Bases: <code>Module</code></p> <p>Image to Patch Embedding.</p> <p>This module divides an image into patches and embeds them as a sequence of vectors.</p> <p>Parameters:</p> Name Type Description Default <code>img_size</code> <code>int</code> <p>Size of the input image (assumed to be square).</p> <code>224</code> <code>patch_size</code> <code>int</code> <p>Size of each image patch (assumed to be square).</p> <code>16</code> <code>in_chans</code> <code>int</code> <p>Number of input channels in the image.</p> <code>3</code> <code>embed_dim</code> <code>int</code> <p>Dimension of the output embeddings.</p> <code>768</code> Source code in <code>mmlearn/modules/layers/embedding.py</code> <pre><code>class PatchEmbed(nn.Module):\n    \"\"\"Image to Patch Embedding.\n\n    This module divides an image into patches and embeds them as a sequence of vectors.\n\n    Parameters\n    ----------\n    img_size : int, optional, default=224\n        Size of the input image (assumed to be square).\n    patch_size : int, optional, default=16\n        Size of each image patch (assumed to be square).\n    in_chans : int, optional, default=3\n        Number of input channels in the image.\n    embed_dim : int, optional, default=768\n        Dimension of the output embeddings.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        img_size: int = 224,\n        patch_size: int = 16,\n        in_chans: int = 3,\n        embed_dim: int = 768,\n    ) -&gt; None:\n        super().__init__()\n        num_patches = (img_size // patch_size) * (img_size // patch_size)\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n\n        self.proj = nn.Conv2d(\n            in_chans, embed_dim, kernel_size=patch_size, stride=patch_size\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass to convert an image into patch embeddings.\"\"\"\n        return self.proj(x).flatten(2).transpose(1, 2)\n</code></pre> <code></code> forward \u00b6 <pre><code>forward(x)\n</code></pre> <p>Forward pass to convert an image into patch embeddings.</p> Source code in <code>mmlearn/modules/layers/embedding.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass to convert an image into patch embeddings.\"\"\"\n    return self.proj(x).flatten(2).transpose(1, 2)\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.embedding.ConvEmbed","title":"ConvEmbed","text":"<p>               Bases: <code>Module</code></p> <p>3x3 Convolution stems for ViT following ViTC models.</p> <p>This module builds convolutional stems for Vision Transformers (ViT) with intermediate batch normalization and ReLU activation.</p> <p>Parameters:</p> Name Type Description Default <code>channels</code> <code>list[int]</code> <p>list of channel sizes for each convolution layer.</p> required <code>strides</code> <code>list[int]</code> <p>list of stride sizes for each convolution layer.</p> required <code>img_size</code> <code>int</code> <p>Size of the input image (assumed to be square).</p> <code>224</code> <code>in_chans</code> <code>int</code> <p>Number of input channels in the image.</p> <code>3</code> <code>batch_norm</code> <code>bool</code> <p>Whether to include batch normalization after each convolution layer.</p> <code>True</code> Source code in <code>mmlearn/modules/layers/embedding.py</code> <pre><code>class ConvEmbed(nn.Module):\n    \"\"\"3x3 Convolution stems for ViT following ViTC models.\n\n    This module builds convolutional stems for Vision Transformers (ViT)\n    with intermediate batch normalization and ReLU activation.\n\n    Parameters\n    ----------\n    channels : list[int]\n        list of channel sizes for each convolution layer.\n    strides : list[int]\n        list of stride sizes for each convolution layer.\n    img_size : int, optional, default=224\n        Size of the input image (assumed to be square).\n    in_chans : int, optional, default=3\n        Number of input channels in the image.\n    batch_norm : bool, optional, default=True\n        Whether to include batch normalization after each convolution layer.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        channels: list[int],\n        strides: list[int],\n        img_size: int = 224,\n        in_chans: int = 3,\n        batch_norm: bool = True,\n    ) -&gt; None:\n        super().__init__()\n        # Build the stems\n        stem = []\n        channels = [in_chans] + channels\n        for i in range(len(channels) - 2):\n            stem += [\n                nn.Conv2d(\n                    channels[i],\n                    channels[i + 1],\n                    kernel_size=3,\n                    stride=strides[i],\n                    padding=1,\n                    bias=(not batch_norm),\n                )\n            ]\n            if batch_norm:\n                stem += [nn.BatchNorm2d(channels[i + 1])]\n            stem += [nn.ReLU(inplace=True)]\n        stem += [\n            nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])\n        ]\n        self.stem = nn.Sequential(*stem)\n\n        # Compute the number of patches\n        stride_prod = int(np.prod(strides))\n        self.num_patches = (img_size // stride_prod) ** 2\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass through the convolutional embedding layers.\"\"\"\n        p = self.stem(x)\n        return p.flatten(2).transpose(1, 2)\n</code></pre> <code></code> forward \u00b6 <pre><code>forward(x)\n</code></pre> <p>Forward pass through the convolutional embedding layers.</p> Source code in <code>mmlearn/modules/layers/embedding.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass through the convolutional embedding layers.\"\"\"\n    p = self.stem(x)\n    return p.flatten(2).transpose(1, 2)\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.embedding.get_2d_sincos_pos_embed","title":"get_2d_sincos_pos_embed","text":"<pre><code>get_2d_sincos_pos_embed(\n    embed_dim, grid_size, cls_token=False\n)\n</code></pre> <p>Generate 2D sine-cosine positional embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>embed_dim</code> <code>int</code> <p>The dimension of the embeddings.</p> required <code>grid_size</code> <code>int</code> <p>The size of the grid (both height and width).</p> required <code>cls_token</code> <code>bool</code> <p>Whether to include a class token in the embeddings.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>pos_embed</code> <code>ndarray</code> <p>Positional embeddings with shape [grid_sizegrid_size, embed_dim] or [1 + grid_sizegrid_size, embed_dim] if cls_token is True.</p> Source code in <code>mmlearn/modules/layers/embedding.py</code> <pre><code>def get_2d_sincos_pos_embed(\n    embed_dim: int, grid_size: int, cls_token: bool = False\n) -&gt; np.ndarray:\n    \"\"\"\n    Generate 2D sine-cosine positional embeddings.\n\n    Parameters\n    ----------\n    embed_dim : int\n        The dimension of the embeddings.\n    grid_size : int\n        The size of the grid (both height and width).\n    cls_token : bool, optional, default=False\n        Whether to include a class token in the embeddings.\n\n    Returns\n    -------\n    pos_embed : np.ndarray\n        Positional embeddings with shape [grid_size*grid_size, embed_dim] or\n        [1 + grid_size*grid_size, embed_dim] if cls_token is True.\n    \"\"\"\n    grid_h = np.arange(grid_size, dtype=float)\n    grid_w = np.arange(grid_size, dtype=float)\n    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n    grid = np.stack(grid, axis=0)\n\n    grid = grid.reshape([2, 1, grid_size, grid_size])\n    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n    if cls_token:\n        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n    return pos_embed\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.embedding.get_2d_sincos_pos_embed_from_grid","title":"get_2d_sincos_pos_embed_from_grid","text":"<pre><code>get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n</code></pre> <p>Generate 2D sine-cosine positional embeddings from a grid.</p> <p>Parameters:</p> Name Type Description Default <code>embed_dim</code> <code>int</code> <p>The dimension of the embeddings.</p> required <code>grid</code> <code>ndarray</code> <p>The grid of positions with shape [2, 1, grid_size, grid_size].</p> required <p>Returns:</p> Name Type Description <code>emb</code> <code>ndarray</code> <p>Positional embeddings with shape [grid_size*grid_size, embed_dim].</p> Source code in <code>mmlearn/modules/layers/embedding.py</code> <pre><code>def get_2d_sincos_pos_embed_from_grid(embed_dim: int, grid: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Generate 2D sine-cosine positional embeddings from a grid.\n\n    Parameters\n    ----------\n    embed_dim : int\n        The dimension of the embeddings.\n    grid : np.ndarray\n        The grid of positions with shape [2, 1, grid_size, grid_size].\n\n    Returns\n    -------\n    emb : np.ndarray\n        Positional embeddings with shape [grid_size*grid_size, embed_dim].\n    \"\"\"\n    assert embed_dim % 2 == 0\n\n    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n\n    return np.concatenate([emb_h, emb_w], axis=1)\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.embedding.get_1d_sincos_pos_embed","title":"get_1d_sincos_pos_embed","text":"<pre><code>get_1d_sincos_pos_embed(\n    embed_dim, grid_size, cls_token=False\n)\n</code></pre> <p>Generate 1D sine-cosine positional embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>embed_dim</code> <code>int</code> <p>The dimension of the embeddings.</p> required <code>grid_size</code> <code>int</code> <p>The size of the grid.</p> required <code>cls_token</code> <code>bool</code> <p>Whether to include a class token in the embeddings.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>pos_embed</code> <code>ndarray</code> <p>Positional embeddings with shape [grid_size, embed_dim] or [1 + grid_size, embed_dim] if cls_token is True.</p> Source code in <code>mmlearn/modules/layers/embedding.py</code> <pre><code>def get_1d_sincos_pos_embed(\n    embed_dim: int, grid_size: int, cls_token: bool = False\n) -&gt; np.ndarray:\n    \"\"\"\n    Generate 1D sine-cosine positional embeddings.\n\n    Parameters\n    ----------\n    embed_dim : int\n        The dimension of the embeddings.\n    grid_size : int\n        The size of the grid.\n    cls_token : bool, optional, default=False\n        Whether to include a class token in the embeddings.\n\n    Returns\n    -------\n    pos_embed : np.ndarray\n        Positional embeddings with shape [grid_size, embed_dim] or\n        [1 + grid_size, embed_dim] if cls_token is True.\n    \"\"\"\n    grid = np.arange(grid_size, dtype=float)\n    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n    if cls_token:\n        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n    return pos_embed\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.embedding.get_1d_sincos_pos_embed_from_grid","title":"get_1d_sincos_pos_embed_from_grid","text":"<pre><code>get_1d_sincos_pos_embed_from_grid(embed_dim, pos)\n</code></pre> <p>Generate 1D sine-cosine positional embeddings from a grid.</p> <p>Parameters:</p> Name Type Description Default <code>embed_dim</code> <code>int</code> <p>The dimension of the embeddings.</p> required <code>pos</code> <code>ndarray</code> <p>A list of positions to be encoded, with shape [M,].</p> required <p>Returns:</p> Name Type Description <code>emb</code> <code>ndarray</code> <p>Positional embeddings with shape [M, embed_dim].</p> Source code in <code>mmlearn/modules/layers/embedding.py</code> <pre><code>def get_1d_sincos_pos_embed_from_grid(embed_dim: int, pos: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Generate 1D sine-cosine positional embeddings from a grid.\n\n    Parameters\n    ----------\n    embed_dim : int\n        The dimension of the embeddings.\n    pos : np.ndarray\n        A list of positions to be encoded, with shape [M,].\n\n    Returns\n    -------\n    emb : np.ndarray\n        Positional embeddings with shape [M, embed_dim].\n    \"\"\"\n    assert embed_dim % 2 == 0\n    omega = np.arange(embed_dim // 2, dtype=float)\n    omega /= embed_dim / 2.0\n    omega = 1.0 / 10000**omega  # (D/2,)\n\n    pos = pos.reshape(-1)  # (M,)\n    out = np.einsum(\"m,d-&gt;md\", pos, omega)  # (M, D/2), outer product\n\n    emb_sin = np.sin(out)  # (M, D/2)\n    emb_cos = np.cos(out)  # (M, D/2)\n\n    return np.concatenate([emb_sin, emb_cos], axis=1)\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.logit_scaling","title":"logit_scaling","text":"<p>Learnable logit scaling layer.</p>"},{"location":"api/#mmlearn.modules.layers.logit_scaling.LearnableLogitScaling","title":"LearnableLogitScaling","text":"<p>               Bases: <code>Module</code></p> <p>Logit scaling layer.</p> <p>Parameters:</p> Name Type Description Default <code>init_logit_scale</code> <code>float</code> <p>Initial value of the logit scale.</p> <code>1/0.07</code> <code>learnable</code> <code>bool</code> <p>If True, the logit scale is learnable. Otherwise, it is fixed.</p> <code>True</code> <code>max_logit_scale</code> <code>float</code> <p>Maximum value of the logit scale.</p> <code>100</code> Source code in <code>mmlearn/modules/layers/logit_scaling.py</code> <pre><code>@store(group=\"modules/layers\", provider=\"mmlearn\")\nclass LearnableLogitScaling(torch.nn.Module):\n    \"\"\"Logit scaling layer.\n\n    Parameters\n    ----------\n    init_logit_scale : float, optional, default=1/0.07\n        Initial value of the logit scale.\n    learnable : bool, optional, default=True\n        If True, the logit scale is learnable. Otherwise, it is fixed.\n    max_logit_scale : float, optional, default=100\n        Maximum value of the logit scale.\n    \"\"\"\n\n    def __init__(\n        self,\n        init_logit_scale: float = 1 / 0.07,\n        max_logit_scale: float = 100,\n        learnable: bool = True,\n    ) -&gt; None:\n        super().__init__()\n        self.max_logit_scale = max_logit_scale\n        self.init_logit_scale = init_logit_scale\n        self.learnable = learnable\n        log_logit_scale = torch.ones([]) * np.log(self.init_logit_scale)\n        if learnable:\n            self.log_logit_scale = torch.nn.Parameter(log_logit_scale)\n        else:\n            self.register_buffer(\"log_logit_scale\", log_logit_scale)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Apply the logit scaling to the input tensor.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor of shape ``(batch_sz, seq_len, dim)``.\n        \"\"\"\n        return torch.clip(self.log_logit_scale.exp(), max=self.max_logit_scale) * x\n\n    def extra_repr(self) -&gt; str:\n        \"\"\"Return the string representation of the layer.\"\"\"\n        return (\n            f\"logit_scale_init={self.init_logit_scale},learnable={self.learnable},\"\n            f\" max_logit_scale={self.max_logit_scale}\"\n        )\n</code></pre> <code></code> forward \u00b6 <pre><code>forward(x)\n</code></pre> <p>Apply the logit scaling to the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape <code>(batch_sz, seq_len, dim)</code>.</p> required Source code in <code>mmlearn/modules/layers/logit_scaling.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Apply the logit scaling to the input tensor.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape ``(batch_sz, seq_len, dim)``.\n    \"\"\"\n    return torch.clip(self.log_logit_scale.exp(), max=self.max_logit_scale) * x\n</code></pre> <code></code> extra_repr \u00b6 <pre><code>extra_repr()\n</code></pre> <p>Return the string representation of the layer.</p> Source code in <code>mmlearn/modules/layers/logit_scaling.py</code> <pre><code>def extra_repr(self) -&gt; str:\n    \"\"\"Return the string representation of the layer.\"\"\"\n    return (\n        f\"logit_scale_init={self.init_logit_scale},learnable={self.learnable},\"\n        f\" max_logit_scale={self.max_logit_scale}\"\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.mlp","title":"mlp","text":"<p>Multi-layer perceptron (MLP).</p>"},{"location":"api/#mmlearn.modules.layers.mlp.MLP","title":"MLP","text":"<p>               Bases: <code>Sequential</code></p> <p>Multi-layer perceptron (MLP).</p> <p>This module will create a block of <code>Linear -&gt; Normalization -&gt; Activation -&gt; Dropout</code> layers.</p> <p>Parameters:</p> Name Type Description Default <code>in_dim</code> <code>int</code> <p>The input dimension.</p> required <code>out_dim</code> <code>Optional[int]</code> <p>The output dimension. If not specified, it is set to :attr:<code>in_dim</code>.</p> <code>None</code> <code>hidden_dims</code> <code>Optional[list]</code> <p>The dimensions of the hidden layers. The length of the list determines the number of hidden layers. This parameter is mutually exclusive with :attr:<code>hidden_dims_multiplier</code>.</p> <code>None</code> <code>hidden_dims_multiplier</code> <code>Optional[list]</code> <p>The multipliers to apply to the input dimension to get the dimensions of the hidden layers. The length of the list determines the number of hidden layers. The multipliers will be used to get the dimensions of the hidden layers. This parameter is mutually exclusive with <code>hidden_dims</code>.</p> <code>None</code> <code>apply_multiplier_to_in_dim</code> <code>bool</code> <p>Whether to apply the :attr:<code>hidden_dims_multiplier</code> to :attr:<code>in_dim</code> to get the dimensions of the hidden layers. If <code>False</code>, the multipliers will be applied to the dimensions of the previous hidden layer, starting from :attr:<code>in_dim</code>. This parameter is only relevant when :attr:<code>hidden_dims_multiplier</code> is specified.</p> <code>False</code> <code>norm_layer</code> <code>Optional[Callable[..., Module]]</code> <p>The normalization layer to use. If not specified, no normalization is used. Partial functions can be used to specify the normalization layer with specific parameters.</p> <code>None</code> <code>activation_layer</code> <code>Optional[Callable[..., Module]]</code> <p>The activation layer to use. If not specified, ReLU is used. Partial functions can be used to specify the activation layer with specific parameters.</p> <code>torch.nn.ReLU</code> <code>bias</code> <code>Union[bool, list[bool]]</code> <p>Whether to use bias in the linear layers.</p> <code>True</code> <code>dropout</code> <code>Union[float, list[float]]</code> <p>The dropout probability to use.</p> <code>0.0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If both :attr:<code>hidden_dims</code> and :attr:<code>hidden_dims_multiplier</code> are specified or if the lengths of :attr:<code>bias</code> and :attr:<code>hidden_dims</code> do not match or if the lengths of :attr:<code>dropout</code> and :attr:<code>hidden_dims</code> do not match.</p> Source code in <code>mmlearn/modules/layers/mlp.py</code> <pre><code>@store(group=\"modules/layers\", provider=\"mmlearn\")\nclass MLP(torch.nn.Sequential):\n    \"\"\"Multi-layer perceptron (MLP).\n\n    This module will create a block of ``Linear -&gt; Normalization -&gt; Activation -&gt; Dropout``\n    layers.\n\n    Parameters\n    ----------\n    in_dim : int\n        The input dimension.\n    out_dim : Optional[int], optional, default=None\n        The output dimension. If not specified, it is set to :attr:`in_dim`.\n    hidden_dims : Optional[list], optional, default=None\n        The dimensions of the hidden layers. The length of the list determines the\n        number of hidden layers. This parameter is mutually exclusive with\n        :attr:`hidden_dims_multiplier`.\n    hidden_dims_multiplier : Optional[list], optional, default=None\n        The multipliers to apply to the input dimension to get the dimensions of\n        the hidden layers. The length of the list determines the number of hidden\n        layers. The multipliers will be used to get the dimensions of the hidden\n        layers. This parameter is mutually exclusive with `hidden_dims`.\n    apply_multiplier_to_in_dim : bool, optional, default=False\n        Whether to apply the :attr:`hidden_dims_multiplier` to :attr:`in_dim` to get the\n        dimensions of the hidden layers. If ``False``, the multipliers will be applied\n        to the dimensions of the previous hidden layer, starting from :attr:`in_dim`.\n        This parameter is only relevant when :attr:`hidden_dims_multiplier` is\n        specified.\n    norm_layer : Optional[Callable[..., torch.nn.Module]], optional, default=None\n        The normalization layer to use. If not specified, no normalization is used.\n        Partial functions can be used to specify the normalization layer with specific\n        parameters.\n    activation_layer : Optional[Callable[..., torch.nn.Module]], optional, default=torch.nn.ReLU\n        The activation layer to use. If not specified, ReLU is used. Partial functions\n        can be used to specify the activation layer with specific parameters.\n    bias : Union[bool, list[bool]], optional, default=True\n        Whether to use bias in the linear layers.\n    dropout : Union[float, list[float]], optional, default=0.0\n        The dropout probability to use.\n\n    Raises\n    ------\n    ValueError\n        If both :attr:`hidden_dims` and :attr:`hidden_dims_multiplier` are specified\n        or if the lengths of :attr:`bias` and :attr:`hidden_dims` do not match or if\n        the lengths of :attr:`dropout` and :attr:`hidden_dims` do not match.\n\n    \"\"\"  # noqa: W505\n\n    def __init__(  # noqa: PLR0912\n        self,\n        in_dim: int,\n        out_dim: Optional[int] = None,\n        hidden_dims: Optional[list[int]] = None,\n        hidden_dims_multiplier: Optional[list[float]] = None,\n        apply_multiplier_to_in_dim: bool = False,\n        norm_layer: Optional[Callable[..., torch.nn.Module]] = None,\n        activation_layer: Optional[Callable[..., torch.nn.Module]] = torch.nn.ReLU,\n        bias: Union[bool, list[bool]] = True,\n        dropout: Union[float, list[float]] = 0.0,\n    ) -&gt; None:\n        if hidden_dims is None and hidden_dims_multiplier is None:\n            hidden_dims = []\n        if hidden_dims is not None and hidden_dims_multiplier is not None:\n            raise ValueError(\n                \"Only one of `hidden_dims` or `hidden_dims_multiplier` must be specified.\"\n            )\n\n        if hidden_dims is None and hidden_dims_multiplier is not None:\n            if apply_multiplier_to_in_dim:\n                hidden_dims = [\n                    int(in_dim * multiplier) for multiplier in hidden_dims_multiplier\n                ]\n            else:\n                hidden_dims = [int(in_dim * hidden_dims_multiplier[0])]\n                for multiplier in hidden_dims_multiplier[1:]:\n                    hidden_dims.append(int(hidden_dims[-1] * multiplier))\n\n        if isinstance(bias, bool):\n            bias_list: list[bool] = [bias] * (len(hidden_dims) + 1)  # type: ignore[arg-type]\n        else:\n            bias_list = bias\n        if len(bias_list) != len(hidden_dims) + 1:  # type: ignore[arg-type]\n            raise ValueError(\n                \"Expected `bias` to be a boolean or a list of booleans with length \"\n                \"equal to the number of linear layers in the MLP.\"\n            )\n\n        if isinstance(dropout, float):\n            dropout_list: list[float] = [dropout] * (len(hidden_dims) + 1)  # type: ignore[arg-type]\n        else:\n            dropout_list = dropout\n        if len(dropout_list) != len(hidden_dims) + 1:  # type: ignore[arg-type]\n            raise ValueError(\n                \"Expected `dropout` to be a float or a list of floats with length \"\n                \"equal to the number of linear layers in the MLP.\"\n            )\n\n        # construct list of dimensions for the layers\n        dims = [in_dim] + hidden_dims  # type: ignore[operator]\n        layers = []\n        for layer_idx, (in_features, hidden_features) in enumerate(\n            zip(dims[:-1], dims[1:], strict=False)\n        ):\n            layers.append(\n                torch.nn.Linear(in_features, hidden_features, bias=bias_list[layer_idx])\n            )\n            if norm_layer is not None:\n                layers.append(norm_layer(hidden_features))\n            if activation_layer is not None:\n                layers.append(activation_layer())\n            layers.append(torch.nn.Dropout(dropout_list[layer_idx]))\n\n        out_dim = out_dim or in_dim\n\n        layers.append(torch.nn.Linear(dims[-1], out_dim, bias=bias_list[-1]))\n        layers.append(torch.nn.Dropout(dropout_list[-1]))\n\n        super().__init__(*layers)\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.normalization","title":"normalization","text":"<p>Normalization layers.</p>"},{"location":"api/#mmlearn.modules.layers.normalization.L2Norm","title":"L2Norm","text":"<p>               Bases: <code>Module</code></p> <p>L2 normalization.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>The dimension along which to normalize.</p> required Source code in <code>mmlearn/modules/layers/normalization.py</code> <pre><code>@store(group=\"modules/layers\", provider=\"mmlearn\")\nclass L2Norm(torch.nn.Module):\n    \"\"\"L2 normalization.\n\n    Parameters\n    ----------\n    dim : int\n        The dimension along which to normalize.\n    \"\"\"\n\n    def __init__(self, dim: int) -&gt; None:\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Apply L2 normalization to the input tensor.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor of shape ``(batch_sz, seq_len, dim)``.\n\n        Returns\n        -------\n        torch.Tensor\n            Normalized tensor of shape ``(batch_sz, seq_len, dim)``.\n        \"\"\"\n        return torch.nn.functional.normalize(x, dim=self.dim, p=2)\n</code></pre> <code></code> forward \u00b6 <pre><code>forward(x)\n</code></pre> <p>Apply L2 normalization to the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape <code>(batch_sz, seq_len, dim)</code>.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized tensor of shape <code>(batch_sz, seq_len, dim)</code>.</p> Source code in <code>mmlearn/modules/layers/normalization.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Apply L2 normalization to the input tensor.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape ``(batch_sz, seq_len, dim)``.\n\n    Returns\n    -------\n    torch.Tensor\n        Normalized tensor of shape ``(batch_sz, seq_len, dim)``.\n    \"\"\"\n    return torch.nn.functional.normalize(x, dim=self.dim, p=2)\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.patch_dropout","title":"patch_dropout","text":"<p>Patch dropout layer.</p>"},{"location":"api/#mmlearn.modules.layers.patch_dropout.PatchDropout","title":"PatchDropout","text":"<p>               Bases: <code>Module</code></p> <p>Patch dropout layer.</p> <p>Drops patch tokens (after embedding and adding CLS token) from the input tensor. Usually used in vision transformers to reduce the number of tokens. [1]_</p> <p>Parameters:</p> Name Type Description Default <code>keep_rate</code> <code>float</code> <p>The proportion of tokens to keep.</p> <code>0.5</code> <code>bias</code> <code>Optional[float]</code> <p>The bias to add to the random noise before sorting.</p> <code>None</code> <code>token_shuffling</code> <code>bool</code> <p>If True, the tokens are shuffled.</p> <code>False</code> References <p>.. [1] Liu, Y., Matsoukas, C., Strand, F., Azizpour, H., &amp; Smith, K. (2023).    Patchdropout: Economizing vision transformers using patch dropout. In Proceedings    of the IEEE/CVF Winter Conference on Applications of Computer Vision    (pp. 3953-3962).</p> Source code in <code>mmlearn/modules/layers/patch_dropout.py</code> <pre><code>class PatchDropout(torch.nn.Module):\n    \"\"\"Patch dropout layer.\n\n    Drops patch tokens (after embedding and adding CLS token) from the input tensor.\n    Usually used in vision transformers to reduce the number of tokens. [1]_\n\n    Parameters\n    ----------\n    keep_rate : float, optional, default=0.5\n        The proportion of tokens to keep.\n    bias : Optional[float], optional, default=None\n        The bias to add to the random noise before sorting.\n    token_shuffling : bool, optional, default=False\n        If True, the tokens are shuffled.\n\n    References\n    ----------\n    .. [1] Liu, Y., Matsoukas, C., Strand, F., Azizpour, H., &amp; Smith, K. (2023).\n       Patchdropout: Economizing vision transformers using patch dropout. In Proceedings\n       of the IEEE/CVF Winter Conference on Applications of Computer Vision\n       (pp. 3953-3962).\n    \"\"\"\n\n    def __init__(\n        self,\n        keep_rate: float = 0.5,\n        bias: Optional[float] = None,\n        token_shuffling: bool = False,\n    ):\n        super().__init__()\n        assert 0 &lt; keep_rate &lt;= 1, \"The keep_rate must be in (0,1]\"\n\n        self.bias = bias\n        self.keep_rate = keep_rate\n        self.token_shuffling = token_shuffling\n\n    def forward(self, x: torch.Tensor, force_drop: bool = False) -&gt; torch.Tensor:\n        \"\"\"Drop tokens from the input tensor.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor of shape ``(batch_sz, seq_len, dim)``.\n        force_drop : bool, optional, default=False\n            If True, the tokens are always dropped, even when the model is in\n            evaluation mode.\n\n        Returns\n        -------\n        torch.Tensor\n            Tensor of shape ``(batch_sz, keep_len, dim)`` containing the kept tokens.\n        \"\"\"\n        if (not self.training and not force_drop) or self.keep_rate == 1:\n            return x\n\n        batch_sz, _, dim = x.shape\n\n        cls_mask = torch.zeros(  # assumes that CLS is always the 1st element\n            batch_sz, 1, dtype=torch.int64, device=x.device\n        )\n        patch_mask = self.uniform_mask(x)\n        patch_mask = torch.hstack([cls_mask, patch_mask])\n\n        return torch.gather(x, dim=1, index=patch_mask.unsqueeze(-1).repeat(1, 1, dim))\n\n    def uniform_mask(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Generate token ids to keep from uniform random distribution.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor of shape ``(batch_sz, seq_len, dim)``.\n\n        Returns\n        -------\n        torch.Tensor\n            Tensor of shape ``(batch_sz, keep_len)`` containing the token ids to keep.\n\n        \"\"\"\n        batch_sz, seq_len, _ = x.shape\n        seq_len = seq_len - 1  # patch length (without CLS)\n\n        keep_len = int(seq_len * self.keep_rate)\n        noise = torch.rand(batch_sz, seq_len, device=x.device)\n        if self.bias is not None:\n            noise += self.bias\n        ids = torch.argsort(noise, dim=1)\n        keep_ids = ids[:, :keep_len]\n        if not self.token_shuffling:\n            keep_ids = keep_ids.sort(1)[0]\n        return keep_ids\n</code></pre> <code></code> forward \u00b6 <pre><code>forward(x, force_drop=False)\n</code></pre> <p>Drop tokens from the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape <code>(batch_sz, seq_len, dim)</code>.</p> required <code>force_drop</code> <code>bool</code> <p>If True, the tokens are always dropped, even when the model is in evaluation mode.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of shape <code>(batch_sz, keep_len, dim)</code> containing the kept tokens.</p> Source code in <code>mmlearn/modules/layers/patch_dropout.py</code> <pre><code>def forward(self, x: torch.Tensor, force_drop: bool = False) -&gt; torch.Tensor:\n    \"\"\"Drop tokens from the input tensor.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape ``(batch_sz, seq_len, dim)``.\n    force_drop : bool, optional, default=False\n        If True, the tokens are always dropped, even when the model is in\n        evaluation mode.\n\n    Returns\n    -------\n    torch.Tensor\n        Tensor of shape ``(batch_sz, keep_len, dim)`` containing the kept tokens.\n    \"\"\"\n    if (not self.training and not force_drop) or self.keep_rate == 1:\n        return x\n\n    batch_sz, _, dim = x.shape\n\n    cls_mask = torch.zeros(  # assumes that CLS is always the 1st element\n        batch_sz, 1, dtype=torch.int64, device=x.device\n    )\n    patch_mask = self.uniform_mask(x)\n    patch_mask = torch.hstack([cls_mask, patch_mask])\n\n    return torch.gather(x, dim=1, index=patch_mask.unsqueeze(-1).repeat(1, 1, dim))\n</code></pre> <code></code> uniform_mask \u00b6 <pre><code>uniform_mask(x)\n</code></pre> <p>Generate token ids to keep from uniform random distribution.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape <code>(batch_sz, seq_len, dim)</code>.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of shape <code>(batch_sz, keep_len)</code> containing the token ids to keep.</p> Source code in <code>mmlearn/modules/layers/patch_dropout.py</code> <pre><code>def uniform_mask(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Generate token ids to keep from uniform random distribution.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape ``(batch_sz, seq_len, dim)``.\n\n    Returns\n    -------\n    torch.Tensor\n        Tensor of shape ``(batch_sz, keep_len)`` containing the token ids to keep.\n\n    \"\"\"\n    batch_sz, seq_len, _ = x.shape\n    seq_len = seq_len - 1  # patch length (without CLS)\n\n    keep_len = int(seq_len * self.keep_rate)\n    noise = torch.rand(batch_sz, seq_len, device=x.device)\n    if self.bias is not None:\n        noise += self.bias\n    ids = torch.argsort(noise, dim=1)\n    keep_ids = ids[:, :keep_len]\n    if not self.token_shuffling:\n        keep_ids = keep_ids.sort(1)[0]\n    return keep_ids\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.transformer_block","title":"transformer_block","text":"<p>Transformer Block and Embedding Modules for Vision Transformers (ViT).</p>"},{"location":"api/#mmlearn.modules.layers.transformer_block.DropPath","title":"DropPath","text":"<p>               Bases: <code>Module</code></p> <p>Drop paths (Stochastic Depth) per sample.</p> <p>Parameters:</p> Name Type Description Default <code>drop_prob</code> <code>float</code> <p>Probability of dropping paths.</p> <code>0.0</code> Source code in <code>mmlearn/modules/layers/transformer_block.py</code> <pre><code>class DropPath(nn.Module):\n    \"\"\"Drop paths (Stochastic Depth) per sample.\n\n    Parameters\n    ----------\n    drop_prob : float, optional, default=0.0\n        Probability of dropping paths.\n    \"\"\"\n\n    def __init__(self, drop_prob: float = 0.0) -&gt; None:\n        super().__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass through DropPath module.\"\"\"\n        return drop_path(x, self.drop_prob, self.training)\n</code></pre> <code></code> forward \u00b6 <pre><code>forward(x)\n</code></pre> <p>Forward pass through DropPath module.</p> Source code in <code>mmlearn/modules/layers/transformer_block.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass through DropPath module.\"\"\"\n    return drop_path(x, self.drop_prob, self.training)\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.transformer_block.Block","title":"Block","text":"<p>               Bases: <code>Module</code></p> <p>Transformer Block.</p> <p>This module represents a Transformer block that includes self-attention, normalization layers, and a feedforward multi-layer perceptron (MLP) network.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>The input and output dimension of the block.</p> required <code>num_heads</code> <code>int</code> <p>Number of attention heads.</p> required <code>mlp_ratio</code> <code>float</code> <p>Ratio of hidden dimension to the input dimension in the MLP.</p> <code>4.0</code> <code>qkv_bias</code> <code>bool</code> <p>If True, add a learnable bias to the query, key, value projections.</p> <code>False</code> <code>qk_scale</code> <code>Optional[float]</code> <p>Override default qk scale of head_dim ** -0.5 if set.</p> <code>None</code> <code>drop</code> <code>float</code> <p>Dropout probability for the output of attention and MLP layers.</p> <code>0.0</code> <code>attn_drop</code> <code>float</code> <p>Dropout probability for the attention scores.</p> <code>0.0</code> <code>drop_path</code> <code>float</code> <p>Stochastic depth rate, a form of layer dropout.</p> <code>0.0</code> <code>act_layer</code> <code>Callable[..., Module]</code> <p>Activation layer in the MLP.</p> <code>nn.GELU</code> <code>norm_layer</code> <code>Callable[..., Module]</code> <p>Normalization layer.</p> <code>torch.nn.LayerNorm</code> Source code in <code>mmlearn/modules/layers/transformer_block.py</code> <pre><code>class Block(nn.Module):\n    \"\"\"Transformer Block.\n\n    This module represents a Transformer block that includes self-attention,\n    normalization layers, and a feedforward multi-layer perceptron (MLP) network.\n\n    Parameters\n    ----------\n    dim : int\n        The input and output dimension of the block.\n    num_heads : int\n        Number of attention heads.\n    mlp_ratio : float, optional, default=4.0\n        Ratio of hidden dimension to the input dimension in the MLP.\n    qkv_bias : bool, optional, default=False\n        If True, add a learnable bias to the query, key, value projections.\n    qk_scale : Optional[float], optional, default=None\n        Override default qk scale of head_dim ** -0.5 if set.\n    drop : float, optional, default=0.0\n        Dropout probability for the output of attention and MLP layers.\n    attn_drop : float, optional, default=0.0\n        Dropout probability for the attention scores.\n    drop_path : float, optional, default=0.0\n        Stochastic depth rate, a form of layer dropout.\n    act_layer : Callable[..., torch.nn.Module], optional, default=nn.GELU\n        Activation layer in the MLP.\n    norm_layer : Callable[..., torch.nn.Module], optional, default=torch.nn.LayerNorm\n        Normalization layer.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int,\n        mlp_ratio: float = 4.0,\n        qkv_bias: bool = False,\n        qk_scale: Optional[float] = None,\n        drop: float = 0.0,\n        attn_drop: float = 0.0,\n        drop_path: float = 0.0,\n        act_layer: Callable[..., nn.Module] = nn.GELU,\n        norm_layer: Callable[..., nn.Module] = nn.LayerNorm,\n    ) -&gt; None:\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(\n            dim,\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            qk_scale=qk_scale,\n            attn_drop=attn_drop,\n            proj_drop=drop,\n        )\n        self.drop_path = DropPath(drop_path) if drop_path &gt; 0.0 else nn.Identity()\n        self.norm2 = norm_layer(dim)\n\n        self.mlp = MLP(\n            in_dim=dim,\n            hidden_dims_multiplier=[mlp_ratio],\n            activation_layer=act_layer,\n            bias=True,\n            dropout=drop,\n        )\n\n    def forward(\n        self, x: torch.Tensor, return_attention: bool = False\n    ) -&gt; Union[torch.Tensor, torch.Tensor]:\n        \"\"\"Forward pass through the Transformer Block.\"\"\"\n        y, attn = self.attn(self.norm1(x))\n        if return_attention:\n            return attn\n        x = x + self.drop_path(y)\n        return x + self.drop_path(self.mlp(self.norm2(x)))\n</code></pre> <code></code> forward \u00b6 <pre><code>forward(x, return_attention=False)\n</code></pre> <p>Forward pass through the Transformer Block.</p> Source code in <code>mmlearn/modules/layers/transformer_block.py</code> <pre><code>def forward(\n    self, x: torch.Tensor, return_attention: bool = False\n) -&gt; Union[torch.Tensor, torch.Tensor]:\n    \"\"\"Forward pass through the Transformer Block.\"\"\"\n    y, attn = self.attn(self.norm1(x))\n    if return_attention:\n        return attn\n    x = x + self.drop_path(y)\n    return x + self.drop_path(self.mlp(self.norm2(x)))\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.transformer_block.drop_path","title":"drop_path","text":"<pre><code>drop_path(x, drop_prob=0.0, training=False)\n</code></pre> <p>Drop paths (Stochastic Depth) for regularization during training.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <code>drop_prob</code> <code>float</code> <p>Probability of dropping paths.</p> <code>0.0</code> <code>training</code> <code>bool</code> <p>Whether the model is in training mode.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>output</code> <code>Tensor</code> <p>Output tensor after applying drop path.</p> Source code in <code>mmlearn/modules/layers/transformer_block.py</code> <pre><code>def drop_path(\n    x: torch.Tensor, drop_prob: float = 0.0, training: bool = False\n) -&gt; torch.Tensor:\n    \"\"\"Drop paths (Stochastic Depth) for regularization during training.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor.\n    drop_prob : float, optional, default=0.0\n        Probability of dropping paths.\n    training : bool, optional, default=False\n        Whether the model is in training mode.\n\n    Returns\n    -------\n    output : torch.Tensor\n        Output tensor after applying drop path.\n    \"\"\"\n    if drop_prob == 0.0 or not training:\n        return x\n    keep_prob = 1 - drop_prob\n    shape = (x.shape[0],) + (1,) * (\n        x.ndim - 1\n    )  # work with diff dim tensors, not just 2D ConvNets\n    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n    random_tensor.floor_()  # binarize\n    return x.div(keep_prob) * random_tensor\n</code></pre>"},{"location":"api/#mmlearn.modules.losses","title":"losses","text":"<p>Loss functions.</p>"},{"location":"api/#mmlearn.modules.losses.ContrastiveLoss","title":"ContrastiveLoss","text":"<p>               Bases: <code>Module</code></p> <p>Contrastive Loss.</p> <p>Parameters:</p> Name Type Description Default <code>l2_normalize</code> <code>bool</code> <p>Whether to L2 normalize the features.</p> <code>False</code> <code>local_loss</code> <code>bool</code> <p>Whether to calculate the loss locally i.e. <code>local_features@global_features</code>.</p> <code>False</code> <code>gather_with_grad</code> <code>bool</code> <p>Whether to gather tensors with gradients.</p> <code>False</code> <code>modality_alignment</code> <code>bool</code> <p>Whether to include modality alignment loss. This loss considers all features from the same modality as positive pairs and all features from different modalities as negative pairs.</p> <code>False</code> <code>cache_labels</code> <code>bool</code> <p>Whether to cache the labels.</p> <code>False</code> Source code in <code>mmlearn/modules/losses/contrastive.py</code> <pre><code>@store(group=\"modules/losses\", provider=\"mmlearn\")\nclass ContrastiveLoss(nn.Module):\n    \"\"\"Contrastive Loss.\n\n    Parameters\n    ----------\n    l2_normalize : bool, optional, default=False\n        Whether to L2 normalize the features.\n    local_loss : bool, optional, default=False\n        Whether to calculate the loss locally i.e. ``local_features@global_features``.\n    gather_with_grad : bool, optional, default=False\n        Whether to gather tensors with gradients.\n    modality_alignment : bool, optional, default=False\n        Whether to include modality alignment loss. This loss considers all features\n        from the same modality as positive pairs and all features from different\n        modalities as negative pairs.\n    cache_labels : bool, optional, default=False\n        Whether to cache the labels.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        l2_normalize: bool = False,\n        local_loss: bool = False,\n        gather_with_grad: bool = False,\n        modality_alignment: bool = False,\n        cache_labels: bool = False,\n    ):\n        super().__init__()\n        self.local_loss = local_loss\n        self.gather_with_grad = gather_with_grad\n        self.cache_labels = cache_labels\n        self.l2_normalize = l2_normalize\n        self.modality_alignment = modality_alignment\n\n        # cache state\n        self._prev_num_logits = 0\n        self._labels: dict[torch.device, torch.Tensor] = {}\n\n    def forward(\n        self,\n        embeddings: dict[str, torch.Tensor],\n        example_ids: dict[str, torch.Tensor],\n        logit_scale: torch.Tensor,\n        modality_loss_pairs: list[LossPairSpec],\n    ) -&gt; torch.Tensor:\n        \"\"\"Calculate the contrastive loss.\n\n        Parameters\n        ----------\n        embeddings : dict[str, torch.Tensor]\n            Dictionary of embeddings, where the key is the modality name and the value\n            is the corresponding embedding tensor.\n        example_ids : dict[str, torch.Tensor]\n            Dictionary of example IDs, where the key is the modality name and the value\n            is a tensor tuple of the dataset index and the example index.\n        logit_scale : torch.Tensor\n            Scale factor for the logits.\n        modality_loss_pairs : list[LossPairSpec]\n            Specification of the modality pairs for which the loss should be calculated.\n\n        Returns\n        -------\n        torch.Tensor\n            The contrastive loss.\n        \"\"\"\n        world_size = dist.get_world_size() if dist.is_initialized() else 1\n        rank = dist.get_rank() if world_size &gt; 1 else 0\n\n        if self.l2_normalize:\n            embeddings = {k: F.normalize(v, p=2, dim=-1) for k, v in embeddings.items()}\n\n        if world_size &gt; 1:  # gather embeddings and example_ids across all processes\n            # NOTE: gathering dictionaries of tensors across all processes\n            # (keys + values, as opposed to just values) is especially important\n            # for the modality_alignment loss, which requires all embeddings\n            all_embeddings = _gather_dicts(\n                embeddings,\n                local_loss=self.local_loss,\n                gather_with_grad=self.gather_with_grad,\n                rank=rank,\n            )\n            all_example_ids = _gather_dicts(\n                example_ids,\n                local_loss=self.local_loss,\n                gather_with_grad=self.gather_with_grad,\n                rank=rank,\n            )\n        else:\n            all_embeddings = embeddings\n            all_example_ids = example_ids\n\n        losses = []\n        for loss_pairs in modality_loss_pairs:\n            logits_per_feature_a, logits_per_feature_b, skip_flag = self._get_logits(\n                loss_pairs.modalities,\n                per_device_embeddings=embeddings,\n                all_embeddings=all_embeddings,\n                per_device_example_ids=example_ids,\n                all_example_ids=all_example_ids,\n                logit_scale=logit_scale,\n                world_size=world_size,\n            )\n            if logits_per_feature_a is None or logits_per_feature_b is None:\n                continue\n\n            labels = self._get_ground_truth(\n                logits_per_feature_a.shape,\n                device=logits_per_feature_a.device,\n                rank=rank,\n                world_size=world_size,\n                skipped_process=skip_flag,\n            )\n\n            if labels.numel() != 0:\n                losses.append(\n                    (\n                        (\n                            F.cross_entropy(logits_per_feature_a, labels)\n                            + F.cross_entropy(logits_per_feature_b, labels)\n                        )\n                        / 2\n                    )\n                    * loss_pairs.weight\n                )\n\n        if self.modality_alignment:\n            losses.append(\n                self._compute_modality_alignment_loss(all_embeddings, logit_scale)\n            )\n\n        if not losses:  # no loss to compute (e.g. no paired data in batch)\n            losses.append(\n                torch.tensor(\n                    0.0,\n                    device=logit_scale.device,\n                    dtype=next(iter(embeddings.values())).dtype,\n                )\n            )\n\n        return torch.stack(losses).sum()\n\n    def _get_ground_truth(\n        self,\n        logits_shape: tuple[int, int],\n        device: torch.device,\n        rank: int,\n        world_size: int,\n        skipped_process: bool,\n    ) -&gt; torch.Tensor:\n        \"\"\"Return the ground-truth labels.\n\n        Parameters\n        ----------\n        logits_shape : tuple[int, int]\n            Shape of the logits tensor.\n        device : torch.device\n            Device on which the labels should be created.\n        rank : int\n            Rank of the current process.\n        world_size : int\n            Number of processes.\n        skipped_process : bool\n            Whether the current process skipped the computation due to lack of data.\n\n        Returns\n        -------\n        torch.Tensor\n            Ground-truth labels.\n        \"\"\"\n        num_logits = logits_shape[-1]\n\n        # calculate ground-truth and cache if enabled\n        if self._prev_num_logits != num_logits or device not in self._labels:\n            labels = torch.arange(num_logits, device=device, dtype=torch.long)\n\n            if world_size &gt; 1 and self.local_loss:\n                local_size = torch.tensor(\n                    0 if skipped_process else logits_shape[0], device=device\n                )\n                # NOTE: all processes must participate in the all_gather operation\n                # even if they have no data to contribute.\n                sizes = torch.stack(\n                    _simple_gather_all_tensors(\n                        local_size, group=dist.group.WORLD, world_size=world_size\n                    )\n                )\n                sizes = torch.cat(\n                    [torch.tensor([0], device=sizes.device), torch.cumsum(sizes, dim=0)]\n                )\n                labels = labels[\n                    sizes[rank] : sizes[rank + 1] if rank + 1 &lt; world_size else None\n                ]\n\n            if self.cache_labels:\n                self._labels[device] = labels\n                self._prev_num_logits = num_logits\n        else:\n            labels = self._labels[device]\n        return labels\n\n    def _get_logits(  # noqa: PLR0912\n        self,\n        modalities: tuple[str, str],\n        per_device_embeddings: dict[str, torch.Tensor],\n        all_embeddings: dict[str, torch.Tensor],\n        per_device_example_ids: dict[str, torch.Tensor],\n        all_example_ids: dict[str, torch.Tensor],\n        logit_scale: torch.Tensor,\n        world_size: int,\n    ) -&gt; tuple[Optional[torch.Tensor], Optional[torch.Tensor], bool]:\n        \"\"\"Calculate the logits for the given modalities.\n\n        Parameters\n        ----------\n        modalities : tuple[str, str]\n            Tuple of modality names.\n        per_device_embeddings : dict[str, torch.Tensor]\n            Dictionary of embeddings, where the key is the modality name and the value\n            is the corresponding embedding tensor.\n        all_embeddings : dict[str, torch.Tensor]\n            Dictionary of embeddings, where the key is the modality name and the value\n            is the corresponding embedding tensor. In distributed mode, this contains\n            embeddings from all processes.\n        per_device_example_ids : dict[str, torch.Tensor]\n            Dictionary of example IDs, where the key is the modality name and the value\n            is a tensor tuple of the dataset index and the example index.\n        all_example_ids : dict[str, torch.Tensor]\n            Dictionary of example IDs, where the key is the modality name and the value\n            is a tensor tuple of the dataset index and the example index. In distributed\n            mode, this contains example IDs from all processes.\n        logit_scale : torch.Tensor\n            Scale factor for the logits.\n        world_size : int\n            Number of processes.\n\n        Returns\n        -------\n        tuple[Optional[torch.Tensor], Optional[torch.Tensor], bool]\n            Tuple of logits for the given modalities. If embeddings for the given\n            modalities are not available, returns `None` for the logits. The last\n            element is a flag indicating whether the process skipped the computation\n            due to lack of data.\n        \"\"\"\n        modality_a = Modalities.get_modality(modalities[0])\n        modality_b = Modalities.get_modality(modalities[1])\n        skip_flag = False\n\n        if self.local_loss or world_size == 1:\n            if not (\n                modality_a.embedding in per_device_embeddings\n                and modality_b.embedding in per_device_embeddings\n            ):\n                if world_size &gt; 1:  # NOTE: not all processes exit here, hence skip_flag\n                    skip_flag = True\n                else:\n                    return None, None, skip_flag\n\n            if not skip_flag:\n                indices_a, indices_b = find_matching_indices(\n                    per_device_example_ids[modality_a.name],\n                    per_device_example_ids[modality_b.name],\n                )\n                if indices_a.numel() == 0 or indices_b.numel() == 0:\n                    if world_size &gt; 1:  # not all processes exit here\n                        skip_flag = True\n                    else:\n                        return None, None, skip_flag\n\n            if not skip_flag:\n                features_a = per_device_embeddings[modality_a.embedding][indices_a]\n                features_b = per_device_embeddings[modality_b.embedding][indices_b]\n            else:\n                # all processes must participate in the all_gather operation\n                # that follows, even if they have no data to contribute. So,\n                # we create empty tensors here.\n                features_a = torch.empty(\n                    0, device=list(per_device_embeddings.values())[0].device\n                )\n                features_b = torch.empty(\n                    0, device=list(per_device_embeddings.values())[0].device\n                )\n\n        if world_size &gt; 1:\n            if not (\n                modality_a.embedding in all_embeddings\n                and modality_b.embedding in all_embeddings\n            ):  # all processes exit here\n                return None, None, skip_flag\n\n            indices_a, indices_b = find_matching_indices(\n                all_example_ids[modality_a.name],\n                all_example_ids[modality_b.name],\n            )\n            if indices_a.numel() == 0 or indices_b.numel() == 0:\n                # all processes exit here\n                return None, None, skip_flag\n\n            all_features_a = all_embeddings[modality_a.embedding][indices_a]\n            all_features_b = all_embeddings[modality_b.embedding][indices_b]\n\n            if self.local_loss:\n                if features_a.numel() == 0:\n                    features_a = all_features_a\n                if features_b.numel() == 0:\n                    features_b = all_features_b\n\n                logits_per_feature_a = logit_scale * _safe_matmul(\n                    features_a, all_features_b\n                )\n                logits_per_feature_b = logit_scale * _safe_matmul(\n                    features_b, all_features_a\n                )\n            else:\n                logits_per_feature_a = logit_scale * _safe_matmul(\n                    all_features_a, all_features_b\n                )\n                logits_per_feature_b = logits_per_feature_a.T\n        else:\n            logits_per_feature_a = logit_scale * _safe_matmul(features_a, features_b)\n            logits_per_feature_b = logit_scale * _safe_matmul(features_b, features_a)\n\n        return logits_per_feature_a, logits_per_feature_b, skip_flag\n\n    def _compute_modality_alignment_loss(\n        self, all_embeddings: dict[str, torch.Tensor], logit_scale: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"Compute the modality alignment loss.\n\n        This loss considers all features from the same modality as positive pairs\n        and all features from different modalities as negative pairs.\n\n        Parameters\n        ----------\n        all_embeddings : dict[str, torch.Tensor]\n            Dictionary of embeddings, where the key is the modality name and the value\n            is the corresponding embedding tensor.\n        logit_scale : torch.Tensor\n            Scale factor for the logits.\n\n        Returns\n        -------\n        torch.Tensor\n            Modality alignment loss.\n\n        Notes\n        -----\n        This loss does not support `local_loss=True`.\n        \"\"\"\n        available_modalities = list(all_embeddings.keys())\n        # TODO: support local_loss for modality_alignment?\n        # if world_size == 1, all_embeddings == embeddings\n        all_features = torch.cat(list(all_embeddings.values()), dim=0)\n\n        positive_indices = torch.tensor(\n            [\n                (i, j)\n                if idx == 0\n                else (\n                    i + all_embeddings[available_modalities[idx - 1]].size(0),\n                    j + all_embeddings[available_modalities[idx - 1]].size(0),\n                )\n                for idx, k in enumerate(all_embeddings)\n                for i, j in itertools.combinations(range(all_embeddings[k].size(0)), 2)\n            ],\n            device=all_features.device,\n        )\n        logits = logit_scale * _safe_matmul(all_features, all_features)\n\n        target = torch.eye(all_features.size(0), device=all_features.device)\n        target[positive_indices[:, 0], positive_indices[:, 1]] = 1\n\n        modality_loss = torch.nn.functional.binary_cross_entropy_with_logits(\n            logits, target, reduction=\"none\"\n        )\n\n        target_pos = target.bool()\n        target_neg = ~target_pos\n\n        # loss_pos and loss_neg below contain non-zero values only for those\n        # elements that are positive pairs and negative pairs respectively.\n        loss_pos = torch.zeros(\n            logits.size(0), logits.size(0), device=target.device\n        ).masked_scatter(target_pos, modality_loss[target_pos])\n        loss_neg = torch.zeros(\n            logits.size(0), logits.size(0), device=target.device\n        ).masked_scatter(target_neg, modality_loss[target_neg])\n\n        loss_pos = loss_pos.sum(dim=1)\n        loss_neg = loss_neg.sum(dim=1)\n        num_pos = target.sum(dim=1)\n        num_neg = logits.size(0) - num_pos\n\n        return ((loss_pos / num_pos) + (loss_neg / num_neg)).mean()\n</code></pre>"},{"location":"api/#mmlearn.modules.losses.ContrastiveLoss.forward","title":"forward","text":"<pre><code>forward(\n    embeddings,\n    example_ids,\n    logit_scale,\n    modality_loss_pairs,\n)\n</code></pre> <p>Calculate the contrastive loss.</p> <p>Parameters:</p> Name Type Description Default <code>embeddings</code> <code>dict[str, Tensor]</code> <p>Dictionary of embeddings, where the key is the modality name and the value is the corresponding embedding tensor.</p> required <code>example_ids</code> <code>dict[str, Tensor]</code> <p>Dictionary of example IDs, where the key is the modality name and the value is a tensor tuple of the dataset index and the example index.</p> required <code>logit_scale</code> <code>Tensor</code> <p>Scale factor for the logits.</p> required <code>modality_loss_pairs</code> <code>list[LossPairSpec]</code> <p>Specification of the modality pairs for which the loss should be calculated.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The contrastive loss.</p> Source code in <code>mmlearn/modules/losses/contrastive.py</code> <pre><code>def forward(\n    self,\n    embeddings: dict[str, torch.Tensor],\n    example_ids: dict[str, torch.Tensor],\n    logit_scale: torch.Tensor,\n    modality_loss_pairs: list[LossPairSpec],\n) -&gt; torch.Tensor:\n    \"\"\"Calculate the contrastive loss.\n\n    Parameters\n    ----------\n    embeddings : dict[str, torch.Tensor]\n        Dictionary of embeddings, where the key is the modality name and the value\n        is the corresponding embedding tensor.\n    example_ids : dict[str, torch.Tensor]\n        Dictionary of example IDs, where the key is the modality name and the value\n        is a tensor tuple of the dataset index and the example index.\n    logit_scale : torch.Tensor\n        Scale factor for the logits.\n    modality_loss_pairs : list[LossPairSpec]\n        Specification of the modality pairs for which the loss should be calculated.\n\n    Returns\n    -------\n    torch.Tensor\n        The contrastive loss.\n    \"\"\"\n    world_size = dist.get_world_size() if dist.is_initialized() else 1\n    rank = dist.get_rank() if world_size &gt; 1 else 0\n\n    if self.l2_normalize:\n        embeddings = {k: F.normalize(v, p=2, dim=-1) for k, v in embeddings.items()}\n\n    if world_size &gt; 1:  # gather embeddings and example_ids across all processes\n        # NOTE: gathering dictionaries of tensors across all processes\n        # (keys + values, as opposed to just values) is especially important\n        # for the modality_alignment loss, which requires all embeddings\n        all_embeddings = _gather_dicts(\n            embeddings,\n            local_loss=self.local_loss,\n            gather_with_grad=self.gather_with_grad,\n            rank=rank,\n        )\n        all_example_ids = _gather_dicts(\n            example_ids,\n            local_loss=self.local_loss,\n            gather_with_grad=self.gather_with_grad,\n            rank=rank,\n        )\n    else:\n        all_embeddings = embeddings\n        all_example_ids = example_ids\n\n    losses = []\n    for loss_pairs in modality_loss_pairs:\n        logits_per_feature_a, logits_per_feature_b, skip_flag = self._get_logits(\n            loss_pairs.modalities,\n            per_device_embeddings=embeddings,\n            all_embeddings=all_embeddings,\n            per_device_example_ids=example_ids,\n            all_example_ids=all_example_ids,\n            logit_scale=logit_scale,\n            world_size=world_size,\n        )\n        if logits_per_feature_a is None or logits_per_feature_b is None:\n            continue\n\n        labels = self._get_ground_truth(\n            logits_per_feature_a.shape,\n            device=logits_per_feature_a.device,\n            rank=rank,\n            world_size=world_size,\n            skipped_process=skip_flag,\n        )\n\n        if labels.numel() != 0:\n            losses.append(\n                (\n                    (\n                        F.cross_entropy(logits_per_feature_a, labels)\n                        + F.cross_entropy(logits_per_feature_b, labels)\n                    )\n                    / 2\n                )\n                * loss_pairs.weight\n            )\n\n    if self.modality_alignment:\n        losses.append(\n            self._compute_modality_alignment_loss(all_embeddings, logit_scale)\n        )\n\n    if not losses:  # no loss to compute (e.g. no paired data in batch)\n        losses.append(\n            torch.tensor(\n                0.0,\n                device=logit_scale.device,\n                dtype=next(iter(embeddings.values())).dtype,\n            )\n        )\n\n    return torch.stack(losses).sum()\n</code></pre>"},{"location":"api/#mmlearn.modules.losses.Data2VecLoss","title":"Data2VecLoss","text":"<p>               Bases: <code>Module</code></p> <p>Data2Vec loss function.</p> <p>Parameters:</p> Name Type Description Default <code>beta</code> <code>float</code> <p>Specifies the beta parameter for smooth L1 loss. If <code>0</code>, MSE loss is used.</p> <code>0</code> <code>loss_scale</code> <code>Optional[float]</code> <p>Scaling factor for the loss. If None, uses <code>1 / sqrt(embedding_dim)</code>.</p> <code>None</code> <code>reduction</code> <code>str</code> <p>Specifies the reduction to apply to the output: <code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>.</p> <code>'none'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the reduction mode is not supported.</p> Source code in <code>mmlearn/modules/losses/data2vec.py</code> <pre><code>@store(group=\"modules/losses\", provider=\"mmlearn\")\nclass Data2VecLoss(nn.Module):\n    \"\"\"Data2Vec loss function.\n\n    Parameters\n    ----------\n    beta : float, optional, default=0\n        Specifies the beta parameter for smooth L1 loss. If ``0``, MSE loss is used.\n    loss_scale : Optional[float], optional, default=None\n        Scaling factor for the loss. If None, uses ``1 / sqrt(embedding_dim)``.\n    reduction : str, optional, default='none'\n        Specifies the reduction to apply to the output:\n        ``'none'`` | ``'mean'`` | ``'sum'``.\n\n    Raises\n    ------\n    ValueError\n        If the reduction mode is not supported.\n    \"\"\"\n\n    def __init__(\n        self,\n        beta: float = 0,\n        loss_scale: Optional[float] = None,\n        reduction: str = \"none\",\n    ) -&gt; None:\n        super().__init__()\n        self.beta = beta\n        self.loss_scale = loss_scale\n        if reduction not in [\"none\", \"mean\", \"sum\"]:\n            raise ValueError(f\"Unsupported reduction mode: {reduction}\")\n        self.reduction = reduction\n\n    def forward(self, x: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute the Data2Vec loss.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Predicted embeddings of shape ``(batch_size, num_patches, embedding_dim)``.\n        y : torch.Tensor\n            Target embeddings of shape ``(batch_size, num_patches, embedding_dim)``.\n\n        Returns\n        -------\n        torch.Tensor\n            Data2Vec loss value.\n\n        Raises\n        ------\n        ValueError\n            If the shapes of x and y do not match.\n        \"\"\"\n        if x.shape != y.shape:\n            raise ValueError(f\"Shape mismatch: x: {x.shape}, y: {y.shape}\")\n\n        x = x.view(-1, x.size(-1)).float()\n        y = y.view(-1, y.size(-1))\n\n        if self.beta == 0:\n            loss = mse_loss(x, y, reduction=\"none\")\n        else:\n            loss = smooth_l1_loss(x, y, reduction=\"none\", beta=self.beta)\n\n        if self.loss_scale is not None:\n            scale = self.loss_scale\n        else:\n            scale = 1 / math.sqrt(x.size(-1))\n\n        loss = loss * scale\n\n        if self.reduction == \"mean\":\n            return loss.mean()\n        if self.reduction == \"sum\":\n            return loss.sum()\n        # 'none'\n        return loss.view(x.size(0), -1).sum(1)\n</code></pre>"},{"location":"api/#mmlearn.modules.losses.Data2VecLoss.forward","title":"forward","text":"<pre><code>forward(x, y)\n</code></pre> <p>Compute the Data2Vec loss.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Predicted embeddings of shape <code>(batch_size, num_patches, embedding_dim)</code>.</p> required <code>y</code> <code>Tensor</code> <p>Target embeddings of shape <code>(batch_size, num_patches, embedding_dim)</code>.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Data2Vec loss value.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the shapes of x and y do not match.</p> Source code in <code>mmlearn/modules/losses/data2vec.py</code> <pre><code>def forward(self, x: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute the Data2Vec loss.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Predicted embeddings of shape ``(batch_size, num_patches, embedding_dim)``.\n    y : torch.Tensor\n        Target embeddings of shape ``(batch_size, num_patches, embedding_dim)``.\n\n    Returns\n    -------\n    torch.Tensor\n        Data2Vec loss value.\n\n    Raises\n    ------\n    ValueError\n        If the shapes of x and y do not match.\n    \"\"\"\n    if x.shape != y.shape:\n        raise ValueError(f\"Shape mismatch: x: {x.shape}, y: {y.shape}\")\n\n    x = x.view(-1, x.size(-1)).float()\n    y = y.view(-1, y.size(-1))\n\n    if self.beta == 0:\n        loss = mse_loss(x, y, reduction=\"none\")\n    else:\n        loss = smooth_l1_loss(x, y, reduction=\"none\", beta=self.beta)\n\n    if self.loss_scale is not None:\n        scale = self.loss_scale\n    else:\n        scale = 1 / math.sqrt(x.size(-1))\n\n    loss = loss * scale\n\n    if self.reduction == \"mean\":\n        return loss.mean()\n    if self.reduction == \"sum\":\n        return loss.sum()\n    # 'none'\n    return loss.view(x.size(0), -1).sum(1)\n</code></pre>"},{"location":"api/#mmlearn.modules.losses.contrastive","title":"contrastive","text":"<p>Implementations of the contrastive loss and its variants.</p>"},{"location":"api/#mmlearn.modules.losses.contrastive.ContrastiveLoss","title":"ContrastiveLoss","text":"<p>               Bases: <code>Module</code></p> <p>Contrastive Loss.</p> <p>Parameters:</p> Name Type Description Default <code>l2_normalize</code> <code>bool</code> <p>Whether to L2 normalize the features.</p> <code>False</code> <code>local_loss</code> <code>bool</code> <p>Whether to calculate the loss locally i.e. <code>local_features@global_features</code>.</p> <code>False</code> <code>gather_with_grad</code> <code>bool</code> <p>Whether to gather tensors with gradients.</p> <code>False</code> <code>modality_alignment</code> <code>bool</code> <p>Whether to include modality alignment loss. This loss considers all features from the same modality as positive pairs and all features from different modalities as negative pairs.</p> <code>False</code> <code>cache_labels</code> <code>bool</code> <p>Whether to cache the labels.</p> <code>False</code> Source code in <code>mmlearn/modules/losses/contrastive.py</code> <pre><code>@store(group=\"modules/losses\", provider=\"mmlearn\")\nclass ContrastiveLoss(nn.Module):\n    \"\"\"Contrastive Loss.\n\n    Parameters\n    ----------\n    l2_normalize : bool, optional, default=False\n        Whether to L2 normalize the features.\n    local_loss : bool, optional, default=False\n        Whether to calculate the loss locally i.e. ``local_features@global_features``.\n    gather_with_grad : bool, optional, default=False\n        Whether to gather tensors with gradients.\n    modality_alignment : bool, optional, default=False\n        Whether to include modality alignment loss. This loss considers all features\n        from the same modality as positive pairs and all features from different\n        modalities as negative pairs.\n    cache_labels : bool, optional, default=False\n        Whether to cache the labels.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        l2_normalize: bool = False,\n        local_loss: bool = False,\n        gather_with_grad: bool = False,\n        modality_alignment: bool = False,\n        cache_labels: bool = False,\n    ):\n        super().__init__()\n        self.local_loss = local_loss\n        self.gather_with_grad = gather_with_grad\n        self.cache_labels = cache_labels\n        self.l2_normalize = l2_normalize\n        self.modality_alignment = modality_alignment\n\n        # cache state\n        self._prev_num_logits = 0\n        self._labels: dict[torch.device, torch.Tensor] = {}\n\n    def forward(\n        self,\n        embeddings: dict[str, torch.Tensor],\n        example_ids: dict[str, torch.Tensor],\n        logit_scale: torch.Tensor,\n        modality_loss_pairs: list[LossPairSpec],\n    ) -&gt; torch.Tensor:\n        \"\"\"Calculate the contrastive loss.\n\n        Parameters\n        ----------\n        embeddings : dict[str, torch.Tensor]\n            Dictionary of embeddings, where the key is the modality name and the value\n            is the corresponding embedding tensor.\n        example_ids : dict[str, torch.Tensor]\n            Dictionary of example IDs, where the key is the modality name and the value\n            is a tensor tuple of the dataset index and the example index.\n        logit_scale : torch.Tensor\n            Scale factor for the logits.\n        modality_loss_pairs : list[LossPairSpec]\n            Specification of the modality pairs for which the loss should be calculated.\n\n        Returns\n        -------\n        torch.Tensor\n            The contrastive loss.\n        \"\"\"\n        world_size = dist.get_world_size() if dist.is_initialized() else 1\n        rank = dist.get_rank() if world_size &gt; 1 else 0\n\n        if self.l2_normalize:\n            embeddings = {k: F.normalize(v, p=2, dim=-1) for k, v in embeddings.items()}\n\n        if world_size &gt; 1:  # gather embeddings and example_ids across all processes\n            # NOTE: gathering dictionaries of tensors across all processes\n            # (keys + values, as opposed to just values) is especially important\n            # for the modality_alignment loss, which requires all embeddings\n            all_embeddings = _gather_dicts(\n                embeddings,\n                local_loss=self.local_loss,\n                gather_with_grad=self.gather_with_grad,\n                rank=rank,\n            )\n            all_example_ids = _gather_dicts(\n                example_ids,\n                local_loss=self.local_loss,\n                gather_with_grad=self.gather_with_grad,\n                rank=rank,\n            )\n        else:\n            all_embeddings = embeddings\n            all_example_ids = example_ids\n\n        losses = []\n        for loss_pairs in modality_loss_pairs:\n            logits_per_feature_a, logits_per_feature_b, skip_flag = self._get_logits(\n                loss_pairs.modalities,\n                per_device_embeddings=embeddings,\n                all_embeddings=all_embeddings,\n                per_device_example_ids=example_ids,\n                all_example_ids=all_example_ids,\n                logit_scale=logit_scale,\n                world_size=world_size,\n            )\n            if logits_per_feature_a is None or logits_per_feature_b is None:\n                continue\n\n            labels = self._get_ground_truth(\n                logits_per_feature_a.shape,\n                device=logits_per_feature_a.device,\n                rank=rank,\n                world_size=world_size,\n                skipped_process=skip_flag,\n            )\n\n            if labels.numel() != 0:\n                losses.append(\n                    (\n                        (\n                            F.cross_entropy(logits_per_feature_a, labels)\n                            + F.cross_entropy(logits_per_feature_b, labels)\n                        )\n                        / 2\n                    )\n                    * loss_pairs.weight\n                )\n\n        if self.modality_alignment:\n            losses.append(\n                self._compute_modality_alignment_loss(all_embeddings, logit_scale)\n            )\n\n        if not losses:  # no loss to compute (e.g. no paired data in batch)\n            losses.append(\n                torch.tensor(\n                    0.0,\n                    device=logit_scale.device,\n                    dtype=next(iter(embeddings.values())).dtype,\n                )\n            )\n\n        return torch.stack(losses).sum()\n\n    def _get_ground_truth(\n        self,\n        logits_shape: tuple[int, int],\n        device: torch.device,\n        rank: int,\n        world_size: int,\n        skipped_process: bool,\n    ) -&gt; torch.Tensor:\n        \"\"\"Return the ground-truth labels.\n\n        Parameters\n        ----------\n        logits_shape : tuple[int, int]\n            Shape of the logits tensor.\n        device : torch.device\n            Device on which the labels should be created.\n        rank : int\n            Rank of the current process.\n        world_size : int\n            Number of processes.\n        skipped_process : bool\n            Whether the current process skipped the computation due to lack of data.\n\n        Returns\n        -------\n        torch.Tensor\n            Ground-truth labels.\n        \"\"\"\n        num_logits = logits_shape[-1]\n\n        # calculate ground-truth and cache if enabled\n        if self._prev_num_logits != num_logits or device not in self._labels:\n            labels = torch.arange(num_logits, device=device, dtype=torch.long)\n\n            if world_size &gt; 1 and self.local_loss:\n                local_size = torch.tensor(\n                    0 if skipped_process else logits_shape[0], device=device\n                )\n                # NOTE: all processes must participate in the all_gather operation\n                # even if they have no data to contribute.\n                sizes = torch.stack(\n                    _simple_gather_all_tensors(\n                        local_size, group=dist.group.WORLD, world_size=world_size\n                    )\n                )\n                sizes = torch.cat(\n                    [torch.tensor([0], device=sizes.device), torch.cumsum(sizes, dim=0)]\n                )\n                labels = labels[\n                    sizes[rank] : sizes[rank + 1] if rank + 1 &lt; world_size else None\n                ]\n\n            if self.cache_labels:\n                self._labels[device] = labels\n                self._prev_num_logits = num_logits\n        else:\n            labels = self._labels[device]\n        return labels\n\n    def _get_logits(  # noqa: PLR0912\n        self,\n        modalities: tuple[str, str],\n        per_device_embeddings: dict[str, torch.Tensor],\n        all_embeddings: dict[str, torch.Tensor],\n        per_device_example_ids: dict[str, torch.Tensor],\n        all_example_ids: dict[str, torch.Tensor],\n        logit_scale: torch.Tensor,\n        world_size: int,\n    ) -&gt; tuple[Optional[torch.Tensor], Optional[torch.Tensor], bool]:\n        \"\"\"Calculate the logits for the given modalities.\n\n        Parameters\n        ----------\n        modalities : tuple[str, str]\n            Tuple of modality names.\n        per_device_embeddings : dict[str, torch.Tensor]\n            Dictionary of embeddings, where the key is the modality name and the value\n            is the corresponding embedding tensor.\n        all_embeddings : dict[str, torch.Tensor]\n            Dictionary of embeddings, where the key is the modality name and the value\n            is the corresponding embedding tensor. In distributed mode, this contains\n            embeddings from all processes.\n        per_device_example_ids : dict[str, torch.Tensor]\n            Dictionary of example IDs, where the key is the modality name and the value\n            is a tensor tuple of the dataset index and the example index.\n        all_example_ids : dict[str, torch.Tensor]\n            Dictionary of example IDs, where the key is the modality name and the value\n            is a tensor tuple of the dataset index and the example index. In distributed\n            mode, this contains example IDs from all processes.\n        logit_scale : torch.Tensor\n            Scale factor for the logits.\n        world_size : int\n            Number of processes.\n\n        Returns\n        -------\n        tuple[Optional[torch.Tensor], Optional[torch.Tensor], bool]\n            Tuple of logits for the given modalities. If embeddings for the given\n            modalities are not available, returns `None` for the logits. The last\n            element is a flag indicating whether the process skipped the computation\n            due to lack of data.\n        \"\"\"\n        modality_a = Modalities.get_modality(modalities[0])\n        modality_b = Modalities.get_modality(modalities[1])\n        skip_flag = False\n\n        if self.local_loss or world_size == 1:\n            if not (\n                modality_a.embedding in per_device_embeddings\n                and modality_b.embedding in per_device_embeddings\n            ):\n                if world_size &gt; 1:  # NOTE: not all processes exit here, hence skip_flag\n                    skip_flag = True\n                else:\n                    return None, None, skip_flag\n\n            if not skip_flag:\n                indices_a, indices_b = find_matching_indices(\n                    per_device_example_ids[modality_a.name],\n                    per_device_example_ids[modality_b.name],\n                )\n                if indices_a.numel() == 0 or indices_b.numel() == 0:\n                    if world_size &gt; 1:  # not all processes exit here\n                        skip_flag = True\n                    else:\n                        return None, None, skip_flag\n\n            if not skip_flag:\n                features_a = per_device_embeddings[modality_a.embedding][indices_a]\n                features_b = per_device_embeddings[modality_b.embedding][indices_b]\n            else:\n                # all processes must participate in the all_gather operation\n                # that follows, even if they have no data to contribute. So,\n                # we create empty tensors here.\n                features_a = torch.empty(\n                    0, device=list(per_device_embeddings.values())[0].device\n                )\n                features_b = torch.empty(\n                    0, device=list(per_device_embeddings.values())[0].device\n                )\n\n        if world_size &gt; 1:\n            if not (\n                modality_a.embedding in all_embeddings\n                and modality_b.embedding in all_embeddings\n            ):  # all processes exit here\n                return None, None, skip_flag\n\n            indices_a, indices_b = find_matching_indices(\n                all_example_ids[modality_a.name],\n                all_example_ids[modality_b.name],\n            )\n            if indices_a.numel() == 0 or indices_b.numel() == 0:\n                # all processes exit here\n                return None, None, skip_flag\n\n            all_features_a = all_embeddings[modality_a.embedding][indices_a]\n            all_features_b = all_embeddings[modality_b.embedding][indices_b]\n\n            if self.local_loss:\n                if features_a.numel() == 0:\n                    features_a = all_features_a\n                if features_b.numel() == 0:\n                    features_b = all_features_b\n\n                logits_per_feature_a = logit_scale * _safe_matmul(\n                    features_a, all_features_b\n                )\n                logits_per_feature_b = logit_scale * _safe_matmul(\n                    features_b, all_features_a\n                )\n            else:\n                logits_per_feature_a = logit_scale * _safe_matmul(\n                    all_features_a, all_features_b\n                )\n                logits_per_feature_b = logits_per_feature_a.T\n        else:\n            logits_per_feature_a = logit_scale * _safe_matmul(features_a, features_b)\n            logits_per_feature_b = logit_scale * _safe_matmul(features_b, features_a)\n\n        return logits_per_feature_a, logits_per_feature_b, skip_flag\n\n    def _compute_modality_alignment_loss(\n        self, all_embeddings: dict[str, torch.Tensor], logit_scale: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"Compute the modality alignment loss.\n\n        This loss considers all features from the same modality as positive pairs\n        and all features from different modalities as negative pairs.\n\n        Parameters\n        ----------\n        all_embeddings : dict[str, torch.Tensor]\n            Dictionary of embeddings, where the key is the modality name and the value\n            is the corresponding embedding tensor.\n        logit_scale : torch.Tensor\n            Scale factor for the logits.\n\n        Returns\n        -------\n        torch.Tensor\n            Modality alignment loss.\n\n        Notes\n        -----\n        This loss does not support `local_loss=True`.\n        \"\"\"\n        available_modalities = list(all_embeddings.keys())\n        # TODO: support local_loss for modality_alignment?\n        # if world_size == 1, all_embeddings == embeddings\n        all_features = torch.cat(list(all_embeddings.values()), dim=0)\n\n        positive_indices = torch.tensor(\n            [\n                (i, j)\n                if idx == 0\n                else (\n                    i + all_embeddings[available_modalities[idx - 1]].size(0),\n                    j + all_embeddings[available_modalities[idx - 1]].size(0),\n                )\n                for idx, k in enumerate(all_embeddings)\n                for i, j in itertools.combinations(range(all_embeddings[k].size(0)), 2)\n            ],\n            device=all_features.device,\n        )\n        logits = logit_scale * _safe_matmul(all_features, all_features)\n\n        target = torch.eye(all_features.size(0), device=all_features.device)\n        target[positive_indices[:, 0], positive_indices[:, 1]] = 1\n\n        modality_loss = torch.nn.functional.binary_cross_entropy_with_logits(\n            logits, target, reduction=\"none\"\n        )\n\n        target_pos = target.bool()\n        target_neg = ~target_pos\n\n        # loss_pos and loss_neg below contain non-zero values only for those\n        # elements that are positive pairs and negative pairs respectively.\n        loss_pos = torch.zeros(\n            logits.size(0), logits.size(0), device=target.device\n        ).masked_scatter(target_pos, modality_loss[target_pos])\n        loss_neg = torch.zeros(\n            logits.size(0), logits.size(0), device=target.device\n        ).masked_scatter(target_neg, modality_loss[target_neg])\n\n        loss_pos = loss_pos.sum(dim=1)\n        loss_neg = loss_neg.sum(dim=1)\n        num_pos = target.sum(dim=1)\n        num_neg = logits.size(0) - num_pos\n\n        return ((loss_pos / num_pos) + (loss_neg / num_neg)).mean()\n</code></pre> <code></code> forward \u00b6 <pre><code>forward(\n    embeddings,\n    example_ids,\n    logit_scale,\n    modality_loss_pairs,\n)\n</code></pre> <p>Calculate the contrastive loss.</p> <p>Parameters:</p> Name Type Description Default <code>embeddings</code> <code>dict[str, Tensor]</code> <p>Dictionary of embeddings, where the key is the modality name and the value is the corresponding embedding tensor.</p> required <code>example_ids</code> <code>dict[str, Tensor]</code> <p>Dictionary of example IDs, where the key is the modality name and the value is a tensor tuple of the dataset index and the example index.</p> required <code>logit_scale</code> <code>Tensor</code> <p>Scale factor for the logits.</p> required <code>modality_loss_pairs</code> <code>list[LossPairSpec]</code> <p>Specification of the modality pairs for which the loss should be calculated.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The contrastive loss.</p> Source code in <code>mmlearn/modules/losses/contrastive.py</code> <pre><code>def forward(\n    self,\n    embeddings: dict[str, torch.Tensor],\n    example_ids: dict[str, torch.Tensor],\n    logit_scale: torch.Tensor,\n    modality_loss_pairs: list[LossPairSpec],\n) -&gt; torch.Tensor:\n    \"\"\"Calculate the contrastive loss.\n\n    Parameters\n    ----------\n    embeddings : dict[str, torch.Tensor]\n        Dictionary of embeddings, where the key is the modality name and the value\n        is the corresponding embedding tensor.\n    example_ids : dict[str, torch.Tensor]\n        Dictionary of example IDs, where the key is the modality name and the value\n        is a tensor tuple of the dataset index and the example index.\n    logit_scale : torch.Tensor\n        Scale factor for the logits.\n    modality_loss_pairs : list[LossPairSpec]\n        Specification of the modality pairs for which the loss should be calculated.\n\n    Returns\n    -------\n    torch.Tensor\n        The contrastive loss.\n    \"\"\"\n    world_size = dist.get_world_size() if dist.is_initialized() else 1\n    rank = dist.get_rank() if world_size &gt; 1 else 0\n\n    if self.l2_normalize:\n        embeddings = {k: F.normalize(v, p=2, dim=-1) for k, v in embeddings.items()}\n\n    if world_size &gt; 1:  # gather embeddings and example_ids across all processes\n        # NOTE: gathering dictionaries of tensors across all processes\n        # (keys + values, as opposed to just values) is especially important\n        # for the modality_alignment loss, which requires all embeddings\n        all_embeddings = _gather_dicts(\n            embeddings,\n            local_loss=self.local_loss,\n            gather_with_grad=self.gather_with_grad,\n            rank=rank,\n        )\n        all_example_ids = _gather_dicts(\n            example_ids,\n            local_loss=self.local_loss,\n            gather_with_grad=self.gather_with_grad,\n            rank=rank,\n        )\n    else:\n        all_embeddings = embeddings\n        all_example_ids = example_ids\n\n    losses = []\n    for loss_pairs in modality_loss_pairs:\n        logits_per_feature_a, logits_per_feature_b, skip_flag = self._get_logits(\n            loss_pairs.modalities,\n            per_device_embeddings=embeddings,\n            all_embeddings=all_embeddings,\n            per_device_example_ids=example_ids,\n            all_example_ids=all_example_ids,\n            logit_scale=logit_scale,\n            world_size=world_size,\n        )\n        if logits_per_feature_a is None or logits_per_feature_b is None:\n            continue\n\n        labels = self._get_ground_truth(\n            logits_per_feature_a.shape,\n            device=logits_per_feature_a.device,\n            rank=rank,\n            world_size=world_size,\n            skipped_process=skip_flag,\n        )\n\n        if labels.numel() != 0:\n            losses.append(\n                (\n                    (\n                        F.cross_entropy(logits_per_feature_a, labels)\n                        + F.cross_entropy(logits_per_feature_b, labels)\n                    )\n                    / 2\n                )\n                * loss_pairs.weight\n            )\n\n    if self.modality_alignment:\n        losses.append(\n            self._compute_modality_alignment_loss(all_embeddings, logit_scale)\n        )\n\n    if not losses:  # no loss to compute (e.g. no paired data in batch)\n        losses.append(\n            torch.tensor(\n                0.0,\n                device=logit_scale.device,\n                dtype=next(iter(embeddings.values())).dtype,\n            )\n        )\n\n    return torch.stack(losses).sum()\n</code></pre>"},{"location":"api/#mmlearn.modules.losses.data2vec","title":"data2vec","text":"<p>Implementation of Data2vec loss function.</p>"},{"location":"api/#mmlearn.modules.losses.data2vec.Data2VecLoss","title":"Data2VecLoss","text":"<p>               Bases: <code>Module</code></p> <p>Data2Vec loss function.</p> <p>Parameters:</p> Name Type Description Default <code>beta</code> <code>float</code> <p>Specifies the beta parameter for smooth L1 loss. If <code>0</code>, MSE loss is used.</p> <code>0</code> <code>loss_scale</code> <code>Optional[float]</code> <p>Scaling factor for the loss. If None, uses <code>1 / sqrt(embedding_dim)</code>.</p> <code>None</code> <code>reduction</code> <code>str</code> <p>Specifies the reduction to apply to the output: <code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>.</p> <code>'none'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the reduction mode is not supported.</p> Source code in <code>mmlearn/modules/losses/data2vec.py</code> <pre><code>@store(group=\"modules/losses\", provider=\"mmlearn\")\nclass Data2VecLoss(nn.Module):\n    \"\"\"Data2Vec loss function.\n\n    Parameters\n    ----------\n    beta : float, optional, default=0\n        Specifies the beta parameter for smooth L1 loss. If ``0``, MSE loss is used.\n    loss_scale : Optional[float], optional, default=None\n        Scaling factor for the loss. If None, uses ``1 / sqrt(embedding_dim)``.\n    reduction : str, optional, default='none'\n        Specifies the reduction to apply to the output:\n        ``'none'`` | ``'mean'`` | ``'sum'``.\n\n    Raises\n    ------\n    ValueError\n        If the reduction mode is not supported.\n    \"\"\"\n\n    def __init__(\n        self,\n        beta: float = 0,\n        loss_scale: Optional[float] = None,\n        reduction: str = \"none\",\n    ) -&gt; None:\n        super().__init__()\n        self.beta = beta\n        self.loss_scale = loss_scale\n        if reduction not in [\"none\", \"mean\", \"sum\"]:\n            raise ValueError(f\"Unsupported reduction mode: {reduction}\")\n        self.reduction = reduction\n\n    def forward(self, x: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute the Data2Vec loss.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Predicted embeddings of shape ``(batch_size, num_patches, embedding_dim)``.\n        y : torch.Tensor\n            Target embeddings of shape ``(batch_size, num_patches, embedding_dim)``.\n\n        Returns\n        -------\n        torch.Tensor\n            Data2Vec loss value.\n\n        Raises\n        ------\n        ValueError\n            If the shapes of x and y do not match.\n        \"\"\"\n        if x.shape != y.shape:\n            raise ValueError(f\"Shape mismatch: x: {x.shape}, y: {y.shape}\")\n\n        x = x.view(-1, x.size(-1)).float()\n        y = y.view(-1, y.size(-1))\n\n        if self.beta == 0:\n            loss = mse_loss(x, y, reduction=\"none\")\n        else:\n            loss = smooth_l1_loss(x, y, reduction=\"none\", beta=self.beta)\n\n        if self.loss_scale is not None:\n            scale = self.loss_scale\n        else:\n            scale = 1 / math.sqrt(x.size(-1))\n\n        loss = loss * scale\n\n        if self.reduction == \"mean\":\n            return loss.mean()\n        if self.reduction == \"sum\":\n            return loss.sum()\n        # 'none'\n        return loss.view(x.size(0), -1).sum(1)\n</code></pre> <code></code> forward \u00b6 <pre><code>forward(x, y)\n</code></pre> <p>Compute the Data2Vec loss.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Predicted embeddings of shape <code>(batch_size, num_patches, embedding_dim)</code>.</p> required <code>y</code> <code>Tensor</code> <p>Target embeddings of shape <code>(batch_size, num_patches, embedding_dim)</code>.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Data2Vec loss value.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the shapes of x and y do not match.</p> Source code in <code>mmlearn/modules/losses/data2vec.py</code> <pre><code>def forward(self, x: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute the Data2Vec loss.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Predicted embeddings of shape ``(batch_size, num_patches, embedding_dim)``.\n    y : torch.Tensor\n        Target embeddings of shape ``(batch_size, num_patches, embedding_dim)``.\n\n    Returns\n    -------\n    torch.Tensor\n        Data2Vec loss value.\n\n    Raises\n    ------\n    ValueError\n        If the shapes of x and y do not match.\n    \"\"\"\n    if x.shape != y.shape:\n        raise ValueError(f\"Shape mismatch: x: {x.shape}, y: {y.shape}\")\n\n    x = x.view(-1, x.size(-1)).float()\n    y = y.view(-1, y.size(-1))\n\n    if self.beta == 0:\n        loss = mse_loss(x, y, reduction=\"none\")\n    else:\n        loss = smooth_l1_loss(x, y, reduction=\"none\", beta=self.beta)\n\n    if self.loss_scale is not None:\n        scale = self.loss_scale\n    else:\n        scale = 1 / math.sqrt(x.size(-1))\n\n    loss = loss * scale\n\n    if self.reduction == \"mean\":\n        return loss.mean()\n    if self.reduction == \"sum\":\n        return loss.sum()\n    # 'none'\n    return loss.view(x.size(0), -1).sum(1)\n</code></pre>"},{"location":"api/#mmlearn.modules.lr_schedulers","title":"lr_schedulers","text":"<p>Learning rate schedulers for training models.</p>"},{"location":"api/#mmlearn.modules.lr_schedulers.linear_warmup_cosine_annealing_lr","title":"linear_warmup_cosine_annealing_lr","text":"<pre><code>linear_warmup_cosine_annealing_lr(\n    optimizer,\n    warmup_steps,\n    max_steps,\n    start_factor=1 / 3,\n    eta_min=0.0,\n    last_epoch=-1,\n)\n</code></pre> <p>Create a linear warmup cosine annealing learning rate scheduler.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>Optimizer</code> <p>The optimizer for which to schedule the learning rate.</p> required <code>warmup_steps</code> <code>int</code> <p>Maximum number of iterations for linear warmup.</p> required <code>max_steps</code> <code>int</code> <p>Maximum number of iterations.</p> required <code>start_factor</code> <code>float</code> <p>Multiplicative factor for the learning rate at the start of the warmup phase.</p> <code>1/3</code> <code>eta_min</code> <code>float</code> <p>Minimum learning rate.</p> <code>0</code> <code>last_epoch</code> <code>int</code> <p>The index of last epoch. If set to <code>-1</code>, it initializes the learning rate as the base learning rate</p> <code>-1</code> <p>Returns:</p> Type Description <code>LRScheduler</code> <p>The learning rate scheduler.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>warmup_steps</code> is greater than or equal to <code>max_steps</code> or if <code>warmup_steps</code> is less than or equal to 0.</p> Source code in <code>mmlearn/modules/lr_schedulers/linear_warmup_cosine_lr.py</code> <pre><code>@store(  # type: ignore[misc]\n    group=\"modules/lr_schedulers\",\n    provider=\"mmlearn\",\n    zen_partial=True,\n    warmup_steps=MISSING,\n    max_steps=MISSING,\n)\ndef linear_warmup_cosine_annealing_lr(\n    optimizer: Optimizer,\n    warmup_steps: int,\n    max_steps: int,\n    start_factor: float = 1 / 3,\n    eta_min: float = 0.0,\n    last_epoch: int = -1,\n) -&gt; LRScheduler:\n    \"\"\"Create a linear warmup cosine annealing learning rate scheduler.\n\n    Parameters\n    ----------\n    optimizer : Optimizer\n        The optimizer for which to schedule the learning rate.\n    warmup_steps : int\n        Maximum number of iterations for linear warmup.\n    max_steps : int\n        Maximum number of iterations.\n    start_factor : float, optional, default=1/3\n        Multiplicative factor for the learning rate at the start of the warmup phase.\n    eta_min : float, optional, default=0\n        Minimum learning rate.\n    last_epoch : int, optional, default=-1\n        The index of last epoch. If set to ``-1``, it initializes the learning rate\n        as the base learning rate\n\n    Returns\n    -------\n    LRScheduler\n        The learning rate scheduler.\n\n    Raises\n    ------\n    ValueError\n        If `warmup_steps` is greater than or equal to `max_steps` or if `warmup_steps`\n        is less than or equal to 0.\n    \"\"\"\n    if warmup_steps &gt;= max_steps:\n        raise ValueError(\n            \"Expected `warmup_steps` to be less than `max_steps` but got \"\n            f\"`warmup_steps={warmup_steps}` and `max_steps={max_steps}`.\"\n        )\n    if warmup_steps &lt;= 0:\n        raise ValueError(\n            \"Expected `warmup_steps` to be positive but got \"\n            f\"`warmup_steps={warmup_steps}`.\"\n        )\n\n    linear_lr = LinearLR(\n        optimizer,\n        start_factor=start_factor,\n        total_iters=warmup_steps,\n        last_epoch=last_epoch,\n    )\n    cosine_lr = CosineAnnealingLR(\n        optimizer,\n        T_max=max_steps - warmup_steps,\n        eta_min=eta_min,\n        last_epoch=last_epoch,\n    )\n    return SequentialLR(\n        optimizer,\n        schedulers=[linear_lr, cosine_lr],\n        milestones=[warmup_steps],\n        last_epoch=last_epoch,\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.lr_schedulers.linear_warmup_cosine_lr","title":"linear_warmup_cosine_lr","text":"<p>Linear warmup cosine annealing learning rate scheduler.</p>"},{"location":"api/#mmlearn.modules.lr_schedulers.linear_warmup_cosine_lr.linear_warmup_cosine_annealing_lr","title":"linear_warmup_cosine_annealing_lr","text":"<pre><code>linear_warmup_cosine_annealing_lr(\n    optimizer,\n    warmup_steps,\n    max_steps,\n    start_factor=1 / 3,\n    eta_min=0.0,\n    last_epoch=-1,\n)\n</code></pre> <p>Create a linear warmup cosine annealing learning rate scheduler.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>Optimizer</code> <p>The optimizer for which to schedule the learning rate.</p> required <code>warmup_steps</code> <code>int</code> <p>Maximum number of iterations for linear warmup.</p> required <code>max_steps</code> <code>int</code> <p>Maximum number of iterations.</p> required <code>start_factor</code> <code>float</code> <p>Multiplicative factor for the learning rate at the start of the warmup phase.</p> <code>1/3</code> <code>eta_min</code> <code>float</code> <p>Minimum learning rate.</p> <code>0</code> <code>last_epoch</code> <code>int</code> <p>The index of last epoch. If set to <code>-1</code>, it initializes the learning rate as the base learning rate</p> <code>-1</code> <p>Returns:</p> Type Description <code>LRScheduler</code> <p>The learning rate scheduler.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>warmup_steps</code> is greater than or equal to <code>max_steps</code> or if <code>warmup_steps</code> is less than or equal to 0.</p> Source code in <code>mmlearn/modules/lr_schedulers/linear_warmup_cosine_lr.py</code> <pre><code>@store(  # type: ignore[misc]\n    group=\"modules/lr_schedulers\",\n    provider=\"mmlearn\",\n    zen_partial=True,\n    warmup_steps=MISSING,\n    max_steps=MISSING,\n)\ndef linear_warmup_cosine_annealing_lr(\n    optimizer: Optimizer,\n    warmup_steps: int,\n    max_steps: int,\n    start_factor: float = 1 / 3,\n    eta_min: float = 0.0,\n    last_epoch: int = -1,\n) -&gt; LRScheduler:\n    \"\"\"Create a linear warmup cosine annealing learning rate scheduler.\n\n    Parameters\n    ----------\n    optimizer : Optimizer\n        The optimizer for which to schedule the learning rate.\n    warmup_steps : int\n        Maximum number of iterations for linear warmup.\n    max_steps : int\n        Maximum number of iterations.\n    start_factor : float, optional, default=1/3\n        Multiplicative factor for the learning rate at the start of the warmup phase.\n    eta_min : float, optional, default=0\n        Minimum learning rate.\n    last_epoch : int, optional, default=-1\n        The index of last epoch. If set to ``-1``, it initializes the learning rate\n        as the base learning rate\n\n    Returns\n    -------\n    LRScheduler\n        The learning rate scheduler.\n\n    Raises\n    ------\n    ValueError\n        If `warmup_steps` is greater than or equal to `max_steps` or if `warmup_steps`\n        is less than or equal to 0.\n    \"\"\"\n    if warmup_steps &gt;= max_steps:\n        raise ValueError(\n            \"Expected `warmup_steps` to be less than `max_steps` but got \"\n            f\"`warmup_steps={warmup_steps}` and `max_steps={max_steps}`.\"\n        )\n    if warmup_steps &lt;= 0:\n        raise ValueError(\n            \"Expected `warmup_steps` to be positive but got \"\n            f\"`warmup_steps={warmup_steps}`.\"\n        )\n\n    linear_lr = LinearLR(\n        optimizer,\n        start_factor=start_factor,\n        total_iters=warmup_steps,\n        last_epoch=last_epoch,\n    )\n    cosine_lr = CosineAnnealingLR(\n        optimizer,\n        T_max=max_steps - warmup_steps,\n        eta_min=eta_min,\n        last_epoch=last_epoch,\n    )\n    return SequentialLR(\n        optimizer,\n        schedulers=[linear_lr, cosine_lr],\n        milestones=[warmup_steps],\n        last_epoch=last_epoch,\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.metrics","title":"metrics","text":"<p>Metrics for evaluating models.</p>"},{"location":"api/#mmlearn.modules.metrics.RetrievalRecallAtK","title":"RetrievalRecallAtK","text":"<p>               Bases: <code>Metric</code></p> <p>Retrieval Recall@K metric.</p> <p>Computes the Recall@K for retrieval tasks. The metric is computed as follows:</p> <ol> <li>Compute the cosine similarity between the query and the database.</li> <li>For each query, sort the database in decreasing order of similarity.</li> <li>Compute the Recall@K as the number of true positives among the top K elements.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>top_k</code> <code>int</code> <p>The number of top elements to consider for computing the Recall@K.</p> required <code>reduction</code> <code>(mean, sum, none, None)</code> <p>Specifies the reduction to apply after computing the pairwise cosine similarity scores.</p> <code>\"mean\"</code> <code>aggregation</code> <code>(mean, median, min, max)</code> <p>Specifies the aggregation function to apply to the Recall@K values computed in batches. If a callable is provided, it should accept a tensor of values and a keyword argument <code>'dim'</code> and return a single scalar value.</p> <code>\"mean\"</code> <code>kwargs</code> <code>Any</code> <p>Additional arguments to be passed to the class:<code>torchmetrics.Metric</code> class.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <ul> <li>If the <code>top_k</code> is not a positive integer or None.</li> <li>If the <code>reduction</code> is not one of {\"mean\", \"sum\", \"none\", None}.</li> <li>If the <code>aggregation</code> is not one of {\"mean\", \"median\", \"min\", \"max\"} or a   custom callable function.</li> </ul> Source code in <code>mmlearn/modules/metrics/retrieval_recall.py</code> <pre><code>@store(group=\"modules/metrics\", provider=\"mmlearn\")\nclass RetrievalRecallAtK(Metric):\n    \"\"\"Retrieval Recall@K metric.\n\n    Computes the Recall@K for retrieval tasks. The metric is computed as follows:\n\n    1. Compute the cosine similarity between the query and the database.\n    2. For each query, sort the database in decreasing order of similarity.\n    3. Compute the Recall@K as the number of true positives among the top K elements.\n\n    Parameters\n    ----------\n    top_k : int\n        The number of top elements to consider for computing the Recall@K.\n    reduction : {\"mean\", \"sum\", \"none\", None}, optional, default=\"sum\"\n        Specifies the reduction to apply after computing the pairwise cosine similarity\n        scores.\n    aggregation : {\"mean\", \"median\", \"min\", \"max\"} or callable, default=\"mean\"\n        Specifies the aggregation function to apply to the Recall@K values computed\n        in batches. If a callable is provided, it should accept a tensor of values\n        and a keyword argument ``'dim'`` and return a single scalar value.\n    kwargs : Any\n        Additional arguments to be passed to the :py:class:`torchmetrics.Metric` class.\n\n    Raises\n    ------\n    ValueError\n\n        - If the `top_k` is not a positive integer or None.\n        - If the `reduction` is not one of {\"mean\", \"sum\", \"none\", None}.\n        - If the `aggregation` is not one of {\"mean\", \"median\", \"min\", \"max\"} or a\n          custom callable function.\n\n    \"\"\"\n\n    is_differentiable: bool = False\n    higher_is_better: bool = True\n    full_state_update: bool = False\n\n    indexes: list[torch.Tensor]\n    x: list[torch.Tensor]\n    y: list[torch.Tensor]\n    num_samples: torch.Tensor\n\n    def __init__(\n        self,\n        top_k: int,\n        reduction: Optional[Literal[\"mean\", \"sum\", \"none\"]] = \"sum\",\n        aggregation: Union[\n            Literal[\"mean\", \"median\", \"min\", \"max\"],\n            Callable[[torch.Tensor, int], torch.Tensor],\n        ] = \"mean\",\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Initialize the metric.\"\"\"\n        super().__init__(**kwargs)\n\n        if top_k is not None and not (isinstance(top_k, int) and top_k &gt; 0):\n            raise ValueError(\"`top_k` has to be a positive integer or None\")\n        self.top_k = top_k\n\n        allowed_reduction = (\"sum\", \"mean\", \"none\", None)\n        if reduction not in allowed_reduction:\n            raise ValueError(\n                f\"Expected argument `reduction` to be one of {allowed_reduction} but got {reduction}\"\n            )\n        self.reduction = reduction\n\n        if not (\n            aggregation in (\"mean\", \"median\", \"min\", \"max\") or callable(aggregation)\n        ):\n            raise ValueError(\n                \"Argument `aggregation` must be one of `mean`, `median`, `min`, `max` or a custom callable function\"\n                f\"which takes tensor of values, but got {aggregation}.\"\n            )\n        self.aggregation = aggregation\n\n        self.add_state(\"x\", default=[], dist_reduce_fx=\"cat\")\n        self.add_state(\"y\", default=[], dist_reduce_fx=\"cat\")\n        self.add_state(\"indexes\", default=[], dist_reduce_fx=\"cat\")\n        self.add_state(\"num_samples\", default=torch.tensor(0), dist_reduce_fx=\"cat\")\n\n        self._batch_size: int = -1\n\n        self.compute_on_cpu = True\n        self.sync_on_compute = False\n        self.dist_sync_on_step = False\n        self._to_sync = self.sync_on_compute\n        self._should_unsync = False\n\n    def update(self, x: torch.Tensor, y: torch.Tensor, indexes: torch.Tensor) -&gt; None:\n        \"\"\"Check shape, convert dtypes and add to accumulators.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Embeddings (unnormalized) of shape ``(N, D)`` where ``N`` is the number\n            of samples and `D` is the number of dimensions.\n        y : torch.Tensor\n            Embeddings (unnormalized) of shape ``(M, D)`` where ``M`` is the number\n            of samples and ``D`` is the number of dimensions.\n        indexes : torch.Tensor\n            Index tensor of shape ``(N,)`` where ``N`` is the number of samples.\n            This specifies which sample in ``y`` is the positive match for each\n            sample in ``x``.\n\n        Raises\n        ------\n        ValueError\n            If `indexes` is None.\n\n        \"\"\"\n        if indexes is None:\n            raise ValueError(\"Argument `indexes` cannot be None\")\n\n        x, y, indexes = _update_batch_inputs(x.clone(), y.clone(), indexes.clone())\n\n        # offset batch indexes by the number of samples seen so far\n        indexes += self.num_samples\n\n        local_batch_size = torch.zeros_like(self.num_samples) + x.size(0)\n        if self._is_distributed():\n            x = dim_zero_cat(gather_all_tensors(x, self.process_group))\n            y = dim_zero_cat(gather_all_tensors(y, self.process_group))\n            indexes = dim_zero_cat(\n                gather_all_tensors(indexes.clone(), self.process_group)\n            )\n\n            # offset indexes for each device\n            bsz_per_device = dim_zero_cat(\n                gather_all_tensors(local_batch_size, self.process_group)\n            )\n            cum_local_bsz = torch.cumsum(bsz_per_device, dim=0)\n            for device_idx in range(1, bsz_per_device.numel()):\n                indexes[cum_local_bsz[device_idx - 1] : cum_local_bsz[device_idx]] += (\n                    cum_local_bsz[device_idx - 1]\n                )\n\n            # update the global sample count\n            self.num_samples += cum_local_bsz[-1]\n\n            self._is_synced = True\n        else:\n            self.num_samples += x.size(0)\n\n        self.x.append(x)\n        self.y.append(y)\n        self.indexes.append(indexes)\n\n        if self._batch_size == -1:\n            self._batch_size = x.size(0)  # global batch size\n\n    def compute(self) -&gt; torch.Tensor:\n        \"\"\"Compute the metric.\n\n        Returns\n        -------\n        torch.Tensor\n            The computed metric.\n        \"\"\"\n        x = dim_zero_cat(self.x)\n        y = dim_zero_cat(self.y)\n\n        # normalize embeddings\n        x /= x.norm(dim=-1, p=2, keepdim=True)\n        y /= y.norm(dim=-1, p=2, keepdim=True)\n\n        # instantiate reduction function\n        reduction_mapping: Dict[\n            Optional[str], Callable[[torch.Tensor], torch.Tensor]\n        ] = {\n            \"sum\": partial(torch.sum, dim=-1),\n            \"mean\": partial(torch.mean, dim=-1),\n            \"none\": lambda x: x,\n            None: lambda x: x,\n        }\n\n        # concatenate indexes of true pairs\n        indexes = dim_zero_cat(self.indexes)\n\n        results: list[torch.Tensor] = []\n        with concurrent.futures.ThreadPoolExecutor(\n            max_workers=os.cpu_count() or 1  # use all available CPUs\n        ) as executor:\n            futures = [\n                executor.submit(\n                    self._process_batch,\n                    start,\n                    x,\n                    y,\n                    indexes,\n                    reduction_mapping,\n                    self.top_k,\n                )\n                for start in tqdm(\n                    range(0, len(x), self._batch_size), desc=f\"Recall@{self.top_k}\"\n                )\n            ]\n            for future in concurrent.futures.as_completed(futures):\n                results.append(future.result())\n\n        return _retrieval_aggregate(\n            (torch.cat([x.float() for x in results]) &gt; 0).float(), self.aggregation\n        )\n\n    def forward(self, *args: Any, **kwargs: Any) -&gt; Any:\n        \"\"\"Forward method is not supported.\n\n        Raises\n        ------\n        NotImplementedError\n            The forward method is not supported for this metric.\n        \"\"\"\n        raise NotImplementedError(\n            \"RetrievalRecallAtK metric does not support forward method\"\n        )\n\n    def _is_distributed(self) -&gt; bool:\n        if self.distributed_available_fn is not None:\n            distributed_available = self.distributed_available_fn\n\n        return distributed_available() if callable(distributed_available) else False\n\n    def _process_batch(\n        self,\n        start: int,\n        x_norm: torch.Tensor,\n        y_norm: torch.Tensor,\n        indexes: torch.Tensor,\n        reduction_mapping: Dict[Optional[str], Callable[[torch.Tensor], torch.Tensor]],\n        top_k: int,\n    ) -&gt; torch.Tensor:\n        \"\"\"Compute the Recall@K for a batch of samples.\"\"\"\n        end = start + self._batch_size\n        x_norm_batch = x_norm[start:end]\n        indexes_batch = indexes[start:end]\n\n        similarity = _safe_matmul(x_norm_batch, y_norm)\n        scores: torch.Tensor = reduction_mapping[self.reduction](similarity)\n\n        with torch.inference_mode():\n            positive_pairs = torch.zeros_like(scores, dtype=torch.bool)\n            positive_pairs[torch.arange(len(scores)), indexes_batch] = True\n\n        return _recall_at_k(scores, positive_pairs, top_k)\n</code></pre>"},{"location":"api/#mmlearn.modules.metrics.RetrievalRecallAtK.__init__","title":"__init__","text":"<pre><code>__init__(\n    top_k, reduction=\"sum\", aggregation=\"mean\", **kwargs\n)\n</code></pre> <p>Initialize the metric.</p> Source code in <code>mmlearn/modules/metrics/retrieval_recall.py</code> <pre><code>def __init__(\n    self,\n    top_k: int,\n    reduction: Optional[Literal[\"mean\", \"sum\", \"none\"]] = \"sum\",\n    aggregation: Union[\n        Literal[\"mean\", \"median\", \"min\", \"max\"],\n        Callable[[torch.Tensor, int], torch.Tensor],\n    ] = \"mean\",\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Initialize the metric.\"\"\"\n    super().__init__(**kwargs)\n\n    if top_k is not None and not (isinstance(top_k, int) and top_k &gt; 0):\n        raise ValueError(\"`top_k` has to be a positive integer or None\")\n    self.top_k = top_k\n\n    allowed_reduction = (\"sum\", \"mean\", \"none\", None)\n    if reduction not in allowed_reduction:\n        raise ValueError(\n            f\"Expected argument `reduction` to be one of {allowed_reduction} but got {reduction}\"\n        )\n    self.reduction = reduction\n\n    if not (\n        aggregation in (\"mean\", \"median\", \"min\", \"max\") or callable(aggregation)\n    ):\n        raise ValueError(\n            \"Argument `aggregation` must be one of `mean`, `median`, `min`, `max` or a custom callable function\"\n            f\"which takes tensor of values, but got {aggregation}.\"\n        )\n    self.aggregation = aggregation\n\n    self.add_state(\"x\", default=[], dist_reduce_fx=\"cat\")\n    self.add_state(\"y\", default=[], dist_reduce_fx=\"cat\")\n    self.add_state(\"indexes\", default=[], dist_reduce_fx=\"cat\")\n    self.add_state(\"num_samples\", default=torch.tensor(0), dist_reduce_fx=\"cat\")\n\n    self._batch_size: int = -1\n\n    self.compute_on_cpu = True\n    self.sync_on_compute = False\n    self.dist_sync_on_step = False\n    self._to_sync = self.sync_on_compute\n    self._should_unsync = False\n</code></pre>"},{"location":"api/#mmlearn.modules.metrics.RetrievalRecallAtK.update","title":"update","text":"<pre><code>update(x, y, indexes)\n</code></pre> <p>Check shape, convert dtypes and add to accumulators.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Embeddings (unnormalized) of shape <code>(N, D)</code> where <code>N</code> is the number of samples and <code>D</code> is the number of dimensions.</p> required <code>y</code> <code>Tensor</code> <p>Embeddings (unnormalized) of shape <code>(M, D)</code> where <code>M</code> is the number of samples and <code>D</code> is the number of dimensions.</p> required <code>indexes</code> <code>Tensor</code> <p>Index tensor of shape <code>(N,)</code> where <code>N</code> is the number of samples. This specifies which sample in <code>y</code> is the positive match for each sample in <code>x</code>.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>indexes</code> is None.</p> Source code in <code>mmlearn/modules/metrics/retrieval_recall.py</code> <pre><code>def update(self, x: torch.Tensor, y: torch.Tensor, indexes: torch.Tensor) -&gt; None:\n    \"\"\"Check shape, convert dtypes and add to accumulators.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Embeddings (unnormalized) of shape ``(N, D)`` where ``N`` is the number\n        of samples and `D` is the number of dimensions.\n    y : torch.Tensor\n        Embeddings (unnormalized) of shape ``(M, D)`` where ``M`` is the number\n        of samples and ``D`` is the number of dimensions.\n    indexes : torch.Tensor\n        Index tensor of shape ``(N,)`` where ``N`` is the number of samples.\n        This specifies which sample in ``y`` is the positive match for each\n        sample in ``x``.\n\n    Raises\n    ------\n    ValueError\n        If `indexes` is None.\n\n    \"\"\"\n    if indexes is None:\n        raise ValueError(\"Argument `indexes` cannot be None\")\n\n    x, y, indexes = _update_batch_inputs(x.clone(), y.clone(), indexes.clone())\n\n    # offset batch indexes by the number of samples seen so far\n    indexes += self.num_samples\n\n    local_batch_size = torch.zeros_like(self.num_samples) + x.size(0)\n    if self._is_distributed():\n        x = dim_zero_cat(gather_all_tensors(x, self.process_group))\n        y = dim_zero_cat(gather_all_tensors(y, self.process_group))\n        indexes = dim_zero_cat(\n            gather_all_tensors(indexes.clone(), self.process_group)\n        )\n\n        # offset indexes for each device\n        bsz_per_device = dim_zero_cat(\n            gather_all_tensors(local_batch_size, self.process_group)\n        )\n        cum_local_bsz = torch.cumsum(bsz_per_device, dim=0)\n        for device_idx in range(1, bsz_per_device.numel()):\n            indexes[cum_local_bsz[device_idx - 1] : cum_local_bsz[device_idx]] += (\n                cum_local_bsz[device_idx - 1]\n            )\n\n        # update the global sample count\n        self.num_samples += cum_local_bsz[-1]\n\n        self._is_synced = True\n    else:\n        self.num_samples += x.size(0)\n\n    self.x.append(x)\n    self.y.append(y)\n    self.indexes.append(indexes)\n\n    if self._batch_size == -1:\n        self._batch_size = x.size(0)  # global batch size\n</code></pre>"},{"location":"api/#mmlearn.modules.metrics.RetrievalRecallAtK.compute","title":"compute","text":"<pre><code>compute()\n</code></pre> <p>Compute the metric.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>The computed metric.</p> Source code in <code>mmlearn/modules/metrics/retrieval_recall.py</code> <pre><code>def compute(self) -&gt; torch.Tensor:\n    \"\"\"Compute the metric.\n\n    Returns\n    -------\n    torch.Tensor\n        The computed metric.\n    \"\"\"\n    x = dim_zero_cat(self.x)\n    y = dim_zero_cat(self.y)\n\n    # normalize embeddings\n    x /= x.norm(dim=-1, p=2, keepdim=True)\n    y /= y.norm(dim=-1, p=2, keepdim=True)\n\n    # instantiate reduction function\n    reduction_mapping: Dict[\n        Optional[str], Callable[[torch.Tensor], torch.Tensor]\n    ] = {\n        \"sum\": partial(torch.sum, dim=-1),\n        \"mean\": partial(torch.mean, dim=-1),\n        \"none\": lambda x: x,\n        None: lambda x: x,\n    }\n\n    # concatenate indexes of true pairs\n    indexes = dim_zero_cat(self.indexes)\n\n    results: list[torch.Tensor] = []\n    with concurrent.futures.ThreadPoolExecutor(\n        max_workers=os.cpu_count() or 1  # use all available CPUs\n    ) as executor:\n        futures = [\n            executor.submit(\n                self._process_batch,\n                start,\n                x,\n                y,\n                indexes,\n                reduction_mapping,\n                self.top_k,\n            )\n            for start in tqdm(\n                range(0, len(x), self._batch_size), desc=f\"Recall@{self.top_k}\"\n            )\n        ]\n        for future in concurrent.futures.as_completed(futures):\n            results.append(future.result())\n\n    return _retrieval_aggregate(\n        (torch.cat([x.float() for x in results]) &gt; 0).float(), self.aggregation\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.metrics.RetrievalRecallAtK.forward","title":"forward","text":"<pre><code>forward(*args, **kwargs)\n</code></pre> <p>Forward method is not supported.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>The forward method is not supported for this metric.</p> Source code in <code>mmlearn/modules/metrics/retrieval_recall.py</code> <pre><code>def forward(self, *args: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"Forward method is not supported.\n\n    Raises\n    ------\n    NotImplementedError\n        The forward method is not supported for this metric.\n    \"\"\"\n    raise NotImplementedError(\n        \"RetrievalRecallAtK metric does not support forward method\"\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.metrics.retrieval_recall","title":"retrieval_recall","text":"<p>Retrieval Recall@K metric.</p>"},{"location":"api/#mmlearn.modules.metrics.retrieval_recall.RetrievalRecallAtK","title":"RetrievalRecallAtK","text":"<p>               Bases: <code>Metric</code></p> <p>Retrieval Recall@K metric.</p> <p>Computes the Recall@K for retrieval tasks. The metric is computed as follows:</p> <ol> <li>Compute the cosine similarity between the query and the database.</li> <li>For each query, sort the database in decreasing order of similarity.</li> <li>Compute the Recall@K as the number of true positives among the top K elements.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>top_k</code> <code>int</code> <p>The number of top elements to consider for computing the Recall@K.</p> required <code>reduction</code> <code>(mean, sum, none, None)</code> <p>Specifies the reduction to apply after computing the pairwise cosine similarity scores.</p> <code>\"mean\"</code> <code>aggregation</code> <code>(mean, median, min, max)</code> <p>Specifies the aggregation function to apply to the Recall@K values computed in batches. If a callable is provided, it should accept a tensor of values and a keyword argument <code>'dim'</code> and return a single scalar value.</p> <code>\"mean\"</code> <code>kwargs</code> <code>Any</code> <p>Additional arguments to be passed to the class:<code>torchmetrics.Metric</code> class.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <ul> <li>If the <code>top_k</code> is not a positive integer or None.</li> <li>If the <code>reduction</code> is not one of {\"mean\", \"sum\", \"none\", None}.</li> <li>If the <code>aggregation</code> is not one of {\"mean\", \"median\", \"min\", \"max\"} or a   custom callable function.</li> </ul> Source code in <code>mmlearn/modules/metrics/retrieval_recall.py</code> <pre><code>@store(group=\"modules/metrics\", provider=\"mmlearn\")\nclass RetrievalRecallAtK(Metric):\n    \"\"\"Retrieval Recall@K metric.\n\n    Computes the Recall@K for retrieval tasks. The metric is computed as follows:\n\n    1. Compute the cosine similarity between the query and the database.\n    2. For each query, sort the database in decreasing order of similarity.\n    3. Compute the Recall@K as the number of true positives among the top K elements.\n\n    Parameters\n    ----------\n    top_k : int\n        The number of top elements to consider for computing the Recall@K.\n    reduction : {\"mean\", \"sum\", \"none\", None}, optional, default=\"sum\"\n        Specifies the reduction to apply after computing the pairwise cosine similarity\n        scores.\n    aggregation : {\"mean\", \"median\", \"min\", \"max\"} or callable, default=\"mean\"\n        Specifies the aggregation function to apply to the Recall@K values computed\n        in batches. If a callable is provided, it should accept a tensor of values\n        and a keyword argument ``'dim'`` and return a single scalar value.\n    kwargs : Any\n        Additional arguments to be passed to the :py:class:`torchmetrics.Metric` class.\n\n    Raises\n    ------\n    ValueError\n\n        - If the `top_k` is not a positive integer or None.\n        - If the `reduction` is not one of {\"mean\", \"sum\", \"none\", None}.\n        - If the `aggregation` is not one of {\"mean\", \"median\", \"min\", \"max\"} or a\n          custom callable function.\n\n    \"\"\"\n\n    is_differentiable: bool = False\n    higher_is_better: bool = True\n    full_state_update: bool = False\n\n    indexes: list[torch.Tensor]\n    x: list[torch.Tensor]\n    y: list[torch.Tensor]\n    num_samples: torch.Tensor\n\n    def __init__(\n        self,\n        top_k: int,\n        reduction: Optional[Literal[\"mean\", \"sum\", \"none\"]] = \"sum\",\n        aggregation: Union[\n            Literal[\"mean\", \"median\", \"min\", \"max\"],\n            Callable[[torch.Tensor, int], torch.Tensor],\n        ] = \"mean\",\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Initialize the metric.\"\"\"\n        super().__init__(**kwargs)\n\n        if top_k is not None and not (isinstance(top_k, int) and top_k &gt; 0):\n            raise ValueError(\"`top_k` has to be a positive integer or None\")\n        self.top_k = top_k\n\n        allowed_reduction = (\"sum\", \"mean\", \"none\", None)\n        if reduction not in allowed_reduction:\n            raise ValueError(\n                f\"Expected argument `reduction` to be one of {allowed_reduction} but got {reduction}\"\n            )\n        self.reduction = reduction\n\n        if not (\n            aggregation in (\"mean\", \"median\", \"min\", \"max\") or callable(aggregation)\n        ):\n            raise ValueError(\n                \"Argument `aggregation` must be one of `mean`, `median`, `min`, `max` or a custom callable function\"\n                f\"which takes tensor of values, but got {aggregation}.\"\n            )\n        self.aggregation = aggregation\n\n        self.add_state(\"x\", default=[], dist_reduce_fx=\"cat\")\n        self.add_state(\"y\", default=[], dist_reduce_fx=\"cat\")\n        self.add_state(\"indexes\", default=[], dist_reduce_fx=\"cat\")\n        self.add_state(\"num_samples\", default=torch.tensor(0), dist_reduce_fx=\"cat\")\n\n        self._batch_size: int = -1\n\n        self.compute_on_cpu = True\n        self.sync_on_compute = False\n        self.dist_sync_on_step = False\n        self._to_sync = self.sync_on_compute\n        self._should_unsync = False\n\n    def update(self, x: torch.Tensor, y: torch.Tensor, indexes: torch.Tensor) -&gt; None:\n        \"\"\"Check shape, convert dtypes and add to accumulators.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Embeddings (unnormalized) of shape ``(N, D)`` where ``N`` is the number\n            of samples and `D` is the number of dimensions.\n        y : torch.Tensor\n            Embeddings (unnormalized) of shape ``(M, D)`` where ``M`` is the number\n            of samples and ``D`` is the number of dimensions.\n        indexes : torch.Tensor\n            Index tensor of shape ``(N,)`` where ``N`` is the number of samples.\n            This specifies which sample in ``y`` is the positive match for each\n            sample in ``x``.\n\n        Raises\n        ------\n        ValueError\n            If `indexes` is None.\n\n        \"\"\"\n        if indexes is None:\n            raise ValueError(\"Argument `indexes` cannot be None\")\n\n        x, y, indexes = _update_batch_inputs(x.clone(), y.clone(), indexes.clone())\n\n        # offset batch indexes by the number of samples seen so far\n        indexes += self.num_samples\n\n        local_batch_size = torch.zeros_like(self.num_samples) + x.size(0)\n        if self._is_distributed():\n            x = dim_zero_cat(gather_all_tensors(x, self.process_group))\n            y = dim_zero_cat(gather_all_tensors(y, self.process_group))\n            indexes = dim_zero_cat(\n                gather_all_tensors(indexes.clone(), self.process_group)\n            )\n\n            # offset indexes for each device\n            bsz_per_device = dim_zero_cat(\n                gather_all_tensors(local_batch_size, self.process_group)\n            )\n            cum_local_bsz = torch.cumsum(bsz_per_device, dim=0)\n            for device_idx in range(1, bsz_per_device.numel()):\n                indexes[cum_local_bsz[device_idx - 1] : cum_local_bsz[device_idx]] += (\n                    cum_local_bsz[device_idx - 1]\n                )\n\n            # update the global sample count\n            self.num_samples += cum_local_bsz[-1]\n\n            self._is_synced = True\n        else:\n            self.num_samples += x.size(0)\n\n        self.x.append(x)\n        self.y.append(y)\n        self.indexes.append(indexes)\n\n        if self._batch_size == -1:\n            self._batch_size = x.size(0)  # global batch size\n\n    def compute(self) -&gt; torch.Tensor:\n        \"\"\"Compute the metric.\n\n        Returns\n        -------\n        torch.Tensor\n            The computed metric.\n        \"\"\"\n        x = dim_zero_cat(self.x)\n        y = dim_zero_cat(self.y)\n\n        # normalize embeddings\n        x /= x.norm(dim=-1, p=2, keepdim=True)\n        y /= y.norm(dim=-1, p=2, keepdim=True)\n\n        # instantiate reduction function\n        reduction_mapping: Dict[\n            Optional[str], Callable[[torch.Tensor], torch.Tensor]\n        ] = {\n            \"sum\": partial(torch.sum, dim=-1),\n            \"mean\": partial(torch.mean, dim=-1),\n            \"none\": lambda x: x,\n            None: lambda x: x,\n        }\n\n        # concatenate indexes of true pairs\n        indexes = dim_zero_cat(self.indexes)\n\n        results: list[torch.Tensor] = []\n        with concurrent.futures.ThreadPoolExecutor(\n            max_workers=os.cpu_count() or 1  # use all available CPUs\n        ) as executor:\n            futures = [\n                executor.submit(\n                    self._process_batch,\n                    start,\n                    x,\n                    y,\n                    indexes,\n                    reduction_mapping,\n                    self.top_k,\n                )\n                for start in tqdm(\n                    range(0, len(x), self._batch_size), desc=f\"Recall@{self.top_k}\"\n                )\n            ]\n            for future in concurrent.futures.as_completed(futures):\n                results.append(future.result())\n\n        return _retrieval_aggregate(\n            (torch.cat([x.float() for x in results]) &gt; 0).float(), self.aggregation\n        )\n\n    def forward(self, *args: Any, **kwargs: Any) -&gt; Any:\n        \"\"\"Forward method is not supported.\n\n        Raises\n        ------\n        NotImplementedError\n            The forward method is not supported for this metric.\n        \"\"\"\n        raise NotImplementedError(\n            \"RetrievalRecallAtK metric does not support forward method\"\n        )\n\n    def _is_distributed(self) -&gt; bool:\n        if self.distributed_available_fn is not None:\n            distributed_available = self.distributed_available_fn\n\n        return distributed_available() if callable(distributed_available) else False\n\n    def _process_batch(\n        self,\n        start: int,\n        x_norm: torch.Tensor,\n        y_norm: torch.Tensor,\n        indexes: torch.Tensor,\n        reduction_mapping: Dict[Optional[str], Callable[[torch.Tensor], torch.Tensor]],\n        top_k: int,\n    ) -&gt; torch.Tensor:\n        \"\"\"Compute the Recall@K for a batch of samples.\"\"\"\n        end = start + self._batch_size\n        x_norm_batch = x_norm[start:end]\n        indexes_batch = indexes[start:end]\n\n        similarity = _safe_matmul(x_norm_batch, y_norm)\n        scores: torch.Tensor = reduction_mapping[self.reduction](similarity)\n\n        with torch.inference_mode():\n            positive_pairs = torch.zeros_like(scores, dtype=torch.bool)\n            positive_pairs[torch.arange(len(scores)), indexes_batch] = True\n\n        return _recall_at_k(scores, positive_pairs, top_k)\n</code></pre> <code></code> __init__ \u00b6 <pre><code>__init__(\n    top_k, reduction=\"sum\", aggregation=\"mean\", **kwargs\n)\n</code></pre> <p>Initialize the metric.</p> Source code in <code>mmlearn/modules/metrics/retrieval_recall.py</code> <pre><code>def __init__(\n    self,\n    top_k: int,\n    reduction: Optional[Literal[\"mean\", \"sum\", \"none\"]] = \"sum\",\n    aggregation: Union[\n        Literal[\"mean\", \"median\", \"min\", \"max\"],\n        Callable[[torch.Tensor, int], torch.Tensor],\n    ] = \"mean\",\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Initialize the metric.\"\"\"\n    super().__init__(**kwargs)\n\n    if top_k is not None and not (isinstance(top_k, int) and top_k &gt; 0):\n        raise ValueError(\"`top_k` has to be a positive integer or None\")\n    self.top_k = top_k\n\n    allowed_reduction = (\"sum\", \"mean\", \"none\", None)\n    if reduction not in allowed_reduction:\n        raise ValueError(\n            f\"Expected argument `reduction` to be one of {allowed_reduction} but got {reduction}\"\n        )\n    self.reduction = reduction\n\n    if not (\n        aggregation in (\"mean\", \"median\", \"min\", \"max\") or callable(aggregation)\n    ):\n        raise ValueError(\n            \"Argument `aggregation` must be one of `mean`, `median`, `min`, `max` or a custom callable function\"\n            f\"which takes tensor of values, but got {aggregation}.\"\n        )\n    self.aggregation = aggregation\n\n    self.add_state(\"x\", default=[], dist_reduce_fx=\"cat\")\n    self.add_state(\"y\", default=[], dist_reduce_fx=\"cat\")\n    self.add_state(\"indexes\", default=[], dist_reduce_fx=\"cat\")\n    self.add_state(\"num_samples\", default=torch.tensor(0), dist_reduce_fx=\"cat\")\n\n    self._batch_size: int = -1\n\n    self.compute_on_cpu = True\n    self.sync_on_compute = False\n    self.dist_sync_on_step = False\n    self._to_sync = self.sync_on_compute\n    self._should_unsync = False\n</code></pre> <code></code> update \u00b6 <pre><code>update(x, y, indexes)\n</code></pre> <p>Check shape, convert dtypes and add to accumulators.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Embeddings (unnormalized) of shape <code>(N, D)</code> where <code>N</code> is the number of samples and <code>D</code> is the number of dimensions.</p> required <code>y</code> <code>Tensor</code> <p>Embeddings (unnormalized) of shape <code>(M, D)</code> where <code>M</code> is the number of samples and <code>D</code> is the number of dimensions.</p> required <code>indexes</code> <code>Tensor</code> <p>Index tensor of shape <code>(N,)</code> where <code>N</code> is the number of samples. This specifies which sample in <code>y</code> is the positive match for each sample in <code>x</code>.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>indexes</code> is None.</p> Source code in <code>mmlearn/modules/metrics/retrieval_recall.py</code> <pre><code>def update(self, x: torch.Tensor, y: torch.Tensor, indexes: torch.Tensor) -&gt; None:\n    \"\"\"Check shape, convert dtypes and add to accumulators.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Embeddings (unnormalized) of shape ``(N, D)`` where ``N`` is the number\n        of samples and `D` is the number of dimensions.\n    y : torch.Tensor\n        Embeddings (unnormalized) of shape ``(M, D)`` where ``M`` is the number\n        of samples and ``D`` is the number of dimensions.\n    indexes : torch.Tensor\n        Index tensor of shape ``(N,)`` where ``N`` is the number of samples.\n        This specifies which sample in ``y`` is the positive match for each\n        sample in ``x``.\n\n    Raises\n    ------\n    ValueError\n        If `indexes` is None.\n\n    \"\"\"\n    if indexes is None:\n        raise ValueError(\"Argument `indexes` cannot be None\")\n\n    x, y, indexes = _update_batch_inputs(x.clone(), y.clone(), indexes.clone())\n\n    # offset batch indexes by the number of samples seen so far\n    indexes += self.num_samples\n\n    local_batch_size = torch.zeros_like(self.num_samples) + x.size(0)\n    if self._is_distributed():\n        x = dim_zero_cat(gather_all_tensors(x, self.process_group))\n        y = dim_zero_cat(gather_all_tensors(y, self.process_group))\n        indexes = dim_zero_cat(\n            gather_all_tensors(indexes.clone(), self.process_group)\n        )\n\n        # offset indexes for each device\n        bsz_per_device = dim_zero_cat(\n            gather_all_tensors(local_batch_size, self.process_group)\n        )\n        cum_local_bsz = torch.cumsum(bsz_per_device, dim=0)\n        for device_idx in range(1, bsz_per_device.numel()):\n            indexes[cum_local_bsz[device_idx - 1] : cum_local_bsz[device_idx]] += (\n                cum_local_bsz[device_idx - 1]\n            )\n\n        # update the global sample count\n        self.num_samples += cum_local_bsz[-1]\n\n        self._is_synced = True\n    else:\n        self.num_samples += x.size(0)\n\n    self.x.append(x)\n    self.y.append(y)\n    self.indexes.append(indexes)\n\n    if self._batch_size == -1:\n        self._batch_size = x.size(0)  # global batch size\n</code></pre> <code></code> compute \u00b6 <pre><code>compute()\n</code></pre> <p>Compute the metric.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>The computed metric.</p> Source code in <code>mmlearn/modules/metrics/retrieval_recall.py</code> <pre><code>def compute(self) -&gt; torch.Tensor:\n    \"\"\"Compute the metric.\n\n    Returns\n    -------\n    torch.Tensor\n        The computed metric.\n    \"\"\"\n    x = dim_zero_cat(self.x)\n    y = dim_zero_cat(self.y)\n\n    # normalize embeddings\n    x /= x.norm(dim=-1, p=2, keepdim=True)\n    y /= y.norm(dim=-1, p=2, keepdim=True)\n\n    # instantiate reduction function\n    reduction_mapping: Dict[\n        Optional[str], Callable[[torch.Tensor], torch.Tensor]\n    ] = {\n        \"sum\": partial(torch.sum, dim=-1),\n        \"mean\": partial(torch.mean, dim=-1),\n        \"none\": lambda x: x,\n        None: lambda x: x,\n    }\n\n    # concatenate indexes of true pairs\n    indexes = dim_zero_cat(self.indexes)\n\n    results: list[torch.Tensor] = []\n    with concurrent.futures.ThreadPoolExecutor(\n        max_workers=os.cpu_count() or 1  # use all available CPUs\n    ) as executor:\n        futures = [\n            executor.submit(\n                self._process_batch,\n                start,\n                x,\n                y,\n                indexes,\n                reduction_mapping,\n                self.top_k,\n            )\n            for start in tqdm(\n                range(0, len(x), self._batch_size), desc=f\"Recall@{self.top_k}\"\n            )\n        ]\n        for future in concurrent.futures.as_completed(futures):\n            results.append(future.result())\n\n    return _retrieval_aggregate(\n        (torch.cat([x.float() for x in results]) &gt; 0).float(), self.aggregation\n    )\n</code></pre> <code></code> forward \u00b6 <pre><code>forward(*args, **kwargs)\n</code></pre> <p>Forward method is not supported.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>The forward method is not supported for this metric.</p> Source code in <code>mmlearn/modules/metrics/retrieval_recall.py</code> <pre><code>def forward(self, *args: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"Forward method is not supported.\n\n    Raises\n    ------\n    NotImplementedError\n        The forward method is not supported for this metric.\n    \"\"\"\n    raise NotImplementedError(\n        \"RetrievalRecallAtK metric does not support forward method\"\n    )\n</code></pre>"},{"location":"api/#mmlearn.tasks","title":"tasks","text":"<p>Modules for pretraining, downstream and evaluation tasks.</p>"},{"location":"api/#mmlearn.tasks.ContrastivePretraining","title":"ContrastivePretraining","text":"<p>               Bases: <code>TrainingTask</code></p> <p>Contrastive pretraining task.</p> <p>This class supports contrastive pretraining with <code>N</code> modalities of data. It allows the sharing of encoders, heads, and postprocessors across modalities. It also supports computing the contrastive loss between specified pairs of modalities, as well as training auxiliary tasks alongside the main contrastive pretraining task.</p> <p>Parameters:</p> Name Type Description Default <code>encoders</code> <code>dict[str, Module]</code> <p>A dictionary of encoders. The keys can be any string, including the names of any supported modalities. If the keys are not supported modalities, the <code>modality_module_mapping</code> parameter must be provided to map the encoders to specific modalities. The encoders are expected to take a dictionary of input values and return a list-like object with the first element being the encoded values. This first element is passed on to the heads or postprocessors and the remaining elements are ignored.</p> required <code>heads</code> <code>Optional[dict[str, Union[Module, dict[str, Module]]]]</code> <p>A dictionary of modules that process the encoder outputs, usually projection heads. If the keys do not correspond to the name of a supported modality, the <code>modality_module_mapping</code> parameter must be provided. If any of the values are dictionaries, they will be wrapped in a class:<code>torch.nn.Sequential</code> module. All head modules are expected to take a single input tensor and return a single output tensor.</p> <code>None</code> <code>postprocessors</code> <code>Optional[dict[str, Union[Module, dict[str, Module]]]]</code> <p>A dictionary of modules that process the head outputs. If the keys do not correspond to the name of a supported modality, the <code>modality_module_mapping</code> parameter must be provided. If any of the values are dictionaries, they will be wrapped in a <code>nn.Sequential</code> module. All postprocessor modules are expected to take a single input tensor and return a single output tensor.</p> <code>None</code> <code>modality_module_mapping</code> <code>Optional[dict[str, ModuleKeySpec]]</code> <p>A dictionary mapping modalities to encoders, heads, and postprocessors. Useful for reusing the same instance of a module across multiple modalities.</p> <code>None</code> <code>optimizer</code> <code>Optional[partial[Optimizer]]</code> <p>The optimizer to use for training. This is expected to be a func:<code>~functools.partial</code> function that takes the model parameters as the only required argument. If not provided, training will continue without an optimizer.</p> <code>None</code> <code>lr_scheduler</code> <code>Optional[Union[dict[str, Union[partial[LRScheduler], Any]], partial[LRScheduler]]]</code> <p>The learning rate scheduler to use for training. This can be a func:<code>~functools.partial</code> function that takes the optimizer as the only required argument or a dictionary with a <code>'scheduler'</code> key that specifies the scheduler and an optional <code>'extras'</code> key that specifies additional arguments to pass to the scheduler. If not provided, the learning rate will not be adjusted during training.</p> <code>None</code> <code>init_logit_scale</code> <code>float</code> <p>The initial value of the logit scale parameter. This is the log of the scale factor applied to the logits before computing the contrastive loss.</p> <code>1 / 0.07</code> <code>max_logit_scale</code> <code>float</code> <p>The maximum value of the logit scale parameter. The logit scale parameter is clamped to the range <code>[0, log(max_logit_scale)]</code>.</p> <code>100</code> <code>learnable_logit_scale</code> <code>bool</code> <p>Whether the logit scale parameter is learnable. If set to False, the logit scale parameter is treated as a constant.</p> <code>True</code> <code>loss</code> <code>Optional[Module]</code> <p>The loss function to use.</p> <code>None</code> <code>modality_loss_pairs</code> <code>Optional[list[LossPairSpec]]</code> <p>A list of pairs of modalities to compute the contrastive loss between and the weight to apply to each pair.</p> <code>None</code> <code>auxiliary_tasks</code> <code>dict[str, AuxiliaryTaskSpec]</code> <p>Auxiliary tasks to run alongside the main contrastive pretraining task.</p> <ul> <li>The auxiliary task module is expected to be a partially-initialized instance   of a class:<code>~lightning.pytorch.core.LightningModule</code> created using   func:<code>functools.partial</code>, such that an initialized encoder can be   passed as the only argument.</li> <li>The <code>modality</code> parameter specifies the modality of the encoder to use   for the auxiliary task. The <code>loss_weight</code> parameter specifies the weight   to apply to the auxiliary task loss.</li> </ul> <code>None</code> <code>log_auxiliary_tasks_loss</code> <code>bool</code> <p>Whether to log the loss of auxiliary tasks to the main logger.</p> <code>False</code> <code>compute_validation_loss</code> <code>bool</code> <p>Whether to compute the validation loss if a validation dataloader is provided. The loss function must be provided to compute the validation loss.</p> <code>True</code> <code>compute_test_loss</code> <code>bool</code> <p>Whether to compute the test loss if a test dataloader is provided. The loss function must be provided to compute the test loss.</p> <code>True</code> <code>evaluation_tasks</code> <code>Optional[dict[str, EvaluationSpec]]</code> <p>Evaluation tasks to run during validation, while training, and during testing.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <ul> <li>If the loss function is not provided and either the validation or test loss   needs to be computed.</li> <li>If the given modality is not supported.</li> <li>If the encoder, head, or postprocessor is not mapped to a modality.</li> <li>If an unsupported modality is found in the loss pair specification.</li> <li>If an unsupported modality is found in the auxiliary tasks.</li> <li>If the auxiliary task is not a partial function.</li> <li>If the evaluation task is not an instance of class:<code>~mmlearn.tasks.hooks.EvaluationHooks</code>.</li> </ul> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>@store(group=\"task\", provider=\"mmlearn\")\nclass ContrastivePretraining(TrainingTask):\n    \"\"\"Contrastive pretraining task.\n\n    This class supports contrastive pretraining with ``N`` modalities of data. It\n    allows the sharing of encoders, heads, and postprocessors across modalities.\n    It also supports computing the contrastive loss between specified pairs of\n    modalities, as well as training auxiliary tasks alongside the main contrastive\n    pretraining task.\n\n    Parameters\n    ----------\n    encoders : dict[str, torch.nn.Module]\n        A dictionary of encoders. The keys can be any string, including the names of\n        any supported modalities. If the keys are not supported modalities, the\n        ``modality_module_mapping`` parameter must be provided to map the encoders to\n        specific modalities. The encoders are expected to take a dictionary of input\n        values and return a list-like object with the first element being the encoded\n        values. This first element is passed on to the heads or postprocessors and\n        the remaining elements are ignored.\n    heads : Optional[dict[str, Union[torch.nn.Module, dict[str, torch.nn.Module]]]], optional, default=None\n        A dictionary of modules that process the encoder outputs, usually projection\n        heads. If the keys do not correspond to the name of a supported modality,\n        the ``modality_module_mapping`` parameter must be provided. If any of the values\n        are dictionaries, they will be wrapped in a :py:class:`torch.nn.Sequential`\n        module. All head modules are expected to take a single input tensor and\n        return a single output tensor.\n    postprocessors : Optional[dict[str, Union[torch.nn.Module, dict[str, torch.nn.Module]]]], optional, default=None\n        A dictionary of modules that process the head outputs. If the keys do not\n        correspond to the name of a supported modality, the `modality_module_mapping`\n        parameter must be provided. If any of the values are dictionaries, they will\n        be wrapped in a `nn.Sequential` module. All postprocessor modules are expected\n        to take a single input tensor and return a single output tensor.\n    modality_module_mapping : Optional[dict[str, ModuleKeySpec]], optional, default=None\n        A dictionary mapping modalities to encoders, heads, and postprocessors.\n        Useful for reusing the same instance of a module across multiple modalities.\n    optimizer : Optional[partial[torch.optim.Optimizer]], optional, default=None\n        The optimizer to use for training. This is expected to be a :py:func:`~functools.partial`\n        function that takes the model parameters as the only required argument.\n        If not provided, training will continue without an optimizer.\n    lr_scheduler : Optional[Union[dict[str, Union[partial[torch.optim.lr_scheduler.LRScheduler], Any]], partial[torch.optim.lr_scheduler.LRScheduler]]], optional, default=None\n        The learning rate scheduler to use for training. This can be a\n        :py:func:`~functools.partial` function that takes the optimizer as the only\n        required argument or a dictionary with a ``'scheduler'`` key that specifies\n        the scheduler and an optional ``'extras'`` key that specifies additional\n        arguments to pass to the scheduler. If not provided, the learning rate will\n        not be adjusted during training.\n    init_logit_scale : float, optional, default=1 / 0.07\n        The initial value of the logit scale parameter. This is the log of the scale\n        factor applied to the logits before computing the contrastive loss.\n    max_logit_scale : float, optional, default=100\n        The maximum value of the logit scale parameter. The logit scale parameter\n        is clamped to the range ``[0, log(max_logit_scale)]``.\n    learnable_logit_scale : bool, optional, default=True\n        Whether the logit scale parameter is learnable. If set to False, the logit\n        scale parameter is treated as a constant.\n    loss : Optional[torch.nn.Module], optional, default=None\n        The loss function to use.\n    modality_loss_pairs : Optional[list[LossPairSpec]], optional, default=None\n        A list of pairs of modalities to compute the contrastive loss between and\n        the weight to apply to each pair.\n    auxiliary_tasks : dict[str, AuxiliaryTaskSpec], optional, default=None\n        Auxiliary tasks to run alongside the main contrastive pretraining task.\n\n        - The auxiliary task module is expected to be a partially-initialized instance\n          of a :py:class:`~lightning.pytorch.core.LightningModule` created using\n          :py:func:`functools.partial`, such that an initialized encoder can be\n          passed as the only argument.\n        - The ``modality`` parameter specifies the modality of the encoder to use\n          for the auxiliary task. The ``loss_weight`` parameter specifies the weight\n          to apply to the auxiliary task loss.\n    log_auxiliary_tasks_loss : bool, optional, default=False\n        Whether to log the loss of auxiliary tasks to the main logger.\n    compute_validation_loss : bool, optional, default=True\n        Whether to compute the validation loss if a validation dataloader is provided.\n        The loss function must be provided to compute the validation loss.\n    compute_test_loss : bool, optional, default=True\n        Whether to compute the test loss if a test dataloader is provided. The loss\n        function must be provided to compute the test loss.\n    evaluation_tasks : Optional[dict[str, EvaluationSpec]], optional, default=None\n        Evaluation tasks to run during validation, while training, and during testing.\n\n    Raises\n    ------\n    ValueError\n\n        - If the loss function is not provided and either the validation or test loss\n          needs to be computed.\n        - If the given modality is not supported.\n        - If the encoder, head, or postprocessor is not mapped to a modality.\n        - If an unsupported modality is found in the loss pair specification.\n        - If an unsupported modality is found in the auxiliary tasks.\n        - If the auxiliary task is not a partial function.\n        - If the evaluation task is not an instance of :py:class:`~mmlearn.tasks.hooks.EvaluationHooks`.\n\n    \"\"\"  # noqa: W505\n\n    def __init__(  # noqa: PLR0912, PLR0915\n        self,\n        encoders: dict[str, nn.Module],\n        heads: Optional[dict[str, Union[nn.Module, dict[str, nn.Module]]]] = None,\n        postprocessors: Optional[\n            dict[str, Union[nn.Module, dict[str, nn.Module]]]\n        ] = None,\n        modality_module_mapping: Optional[dict[str, ModuleKeySpec]] = None,\n        optimizer: Optional[partial[torch.optim.Optimizer]] = None,\n        lr_scheduler: Optional[\n            Union[\n                dict[str, Union[partial[torch.optim.lr_scheduler.LRScheduler], Any]],\n                partial[torch.optim.lr_scheduler.LRScheduler],\n            ]\n        ] = None,\n        init_logit_scale: float = 1 / 0.07,\n        max_logit_scale: float = 100,\n        learnable_logit_scale: bool = True,\n        loss: Optional[nn.Module] = None,\n        modality_loss_pairs: Optional[list[LossPairSpec]] = None,\n        auxiliary_tasks: Optional[dict[str, AuxiliaryTaskSpec]] = None,\n        log_auxiliary_tasks_loss: bool = False,\n        compute_validation_loss: bool = True,\n        compute_test_loss: bool = True,\n        evaluation_tasks: Optional[dict[str, EvaluationSpec]] = None,\n    ) -&gt; None:\n        super().__init__(\n            optimizer=optimizer,\n            lr_scheduler=lr_scheduler,\n            loss_fn=loss,\n            compute_validation_loss=compute_validation_loss,\n            compute_test_loss=compute_test_loss,\n        )\n\n        self.save_hyperparameters(\n            ignore=[\n                \"encoders\",\n                \"heads\",\n                \"postprocessors\",\n                \"modality_module_mapping\",\n                \"loss\",\n                \"auxiliary_tasks\",\n                \"evaluation_tasks\",\n                \"modality_loss_pairs\",\n            ]\n        )\n\n        if modality_module_mapping is None:\n            # assume all the module dictionaries use the same keys corresponding\n            # to modalities\n            modality_module_mapping = {}\n            for key in encoders:\n                modality_module_mapping[key] = ModuleKeySpec(\n                    encoder_key=key,\n                    head_key=key,\n                    postprocessor_key=key,\n                )\n\n        # match modalities to encoders, heads, and postprocessors\n        modality_encoder_mapping: dict[str, Optional[str]] = {}\n        modality_head_mapping: dict[str, Optional[str]] = {}\n        modality_postprocessor_mapping: dict[str, Optional[str]] = {}\n        for modality_key, module_mapping in modality_module_mapping.items():\n            if not Modalities.has_modality(modality_key):\n                raise ValueError(_unsupported_modality_error.format(modality_key))\n            modality_encoder_mapping[modality_key] = module_mapping.encoder_key\n            modality_head_mapping[modality_key] = module_mapping.head_key\n            modality_postprocessor_mapping[modality_key] = (\n                module_mapping.postprocessor_key\n            )\n\n        # ensure all modules are mapped to a modality\n        for key in encoders:\n            if key not in modality_encoder_mapping.values():\n                if not Modalities.has_modality(key):\n                    raise ValueError(_unsupported_modality_error.format(key))\n                modality_encoder_mapping[key] = key\n\n        if heads is not None:\n            for key in heads:\n                if key not in modality_head_mapping.values():\n                    if not Modalities.has_modality(key):\n                        raise ValueError(_unsupported_modality_error.format(key))\n                    modality_head_mapping[key] = key\n\n        if postprocessors is not None:\n            for key in postprocessors:\n                if key not in modality_postprocessor_mapping.values():\n                    if not Modalities.has_modality(key):\n                        raise ValueError(_unsupported_modality_error.format(key))\n                    modality_postprocessor_mapping[key] = key\n\n        self._available_modalities: list[Modality] = [\n            Modalities.get_modality(modality_key)\n            for modality_key in modality_encoder_mapping\n        ]\n        assert len(self._available_modalities) &gt;= 2, (\n            \"Expected at least two modalities to be available. \"\n        )\n\n        #: A :py:class:`~torch.nn.ModuleDict`, where the keys are the names of the\n        #: modalities and the values are the encoder modules.\n        self.encoders = nn.ModuleDict(\n            {\n                Modalities.get_modality(modality_key).name: encoders[encoder_key]\n                for modality_key, encoder_key in modality_encoder_mapping.items()\n                if encoder_key is not None\n            }\n        )\n\n        #: A :py:class:`~torch.nn.ModuleDict`, where the keys are the names of the\n        #: modalities and the values are the projection head modules. This can be\n        #: ``None`` if no heads modules are provided.\n        self.heads = None\n        if heads is not None:\n            self.heads = nn.ModuleDict(\n                {\n                    Modalities.get_modality(modality_key).name: heads[head_key]\n                    if isinstance(heads[head_key], nn.Module)\n                    else nn.Sequential(*heads[head_key].values())\n                    for modality_key, head_key in modality_head_mapping.items()\n                    if head_key is not None and head_key in heads\n                }\n            )\n\n        #: A :py:class:`~torch.nn.ModuleDict`, where the keys are the names of the\n        #: modalities and the values are the postprocessor modules. This can be\n        #: ``None`` if no postprocessor modules are provided.\n        self.postprocessors = None\n        if postprocessors is not None:\n            self.postprocessors = nn.ModuleDict(\n                {\n                    Modalities.get_modality(modality_key).name: postprocessors[\n                        postprocessor_key\n                    ]\n                    if isinstance(postprocessors[postprocessor_key], nn.Module)\n                    else nn.Sequential(*postprocessors[postprocessor_key].values())\n                    for modality_key, postprocessor_key in modality_postprocessor_mapping.items()\n                    if postprocessor_key is not None\n                    and postprocessor_key in postprocessors\n                }\n            )\n\n        # set up logit scaling\n        log_logit_scale = torch.ones([]) * np.log(init_logit_scale)\n        self.max_logit_scale = max_logit_scale\n        self.learnable_logit_scale = learnable_logit_scale\n\n        if self.learnable_logit_scale:\n            self.log_logit_scale = torch.nn.Parameter(\n                log_logit_scale, requires_grad=True\n            )\n        else:\n            self.register_buffer(\"log_logit_scale\", log_logit_scale)\n\n        # set up contrastive loss pairs\n        if modality_loss_pairs is None:\n            modality_loss_pairs = [\n                LossPairSpec(modalities=(m1.name, m2.name))\n                for m1, m2 in itertools.combinations(self._available_modalities, 2)\n            ]\n\n        for modality_pair in modality_loss_pairs:\n            if not all(\n                Modalities.get_modality(modality) in self._available_modalities\n                for modality in modality_pair.modalities\n            ):\n                raise ValueError(\n                    \"Found unspecified modality in the loss pair specification \"\n                    f\"{modality_pair.modalities}. Available modalities are \"\n                    f\"{self._available_modalities}.\"\n                )\n\n        #: A list :py:class:`LossPairSpec` instances specifying the pairs of\n        #: modalities to compute the contrastive loss between and the weight to\n        #: apply to each pair.\n        self.modality_loss_pairs = modality_loss_pairs\n\n        # set up auxiliary tasks\n        self.aux_task_specs = auxiliary_tasks or {}\n        self.auxiliary_tasks: nn.ModuleDict[str, L.LightningModule] = nn.ModuleDict()\n        for task_name, task_spec in self.aux_task_specs.items():\n            if not Modalities.has_modality(task_spec.modality):\n                raise ValueError(\n                    f\"Found unsupported modality `{task_spec.modality}` in the auxiliary tasks. \"\n                    f\"Available modalities are {self._available_modalities}.\"\n                )\n            if not isinstance(task_spec.task, partial):\n                raise TypeError(\n                    f\"Expected auxiliary task to be a partial function, but got {type(task_spec.task)}.\"\n                )\n\n            self.auxiliary_tasks[task_name] = task_spec.task(\n                self.encoders[Modalities.get_modality(task_spec.modality).name]\n            )\n\n        self.log_auxiliary_tasks_loss = log_auxiliary_tasks_loss\n\n        if evaluation_tasks is not None:\n            for eval_task_spec in evaluation_tasks.values():\n                if not isinstance(eval_task_spec.task, EvaluationHooks):\n                    raise TypeError(\n                        f\"Expected {eval_task_spec.task} to be an instance of `EvaluationHooks` \"\n                        f\"but got {type(eval_task_spec.task)}.\"\n                    )\n\n        #: A dictionary of evaluation tasks to run during validation, while training,\n        #: or during testing.\n        self.evaluation_tasks = evaluation_tasks\n\n    def configure_model(self) -&gt; None:\n        \"\"\"Configure the model.\"\"\"\n        if self.auxiliary_tasks:\n            for task_name in self.auxiliary_tasks:\n                self.auxiliary_tasks[task_name].configure_model()\n\n    def encode(\n        self, inputs: dict[str, Any], modality: Modality, normalize: bool = False\n    ) -&gt; torch.Tensor:\n        \"\"\"Encode the input values for the given modality.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            Input values.\n        modality : Modality\n            The modality to encode.\n        normalize : bool, optional, default=False\n            Whether to apply L2 normalization to the output (after the head and\n            postprocessor layers, if present).\n\n        Returns\n        -------\n        torch.Tensor\n            The encoded values for the specified modality.\n        \"\"\"\n        output = self.encoders[modality.name](inputs)[0]\n\n        if self.postprocessors and modality.name in self.postprocessors:\n            output = self.postprocessors[modality.name](output)\n\n        if self.heads and modality.name in self.heads:\n            output = self.heads[modality.name](output)\n\n        if normalize:\n            output = torch.nn.functional.normalize(output, p=2, dim=-1)\n\n        return output\n\n    def forward(self, inputs: dict[str, Any]) -&gt; dict[str, torch.Tensor]:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input tensors to encode.\n\n        Returns\n        -------\n        dict[str, torch.Tensor]\n            The encodings for each modality.\n        \"\"\"\n        outputs = {\n            modality.embedding: self.encode(inputs, modality, normalize=True)\n            for modality in self._available_modalities\n            if modality.name in inputs\n        }\n\n        if not all(\n            output.size(-1) == list(outputs.values())[0].size(-1)\n            for output in outputs.values()\n        ):\n            raise ValueError(\"Expected all model outputs to have the same dimension.\")\n\n        return outputs\n\n    def on_train_epoch_start(self) -&gt; None:\n        \"\"\"Prepare for the training epoch.\n\n        This method sets the modules to training mode.\n        \"\"\"\n        self.encoders.train()\n        if self.heads:\n            self.heads.train()\n        if self.postprocessors:\n            self.postprocessors.train()\n\n    def training_step(self, batch: dict[str, Any], batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"Compute the loss for the batch.\n\n        Parameters\n        ----------\n        batch : dict[str, Any]\n            The batch of data to process.\n        batch_idx : int\n            The index of the batch.\n\n        Returns\n        -------\n        torch.Tensor\n            The loss for the batch.\n        \"\"\"\n        outputs = self(batch)\n\n        with torch.no_grad():\n            self.log_logit_scale.clamp_(0, math.log(self.max_logit_scale))\n\n        loss = self._compute_loss(batch, batch_idx, outputs)\n\n        if loss is None:\n            raise ValueError(\"The loss function must be provided for training.\")\n\n        self.log(\"train/loss\", loss, prog_bar=True, sync_dist=True)\n        self.log(\n            \"train/logit_scale\",\n            self.log_logit_scale.exp(),\n            prog_bar=True,\n            on_step=True,\n            on_epoch=False,\n        )\n\n        return loss\n\n    def on_before_zero_grad(self, optimizer: torch.optim.Optimizer) -&gt; None:\n        \"\"\"Zero out the gradients of the model.\"\"\"\n        if self.auxiliary_tasks:\n            for task in self.auxiliary_tasks.values():\n                task.on_before_zero_grad(optimizer)\n\n    def on_validation_epoch_start(self) -&gt; None:\n        \"\"\"Prepare for the validation epoch.\n\n        This method sets the modules to evaluation mode and calls the\n        ``on_evaluation_epoch_start`` method of each evaluation task.\n        \"\"\"\n        self._on_eval_epoch_start(\"val\")\n\n    def validation_step(\n        self, batch: dict[str, torch.Tensor], batch_idx: int\n    ) -&gt; Optional[torch.Tensor]:\n        \"\"\"Run a single validation step.\n\n        Parameters\n        ----------\n        batch : dict[str, torch.Tensor]\n            The batch of data to process.\n        batch_idx : int\n            The index of the batch.\n\n        Returns\n        -------\n        Optional[torch.Tensor]\n            The loss for the batch or ``None`` if the loss function is not provided.\n        \"\"\"\n        return self._shared_eval_step(batch, batch_idx, \"val\")\n\n    def on_validation_epoch_end(self) -&gt; None:\n        \"\"\"Compute and log epoch-level metrics at the end of the validation epoch.\"\"\"\n        self._on_eval_epoch_end(\"val\")\n\n    def on_test_epoch_start(self) -&gt; None:\n        \"\"\"Prepare for the test epoch.\"\"\"\n        self._on_eval_epoch_start(\"test\")\n\n    def test_step(\n        self, batch: dict[str, torch.Tensor], batch_idx: int\n    ) -&gt; Optional[torch.Tensor]:\n        \"\"\"Run a single test step.\n\n        Parameters\n        ----------\n        batch : dict[str, torch.Tensor]\n            The batch of data to process.\n        batch_idx : int\n            The index of the batch.\n\n        Returns\n        -------\n        Optional[torch.Tensor]\n            The loss for the batch or ``None`` if the loss function is not provided.\n        \"\"\"\n        return self._shared_eval_step(batch, batch_idx, \"test\")\n\n    def on_test_epoch_end(self) -&gt; None:\n        \"\"\"Compute and log epoch-level metrics at the end of the test epoch.\"\"\"\n        self._on_eval_epoch_end(\"test\")\n\n    def on_load_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n        \"\"\"Modify the model checkpoint after loading.\n\n        The `on_load_checkpoint` method of auxiliary tasks is called here to allow\n        them to modify the checkpoint after loading.\n\n        Parameters\n        ----------\n        checkpoint : Dict[str, Any]\n            The loaded checkpoint.\n        \"\"\"\n        if self.auxiliary_tasks:\n            for task in self.auxiliary_tasks.values():\n                task.on_load_checkpoint(checkpoint)\n\n    def on_save_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n        \"\"\"Modify the checkpoint before saving.\n\n        The `on_save_checkpoint` method of auxiliary tasks is called here to allow\n        them to modify the checkpoint before saving.\n\n        Parameters\n        ----------\n        checkpoint : Dict[str, Any]\n            The checkpoint to save.\n        \"\"\"\n        if self.auxiliary_tasks:\n            for task in self.auxiliary_tasks.values():\n                task.on_save_checkpoint(checkpoint)\n\n    def _compute_loss(\n        self, batch: dict[str, Any], batch_idx: int, outputs: dict[str, torch.Tensor]\n    ) -&gt; Optional[torch.Tensor]:\n        if self.loss_fn is None:\n            return None\n\n        contrastive_loss = self.loss_fn(\n            outputs,\n            batch[\"example_ids\"],\n            self.log_logit_scale.exp(),\n            self.modality_loss_pairs,\n        )\n\n        auxiliary_losses: list[torch.Tensor] = []\n        if self.auxiliary_tasks:\n            for task_name, task_spec in self.aux_task_specs.items():\n                auxiliary_task_output = self.auxiliary_tasks[task_name].training_step(\n                    batch, batch_idx\n                )\n                if isinstance(auxiliary_task_output, torch.Tensor):\n                    auxiliary_task_loss = auxiliary_task_output\n                elif isinstance(auxiliary_task_output, Mapping):\n                    auxiliary_task_loss = auxiliary_task_output[\"loss\"]\n                else:\n                    raise ValueError(\n                        \"Expected auxiliary task output to be a tensor or a mapping \"\n                        f\"containing a 'loss' key, but got {type(auxiliary_task_output)}.\"\n                    )\n\n                auxiliary_task_loss *= task_spec.loss_weight\n                auxiliary_losses.append(auxiliary_task_loss)\n                if self.log_auxiliary_tasks_loss:\n                    self.log(\n                        f\"train/{task_name}_loss\", auxiliary_task_loss, sync_dist=True\n                    )\n\n        if not auxiliary_losses:\n            return contrastive_loss\n\n        return torch.stack(auxiliary_losses).sum() + contrastive_loss\n\n    def _on_eval_epoch_start(self, eval_type: Literal[\"val\", \"test\"]) -&gt; None:\n        \"\"\"Prepare for the evaluation epoch.\"\"\"\n        self.encoders.eval()\n        if self.heads:\n            self.heads.eval()\n        if self.postprocessors:\n            self.postprocessors.eval()\n        if self.evaluation_tasks:\n            for task_spec in self.evaluation_tasks.values():\n                if (eval_type == \"val\" and task_spec.run_on_validation) or (\n                    eval_type == \"test\" and task_spec.run_on_test\n                ):\n                    task_spec.task.on_evaluation_epoch_start(self)\n\n    def _shared_eval_step(\n        self,\n        batch: dict[str, torch.Tensor],\n        batch_idx: int,\n        eval_type: Literal[\"val\", \"test\"],\n    ) -&gt; Optional[torch.Tensor]:\n        \"\"\"Run a single evaluation step.\n\n        Parameters\n        ----------\n        batch : dict[str, torch.Tensor]\n            The batch of data to process.\n        batch_idx : int\n            The index of the batch.\n\n        Returns\n        -------\n        Optional[torch.Tensor]\n            The loss for the batch or ``None`` if the loss function is not provided.\n        \"\"\"\n        loss: Optional[torch.Tensor] = None\n        if (eval_type == \"val\" and self.compute_validation_loss) or (\n            eval_type == \"test\" and self.compute_test_loss\n        ):\n            outputs = self(batch)\n            loss = self._compute_loss(batch, batch_idx, outputs)\n            if loss is not None and not self.trainer.sanity_checking:\n                self.log(f\"{eval_type}/loss\", loss, prog_bar=True, sync_dist=True)\n\n        if self.evaluation_tasks:\n            for task_spec in self.evaluation_tasks.values():\n                if (eval_type == \"val\" and task_spec.run_on_validation) or (\n                    eval_type == \"test\" and task_spec.run_on_test\n                ):\n                    task_spec.task.evaluation_step(self, batch, batch_idx)\n\n        return loss\n\n    def _on_eval_epoch_end(self, eval_type: Literal[\"val\", \"test\"]) -&gt; None:\n        \"\"\"Compute and log epoch-level metrics at the end of the evaluation epoch.\"\"\"\n        if self.evaluation_tasks:\n            for task_spec in self.evaluation_tasks.values():\n                if (eval_type == \"val\" and task_spec.run_on_validation) or (\n                    eval_type == \"test\" and task_spec.run_on_test\n                ):\n                    task_spec.task.on_evaluation_epoch_end(self)\n</code></pre>"},{"location":"api/#mmlearn.tasks.ContrastivePretraining.configure_model","title":"configure_model","text":"<pre><code>configure_model()\n</code></pre> <p>Configure the model.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def configure_model(self) -&gt; None:\n    \"\"\"Configure the model.\"\"\"\n    if self.auxiliary_tasks:\n        for task_name in self.auxiliary_tasks:\n            self.auxiliary_tasks[task_name].configure_model()\n</code></pre>"},{"location":"api/#mmlearn.tasks.ContrastivePretraining.encode","title":"encode","text":"<pre><code>encode(inputs, modality, normalize=False)\n</code></pre> <p>Encode the input values for the given modality.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>Input values.</p> required <code>modality</code> <code>Modality</code> <p>The modality to encode.</p> required <code>normalize</code> <code>bool</code> <p>Whether to apply L2 normalization to the output (after the head and postprocessor layers, if present).</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The encoded values for the specified modality.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def encode(\n    self, inputs: dict[str, Any], modality: Modality, normalize: bool = False\n) -&gt; torch.Tensor:\n    \"\"\"Encode the input values for the given modality.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        Input values.\n    modality : Modality\n        The modality to encode.\n    normalize : bool, optional, default=False\n        Whether to apply L2 normalization to the output (after the head and\n        postprocessor layers, if present).\n\n    Returns\n    -------\n    torch.Tensor\n        The encoded values for the specified modality.\n    \"\"\"\n    output = self.encoders[modality.name](inputs)[0]\n\n    if self.postprocessors and modality.name in self.postprocessors:\n        output = self.postprocessors[modality.name](output)\n\n    if self.heads and modality.name in self.heads:\n        output = self.heads[modality.name](output)\n\n    if normalize:\n        output = torch.nn.functional.normalize(output, p=2, dim=-1)\n\n    return output\n</code></pre>"},{"location":"api/#mmlearn.tasks.ContrastivePretraining.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input tensors to encode.</p> required <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>The encodings for each modality.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; dict[str, torch.Tensor]:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input tensors to encode.\n\n    Returns\n    -------\n    dict[str, torch.Tensor]\n        The encodings for each modality.\n    \"\"\"\n    outputs = {\n        modality.embedding: self.encode(inputs, modality, normalize=True)\n        for modality in self._available_modalities\n        if modality.name in inputs\n    }\n\n    if not all(\n        output.size(-1) == list(outputs.values())[0].size(-1)\n        for output in outputs.values()\n    ):\n        raise ValueError(\"Expected all model outputs to have the same dimension.\")\n\n    return outputs\n</code></pre>"},{"location":"api/#mmlearn.tasks.ContrastivePretraining.on_train_epoch_start","title":"on_train_epoch_start","text":"<pre><code>on_train_epoch_start()\n</code></pre> <p>Prepare for the training epoch.</p> <p>This method sets the modules to training mode.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def on_train_epoch_start(self) -&gt; None:\n    \"\"\"Prepare for the training epoch.\n\n    This method sets the modules to training mode.\n    \"\"\"\n    self.encoders.train()\n    if self.heads:\n        self.heads.train()\n    if self.postprocessors:\n        self.postprocessors.train()\n</code></pre>"},{"location":"api/#mmlearn.tasks.ContrastivePretraining.training_step","title":"training_step","text":"<pre><code>training_step(batch, batch_idx)\n</code></pre> <p>Compute the loss for the batch.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict[str, Any]</code> <p>The batch of data to process.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The loss for the batch.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def training_step(self, batch: dict[str, Any], batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"Compute the loss for the batch.\n\n    Parameters\n    ----------\n    batch : dict[str, Any]\n        The batch of data to process.\n    batch_idx : int\n        The index of the batch.\n\n    Returns\n    -------\n    torch.Tensor\n        The loss for the batch.\n    \"\"\"\n    outputs = self(batch)\n\n    with torch.no_grad():\n        self.log_logit_scale.clamp_(0, math.log(self.max_logit_scale))\n\n    loss = self._compute_loss(batch, batch_idx, outputs)\n\n    if loss is None:\n        raise ValueError(\"The loss function must be provided for training.\")\n\n    self.log(\"train/loss\", loss, prog_bar=True, sync_dist=True)\n    self.log(\n        \"train/logit_scale\",\n        self.log_logit_scale.exp(),\n        prog_bar=True,\n        on_step=True,\n        on_epoch=False,\n    )\n\n    return loss\n</code></pre>"},{"location":"api/#mmlearn.tasks.ContrastivePretraining.on_before_zero_grad","title":"on_before_zero_grad","text":"<pre><code>on_before_zero_grad(optimizer)\n</code></pre> <p>Zero out the gradients of the model.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def on_before_zero_grad(self, optimizer: torch.optim.Optimizer) -&gt; None:\n    \"\"\"Zero out the gradients of the model.\"\"\"\n    if self.auxiliary_tasks:\n        for task in self.auxiliary_tasks.values():\n            task.on_before_zero_grad(optimizer)\n</code></pre>"},{"location":"api/#mmlearn.tasks.ContrastivePretraining.on_validation_epoch_start","title":"on_validation_epoch_start","text":"<pre><code>on_validation_epoch_start()\n</code></pre> <p>Prepare for the validation epoch.</p> <p>This method sets the modules to evaluation mode and calls the <code>on_evaluation_epoch_start</code> method of each evaluation task.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def on_validation_epoch_start(self) -&gt; None:\n    \"\"\"Prepare for the validation epoch.\n\n    This method sets the modules to evaluation mode and calls the\n    ``on_evaluation_epoch_start`` method of each evaluation task.\n    \"\"\"\n    self._on_eval_epoch_start(\"val\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.ContrastivePretraining.validation_step","title":"validation_step","text":"<pre><code>validation_step(batch, batch_idx)\n</code></pre> <p>Run a single validation step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict[str, Tensor]</code> <p>The batch of data to process.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required <p>Returns:</p> Type Description <code>Optional[Tensor]</code> <p>The loss for the batch or <code>None</code> if the loss function is not provided.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def validation_step(\n    self, batch: dict[str, torch.Tensor], batch_idx: int\n) -&gt; Optional[torch.Tensor]:\n    \"\"\"Run a single validation step.\n\n    Parameters\n    ----------\n    batch : dict[str, torch.Tensor]\n        The batch of data to process.\n    batch_idx : int\n        The index of the batch.\n\n    Returns\n    -------\n    Optional[torch.Tensor]\n        The loss for the batch or ``None`` if the loss function is not provided.\n    \"\"\"\n    return self._shared_eval_step(batch, batch_idx, \"val\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.ContrastivePretraining.on_validation_epoch_end","title":"on_validation_epoch_end","text":"<pre><code>on_validation_epoch_end()\n</code></pre> <p>Compute and log epoch-level metrics at the end of the validation epoch.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def on_validation_epoch_end(self) -&gt; None:\n    \"\"\"Compute and log epoch-level metrics at the end of the validation epoch.\"\"\"\n    self._on_eval_epoch_end(\"val\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.ContrastivePretraining.on_test_epoch_start","title":"on_test_epoch_start","text":"<pre><code>on_test_epoch_start()\n</code></pre> <p>Prepare for the test epoch.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def on_test_epoch_start(self) -&gt; None:\n    \"\"\"Prepare for the test epoch.\"\"\"\n    self._on_eval_epoch_start(\"test\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.ContrastivePretraining.test_step","title":"test_step","text":"<pre><code>test_step(batch, batch_idx)\n</code></pre> <p>Run a single test step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict[str, Tensor]</code> <p>The batch of data to process.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required <p>Returns:</p> Type Description <code>Optional[Tensor]</code> <p>The loss for the batch or <code>None</code> if the loss function is not provided.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def test_step(\n    self, batch: dict[str, torch.Tensor], batch_idx: int\n) -&gt; Optional[torch.Tensor]:\n    \"\"\"Run a single test step.\n\n    Parameters\n    ----------\n    batch : dict[str, torch.Tensor]\n        The batch of data to process.\n    batch_idx : int\n        The index of the batch.\n\n    Returns\n    -------\n    Optional[torch.Tensor]\n        The loss for the batch or ``None`` if the loss function is not provided.\n    \"\"\"\n    return self._shared_eval_step(batch, batch_idx, \"test\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.ContrastivePretraining.on_test_epoch_end","title":"on_test_epoch_end","text":"<pre><code>on_test_epoch_end()\n</code></pre> <p>Compute and log epoch-level metrics at the end of the test epoch.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def on_test_epoch_end(self) -&gt; None:\n    \"\"\"Compute and log epoch-level metrics at the end of the test epoch.\"\"\"\n    self._on_eval_epoch_end(\"test\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.ContrastivePretraining.on_load_checkpoint","title":"on_load_checkpoint","text":"<pre><code>on_load_checkpoint(checkpoint)\n</code></pre> <p>Modify the model checkpoint after loading.</p> <p>The <code>on_load_checkpoint</code> method of auxiliary tasks is called here to allow them to modify the checkpoint after loading.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <code>Dict[str, Any]</code> <p>The loaded checkpoint.</p> required Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def on_load_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n    \"\"\"Modify the model checkpoint after loading.\n\n    The `on_load_checkpoint` method of auxiliary tasks is called here to allow\n    them to modify the checkpoint after loading.\n\n    Parameters\n    ----------\n    checkpoint : Dict[str, Any]\n        The loaded checkpoint.\n    \"\"\"\n    if self.auxiliary_tasks:\n        for task in self.auxiliary_tasks.values():\n            task.on_load_checkpoint(checkpoint)\n</code></pre>"},{"location":"api/#mmlearn.tasks.ContrastivePretraining.on_save_checkpoint","title":"on_save_checkpoint","text":"<pre><code>on_save_checkpoint(checkpoint)\n</code></pre> <p>Modify the checkpoint before saving.</p> <p>The <code>on_save_checkpoint</code> method of auxiliary tasks is called here to allow them to modify the checkpoint before saving.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <code>Dict[str, Any]</code> <p>The checkpoint to save.</p> required Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def on_save_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n    \"\"\"Modify the checkpoint before saving.\n\n    The `on_save_checkpoint` method of auxiliary tasks is called here to allow\n    them to modify the checkpoint before saving.\n\n    Parameters\n    ----------\n    checkpoint : Dict[str, Any]\n        The checkpoint to save.\n    \"\"\"\n    if self.auxiliary_tasks:\n        for task in self.auxiliary_tasks.values():\n            task.on_save_checkpoint(checkpoint)\n</code></pre>"},{"location":"api/#mmlearn.tasks.IJEPA","title":"IJEPA","text":"<p>               Bases: <code>TrainingTask</code></p> <p>Pretraining module for IJEPA.</p> <p>This class implements the IJEPA (Image Joint-Embedding Predictive Architecture) pretraining task using PyTorch Lightning. It trains an encoder and a predictor to reconstruct masked regions of an image based on its unmasked context.</p> <p>Parameters:</p> Name Type Description Default <code>encoder</code> <code>VisionTransformer</code> <p>Vision transformer encoder.</p> required <code>predictor</code> <code>VisionTransformerPredictor</code> <p>Vision transformer predictor.</p> required <code>optimizer</code> <code>Optional[partial[Optimizer]]</code> <p>The optimizer to use for training. This is expected to be a func:<code>~functools.partial</code> function that takes the model parameters as the only required argument. If not provided, training will continue without an optimizer.</p> <code>None</code> <code>lr_scheduler</code> <code>Optional[Union[dict[str, Union[partial[LRScheduler], Any]], partial[LRScheduler]]]</code> <p>The learning rate scheduler to use for training. This can be a func:<code>~functools.partial</code> function that takes the optimizer as the only required argument or a dictionary with a <code>'scheduler'</code> key that specifies the scheduler and an optional <code>'extras'</code> key that specifies additional arguments to pass to the scheduler. If not provided, the learning rate will not be adjusted during training.</p> <code>None</code> <code>ema_decay</code> <code>float</code> <p>Initial momentum for EMA of target encoder.</p> <code>0.996</code> <code>ema_decay_end</code> <code>float</code> <p>Final momentum for EMA of target encoder.</p> <code>1.0</code> <code>ema_anneal_end_step</code> <code>int</code> <p>Number of steps to anneal EMA momentum to <code>ema_decay_end</code>.</p> <code>1000</code> <code>loss_fn</code> <code>Optional[Callable[[Tensor, Tensor], Tensor]]</code> <p>Loss function to use. If not provided, defaults to func:<code>~torch.nn.functional.smooth_l1_loss</code>.</p> <code>None</code> <code>compute_validation_loss</code> <code>bool</code> <p>Whether to compute validation loss.</p> <code>True</code> <code>compute_test_loss</code> <code>bool</code> <p>Whether to compute test loss.</p> <code>True</code> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>@store(group=\"task\", provider=\"mmlearn\", zen_partial=False)\nclass IJEPA(TrainingTask):\n    \"\"\"Pretraining module for IJEPA.\n\n    This class implements the IJEPA (Image Joint-Embedding Predictive Architecture)\n    pretraining task using PyTorch Lightning. It trains an encoder and a predictor to\n    reconstruct masked regions of an image based on its unmasked context.\n\n    Parameters\n    ----------\n    encoder : VisionTransformer\n        Vision transformer encoder.\n    predictor : VisionTransformerPredictor\n        Vision transformer predictor.\n    optimizer : Optional[partial[torch.optim.Optimizer]], optional, default=None\n        The optimizer to use for training. This is expected to be a :py:func:`~functools.partial`\n        function that takes the model parameters as the only required argument.\n        If not provided, training will continue without an optimizer.\n    lr_scheduler : Optional[Union[dict[str, Union[partial[torch.optim.lr_scheduler.LRScheduler], Any]], partial[torch.optim.lr_scheduler.LRScheduler]]], optional, default=None\n        The learning rate scheduler to use for training. This can be a\n        :py:func:`~functools.partial` function that takes the optimizer as the only\n        required argument or a dictionary with a ``'scheduler'`` key that specifies\n        the scheduler and an optional ``'extras'`` key that specifies additional\n        arguments to pass to the scheduler. If not provided, the learning rate will\n        not be adjusted during training.\n    ema_decay : float, optional, default=0.996\n        Initial momentum for EMA of target encoder.\n    ema_decay_end : float, optional, default=1.0\n        Final momentum for EMA of target encoder.\n    ema_anneal_end_step : int, optional, default=1000\n        Number of steps to anneal EMA momentum to ``ema_decay_end``.\n    loss_fn : Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]], optional\n        Loss function to use. If not provided, defaults to\n        :py:func:`~torch.nn.functional.smooth_l1_loss`.\n    compute_validation_loss : bool, optional, default=True\n        Whether to compute validation loss.\n    compute_test_loss : bool, optional, default=True\n        Whether to compute test loss.\n    \"\"\"  # noqa: W505\n\n    def __init__(\n        self,\n        encoder: VisionTransformer,\n        predictor: VisionTransformerPredictor,\n        modality: str = \"RGB\",\n        optimizer: Optional[partial[torch.optim.Optimizer]] = None,\n        lr_scheduler: Optional[\n            Union[\n                dict[str, Union[partial[torch.optim.lr_scheduler.LRScheduler], Any]],\n                partial[torch.optim.lr_scheduler.LRScheduler],\n            ]\n        ] = None,\n        ema_decay: float = 0.996,\n        ema_decay_end: float = 1.0,\n        ema_anneal_end_step: int = 1000,\n        loss_fn: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None,\n        compute_validation_loss: bool = True,\n        compute_test_loss: bool = True,\n    ):\n        super().__init__(\n            optimizer=optimizer,\n            lr_scheduler=lr_scheduler,\n            loss_fn=loss_fn if loss_fn is not None else F.smooth_l1_loss,\n            compute_validation_loss=compute_validation_loss,\n            compute_test_loss=compute_test_loss,\n        )\n        self.modality = Modalities.get_modality(modality)\n        self.mask_generator = IJEPAMaskGenerator()\n\n        self.encoder = encoder\n        self.predictor = predictor\n\n        self.predictor.num_patches = encoder.patch_embed.num_patches\n        self.predictor.embed_dim = encoder.embed_dim\n        self.predictor.num_heads = encoder.num_heads\n\n        self.target_encoder = ExponentialMovingAverage(\n            self.encoder, ema_decay, ema_decay_end, ema_anneal_end_step\n        )\n\n    def configure_model(self) -&gt; None:\n        \"\"\"Configure the model.\"\"\"\n        self.target_encoder.configure_model(self.device)\n\n    def on_before_zero_grad(self, optimizer: torch.optim.Optimizer) -&gt; None:\n        \"\"\"Perform exponential moving average update of target encoder.\n\n        This is done right after the ``optimizer.step()`, which comes just before\n        ``optimizer.zero_grad()`` to account for gradient accumulation.\n        \"\"\"\n        if self.target_encoder is not None:\n            self.target_encoder.step(self.encoder)\n\n    def training_step(self, batch: dict[str, Any], batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"Perform a single training step.\n\n        Parameters\n        ----------\n        batch : dict[str, Any]\n            A batch of data.\n        batch_idx : int\n            Index of the batch.\n\n        Returns\n        -------\n        torch.Tensor\n            Loss value.\n        \"\"\"\n        return self._shared_step(batch, batch_idx, step_type=\"train\")\n\n    def validation_step(\n        self, batch: dict[str, Any], batch_idx: int\n    ) -&gt; Optional[torch.Tensor]:\n        \"\"\"Run a single validation step.\n\n        Parameters\n        ----------\n        batch : dict[str, Any]\n            A batch of data.\n        batch_idx : int\n            Index of the batch.\n\n        Returns\n        -------\n        Optional[torch.Tensor]\n            Loss value or ``None`` if no loss is computed.\n        \"\"\"\n        return self._shared_step(batch, batch_idx, step_type=\"val\")\n\n    def test_step(\n        self, batch: dict[str, Any], batch_idx: int\n    ) -&gt; Optional[torch.Tensor]:\n        \"\"\"Run a single test step.\n\n        Parameters\n        ----------\n        batch : dict[str, Any]\n            A batch of data.\n        batch_idx : int\n            Index of the batch.\n\n        Returns\n        -------\n        Optional[torch.Tensor]\n            Loss value or ``None`` if no loss is computed\n        \"\"\"\n        return self._shared_step(batch, batch_idx, step_type=\"test\")\n\n    def on_validation_epoch_start(self) -&gt; None:\n        \"\"\"Prepare for the validation epoch.\"\"\"\n        self._on_eval_epoch_start(\"val\")\n\n    def on_validation_epoch_end(self) -&gt; None:\n        \"\"\"Actions at the end of the validation epoch.\"\"\"\n        self._on_eval_epoch_end(\"val\")\n\n    def on_test_epoch_start(self) -&gt; None:\n        \"\"\"Prepare for the test epoch.\"\"\"\n        self._on_eval_epoch_start(\"test\")\n\n    def on_test_epoch_end(self) -&gt; None:\n        \"\"\"Actions at the end of the test epoch.\"\"\"\n        self._on_eval_epoch_end(\"test\")\n\n    def on_save_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n        \"\"\"Add relevant EMA state to the checkpoint.\n\n        Parameters\n        ----------\n        checkpoint : dict[str, Any]\n            The state dictionary to save the EMA state to.\n        \"\"\"\n        if self.target_encoder is not None:\n            checkpoint[\"ema_params\"] = {\n                \"decay\": self.target_encoder.decay,\n                \"num_updates\": self.target_encoder.num_updates,\n            }\n\n    def on_load_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n        \"\"\"Restore EMA state from the checkpoint.\n\n        Parameters\n        ----------\n        checkpoint : dict[str, Any]\n            The state dictionary to restore the EMA state from.\n        \"\"\"\n        if \"ema_params\" in checkpoint and self.target_encoder is not None:\n            ema_params = checkpoint.pop(\"ema_params\")\n            self.target_encoder.decay = ema_params[\"decay\"]\n            self.target_encoder.num_updates = ema_params[\"num_updates\"]\n\n            self.target_encoder.restore(self.encoder)\n\n    def _shared_step(\n        self, batch: dict[str, Any], batch_idx: int, step_type: str\n    ) -&gt; Optional[torch.Tensor]:\n        images = batch[self.modality.name]\n\n        # Generate masks\n        batch_size = images.size(0)\n        mask_info = self.mask_generator(batch_size=batch_size)\n\n        # Extract masks and move to device\n        device = images.device\n        encoder_masks = [mask.to(device) for mask in mask_info[\"encoder_masks\"]]\n        predictor_masks = [mask.to(device) for mask in mask_info[\"predictor_masks\"]]\n\n        # Forward pass through target encoder to get h\n        with torch.no_grad():\n            h = self.target_encoder.model(batch)[0]\n            h = F.layer_norm(h, h.size()[-1:])\n            h_masked = apply_masks(h, predictor_masks)\n            h_masked = repeat_interleave_batch(\n                h_masked, images.size(0), repeat=len(encoder_masks)\n            )\n\n        # Forward pass through encoder with encoder_masks\n        batch[self.modality.mask] = encoder_masks\n        z = self.encoder(batch)[0]\n\n        # Pass z through predictor with encoder_masks and predictor_masks\n        z_pred = self.predictor(z, encoder_masks, predictor_masks)\n\n        if step_type == \"train\":\n            self.log(\"train/ema_decay\", self.target_encoder.decay, prog_bar=True)\n\n        if self.loss_fn is not None and (\n            step_type == \"train\"\n            or (step_type == \"val\" and self.compute_validation_loss)\n            or (step_type == \"test\" and self.compute_test_loss)\n        ):\n            # Compute loss between z_pred and h_masked\n            loss = self.loss_fn(z_pred, h_masked)\n\n            # Log loss\n            self.log(f\"{step_type}/loss\", loss, prog_bar=True, sync_dist=True)\n\n            return loss\n\n        return None\n\n    def _on_eval_epoch_start(self, step_type: str) -&gt; None:\n        \"\"\"Initialize states or configurations at the start of an evaluation epoch.\n\n        Parameters\n        ----------\n        step_type : str\n            Type of the evaluation phase (\"val\" or \"test\").\n        \"\"\"\n        if (\n            step_type == \"val\"\n            and self.compute_validation_loss\n            or step_type == \"test\"\n            and self.compute_test_loss\n        ):\n            self.log(f\"{step_type}/start\", 1, prog_bar=True, sync_dist=True)\n\n    def _on_eval_epoch_end(self, step_type: str) -&gt; None:\n        \"\"\"Finalize states or logging at the end of an evaluation epoch.\n\n        Parameters\n        ----------\n        step_type : str\n            Type of the evaluation phase (\"val\" or \"test\").\n        \"\"\"\n        if (\n            step_type == \"val\"\n            and self.compute_validation_loss\n            or step_type == \"test\"\n            and self.compute_test_loss\n        ):\n            self.log(f\"{step_type}/end\", 1, prog_bar=True, sync_dist=True)\n</code></pre>"},{"location":"api/#mmlearn.tasks.IJEPA.configure_model","title":"configure_model","text":"<pre><code>configure_model()\n</code></pre> <p>Configure the model.</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def configure_model(self) -&gt; None:\n    \"\"\"Configure the model.\"\"\"\n    self.target_encoder.configure_model(self.device)\n</code></pre>"},{"location":"api/#mmlearn.tasks.IJEPA.on_before_zero_grad","title":"on_before_zero_grad","text":"<pre><code>on_before_zero_grad(optimizer)\n</code></pre> <p>Perform exponential moving average update of target encoder.</p> <p>This is done right after the <code>optimizer.step()`, which comes just before</code>optimizer.zero_grad()`` to account for gradient accumulation.</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def on_before_zero_grad(self, optimizer: torch.optim.Optimizer) -&gt; None:\n    \"\"\"Perform exponential moving average update of target encoder.\n\n    This is done right after the ``optimizer.step()`, which comes just before\n    ``optimizer.zero_grad()`` to account for gradient accumulation.\n    \"\"\"\n    if self.target_encoder is not None:\n        self.target_encoder.step(self.encoder)\n</code></pre>"},{"location":"api/#mmlearn.tasks.IJEPA.training_step","title":"training_step","text":"<pre><code>training_step(batch, batch_idx)\n</code></pre> <p>Perform a single training step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict[str, Any]</code> <p>A batch of data.</p> required <code>batch_idx</code> <code>int</code> <p>Index of the batch.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Loss value.</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def training_step(self, batch: dict[str, Any], batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"Perform a single training step.\n\n    Parameters\n    ----------\n    batch : dict[str, Any]\n        A batch of data.\n    batch_idx : int\n        Index of the batch.\n\n    Returns\n    -------\n    torch.Tensor\n        Loss value.\n    \"\"\"\n    return self._shared_step(batch, batch_idx, step_type=\"train\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.IJEPA.validation_step","title":"validation_step","text":"<pre><code>validation_step(batch, batch_idx)\n</code></pre> <p>Run a single validation step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict[str, Any]</code> <p>A batch of data.</p> required <code>batch_idx</code> <code>int</code> <p>Index of the batch.</p> required <p>Returns:</p> Type Description <code>Optional[Tensor]</code> <p>Loss value or <code>None</code> if no loss is computed.</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def validation_step(\n    self, batch: dict[str, Any], batch_idx: int\n) -&gt; Optional[torch.Tensor]:\n    \"\"\"Run a single validation step.\n\n    Parameters\n    ----------\n    batch : dict[str, Any]\n        A batch of data.\n    batch_idx : int\n        Index of the batch.\n\n    Returns\n    -------\n    Optional[torch.Tensor]\n        Loss value or ``None`` if no loss is computed.\n    \"\"\"\n    return self._shared_step(batch, batch_idx, step_type=\"val\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.IJEPA.test_step","title":"test_step","text":"<pre><code>test_step(batch, batch_idx)\n</code></pre> <p>Run a single test step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict[str, Any]</code> <p>A batch of data.</p> required <code>batch_idx</code> <code>int</code> <p>Index of the batch.</p> required <p>Returns:</p> Type Description <code>Optional[Tensor]</code> <p>Loss value or <code>None</code> if no loss is computed</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def test_step(\n    self, batch: dict[str, Any], batch_idx: int\n) -&gt; Optional[torch.Tensor]:\n    \"\"\"Run a single test step.\n\n    Parameters\n    ----------\n    batch : dict[str, Any]\n        A batch of data.\n    batch_idx : int\n        Index of the batch.\n\n    Returns\n    -------\n    Optional[torch.Tensor]\n        Loss value or ``None`` if no loss is computed\n    \"\"\"\n    return self._shared_step(batch, batch_idx, step_type=\"test\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.IJEPA.on_validation_epoch_start","title":"on_validation_epoch_start","text":"<pre><code>on_validation_epoch_start()\n</code></pre> <p>Prepare for the validation epoch.</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def on_validation_epoch_start(self) -&gt; None:\n    \"\"\"Prepare for the validation epoch.\"\"\"\n    self._on_eval_epoch_start(\"val\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.IJEPA.on_validation_epoch_end","title":"on_validation_epoch_end","text":"<pre><code>on_validation_epoch_end()\n</code></pre> <p>Actions at the end of the validation epoch.</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def on_validation_epoch_end(self) -&gt; None:\n    \"\"\"Actions at the end of the validation epoch.\"\"\"\n    self._on_eval_epoch_end(\"val\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.IJEPA.on_test_epoch_start","title":"on_test_epoch_start","text":"<pre><code>on_test_epoch_start()\n</code></pre> <p>Prepare for the test epoch.</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def on_test_epoch_start(self) -&gt; None:\n    \"\"\"Prepare for the test epoch.\"\"\"\n    self._on_eval_epoch_start(\"test\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.IJEPA.on_test_epoch_end","title":"on_test_epoch_end","text":"<pre><code>on_test_epoch_end()\n</code></pre> <p>Actions at the end of the test epoch.</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def on_test_epoch_end(self) -&gt; None:\n    \"\"\"Actions at the end of the test epoch.\"\"\"\n    self._on_eval_epoch_end(\"test\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.IJEPA.on_save_checkpoint","title":"on_save_checkpoint","text":"<pre><code>on_save_checkpoint(checkpoint)\n</code></pre> <p>Add relevant EMA state to the checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <code>dict[str, Any]</code> <p>The state dictionary to save the EMA state to.</p> required Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def on_save_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n    \"\"\"Add relevant EMA state to the checkpoint.\n\n    Parameters\n    ----------\n    checkpoint : dict[str, Any]\n        The state dictionary to save the EMA state to.\n    \"\"\"\n    if self.target_encoder is not None:\n        checkpoint[\"ema_params\"] = {\n            \"decay\": self.target_encoder.decay,\n            \"num_updates\": self.target_encoder.num_updates,\n        }\n</code></pre>"},{"location":"api/#mmlearn.tasks.IJEPA.on_load_checkpoint","title":"on_load_checkpoint","text":"<pre><code>on_load_checkpoint(checkpoint)\n</code></pre> <p>Restore EMA state from the checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <code>dict[str, Any]</code> <p>The state dictionary to restore the EMA state from.</p> required Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def on_load_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n    \"\"\"Restore EMA state from the checkpoint.\n\n    Parameters\n    ----------\n    checkpoint : dict[str, Any]\n        The state dictionary to restore the EMA state from.\n    \"\"\"\n    if \"ema_params\" in checkpoint and self.target_encoder is not None:\n        ema_params = checkpoint.pop(\"ema_params\")\n        self.target_encoder.decay = ema_params[\"decay\"]\n        self.target_encoder.num_updates = ema_params[\"num_updates\"]\n\n        self.target_encoder.restore(self.encoder)\n</code></pre>"},{"location":"api/#mmlearn.tasks.ZeroShotClassification","title":"ZeroShotClassification","text":"<p>               Bases: <code>EvaluationHooks</code></p> <p>Zero-shot classification evaluation task.</p> <p>This task evaluates the zero-shot classification performance.</p> <p>Parameters:</p> Name Type Description Default <code>task_specs</code> <code>list[ClassificationTaskSpec]</code> <p>A list of classification task specifications.</p> required <code>tokenizer</code> <code>Callable[[Union[str, list[str]]], Union[Tensor, dict[str, Tensor]]]</code> <p>A function to tokenize text inputs.</p> required Source code in <code>mmlearn/tasks/zero_shot_classification.py</code> <pre><code>@store(group=\"eval_task\", provider=\"mmlearn\")\nclass ZeroShotClassification(EvaluationHooks):\n    \"\"\"Zero-shot classification evaluation task.\n\n    This task evaluates the zero-shot classification performance.\n\n    Parameters\n    ----------\n    task_specs : list[ClassificationTaskSpec]\n        A list of classification task specifications.\n    tokenizer : Callable[[Union[str, list[str]]], Union[torch.Tensor, dict[str, torch.Tensor]]]\n        A function to tokenize text inputs.\n    \"\"\"  # noqa: W505\n\n    def __init__(\n        self,\n        task_specs: list[ClassificationTaskSpec],\n        tokenizer: Callable[\n            [Union[str, list[str]]], Union[torch.Tensor, dict[str, torch.Tensor]]\n        ],\n    ) -&gt; None:\n        super().__init__()\n        self.tokenizer = tokenizer\n        self.task_specs = task_specs\n        for spec in self.task_specs:\n            assert Modalities.has_modality(spec.query_modality)\n\n        self.metrics: dict[tuple[str, int], MetricCollection] = {}\n        self._embeddings_store: dict[int, torch.Tensor] = {}\n\n    def on_evaluation_epoch_start(self, pl_module: LightningModule) -&gt; None:\n        \"\"\"Set up the evaluation task.\n\n        Parameters\n        ----------\n        pl_module : pl.LightningModule\n            A reference to the Lightning module being evaluated.\n\n        Raises\n        ------\n        ValueError\n            - If the task is not being run for validation or testing.\n            - If the dataset does not have the required attributes to perform zero-shot\n              classification (i.e ``id2label`` and ``zero_shot_prompt_templates``).\n        \"\"\"\n        if pl_module.trainer.validating:\n            eval_dataset: CombinedDataset = pl_module.trainer.val_dataloaders.dataset\n        elif pl_module.trainer.testing:\n            eval_dataset = pl_module.trainer.test_dataloaders.dataset\n        else:\n            raise ValueError(\n                \"ZeroShotClassification task is only supported for validation and testing.\"\n            )\n\n        self.all_dataset_info = {}\n\n        # create metrics for each dataset/query_modality combination\n        if not self.metrics:\n            for dataset_index, dataset in enumerate(eval_dataset.datasets):\n                dataset_name = getattr(dataset, \"name\", dataset.__class__.__name__)\n                try:\n                    id2label: dict[int, str] = dataset.id2label\n                except AttributeError:\n                    raise ValueError(\n                        f\"Dataset '{dataset_name}' must have a `id2label` attribute \"\n                        \"to perform zero-shot classification.\"\n                    ) from None\n\n                try:\n                    zero_shot_prompt_templates: list[str] = (\n                        dataset.zero_shot_prompt_templates\n                    )\n                except AttributeError:\n                    raise ValueError(\n                        \"Dataset must have a `zero_shot_prompt_templates` attribute to perform zero-shot classification.\"\n                    ) from None\n\n                num_classes = len(id2label)\n\n                self.all_dataset_info[dataset_index] = {\n                    \"name\": dataset_name,\n                    \"id2label\": id2label,\n                    \"prompt_templates\": zero_shot_prompt_templates,\n                    \"num_classes\": num_classes,\n                }\n\n                for spec in self.task_specs:\n                    query_modality = Modalities.get_modality(spec.query_modality).name\n                    self.metrics[(query_modality, dataset_index)] = (\n                        self._create_metrics(\n                            num_classes,\n                            spec.top_k,\n                            prefix=f\"{dataset_name}/{query_modality}_\",\n                            postfix=\"\",\n                        )\n                    )\n\n        for metric in self.metrics.values():\n            metric.to(pl_module.device)\n\n        for dataset_index, dataset_info in self.all_dataset_info.items():\n            id2label = dataset_info[\"id2label\"]\n            prompt_templates: list[str] = dataset_info[\"prompt_templates\"]\n            labels = list(id2label.values())\n\n            with torch.no_grad():\n                chunk_size = 10\n                all_embeddings = []\n\n                for i in tqdm(\n                    range(0, len(labels), chunk_size),\n                    desc=\"Encoding class descriptions\",\n                ):\n                    batch_labels = labels[i : min(i + chunk_size, len(labels))]\n                    descriptions = [\n                        template.format(label)\n                        for label in batch_labels\n                        for template in prompt_templates\n                    ]\n                    tokenized_descriptions = move_data_to_device(\n                        self.tokenizer(descriptions),\n                        pl_module.device,\n                    )\n\n                    # Encode the chunk using the pl_module's encode method\n                    chunk_embeddings = pl_module.encode(\n                        tokenized_descriptions, Modalities.TEXT\n                    )  # shape: [chunk_size x len(prompt_templates), embed_dim]\n                    chunk_embeddings /= chunk_embeddings.norm(p=2, dim=-1, keepdim=True)\n                    chunk_embeddings = chunk_embeddings.reshape(\n                        len(batch_labels), len(prompt_templates), -1\n                    ).mean(dim=1)  # shape: [chunk_size, embed_dim]\n                    chunk_embeddings /= chunk_embeddings.norm(p=2, dim=-1, keepdim=True)\n\n                    # Append the chunk embeddings to the list\n                    all_embeddings.append(chunk_embeddings)\n\n                # Concatenate all chunk embeddings into a single tensor\n                class_embeddings = torch.cat(all_embeddings, dim=0)\n\n            self._embeddings_store[dataset_index] = class_embeddings\n\n    def evaluation_step(\n        self, pl_module: LightningModule, batch: dict[str, torch.Tensor], batch_idx: int\n    ) -&gt; None:\n        \"\"\"Compute logits and update metrics.\n\n        Parameters\n        ----------\n        pl_module : pl.LightningModule\n            A reference to the Lightning module being evaluated.\n        batch : dict[str, torch.Tensor]\n            A batch of data.\n        batch_idx : int\n            The index of the batch.\n        \"\"\"\n        if pl_module.trainer.sanity_checking:\n            return\n\n        for (query_modality, dataset_index), metric_collection in self.metrics.items():\n            matching_indices = torch.where(batch[\"dataset_index\"] == dataset_index)[0]\n\n            if not matching_indices.numel():\n                continue\n\n            class_embeddings = self._embeddings_store[dataset_index]\n            query_embeddings: torch.Tensor = pl_module.encode(\n                batch, Modalities.get_modality(query_modality)\n            )\n            query_embeddings /= query_embeddings.norm(p=2, dim=-1, keepdim=True)\n            query_embeddings = query_embeddings[matching_indices]\n\n            if self.all_dataset_info[dataset_index][\"num_classes\"] == 2:\n                softmax_output = _safe_matmul(\n                    query_embeddings, class_embeddings\n                ).softmax(dim=-1)\n                logits = softmax_output[:, 1] - softmax_output[:, 0]\n            else:\n                logits = 100.0 * _safe_matmul(query_embeddings, class_embeddings)\n            targets = batch[Modalities.get_modality(query_modality).target][\n                matching_indices\n            ]\n\n            metric_collection.update(logits, targets)\n\n    def on_evaluation_epoch_end(self, pl_module: LightningModule) -&gt; dict[str, Any]:\n        \"\"\"Compute and reset metrics.\n\n        Parameters\n        ----------\n        pl_module : pl.LightningModule\n            A reference to the Lightning module being evaluated.\n\n        Returns\n        -------\n        dict[str, Any]\n            The computed metrics.\n        \"\"\"\n        results = {}\n        for metric_collection in self.metrics.values():\n            results.update(metric_collection.compute())\n            metric_collection.reset()\n\n        self._embeddings_store.clear()\n\n        eval_type = \"val\" if pl_module.trainer.validating else \"test\"\n        for key, value in results.items():\n            pl_module.log(f\"{eval_type}/{key}\", value)\n\n        return results\n\n    @staticmethod\n    def _create_metrics(\n        num_classes: int, top_k: list[int], prefix: str, postfix: str\n    ) -&gt; MetricCollection:\n        \"\"\"Create a collection of classification metrics.\"\"\"\n        task_type = \"binary\" if num_classes == 2 else \"multiclass\"\n        acc_metrics = (\n            {\n                f\"top{k}_accuracy\": Accuracy(\n                    task=task_type, num_classes=num_classes, top_k=k, average=\"micro\"\n                )\n                for k in top_k\n            }\n            if num_classes &gt; 2\n            else {\"accuracy\": Accuracy(task=task_type, num_classes=num_classes)}\n        )\n        return MetricCollection(\n            {\n                \"precision\": Precision(\n                    task=task_type,\n                    num_classes=num_classes,\n                    average=\"macro\" if num_classes &gt; 2 else \"micro\",\n                ),\n                \"recall\": Recall(\n                    task=task_type,\n                    num_classes=num_classes,\n                    average=\"macro\" if num_classes &gt; 2 else \"micro\",\n                ),\n                \"f1_score_macro\": F1Score(\n                    task=task_type,\n                    num_classes=num_classes,\n                    average=\"macro\" if num_classes &gt; 2 else \"micro\",\n                ),\n                \"aucroc\": AUROC(task=task_type, num_classes=num_classes),\n                **acc_metrics,\n            },\n            prefix=prefix,\n            postfix=postfix,\n            compute_groups=True,\n        )\n</code></pre>"},{"location":"api/#mmlearn.tasks.ZeroShotClassification.on_evaluation_epoch_start","title":"on_evaluation_epoch_start","text":"<pre><code>on_evaluation_epoch_start(pl_module)\n</code></pre> <p>Set up the evaluation task.</p> <p>Parameters:</p> Name Type Description Default <code>pl_module</code> <code>LightningModule</code> <p>A reference to the Lightning module being evaluated.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <ul> <li>If the task is not being run for validation or testing.</li> <li>If the dataset does not have the required attributes to perform zero-shot   classification (i.e <code>id2label</code> and <code>zero_shot_prompt_templates</code>).</li> </ul> Source code in <code>mmlearn/tasks/zero_shot_classification.py</code> <pre><code>def on_evaluation_epoch_start(self, pl_module: LightningModule) -&gt; None:\n    \"\"\"Set up the evaluation task.\n\n    Parameters\n    ----------\n    pl_module : pl.LightningModule\n        A reference to the Lightning module being evaluated.\n\n    Raises\n    ------\n    ValueError\n        - If the task is not being run for validation or testing.\n        - If the dataset does not have the required attributes to perform zero-shot\n          classification (i.e ``id2label`` and ``zero_shot_prompt_templates``).\n    \"\"\"\n    if pl_module.trainer.validating:\n        eval_dataset: CombinedDataset = pl_module.trainer.val_dataloaders.dataset\n    elif pl_module.trainer.testing:\n        eval_dataset = pl_module.trainer.test_dataloaders.dataset\n    else:\n        raise ValueError(\n            \"ZeroShotClassification task is only supported for validation and testing.\"\n        )\n\n    self.all_dataset_info = {}\n\n    # create metrics for each dataset/query_modality combination\n    if not self.metrics:\n        for dataset_index, dataset in enumerate(eval_dataset.datasets):\n            dataset_name = getattr(dataset, \"name\", dataset.__class__.__name__)\n            try:\n                id2label: dict[int, str] = dataset.id2label\n            except AttributeError:\n                raise ValueError(\n                    f\"Dataset '{dataset_name}' must have a `id2label` attribute \"\n                    \"to perform zero-shot classification.\"\n                ) from None\n\n            try:\n                zero_shot_prompt_templates: list[str] = (\n                    dataset.zero_shot_prompt_templates\n                )\n            except AttributeError:\n                raise ValueError(\n                    \"Dataset must have a `zero_shot_prompt_templates` attribute to perform zero-shot classification.\"\n                ) from None\n\n            num_classes = len(id2label)\n\n            self.all_dataset_info[dataset_index] = {\n                \"name\": dataset_name,\n                \"id2label\": id2label,\n                \"prompt_templates\": zero_shot_prompt_templates,\n                \"num_classes\": num_classes,\n            }\n\n            for spec in self.task_specs:\n                query_modality = Modalities.get_modality(spec.query_modality).name\n                self.metrics[(query_modality, dataset_index)] = (\n                    self._create_metrics(\n                        num_classes,\n                        spec.top_k,\n                        prefix=f\"{dataset_name}/{query_modality}_\",\n                        postfix=\"\",\n                    )\n                )\n\n    for metric in self.metrics.values():\n        metric.to(pl_module.device)\n\n    for dataset_index, dataset_info in self.all_dataset_info.items():\n        id2label = dataset_info[\"id2label\"]\n        prompt_templates: list[str] = dataset_info[\"prompt_templates\"]\n        labels = list(id2label.values())\n\n        with torch.no_grad():\n            chunk_size = 10\n            all_embeddings = []\n\n            for i in tqdm(\n                range(0, len(labels), chunk_size),\n                desc=\"Encoding class descriptions\",\n            ):\n                batch_labels = labels[i : min(i + chunk_size, len(labels))]\n                descriptions = [\n                    template.format(label)\n                    for label in batch_labels\n                    for template in prompt_templates\n                ]\n                tokenized_descriptions = move_data_to_device(\n                    self.tokenizer(descriptions),\n                    pl_module.device,\n                )\n\n                # Encode the chunk using the pl_module's encode method\n                chunk_embeddings = pl_module.encode(\n                    tokenized_descriptions, Modalities.TEXT\n                )  # shape: [chunk_size x len(prompt_templates), embed_dim]\n                chunk_embeddings /= chunk_embeddings.norm(p=2, dim=-1, keepdim=True)\n                chunk_embeddings = chunk_embeddings.reshape(\n                    len(batch_labels), len(prompt_templates), -1\n                ).mean(dim=1)  # shape: [chunk_size, embed_dim]\n                chunk_embeddings /= chunk_embeddings.norm(p=2, dim=-1, keepdim=True)\n\n                # Append the chunk embeddings to the list\n                all_embeddings.append(chunk_embeddings)\n\n            # Concatenate all chunk embeddings into a single tensor\n            class_embeddings = torch.cat(all_embeddings, dim=0)\n\n        self._embeddings_store[dataset_index] = class_embeddings\n</code></pre>"},{"location":"api/#mmlearn.tasks.ZeroShotClassification.evaluation_step","title":"evaluation_step","text":"<pre><code>evaluation_step(pl_module, batch, batch_idx)\n</code></pre> <p>Compute logits and update metrics.</p> <p>Parameters:</p> Name Type Description Default <code>pl_module</code> <code>LightningModule</code> <p>A reference to the Lightning module being evaluated.</p> required <code>batch</code> <code>dict[str, Tensor]</code> <p>A batch of data.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required Source code in <code>mmlearn/tasks/zero_shot_classification.py</code> <pre><code>def evaluation_step(\n    self, pl_module: LightningModule, batch: dict[str, torch.Tensor], batch_idx: int\n) -&gt; None:\n    \"\"\"Compute logits and update metrics.\n\n    Parameters\n    ----------\n    pl_module : pl.LightningModule\n        A reference to the Lightning module being evaluated.\n    batch : dict[str, torch.Tensor]\n        A batch of data.\n    batch_idx : int\n        The index of the batch.\n    \"\"\"\n    if pl_module.trainer.sanity_checking:\n        return\n\n    for (query_modality, dataset_index), metric_collection in self.metrics.items():\n        matching_indices = torch.where(batch[\"dataset_index\"] == dataset_index)[0]\n\n        if not matching_indices.numel():\n            continue\n\n        class_embeddings = self._embeddings_store[dataset_index]\n        query_embeddings: torch.Tensor = pl_module.encode(\n            batch, Modalities.get_modality(query_modality)\n        )\n        query_embeddings /= query_embeddings.norm(p=2, dim=-1, keepdim=True)\n        query_embeddings = query_embeddings[matching_indices]\n\n        if self.all_dataset_info[dataset_index][\"num_classes\"] == 2:\n            softmax_output = _safe_matmul(\n                query_embeddings, class_embeddings\n            ).softmax(dim=-1)\n            logits = softmax_output[:, 1] - softmax_output[:, 0]\n        else:\n            logits = 100.0 * _safe_matmul(query_embeddings, class_embeddings)\n        targets = batch[Modalities.get_modality(query_modality).target][\n            matching_indices\n        ]\n\n        metric_collection.update(logits, targets)\n</code></pre>"},{"location":"api/#mmlearn.tasks.ZeroShotClassification.on_evaluation_epoch_end","title":"on_evaluation_epoch_end","text":"<pre><code>on_evaluation_epoch_end(pl_module)\n</code></pre> <p>Compute and reset metrics.</p> <p>Parameters:</p> Name Type Description Default <code>pl_module</code> <code>LightningModule</code> <p>A reference to the Lightning module being evaluated.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>The computed metrics.</p> Source code in <code>mmlearn/tasks/zero_shot_classification.py</code> <pre><code>def on_evaluation_epoch_end(self, pl_module: LightningModule) -&gt; dict[str, Any]:\n    \"\"\"Compute and reset metrics.\n\n    Parameters\n    ----------\n    pl_module : pl.LightningModule\n        A reference to the Lightning module being evaluated.\n\n    Returns\n    -------\n    dict[str, Any]\n        The computed metrics.\n    \"\"\"\n    results = {}\n    for metric_collection in self.metrics.values():\n        results.update(metric_collection.compute())\n        metric_collection.reset()\n\n    self._embeddings_store.clear()\n\n    eval_type = \"val\" if pl_module.trainer.validating else \"test\"\n    for key, value in results.items():\n        pl_module.log(f\"{eval_type}/{key}\", value)\n\n    return results\n</code></pre>"},{"location":"api/#mmlearn.tasks.ZeroShotCrossModalRetrieval","title":"ZeroShotCrossModalRetrieval","text":"<p>               Bases: <code>EvaluationHooks</code></p> <p>Zero-shot cross-modal retrieval evaluation task.</p> <p>This task evaluates the retrieval performance of a model on a set of query-target pairs. The model is expected to produce embeddings for both the query and target modalities. The task computes the retrieval recall at <code>k</code> for each pair of modalities.</p> <p>Parameters:</p> Name Type Description Default <code>task_specs</code> <code>list[RetrievalTaskSpec]</code> <p>A list of retrieval task specifications. Each specification defines the query and target modalities, as well as the top-k values for which to compute the retrieval recall metrics.</p> required Source code in <code>mmlearn/tasks/zero_shot_retrieval.py</code> <pre><code>@store(group=\"eval_task\", provider=\"mmlearn\")\nclass ZeroShotCrossModalRetrieval(EvaluationHooks):\n    \"\"\"Zero-shot cross-modal retrieval evaluation task.\n\n    This task evaluates the retrieval performance of a model on a set of query-target\n    pairs. The model is expected to produce embeddings for both the query and target\n    modalities. The task computes the retrieval recall at `k` for each pair of\n    modalities.\n\n    Parameters\n    ----------\n    task_specs : list[RetrievalTaskSpec]\n        A list of retrieval task specifications. Each specification defines the query\n        and target modalities, as well as the top-k values for which to compute the\n        retrieval recall metrics.\n\n    \"\"\"\n\n    def __init__(self, task_specs: list[RetrievalTaskSpec]) -&gt; None:\n        super().__init__()\n\n        self.task_specs = task_specs\n        self.metrics: dict[tuple[str, str], MetricCollection] = {}\n        self._available_modalities = set()\n\n        for spec in self.task_specs:\n            query_modality = spec.query_modality\n            target_modality = spec.target_modality\n            assert Modalities.has_modality(query_modality)\n            assert Modalities.has_modality(target_modality)\n\n            self.metrics[(query_modality, target_modality)] = MetricCollection(\n                {\n                    f\"{query_modality}_to_{target_modality}_R@{k}\": RetrievalRecallAtK(\n                        top_k=k, aggregation=\"mean\", reduction=\"none\"\n                    )\n                    for k in spec.top_k\n                }\n            )\n            self._available_modalities.add(query_modality)\n            self._available_modalities.add(target_modality)\n\n    def on_evaluation_epoch_start(self, pl_module: pl.LightningModule) -&gt; None:\n        \"\"\"Move the metrics to the device of the Lightning module.\"\"\"\n        for metric in self.metrics.values():\n            metric.to(pl_module.device)\n\n    def evaluation_step(\n        self,\n        pl_module: pl.LightningModule,\n        batch: dict[str, torch.Tensor],\n        batch_idx: int,\n    ) -&gt; None:\n        \"\"\"Run the forward pass and update retrieval recall metrics.\n\n        Parameters\n        ----------\n        pl_module : pl.LightningModule\n            A reference to the Lightning module being evaluated.\n        batch : dict[str, torch.Tensor]\n            A dictionary of batched input tensors.\n        batch_idx : int\n            The index of the batch.\n\n        \"\"\"\n        if pl_module.trainer.sanity_checking:\n            return\n\n        outputs: dict[str, Any] = {}\n        for modality_name in self._available_modalities:\n            if modality_name in batch:\n                outputs[modality_name] = pl_module.encode(\n                    batch, Modalities.get_modality(modality_name), normalize=False\n                )\n        for (query_modality, target_modality), metric in self.metrics.items():\n            if query_modality not in outputs or target_modality not in outputs:\n                continue\n            query_embeddings: torch.Tensor = outputs[query_modality]\n            target_embeddings: torch.Tensor = outputs[target_modality]\n            indexes = torch.arange(query_embeddings.size(0), device=pl_module.device)\n\n            metric.update(query_embeddings, target_embeddings, indexes)\n\n    def on_evaluation_epoch_end(\n        self, pl_module: pl.LightningModule\n    ) -&gt; Optional[dict[str, Any]]:\n        \"\"\"Compute the retrieval recall metrics.\n\n        Parameters\n        ----------\n        pl_module : pl.LightningModule\n            A reference to the Lightning module being evaluated.\n\n        Returns\n        -------\n        Optional[dict[str, Any]]\n            A dictionary of evaluation results or `None` if no results are available.\n        \"\"\"\n        if pl_module.trainer.sanity_checking:\n            return None\n\n        results = {}\n        for metric in self.metrics.values():\n            results.update(metric.compute())\n            metric.reset()\n\n        eval_type = \"val\" if pl_module.trainer.validating else \"test\"\n\n        for key, value in results.items():\n            pl_module.log(f\"{eval_type}/{key}\", value)\n\n        return results\n</code></pre>"},{"location":"api/#mmlearn.tasks.ZeroShotCrossModalRetrieval.on_evaluation_epoch_start","title":"on_evaluation_epoch_start","text":"<pre><code>on_evaluation_epoch_start(pl_module)\n</code></pre> <p>Move the metrics to the device of the Lightning module.</p> Source code in <code>mmlearn/tasks/zero_shot_retrieval.py</code> <pre><code>def on_evaluation_epoch_start(self, pl_module: pl.LightningModule) -&gt; None:\n    \"\"\"Move the metrics to the device of the Lightning module.\"\"\"\n    for metric in self.metrics.values():\n        metric.to(pl_module.device)\n</code></pre>"},{"location":"api/#mmlearn.tasks.ZeroShotCrossModalRetrieval.evaluation_step","title":"evaluation_step","text":"<pre><code>evaluation_step(pl_module, batch, batch_idx)\n</code></pre> <p>Run the forward pass and update retrieval recall metrics.</p> <p>Parameters:</p> Name Type Description Default <code>pl_module</code> <code>LightningModule</code> <p>A reference to the Lightning module being evaluated.</p> required <code>batch</code> <code>dict[str, Tensor]</code> <p>A dictionary of batched input tensors.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required Source code in <code>mmlearn/tasks/zero_shot_retrieval.py</code> <pre><code>def evaluation_step(\n    self,\n    pl_module: pl.LightningModule,\n    batch: dict[str, torch.Tensor],\n    batch_idx: int,\n) -&gt; None:\n    \"\"\"Run the forward pass and update retrieval recall metrics.\n\n    Parameters\n    ----------\n    pl_module : pl.LightningModule\n        A reference to the Lightning module being evaluated.\n    batch : dict[str, torch.Tensor]\n        A dictionary of batched input tensors.\n    batch_idx : int\n        The index of the batch.\n\n    \"\"\"\n    if pl_module.trainer.sanity_checking:\n        return\n\n    outputs: dict[str, Any] = {}\n    for modality_name in self._available_modalities:\n        if modality_name in batch:\n            outputs[modality_name] = pl_module.encode(\n                batch, Modalities.get_modality(modality_name), normalize=False\n            )\n    for (query_modality, target_modality), metric in self.metrics.items():\n        if query_modality not in outputs or target_modality not in outputs:\n            continue\n        query_embeddings: torch.Tensor = outputs[query_modality]\n        target_embeddings: torch.Tensor = outputs[target_modality]\n        indexes = torch.arange(query_embeddings.size(0), device=pl_module.device)\n\n        metric.update(query_embeddings, target_embeddings, indexes)\n</code></pre>"},{"location":"api/#mmlearn.tasks.ZeroShotCrossModalRetrieval.on_evaluation_epoch_end","title":"on_evaluation_epoch_end","text":"<pre><code>on_evaluation_epoch_end(pl_module)\n</code></pre> <p>Compute the retrieval recall metrics.</p> <p>Parameters:</p> Name Type Description Default <code>pl_module</code> <code>LightningModule</code> <p>A reference to the Lightning module being evaluated.</p> required <p>Returns:</p> Type Description <code>Optional[dict[str, Any]]</code> <p>A dictionary of evaluation results or <code>None</code> if no results are available.</p> Source code in <code>mmlearn/tasks/zero_shot_retrieval.py</code> <pre><code>def on_evaluation_epoch_end(\n    self, pl_module: pl.LightningModule\n) -&gt; Optional[dict[str, Any]]:\n    \"\"\"Compute the retrieval recall metrics.\n\n    Parameters\n    ----------\n    pl_module : pl.LightningModule\n        A reference to the Lightning module being evaluated.\n\n    Returns\n    -------\n    Optional[dict[str, Any]]\n        A dictionary of evaluation results or `None` if no results are available.\n    \"\"\"\n    if pl_module.trainer.sanity_checking:\n        return None\n\n    results = {}\n    for metric in self.metrics.values():\n        results.update(metric.compute())\n        metric.reset()\n\n    eval_type = \"val\" if pl_module.trainer.validating else \"test\"\n\n    for key, value in results.items():\n        pl_module.log(f\"{eval_type}/{key}\", value)\n\n    return results\n</code></pre>"},{"location":"api/#mmlearn.tasks.base","title":"base","text":"<p>Base class for all tasks in mmlearn that require training.</p>"},{"location":"api/#mmlearn.tasks.base.TrainingTask","title":"TrainingTask","text":"<p>               Bases: <code>LightningModule</code></p> <p>Base class for all tasks in mmlearn that require training.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>Optional[partial[Optimizer]]</code> <p>The optimizer to use for training. This is expected to be a partial function, created using <code>functools.partial</code>, that takes the model parameters as the only required argument. If not provided, training will continue without an optimizer.</p> <code>None</code> <code>lr_scheduler</code> <code>Optional[Union[dict[str, Union[partial[LRScheduler], Any]], partial[LRScheduler]]]</code> <p>The learning rate scheduler to use for training. This can be a partial function that takes the optimizer as the only required argument or a dictionary with a <code>scheduler</code> key that specifies the scheduler and an optional <code>extras</code> key that specifies additional arguments to pass to the scheduler. If not provided, the learning rate will not be adjusted during training.</p> <code>None</code> <code>loss_fn</code> <code>Optional[Module]</code> <p>Loss function to use for training.</p> <code>None</code> <code>compute_validation_loss</code> <code>bool</code> <p>Whether to compute the validation loss if a validation dataloader is provided. The loss function must be provided to compute the validation loss.</p> <code>True</code> <code>compute_test_loss</code> <code>bool</code> <p>Whether to compute the test loss if a test dataloader is provided. The loss function must be provided to compute the test loss.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the loss function is not provided and either the validation or test loss needs to be computed.</p> Source code in <code>mmlearn/tasks/base.py</code> <pre><code>class TrainingTask(L.LightningModule):\n    \"\"\"Base class for all tasks in mmlearn that require training.\n\n    Parameters\n    ----------\n    optimizer : Optional[partial[torch.optim.Optimizer]], optional, default=None\n        The optimizer to use for training. This is expected to be a partial function,\n        created using `functools.partial`, that takes the model parameters as the\n        only required argument. If not provided, training will continue without an\n        optimizer.\n    lr_scheduler : Optional[Union[dict[str, Union[partial[torch.optim.lr_scheduler.LRScheduler], Any]], partial[torch.optim.lr_scheduler.LRScheduler]]], optional, default=None\n        The learning rate scheduler to use for training. This can be a partial function\n        that takes the optimizer as the only required argument or a dictionary with\n        a `scheduler` key that specifies the scheduler and an optional `extras` key\n        that specifies additional arguments to pass to the scheduler. If not provided,\n        the learning rate will not be adjusted during training.\n    loss_fn : Optional[torch.nn.Module], optional, default=None\n        Loss function to use for training.\n    compute_validation_loss : bool, optional, default=True\n        Whether to compute the validation loss if a validation dataloader is provided.\n        The loss function must be provided to compute the validation loss.\n    compute_test_loss : bool, optional, default=True\n        Whether to compute the test loss if a test dataloader is provided. The loss\n        function must be provided to compute the test loss.\n\n    Raises\n    ------\n    ValueError\n        If the loss function is not provided and either the validation or test loss\n        needs to be computed.\n    \"\"\"  # noqa: W505\n\n    def __init__(\n        self,\n        optimizer: Optional[partial[torch.optim.Optimizer]] = None,\n        lr_scheduler: Optional[\n            Union[\n                dict[str, Union[partial[torch.optim.lr_scheduler.LRScheduler], Any]],\n                partial[torch.optim.lr_scheduler.LRScheduler],\n            ]\n        ] = None,\n        loss_fn: Optional[torch.nn.Module] = None,\n        compute_validation_loss: bool = True,\n        compute_test_loss: bool = True,\n    ):\n        super().__init__()\n        if loss_fn is None and (compute_validation_loss or compute_test_loss):\n            raise ValueError(\n                \"Loss function must be provided to compute validation or test loss.\"\n            )\n\n        self.optimizer = optimizer\n        self.lr_scheduler = lr_scheduler\n        self.loss_fn = loss_fn\n        self.compute_validation_loss = compute_validation_loss\n        self.compute_test_loss = compute_test_loss\n\n    def configure_optimizers(self) -&gt; OptimizerLRScheduler:  # noqa: PLR0912\n        \"\"\"Configure the optimizer and learning rate scheduler.\"\"\"\n        if self.optimizer is None:\n            rank_zero_warn(\n                \"Optimizer not provided. Training will continue without an optimizer. \"\n                \"LR scheduler will not be used.\",\n            )\n            return None\n\n        weight_decay: Optional[float] = self.optimizer.keywords.get(\n            \"weight_decay\", None\n        )\n        if weight_decay is None:  # try getting default value\n            kw_param = inspect.signature(self.optimizer.func).parameters.get(\n                \"weight_decay\"\n            )\n            if kw_param is not None and kw_param.default != inspect.Parameter.empty:\n                weight_decay = kw_param.default\n\n        parameters = [param for param in self.parameters() if param.requires_grad]\n\n        if weight_decay is not None:\n            decay_params = []\n            no_decay_params = []\n\n            for param in self.parameters():\n                if not param.requires_grad:\n                    continue\n\n                if param.ndim &lt; 2:  # includes all bias and normalization parameters\n                    no_decay_params.append(param)\n                else:\n                    decay_params.append(param)\n\n            parameters = [\n                {\n                    \"params\": decay_params,\n                    \"weight_decay\": weight_decay,\n                    \"name\": \"weight_decay_params\",\n                },\n                {\n                    \"params\": no_decay_params,\n                    \"weight_decay\": 0.0,\n                    \"name\": \"no_weight_decay_params\",\n                },\n            ]\n\n        optimizer = self.optimizer(parameters)\n        if not isinstance(optimizer, torch.optim.Optimizer):\n            raise TypeError(\n                \"Expected optimizer to be an instance of `torch.optim.Optimizer`, \"\n                f\"but got {type(optimizer)}.\",\n            )\n\n        if self.lr_scheduler is not None:\n            if isinstance(self.lr_scheduler, dict):\n                if \"scheduler\" not in self.lr_scheduler:\n                    raise ValueError(\n                        \"Expected 'scheduler' key in the learning rate scheduler dictionary.\",\n                    )\n\n                lr_scheduler = self.lr_scheduler[\"scheduler\"](optimizer)\n                if not isinstance(lr_scheduler, torch.optim.lr_scheduler.LRScheduler):\n                    raise TypeError(\n                        \"Expected scheduler to be an instance of `torch.optim.lr_scheduler.LRScheduler`, \"\n                        f\"but got {type(lr_scheduler)}.\",\n                    )\n                lr_scheduler_dict: dict[\n                    str, Union[torch.optim.lr_scheduler.LRScheduler, Any]\n                ] = {\"scheduler\": lr_scheduler}\n\n                if self.lr_scheduler.get(\"extras\"):\n                    lr_scheduler_dict.update(self.lr_scheduler[\"extras\"])\n                return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler_dict}\n\n            lr_scheduler = self.lr_scheduler(optimizer)\n            if not isinstance(lr_scheduler, torch.optim.lr_scheduler.LRScheduler):\n                raise TypeError(\n                    \"Expected scheduler to be an instance of `torch.optim.lr_scheduler.LRScheduler`, \"\n                    f\"but got {type(lr_scheduler)}.\",\n                )\n            return [optimizer], [lr_scheduler]\n\n        return optimizer\n</code></pre>"},{"location":"api/#mmlearn.tasks.base.TrainingTask.configure_optimizers","title":"configure_optimizers","text":"<pre><code>configure_optimizers()\n</code></pre> <p>Configure the optimizer and learning rate scheduler.</p> Source code in <code>mmlearn/tasks/base.py</code> <pre><code>def configure_optimizers(self) -&gt; OptimizerLRScheduler:  # noqa: PLR0912\n    \"\"\"Configure the optimizer and learning rate scheduler.\"\"\"\n    if self.optimizer is None:\n        rank_zero_warn(\n            \"Optimizer not provided. Training will continue without an optimizer. \"\n            \"LR scheduler will not be used.\",\n        )\n        return None\n\n    weight_decay: Optional[float] = self.optimizer.keywords.get(\n        \"weight_decay\", None\n    )\n    if weight_decay is None:  # try getting default value\n        kw_param = inspect.signature(self.optimizer.func).parameters.get(\n            \"weight_decay\"\n        )\n        if kw_param is not None and kw_param.default != inspect.Parameter.empty:\n            weight_decay = kw_param.default\n\n    parameters = [param for param in self.parameters() if param.requires_grad]\n\n    if weight_decay is not None:\n        decay_params = []\n        no_decay_params = []\n\n        for param in self.parameters():\n            if not param.requires_grad:\n                continue\n\n            if param.ndim &lt; 2:  # includes all bias and normalization parameters\n                no_decay_params.append(param)\n            else:\n                decay_params.append(param)\n\n        parameters = [\n            {\n                \"params\": decay_params,\n                \"weight_decay\": weight_decay,\n                \"name\": \"weight_decay_params\",\n            },\n            {\n                \"params\": no_decay_params,\n                \"weight_decay\": 0.0,\n                \"name\": \"no_weight_decay_params\",\n            },\n        ]\n\n    optimizer = self.optimizer(parameters)\n    if not isinstance(optimizer, torch.optim.Optimizer):\n        raise TypeError(\n            \"Expected optimizer to be an instance of `torch.optim.Optimizer`, \"\n            f\"but got {type(optimizer)}.\",\n        )\n\n    if self.lr_scheduler is not None:\n        if isinstance(self.lr_scheduler, dict):\n            if \"scheduler\" not in self.lr_scheduler:\n                raise ValueError(\n                    \"Expected 'scheduler' key in the learning rate scheduler dictionary.\",\n                )\n\n            lr_scheduler = self.lr_scheduler[\"scheduler\"](optimizer)\n            if not isinstance(lr_scheduler, torch.optim.lr_scheduler.LRScheduler):\n                raise TypeError(\n                    \"Expected scheduler to be an instance of `torch.optim.lr_scheduler.LRScheduler`, \"\n                    f\"but got {type(lr_scheduler)}.\",\n                )\n            lr_scheduler_dict: dict[\n                str, Union[torch.optim.lr_scheduler.LRScheduler, Any]\n            ] = {\"scheduler\": lr_scheduler}\n\n            if self.lr_scheduler.get(\"extras\"):\n                lr_scheduler_dict.update(self.lr_scheduler[\"extras\"])\n            return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler_dict}\n\n        lr_scheduler = self.lr_scheduler(optimizer)\n        if not isinstance(lr_scheduler, torch.optim.lr_scheduler.LRScheduler):\n            raise TypeError(\n                \"Expected scheduler to be an instance of `torch.optim.lr_scheduler.LRScheduler`, \"\n                f\"but got {type(lr_scheduler)}.\",\n            )\n        return [optimizer], [lr_scheduler]\n\n    return optimizer\n</code></pre>"},{"location":"api/#mmlearn.tasks.contrastive_pretraining","title":"contrastive_pretraining","text":"<p>Contrastive pretraining task.</p>"},{"location":"api/#mmlearn.tasks.contrastive_pretraining.ModuleKeySpec","title":"ModuleKeySpec  <code>dataclass</code>","text":"<p>Module key specification for mapping modules to modalities.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>@dataclass\nclass ModuleKeySpec:\n    \"\"\"Module key specification for mapping modules to modalities.\"\"\"\n\n    #: The key of the encoder module. If not provided, the modality name is used.\n    encoder_key: Optional[str] = None\n\n    #: The key of the head module. If not provided, the modality name is used.\n    head_key: Optional[str] = None\n\n    #: The key of the postprocessor module. If not provided, the modality name is used.\n    postprocessor_key: Optional[str] = None\n</code></pre>"},{"location":"api/#mmlearn.tasks.contrastive_pretraining.LossPairSpec","title":"LossPairSpec  <code>dataclass</code>","text":"<p>Specification for a pair of modalities to compute the contrastive loss.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>@dataclass\nclass LossPairSpec:\n    \"\"\"Specification for a pair of modalities to compute the contrastive loss.\"\"\"\n\n    #: The pair of modalities to compute the contrastive loss between.\n    modalities: tuple[str, str]\n\n    #: The weight to apply to the contrastive loss for the pair of modalities.\n    weight: float = 1.0\n</code></pre>"},{"location":"api/#mmlearn.tasks.contrastive_pretraining.AuxiliaryTaskSpec","title":"AuxiliaryTaskSpec  <code>dataclass</code>","text":"<p>Specification for an auxiliary task to run alongside the main task.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>@dataclass\nclass AuxiliaryTaskSpec:\n    \"\"\"Specification for an auxiliary task to run alongside the main task.\"\"\"\n\n    #: The modality of the encoder to use for the auxiliary task.\n    modality: str\n\n    #: The auxiliary task module. This is expected to be a partially-initialized\n    #: instance of a :py:class:`~lightning.pytorch.core.LightningModule` created\n    #: using :py:func:`functools.partial`, such that an initialized encoder can be\n    #: passed as the only argument.\n    task: Any  # `functools.partial[L.LightningModule]` expected\n\n    #: The weight to apply to the auxiliary task loss.\n    loss_weight: float = 1.0\n</code></pre>"},{"location":"api/#mmlearn.tasks.contrastive_pretraining.EvaluationSpec","title":"EvaluationSpec  <code>dataclass</code>","text":"<p>Specification for an evaluation task.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>@dataclass\nclass EvaluationSpec:\n    \"\"\"Specification for an evaluation task.\"\"\"\n\n    #: The evaluation task module. This is expected to be an instance of\n    #: :py:class:`~mmlearn.tasks.hooks.EvaluationHooks`.\n    task: Any  # `EvaluationHooks` expected\n\n    #: Whether to run the evaluation task during validation.\n    run_on_validation: bool = True\n\n    #: Whether to run the evaluation task during training.\n    run_on_test: bool = True\n</code></pre>"},{"location":"api/#mmlearn.tasks.contrastive_pretraining.ContrastivePretraining","title":"ContrastivePretraining","text":"<p>               Bases: <code>TrainingTask</code></p> <p>Contrastive pretraining task.</p> <p>This class supports contrastive pretraining with <code>N</code> modalities of data. It allows the sharing of encoders, heads, and postprocessors across modalities. It also supports computing the contrastive loss between specified pairs of modalities, as well as training auxiliary tasks alongside the main contrastive pretraining task.</p> <p>Parameters:</p> Name Type Description Default <code>encoders</code> <code>dict[str, Module]</code> <p>A dictionary of encoders. The keys can be any string, including the names of any supported modalities. If the keys are not supported modalities, the <code>modality_module_mapping</code> parameter must be provided to map the encoders to specific modalities. The encoders are expected to take a dictionary of input values and return a list-like object with the first element being the encoded values. This first element is passed on to the heads or postprocessors and the remaining elements are ignored.</p> required <code>heads</code> <code>Optional[dict[str, Union[Module, dict[str, Module]]]]</code> <p>A dictionary of modules that process the encoder outputs, usually projection heads. If the keys do not correspond to the name of a supported modality, the <code>modality_module_mapping</code> parameter must be provided. If any of the values are dictionaries, they will be wrapped in a class:<code>torch.nn.Sequential</code> module. All head modules are expected to take a single input tensor and return a single output tensor.</p> <code>None</code> <code>postprocessors</code> <code>Optional[dict[str, Union[Module, dict[str, Module]]]]</code> <p>A dictionary of modules that process the head outputs. If the keys do not correspond to the name of a supported modality, the <code>modality_module_mapping</code> parameter must be provided. If any of the values are dictionaries, they will be wrapped in a <code>nn.Sequential</code> module. All postprocessor modules are expected to take a single input tensor and return a single output tensor.</p> <code>None</code> <code>modality_module_mapping</code> <code>Optional[dict[str, ModuleKeySpec]]</code> <p>A dictionary mapping modalities to encoders, heads, and postprocessors. Useful for reusing the same instance of a module across multiple modalities.</p> <code>None</code> <code>optimizer</code> <code>Optional[partial[Optimizer]]</code> <p>The optimizer to use for training. This is expected to be a func:<code>~functools.partial</code> function that takes the model parameters as the only required argument. If not provided, training will continue without an optimizer.</p> <code>None</code> <code>lr_scheduler</code> <code>Optional[Union[dict[str, Union[partial[LRScheduler], Any]], partial[LRScheduler]]]</code> <p>The learning rate scheduler to use for training. This can be a func:<code>~functools.partial</code> function that takes the optimizer as the only required argument or a dictionary with a <code>'scheduler'</code> key that specifies the scheduler and an optional <code>'extras'</code> key that specifies additional arguments to pass to the scheduler. If not provided, the learning rate will not be adjusted during training.</p> <code>None</code> <code>init_logit_scale</code> <code>float</code> <p>The initial value of the logit scale parameter. This is the log of the scale factor applied to the logits before computing the contrastive loss.</p> <code>1 / 0.07</code> <code>max_logit_scale</code> <code>float</code> <p>The maximum value of the logit scale parameter. The logit scale parameter is clamped to the range <code>[0, log(max_logit_scale)]</code>.</p> <code>100</code> <code>learnable_logit_scale</code> <code>bool</code> <p>Whether the logit scale parameter is learnable. If set to False, the logit scale parameter is treated as a constant.</p> <code>True</code> <code>loss</code> <code>Optional[Module]</code> <p>The loss function to use.</p> <code>None</code> <code>modality_loss_pairs</code> <code>Optional[list[LossPairSpec]]</code> <p>A list of pairs of modalities to compute the contrastive loss between and the weight to apply to each pair.</p> <code>None</code> <code>auxiliary_tasks</code> <code>dict[str, AuxiliaryTaskSpec]</code> <p>Auxiliary tasks to run alongside the main contrastive pretraining task.</p> <ul> <li>The auxiliary task module is expected to be a partially-initialized instance   of a class:<code>~lightning.pytorch.core.LightningModule</code> created using   func:<code>functools.partial</code>, such that an initialized encoder can be   passed as the only argument.</li> <li>The <code>modality</code> parameter specifies the modality of the encoder to use   for the auxiliary task. The <code>loss_weight</code> parameter specifies the weight   to apply to the auxiliary task loss.</li> </ul> <code>None</code> <code>log_auxiliary_tasks_loss</code> <code>bool</code> <p>Whether to log the loss of auxiliary tasks to the main logger.</p> <code>False</code> <code>compute_validation_loss</code> <code>bool</code> <p>Whether to compute the validation loss if a validation dataloader is provided. The loss function must be provided to compute the validation loss.</p> <code>True</code> <code>compute_test_loss</code> <code>bool</code> <p>Whether to compute the test loss if a test dataloader is provided. The loss function must be provided to compute the test loss.</p> <code>True</code> <code>evaluation_tasks</code> <code>Optional[dict[str, EvaluationSpec]]</code> <p>Evaluation tasks to run during validation, while training, and during testing.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <ul> <li>If the loss function is not provided and either the validation or test loss   needs to be computed.</li> <li>If the given modality is not supported.</li> <li>If the encoder, head, or postprocessor is not mapped to a modality.</li> <li>If an unsupported modality is found in the loss pair specification.</li> <li>If an unsupported modality is found in the auxiliary tasks.</li> <li>If the auxiliary task is not a partial function.</li> <li>If the evaluation task is not an instance of class:<code>~mmlearn.tasks.hooks.EvaluationHooks</code>.</li> </ul> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>@store(group=\"task\", provider=\"mmlearn\")\nclass ContrastivePretraining(TrainingTask):\n    \"\"\"Contrastive pretraining task.\n\n    This class supports contrastive pretraining with ``N`` modalities of data. It\n    allows the sharing of encoders, heads, and postprocessors across modalities.\n    It also supports computing the contrastive loss between specified pairs of\n    modalities, as well as training auxiliary tasks alongside the main contrastive\n    pretraining task.\n\n    Parameters\n    ----------\n    encoders : dict[str, torch.nn.Module]\n        A dictionary of encoders. The keys can be any string, including the names of\n        any supported modalities. If the keys are not supported modalities, the\n        ``modality_module_mapping`` parameter must be provided to map the encoders to\n        specific modalities. The encoders are expected to take a dictionary of input\n        values and return a list-like object with the first element being the encoded\n        values. This first element is passed on to the heads or postprocessors and\n        the remaining elements are ignored.\n    heads : Optional[dict[str, Union[torch.nn.Module, dict[str, torch.nn.Module]]]], optional, default=None\n        A dictionary of modules that process the encoder outputs, usually projection\n        heads. If the keys do not correspond to the name of a supported modality,\n        the ``modality_module_mapping`` parameter must be provided. If any of the values\n        are dictionaries, they will be wrapped in a :py:class:`torch.nn.Sequential`\n        module. All head modules are expected to take a single input tensor and\n        return a single output tensor.\n    postprocessors : Optional[dict[str, Union[torch.nn.Module, dict[str, torch.nn.Module]]]], optional, default=None\n        A dictionary of modules that process the head outputs. If the keys do not\n        correspond to the name of a supported modality, the `modality_module_mapping`\n        parameter must be provided. If any of the values are dictionaries, they will\n        be wrapped in a `nn.Sequential` module. All postprocessor modules are expected\n        to take a single input tensor and return a single output tensor.\n    modality_module_mapping : Optional[dict[str, ModuleKeySpec]], optional, default=None\n        A dictionary mapping modalities to encoders, heads, and postprocessors.\n        Useful for reusing the same instance of a module across multiple modalities.\n    optimizer : Optional[partial[torch.optim.Optimizer]], optional, default=None\n        The optimizer to use for training. This is expected to be a :py:func:`~functools.partial`\n        function that takes the model parameters as the only required argument.\n        If not provided, training will continue without an optimizer.\n    lr_scheduler : Optional[Union[dict[str, Union[partial[torch.optim.lr_scheduler.LRScheduler], Any]], partial[torch.optim.lr_scheduler.LRScheduler]]], optional, default=None\n        The learning rate scheduler to use for training. This can be a\n        :py:func:`~functools.partial` function that takes the optimizer as the only\n        required argument or a dictionary with a ``'scheduler'`` key that specifies\n        the scheduler and an optional ``'extras'`` key that specifies additional\n        arguments to pass to the scheduler. If not provided, the learning rate will\n        not be adjusted during training.\n    init_logit_scale : float, optional, default=1 / 0.07\n        The initial value of the logit scale parameter. This is the log of the scale\n        factor applied to the logits before computing the contrastive loss.\n    max_logit_scale : float, optional, default=100\n        The maximum value of the logit scale parameter. The logit scale parameter\n        is clamped to the range ``[0, log(max_logit_scale)]``.\n    learnable_logit_scale : bool, optional, default=True\n        Whether the logit scale parameter is learnable. If set to False, the logit\n        scale parameter is treated as a constant.\n    loss : Optional[torch.nn.Module], optional, default=None\n        The loss function to use.\n    modality_loss_pairs : Optional[list[LossPairSpec]], optional, default=None\n        A list of pairs of modalities to compute the contrastive loss between and\n        the weight to apply to each pair.\n    auxiliary_tasks : dict[str, AuxiliaryTaskSpec], optional, default=None\n        Auxiliary tasks to run alongside the main contrastive pretraining task.\n\n        - The auxiliary task module is expected to be a partially-initialized instance\n          of a :py:class:`~lightning.pytorch.core.LightningModule` created using\n          :py:func:`functools.partial`, such that an initialized encoder can be\n          passed as the only argument.\n        - The ``modality`` parameter specifies the modality of the encoder to use\n          for the auxiliary task. The ``loss_weight`` parameter specifies the weight\n          to apply to the auxiliary task loss.\n    log_auxiliary_tasks_loss : bool, optional, default=False\n        Whether to log the loss of auxiliary tasks to the main logger.\n    compute_validation_loss : bool, optional, default=True\n        Whether to compute the validation loss if a validation dataloader is provided.\n        The loss function must be provided to compute the validation loss.\n    compute_test_loss : bool, optional, default=True\n        Whether to compute the test loss if a test dataloader is provided. The loss\n        function must be provided to compute the test loss.\n    evaluation_tasks : Optional[dict[str, EvaluationSpec]], optional, default=None\n        Evaluation tasks to run during validation, while training, and during testing.\n\n    Raises\n    ------\n    ValueError\n\n        - If the loss function is not provided and either the validation or test loss\n          needs to be computed.\n        - If the given modality is not supported.\n        - If the encoder, head, or postprocessor is not mapped to a modality.\n        - If an unsupported modality is found in the loss pair specification.\n        - If an unsupported modality is found in the auxiliary tasks.\n        - If the auxiliary task is not a partial function.\n        - If the evaluation task is not an instance of :py:class:`~mmlearn.tasks.hooks.EvaluationHooks`.\n\n    \"\"\"  # noqa: W505\n\n    def __init__(  # noqa: PLR0912, PLR0915\n        self,\n        encoders: dict[str, nn.Module],\n        heads: Optional[dict[str, Union[nn.Module, dict[str, nn.Module]]]] = None,\n        postprocessors: Optional[\n            dict[str, Union[nn.Module, dict[str, nn.Module]]]\n        ] = None,\n        modality_module_mapping: Optional[dict[str, ModuleKeySpec]] = None,\n        optimizer: Optional[partial[torch.optim.Optimizer]] = None,\n        lr_scheduler: Optional[\n            Union[\n                dict[str, Union[partial[torch.optim.lr_scheduler.LRScheduler], Any]],\n                partial[torch.optim.lr_scheduler.LRScheduler],\n            ]\n        ] = None,\n        init_logit_scale: float = 1 / 0.07,\n        max_logit_scale: float = 100,\n        learnable_logit_scale: bool = True,\n        loss: Optional[nn.Module] = None,\n        modality_loss_pairs: Optional[list[LossPairSpec]] = None,\n        auxiliary_tasks: Optional[dict[str, AuxiliaryTaskSpec]] = None,\n        log_auxiliary_tasks_loss: bool = False,\n        compute_validation_loss: bool = True,\n        compute_test_loss: bool = True,\n        evaluation_tasks: Optional[dict[str, EvaluationSpec]] = None,\n    ) -&gt; None:\n        super().__init__(\n            optimizer=optimizer,\n            lr_scheduler=lr_scheduler,\n            loss_fn=loss,\n            compute_validation_loss=compute_validation_loss,\n            compute_test_loss=compute_test_loss,\n        )\n\n        self.save_hyperparameters(\n            ignore=[\n                \"encoders\",\n                \"heads\",\n                \"postprocessors\",\n                \"modality_module_mapping\",\n                \"loss\",\n                \"auxiliary_tasks\",\n                \"evaluation_tasks\",\n                \"modality_loss_pairs\",\n            ]\n        )\n\n        if modality_module_mapping is None:\n            # assume all the module dictionaries use the same keys corresponding\n            # to modalities\n            modality_module_mapping = {}\n            for key in encoders:\n                modality_module_mapping[key] = ModuleKeySpec(\n                    encoder_key=key,\n                    head_key=key,\n                    postprocessor_key=key,\n                )\n\n        # match modalities to encoders, heads, and postprocessors\n        modality_encoder_mapping: dict[str, Optional[str]] = {}\n        modality_head_mapping: dict[str, Optional[str]] = {}\n        modality_postprocessor_mapping: dict[str, Optional[str]] = {}\n        for modality_key, module_mapping in modality_module_mapping.items():\n            if not Modalities.has_modality(modality_key):\n                raise ValueError(_unsupported_modality_error.format(modality_key))\n            modality_encoder_mapping[modality_key] = module_mapping.encoder_key\n            modality_head_mapping[modality_key] = module_mapping.head_key\n            modality_postprocessor_mapping[modality_key] = (\n                module_mapping.postprocessor_key\n            )\n\n        # ensure all modules are mapped to a modality\n        for key in encoders:\n            if key not in modality_encoder_mapping.values():\n                if not Modalities.has_modality(key):\n                    raise ValueError(_unsupported_modality_error.format(key))\n                modality_encoder_mapping[key] = key\n\n        if heads is not None:\n            for key in heads:\n                if key not in modality_head_mapping.values():\n                    if not Modalities.has_modality(key):\n                        raise ValueError(_unsupported_modality_error.format(key))\n                    modality_head_mapping[key] = key\n\n        if postprocessors is not None:\n            for key in postprocessors:\n                if key not in modality_postprocessor_mapping.values():\n                    if not Modalities.has_modality(key):\n                        raise ValueError(_unsupported_modality_error.format(key))\n                    modality_postprocessor_mapping[key] = key\n\n        self._available_modalities: list[Modality] = [\n            Modalities.get_modality(modality_key)\n            for modality_key in modality_encoder_mapping\n        ]\n        assert len(self._available_modalities) &gt;= 2, (\n            \"Expected at least two modalities to be available. \"\n        )\n\n        #: A :py:class:`~torch.nn.ModuleDict`, where the keys are the names of the\n        #: modalities and the values are the encoder modules.\n        self.encoders = nn.ModuleDict(\n            {\n                Modalities.get_modality(modality_key).name: encoders[encoder_key]\n                for modality_key, encoder_key in modality_encoder_mapping.items()\n                if encoder_key is not None\n            }\n        )\n\n        #: A :py:class:`~torch.nn.ModuleDict`, where the keys are the names of the\n        #: modalities and the values are the projection head modules. This can be\n        #: ``None`` if no heads modules are provided.\n        self.heads = None\n        if heads is not None:\n            self.heads = nn.ModuleDict(\n                {\n                    Modalities.get_modality(modality_key).name: heads[head_key]\n                    if isinstance(heads[head_key], nn.Module)\n                    else nn.Sequential(*heads[head_key].values())\n                    for modality_key, head_key in modality_head_mapping.items()\n                    if head_key is not None and head_key in heads\n                }\n            )\n\n        #: A :py:class:`~torch.nn.ModuleDict`, where the keys are the names of the\n        #: modalities and the values are the postprocessor modules. This can be\n        #: ``None`` if no postprocessor modules are provided.\n        self.postprocessors = None\n        if postprocessors is not None:\n            self.postprocessors = nn.ModuleDict(\n                {\n                    Modalities.get_modality(modality_key).name: postprocessors[\n                        postprocessor_key\n                    ]\n                    if isinstance(postprocessors[postprocessor_key], nn.Module)\n                    else nn.Sequential(*postprocessors[postprocessor_key].values())\n                    for modality_key, postprocessor_key in modality_postprocessor_mapping.items()\n                    if postprocessor_key is not None\n                    and postprocessor_key in postprocessors\n                }\n            )\n\n        # set up logit scaling\n        log_logit_scale = torch.ones([]) * np.log(init_logit_scale)\n        self.max_logit_scale = max_logit_scale\n        self.learnable_logit_scale = learnable_logit_scale\n\n        if self.learnable_logit_scale:\n            self.log_logit_scale = torch.nn.Parameter(\n                log_logit_scale, requires_grad=True\n            )\n        else:\n            self.register_buffer(\"log_logit_scale\", log_logit_scale)\n\n        # set up contrastive loss pairs\n        if modality_loss_pairs is None:\n            modality_loss_pairs = [\n                LossPairSpec(modalities=(m1.name, m2.name))\n                for m1, m2 in itertools.combinations(self._available_modalities, 2)\n            ]\n\n        for modality_pair in modality_loss_pairs:\n            if not all(\n                Modalities.get_modality(modality) in self._available_modalities\n                for modality in modality_pair.modalities\n            ):\n                raise ValueError(\n                    \"Found unspecified modality in the loss pair specification \"\n                    f\"{modality_pair.modalities}. Available modalities are \"\n                    f\"{self._available_modalities}.\"\n                )\n\n        #: A list :py:class:`LossPairSpec` instances specifying the pairs of\n        #: modalities to compute the contrastive loss between and the weight to\n        #: apply to each pair.\n        self.modality_loss_pairs = modality_loss_pairs\n\n        # set up auxiliary tasks\n        self.aux_task_specs = auxiliary_tasks or {}\n        self.auxiliary_tasks: nn.ModuleDict[str, L.LightningModule] = nn.ModuleDict()\n        for task_name, task_spec in self.aux_task_specs.items():\n            if not Modalities.has_modality(task_spec.modality):\n                raise ValueError(\n                    f\"Found unsupported modality `{task_spec.modality}` in the auxiliary tasks. \"\n                    f\"Available modalities are {self._available_modalities}.\"\n                )\n            if not isinstance(task_spec.task, partial):\n                raise TypeError(\n                    f\"Expected auxiliary task to be a partial function, but got {type(task_spec.task)}.\"\n                )\n\n            self.auxiliary_tasks[task_name] = task_spec.task(\n                self.encoders[Modalities.get_modality(task_spec.modality).name]\n            )\n\n        self.log_auxiliary_tasks_loss = log_auxiliary_tasks_loss\n\n        if evaluation_tasks is not None:\n            for eval_task_spec in evaluation_tasks.values():\n                if not isinstance(eval_task_spec.task, EvaluationHooks):\n                    raise TypeError(\n                        f\"Expected {eval_task_spec.task} to be an instance of `EvaluationHooks` \"\n                        f\"but got {type(eval_task_spec.task)}.\"\n                    )\n\n        #: A dictionary of evaluation tasks to run during validation, while training,\n        #: or during testing.\n        self.evaluation_tasks = evaluation_tasks\n\n    def configure_model(self) -&gt; None:\n        \"\"\"Configure the model.\"\"\"\n        if self.auxiliary_tasks:\n            for task_name in self.auxiliary_tasks:\n                self.auxiliary_tasks[task_name].configure_model()\n\n    def encode(\n        self, inputs: dict[str, Any], modality: Modality, normalize: bool = False\n    ) -&gt; torch.Tensor:\n        \"\"\"Encode the input values for the given modality.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            Input values.\n        modality : Modality\n            The modality to encode.\n        normalize : bool, optional, default=False\n            Whether to apply L2 normalization to the output (after the head and\n            postprocessor layers, if present).\n\n        Returns\n        -------\n        torch.Tensor\n            The encoded values for the specified modality.\n        \"\"\"\n        output = self.encoders[modality.name](inputs)[0]\n\n        if self.postprocessors and modality.name in self.postprocessors:\n            output = self.postprocessors[modality.name](output)\n\n        if self.heads and modality.name in self.heads:\n            output = self.heads[modality.name](output)\n\n        if normalize:\n            output = torch.nn.functional.normalize(output, p=2, dim=-1)\n\n        return output\n\n    def forward(self, inputs: dict[str, Any]) -&gt; dict[str, torch.Tensor]:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input tensors to encode.\n\n        Returns\n        -------\n        dict[str, torch.Tensor]\n            The encodings for each modality.\n        \"\"\"\n        outputs = {\n            modality.embedding: self.encode(inputs, modality, normalize=True)\n            for modality in self._available_modalities\n            if modality.name in inputs\n        }\n\n        if not all(\n            output.size(-1) == list(outputs.values())[0].size(-1)\n            for output in outputs.values()\n        ):\n            raise ValueError(\"Expected all model outputs to have the same dimension.\")\n\n        return outputs\n\n    def on_train_epoch_start(self) -&gt; None:\n        \"\"\"Prepare for the training epoch.\n\n        This method sets the modules to training mode.\n        \"\"\"\n        self.encoders.train()\n        if self.heads:\n            self.heads.train()\n        if self.postprocessors:\n            self.postprocessors.train()\n\n    def training_step(self, batch: dict[str, Any], batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"Compute the loss for the batch.\n\n        Parameters\n        ----------\n        batch : dict[str, Any]\n            The batch of data to process.\n        batch_idx : int\n            The index of the batch.\n\n        Returns\n        -------\n        torch.Tensor\n            The loss for the batch.\n        \"\"\"\n        outputs = self(batch)\n\n        with torch.no_grad():\n            self.log_logit_scale.clamp_(0, math.log(self.max_logit_scale))\n\n        loss = self._compute_loss(batch, batch_idx, outputs)\n\n        if loss is None:\n            raise ValueError(\"The loss function must be provided for training.\")\n\n        self.log(\"train/loss\", loss, prog_bar=True, sync_dist=True)\n        self.log(\n            \"train/logit_scale\",\n            self.log_logit_scale.exp(),\n            prog_bar=True,\n            on_step=True,\n            on_epoch=False,\n        )\n\n        return loss\n\n    def on_before_zero_grad(self, optimizer: torch.optim.Optimizer) -&gt; None:\n        \"\"\"Zero out the gradients of the model.\"\"\"\n        if self.auxiliary_tasks:\n            for task in self.auxiliary_tasks.values():\n                task.on_before_zero_grad(optimizer)\n\n    def on_validation_epoch_start(self) -&gt; None:\n        \"\"\"Prepare for the validation epoch.\n\n        This method sets the modules to evaluation mode and calls the\n        ``on_evaluation_epoch_start`` method of each evaluation task.\n        \"\"\"\n        self._on_eval_epoch_start(\"val\")\n\n    def validation_step(\n        self, batch: dict[str, torch.Tensor], batch_idx: int\n    ) -&gt; Optional[torch.Tensor]:\n        \"\"\"Run a single validation step.\n\n        Parameters\n        ----------\n        batch : dict[str, torch.Tensor]\n            The batch of data to process.\n        batch_idx : int\n            The index of the batch.\n\n        Returns\n        -------\n        Optional[torch.Tensor]\n            The loss for the batch or ``None`` if the loss function is not provided.\n        \"\"\"\n        return self._shared_eval_step(batch, batch_idx, \"val\")\n\n    def on_validation_epoch_end(self) -&gt; None:\n        \"\"\"Compute and log epoch-level metrics at the end of the validation epoch.\"\"\"\n        self._on_eval_epoch_end(\"val\")\n\n    def on_test_epoch_start(self) -&gt; None:\n        \"\"\"Prepare for the test epoch.\"\"\"\n        self._on_eval_epoch_start(\"test\")\n\n    def test_step(\n        self, batch: dict[str, torch.Tensor], batch_idx: int\n    ) -&gt; Optional[torch.Tensor]:\n        \"\"\"Run a single test step.\n\n        Parameters\n        ----------\n        batch : dict[str, torch.Tensor]\n            The batch of data to process.\n        batch_idx : int\n            The index of the batch.\n\n        Returns\n        -------\n        Optional[torch.Tensor]\n            The loss for the batch or ``None`` if the loss function is not provided.\n        \"\"\"\n        return self._shared_eval_step(batch, batch_idx, \"test\")\n\n    def on_test_epoch_end(self) -&gt; None:\n        \"\"\"Compute and log epoch-level metrics at the end of the test epoch.\"\"\"\n        self._on_eval_epoch_end(\"test\")\n\n    def on_load_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n        \"\"\"Modify the model checkpoint after loading.\n\n        The `on_load_checkpoint` method of auxiliary tasks is called here to allow\n        them to modify the checkpoint after loading.\n\n        Parameters\n        ----------\n        checkpoint : Dict[str, Any]\n            The loaded checkpoint.\n        \"\"\"\n        if self.auxiliary_tasks:\n            for task in self.auxiliary_tasks.values():\n                task.on_load_checkpoint(checkpoint)\n\n    def on_save_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n        \"\"\"Modify the checkpoint before saving.\n\n        The `on_save_checkpoint` method of auxiliary tasks is called here to allow\n        them to modify the checkpoint before saving.\n\n        Parameters\n        ----------\n        checkpoint : Dict[str, Any]\n            The checkpoint to save.\n        \"\"\"\n        if self.auxiliary_tasks:\n            for task in self.auxiliary_tasks.values():\n                task.on_save_checkpoint(checkpoint)\n\n    def _compute_loss(\n        self, batch: dict[str, Any], batch_idx: int, outputs: dict[str, torch.Tensor]\n    ) -&gt; Optional[torch.Tensor]:\n        if self.loss_fn is None:\n            return None\n\n        contrastive_loss = self.loss_fn(\n            outputs,\n            batch[\"example_ids\"],\n            self.log_logit_scale.exp(),\n            self.modality_loss_pairs,\n        )\n\n        auxiliary_losses: list[torch.Tensor] = []\n        if self.auxiliary_tasks:\n            for task_name, task_spec in self.aux_task_specs.items():\n                auxiliary_task_output = self.auxiliary_tasks[task_name].training_step(\n                    batch, batch_idx\n                )\n                if isinstance(auxiliary_task_output, torch.Tensor):\n                    auxiliary_task_loss = auxiliary_task_output\n                elif isinstance(auxiliary_task_output, Mapping):\n                    auxiliary_task_loss = auxiliary_task_output[\"loss\"]\n                else:\n                    raise ValueError(\n                        \"Expected auxiliary task output to be a tensor or a mapping \"\n                        f\"containing a 'loss' key, but got {type(auxiliary_task_output)}.\"\n                    )\n\n                auxiliary_task_loss *= task_spec.loss_weight\n                auxiliary_losses.append(auxiliary_task_loss)\n                if self.log_auxiliary_tasks_loss:\n                    self.log(\n                        f\"train/{task_name}_loss\", auxiliary_task_loss, sync_dist=True\n                    )\n\n        if not auxiliary_losses:\n            return contrastive_loss\n\n        return torch.stack(auxiliary_losses).sum() + contrastive_loss\n\n    def _on_eval_epoch_start(self, eval_type: Literal[\"val\", \"test\"]) -&gt; None:\n        \"\"\"Prepare for the evaluation epoch.\"\"\"\n        self.encoders.eval()\n        if self.heads:\n            self.heads.eval()\n        if self.postprocessors:\n            self.postprocessors.eval()\n        if self.evaluation_tasks:\n            for task_spec in self.evaluation_tasks.values():\n                if (eval_type == \"val\" and task_spec.run_on_validation) or (\n                    eval_type == \"test\" and task_spec.run_on_test\n                ):\n                    task_spec.task.on_evaluation_epoch_start(self)\n\n    def _shared_eval_step(\n        self,\n        batch: dict[str, torch.Tensor],\n        batch_idx: int,\n        eval_type: Literal[\"val\", \"test\"],\n    ) -&gt; Optional[torch.Tensor]:\n        \"\"\"Run a single evaluation step.\n\n        Parameters\n        ----------\n        batch : dict[str, torch.Tensor]\n            The batch of data to process.\n        batch_idx : int\n            The index of the batch.\n\n        Returns\n        -------\n        Optional[torch.Tensor]\n            The loss for the batch or ``None`` if the loss function is not provided.\n        \"\"\"\n        loss: Optional[torch.Tensor] = None\n        if (eval_type == \"val\" and self.compute_validation_loss) or (\n            eval_type == \"test\" and self.compute_test_loss\n        ):\n            outputs = self(batch)\n            loss = self._compute_loss(batch, batch_idx, outputs)\n            if loss is not None and not self.trainer.sanity_checking:\n                self.log(f\"{eval_type}/loss\", loss, prog_bar=True, sync_dist=True)\n\n        if self.evaluation_tasks:\n            for task_spec in self.evaluation_tasks.values():\n                if (eval_type == \"val\" and task_spec.run_on_validation) or (\n                    eval_type == \"test\" and task_spec.run_on_test\n                ):\n                    task_spec.task.evaluation_step(self, batch, batch_idx)\n\n        return loss\n\n    def _on_eval_epoch_end(self, eval_type: Literal[\"val\", \"test\"]) -&gt; None:\n        \"\"\"Compute and log epoch-level metrics at the end of the evaluation epoch.\"\"\"\n        if self.evaluation_tasks:\n            for task_spec in self.evaluation_tasks.values():\n                if (eval_type == \"val\" and task_spec.run_on_validation) or (\n                    eval_type == \"test\" and task_spec.run_on_test\n                ):\n                    task_spec.task.on_evaluation_epoch_end(self)\n</code></pre>"},{"location":"api/#mmlearn.tasks.contrastive_pretraining.ContrastivePretraining.configure_model","title":"configure_model","text":"<pre><code>configure_model()\n</code></pre> <p>Configure the model.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def configure_model(self) -&gt; None:\n    \"\"\"Configure the model.\"\"\"\n    if self.auxiliary_tasks:\n        for task_name in self.auxiliary_tasks:\n            self.auxiliary_tasks[task_name].configure_model()\n</code></pre>"},{"location":"api/#mmlearn.tasks.contrastive_pretraining.ContrastivePretraining.encode","title":"encode","text":"<pre><code>encode(inputs, modality, normalize=False)\n</code></pre> <p>Encode the input values for the given modality.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>Input values.</p> required <code>modality</code> <code>Modality</code> <p>The modality to encode.</p> required <code>normalize</code> <code>bool</code> <p>Whether to apply L2 normalization to the output (after the head and postprocessor layers, if present).</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The encoded values for the specified modality.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def encode(\n    self, inputs: dict[str, Any], modality: Modality, normalize: bool = False\n) -&gt; torch.Tensor:\n    \"\"\"Encode the input values for the given modality.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        Input values.\n    modality : Modality\n        The modality to encode.\n    normalize : bool, optional, default=False\n        Whether to apply L2 normalization to the output (after the head and\n        postprocessor layers, if present).\n\n    Returns\n    -------\n    torch.Tensor\n        The encoded values for the specified modality.\n    \"\"\"\n    output = self.encoders[modality.name](inputs)[0]\n\n    if self.postprocessors and modality.name in self.postprocessors:\n        output = self.postprocessors[modality.name](output)\n\n    if self.heads and modality.name in self.heads:\n        output = self.heads[modality.name](output)\n\n    if normalize:\n        output = torch.nn.functional.normalize(output, p=2, dim=-1)\n\n    return output\n</code></pre>"},{"location":"api/#mmlearn.tasks.contrastive_pretraining.ContrastivePretraining.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input tensors to encode.</p> required <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>The encodings for each modality.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; dict[str, torch.Tensor]:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input tensors to encode.\n\n    Returns\n    -------\n    dict[str, torch.Tensor]\n        The encodings for each modality.\n    \"\"\"\n    outputs = {\n        modality.embedding: self.encode(inputs, modality, normalize=True)\n        for modality in self._available_modalities\n        if modality.name in inputs\n    }\n\n    if not all(\n        output.size(-1) == list(outputs.values())[0].size(-1)\n        for output in outputs.values()\n    ):\n        raise ValueError(\"Expected all model outputs to have the same dimension.\")\n\n    return outputs\n</code></pre>"},{"location":"api/#mmlearn.tasks.contrastive_pretraining.ContrastivePretraining.on_train_epoch_start","title":"on_train_epoch_start","text":"<pre><code>on_train_epoch_start()\n</code></pre> <p>Prepare for the training epoch.</p> <p>This method sets the modules to training mode.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def on_train_epoch_start(self) -&gt; None:\n    \"\"\"Prepare for the training epoch.\n\n    This method sets the modules to training mode.\n    \"\"\"\n    self.encoders.train()\n    if self.heads:\n        self.heads.train()\n    if self.postprocessors:\n        self.postprocessors.train()\n</code></pre>"},{"location":"api/#mmlearn.tasks.contrastive_pretraining.ContrastivePretraining.training_step","title":"training_step","text":"<pre><code>training_step(batch, batch_idx)\n</code></pre> <p>Compute the loss for the batch.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict[str, Any]</code> <p>The batch of data to process.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The loss for the batch.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def training_step(self, batch: dict[str, Any], batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"Compute the loss for the batch.\n\n    Parameters\n    ----------\n    batch : dict[str, Any]\n        The batch of data to process.\n    batch_idx : int\n        The index of the batch.\n\n    Returns\n    -------\n    torch.Tensor\n        The loss for the batch.\n    \"\"\"\n    outputs = self(batch)\n\n    with torch.no_grad():\n        self.log_logit_scale.clamp_(0, math.log(self.max_logit_scale))\n\n    loss = self._compute_loss(batch, batch_idx, outputs)\n\n    if loss is None:\n        raise ValueError(\"The loss function must be provided for training.\")\n\n    self.log(\"train/loss\", loss, prog_bar=True, sync_dist=True)\n    self.log(\n        \"train/logit_scale\",\n        self.log_logit_scale.exp(),\n        prog_bar=True,\n        on_step=True,\n        on_epoch=False,\n    )\n\n    return loss\n</code></pre>"},{"location":"api/#mmlearn.tasks.contrastive_pretraining.ContrastivePretraining.on_before_zero_grad","title":"on_before_zero_grad","text":"<pre><code>on_before_zero_grad(optimizer)\n</code></pre> <p>Zero out the gradients of the model.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def on_before_zero_grad(self, optimizer: torch.optim.Optimizer) -&gt; None:\n    \"\"\"Zero out the gradients of the model.\"\"\"\n    if self.auxiliary_tasks:\n        for task in self.auxiliary_tasks.values():\n            task.on_before_zero_grad(optimizer)\n</code></pre>"},{"location":"api/#mmlearn.tasks.contrastive_pretraining.ContrastivePretraining.on_validation_epoch_start","title":"on_validation_epoch_start","text":"<pre><code>on_validation_epoch_start()\n</code></pre> <p>Prepare for the validation epoch.</p> <p>This method sets the modules to evaluation mode and calls the <code>on_evaluation_epoch_start</code> method of each evaluation task.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def on_validation_epoch_start(self) -&gt; None:\n    \"\"\"Prepare for the validation epoch.\n\n    This method sets the modules to evaluation mode and calls the\n    ``on_evaluation_epoch_start`` method of each evaluation task.\n    \"\"\"\n    self._on_eval_epoch_start(\"val\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.contrastive_pretraining.ContrastivePretraining.validation_step","title":"validation_step","text":"<pre><code>validation_step(batch, batch_idx)\n</code></pre> <p>Run a single validation step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict[str, Tensor]</code> <p>The batch of data to process.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required <p>Returns:</p> Type Description <code>Optional[Tensor]</code> <p>The loss for the batch or <code>None</code> if the loss function is not provided.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def validation_step(\n    self, batch: dict[str, torch.Tensor], batch_idx: int\n) -&gt; Optional[torch.Tensor]:\n    \"\"\"Run a single validation step.\n\n    Parameters\n    ----------\n    batch : dict[str, torch.Tensor]\n        The batch of data to process.\n    batch_idx : int\n        The index of the batch.\n\n    Returns\n    -------\n    Optional[torch.Tensor]\n        The loss for the batch or ``None`` if the loss function is not provided.\n    \"\"\"\n    return self._shared_eval_step(batch, batch_idx, \"val\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.contrastive_pretraining.ContrastivePretraining.on_validation_epoch_end","title":"on_validation_epoch_end","text":"<pre><code>on_validation_epoch_end()\n</code></pre> <p>Compute and log epoch-level metrics at the end of the validation epoch.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def on_validation_epoch_end(self) -&gt; None:\n    \"\"\"Compute and log epoch-level metrics at the end of the validation epoch.\"\"\"\n    self._on_eval_epoch_end(\"val\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.contrastive_pretraining.ContrastivePretraining.on_test_epoch_start","title":"on_test_epoch_start","text":"<pre><code>on_test_epoch_start()\n</code></pre> <p>Prepare for the test epoch.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def on_test_epoch_start(self) -&gt; None:\n    \"\"\"Prepare for the test epoch.\"\"\"\n    self._on_eval_epoch_start(\"test\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.contrastive_pretraining.ContrastivePretraining.test_step","title":"test_step","text":"<pre><code>test_step(batch, batch_idx)\n</code></pre> <p>Run a single test step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict[str, Tensor]</code> <p>The batch of data to process.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required <p>Returns:</p> Type Description <code>Optional[Tensor]</code> <p>The loss for the batch or <code>None</code> if the loss function is not provided.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def test_step(\n    self, batch: dict[str, torch.Tensor], batch_idx: int\n) -&gt; Optional[torch.Tensor]:\n    \"\"\"Run a single test step.\n\n    Parameters\n    ----------\n    batch : dict[str, torch.Tensor]\n        The batch of data to process.\n    batch_idx : int\n        The index of the batch.\n\n    Returns\n    -------\n    Optional[torch.Tensor]\n        The loss for the batch or ``None`` if the loss function is not provided.\n    \"\"\"\n    return self._shared_eval_step(batch, batch_idx, \"test\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.contrastive_pretraining.ContrastivePretraining.on_test_epoch_end","title":"on_test_epoch_end","text":"<pre><code>on_test_epoch_end()\n</code></pre> <p>Compute and log epoch-level metrics at the end of the test epoch.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def on_test_epoch_end(self) -&gt; None:\n    \"\"\"Compute and log epoch-level metrics at the end of the test epoch.\"\"\"\n    self._on_eval_epoch_end(\"test\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.contrastive_pretraining.ContrastivePretraining.on_load_checkpoint","title":"on_load_checkpoint","text":"<pre><code>on_load_checkpoint(checkpoint)\n</code></pre> <p>Modify the model checkpoint after loading.</p> <p>The <code>on_load_checkpoint</code> method of auxiliary tasks is called here to allow them to modify the checkpoint after loading.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <code>Dict[str, Any]</code> <p>The loaded checkpoint.</p> required Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def on_load_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n    \"\"\"Modify the model checkpoint after loading.\n\n    The `on_load_checkpoint` method of auxiliary tasks is called here to allow\n    them to modify the checkpoint after loading.\n\n    Parameters\n    ----------\n    checkpoint : Dict[str, Any]\n        The loaded checkpoint.\n    \"\"\"\n    if self.auxiliary_tasks:\n        for task in self.auxiliary_tasks.values():\n            task.on_load_checkpoint(checkpoint)\n</code></pre>"},{"location":"api/#mmlearn.tasks.contrastive_pretraining.ContrastivePretraining.on_save_checkpoint","title":"on_save_checkpoint","text":"<pre><code>on_save_checkpoint(checkpoint)\n</code></pre> <p>Modify the checkpoint before saving.</p> <p>The <code>on_save_checkpoint</code> method of auxiliary tasks is called here to allow them to modify the checkpoint before saving.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <code>Dict[str, Any]</code> <p>The checkpoint to save.</p> required Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def on_save_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n    \"\"\"Modify the checkpoint before saving.\n\n    The `on_save_checkpoint` method of auxiliary tasks is called here to allow\n    them to modify the checkpoint before saving.\n\n    Parameters\n    ----------\n    checkpoint : Dict[str, Any]\n        The checkpoint to save.\n    \"\"\"\n    if self.auxiliary_tasks:\n        for task in self.auxiliary_tasks.values():\n            task.on_save_checkpoint(checkpoint)\n</code></pre>"},{"location":"api/#mmlearn.tasks.hooks","title":"hooks","text":"<p>Task-related hooks for Lightning modules.</p>"},{"location":"api/#mmlearn.tasks.hooks.EvaluationHooks","title":"EvaluationHooks","text":"<p>Hooks for evaluation.</p> Source code in <code>mmlearn/tasks/hooks.py</code> <pre><code>class EvaluationHooks:\n    \"\"\"Hooks for evaluation.\"\"\"\n\n    def on_evaluation_epoch_start(self, pl_module: pl.LightningModule) -&gt; None:\n        \"\"\"Prepare the evaluation loop.\n\n        Parameters\n        ----------\n        pl_module : pl.LightningModule\n            A reference to the Lightning module being evaluated.\n        \"\"\"\n\n    def evaluation_step(\n        self, pl_module: pl.LightningModule, batch: Any, batch_idx: int\n    ) -&gt; Optional[Mapping[str, Any]]:\n        \"\"\"Run a single evaluation step.\n\n        Parameters\n        ----------\n        pl_module : pl.LightningModule\n            A reference to the Lightning module being evaluated.\n        batch : Any\n            A batch of data.\n        batch_idx : int\n            The index of the batch.\n\n        Returns\n        -------\n        Optional[Mapping[str, Any]]\n            A dictionary of evaluation results for the batch or ``None`` if no\n            batch results are available.\n\n        \"\"\"\n        rank_zero_warn(\n            f\"`evaluation_step` must be implemented to use {self.__class__.__name__} for evaluation.\"\n        )\n        return None\n\n    def on_evaluation_epoch_end(\n        self, pl_module: pl.LightningModule\n    ) -&gt; Optional[Union[Mapping[str, Any]]]:\n        \"\"\"Run after the evaluation epoch.\n\n        Parameters\n        ----------\n        pl_module : pl.LightningModule\n            A reference to the Lightning module being evaluated.\n\n        Returns\n        -------\n        Optional[Union[Mapping[str, Any]]]\n            A dictionary of evaluation results or ``None`` if no results are available.\n        \"\"\"\n</code></pre>"},{"location":"api/#mmlearn.tasks.hooks.EvaluationHooks.on_evaluation_epoch_start","title":"on_evaluation_epoch_start","text":"<pre><code>on_evaluation_epoch_start(pl_module)\n</code></pre> <p>Prepare the evaluation loop.</p> <p>Parameters:</p> Name Type Description Default <code>pl_module</code> <code>LightningModule</code> <p>A reference to the Lightning module being evaluated.</p> required Source code in <code>mmlearn/tasks/hooks.py</code> <pre><code>def on_evaluation_epoch_start(self, pl_module: pl.LightningModule) -&gt; None:\n    \"\"\"Prepare the evaluation loop.\n\n    Parameters\n    ----------\n    pl_module : pl.LightningModule\n        A reference to the Lightning module being evaluated.\n    \"\"\"\n</code></pre>"},{"location":"api/#mmlearn.tasks.hooks.EvaluationHooks.evaluation_step","title":"evaluation_step","text":"<pre><code>evaluation_step(pl_module, batch, batch_idx)\n</code></pre> <p>Run a single evaluation step.</p> <p>Parameters:</p> Name Type Description Default <code>pl_module</code> <code>LightningModule</code> <p>A reference to the Lightning module being evaluated.</p> required <code>batch</code> <code>Any</code> <p>A batch of data.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required <p>Returns:</p> Type Description <code>Optional[Mapping[str, Any]]</code> <p>A dictionary of evaluation results for the batch or <code>None</code> if no batch results are available.</p> Source code in <code>mmlearn/tasks/hooks.py</code> <pre><code>def evaluation_step(\n    self, pl_module: pl.LightningModule, batch: Any, batch_idx: int\n) -&gt; Optional[Mapping[str, Any]]:\n    \"\"\"Run a single evaluation step.\n\n    Parameters\n    ----------\n    pl_module : pl.LightningModule\n        A reference to the Lightning module being evaluated.\n    batch : Any\n        A batch of data.\n    batch_idx : int\n        The index of the batch.\n\n    Returns\n    -------\n    Optional[Mapping[str, Any]]\n        A dictionary of evaluation results for the batch or ``None`` if no\n        batch results are available.\n\n    \"\"\"\n    rank_zero_warn(\n        f\"`evaluation_step` must be implemented to use {self.__class__.__name__} for evaluation.\"\n    )\n    return None\n</code></pre>"},{"location":"api/#mmlearn.tasks.hooks.EvaluationHooks.on_evaluation_epoch_end","title":"on_evaluation_epoch_end","text":"<pre><code>on_evaluation_epoch_end(pl_module)\n</code></pre> <p>Run after the evaluation epoch.</p> <p>Parameters:</p> Name Type Description Default <code>pl_module</code> <code>LightningModule</code> <p>A reference to the Lightning module being evaluated.</p> required <p>Returns:</p> Type Description <code>Optional[Union[Mapping[str, Any]]]</code> <p>A dictionary of evaluation results or <code>None</code> if no results are available.</p> Source code in <code>mmlearn/tasks/hooks.py</code> <pre><code>def on_evaluation_epoch_end(\n    self, pl_module: pl.LightningModule\n) -&gt; Optional[Union[Mapping[str, Any]]]:\n    \"\"\"Run after the evaluation epoch.\n\n    Parameters\n    ----------\n    pl_module : pl.LightningModule\n        A reference to the Lightning module being evaluated.\n\n    Returns\n    -------\n    Optional[Union[Mapping[str, Any]]]\n        A dictionary of evaluation results or ``None`` if no results are available.\n    \"\"\"\n</code></pre>"},{"location":"api/#mmlearn.tasks.ijepa","title":"ijepa","text":"<p>IJEPA (Image Joint-Embedding Predictive Architecture) pretraining task.</p>"},{"location":"api/#mmlearn.tasks.ijepa.IJEPA","title":"IJEPA","text":"<p>               Bases: <code>TrainingTask</code></p> <p>Pretraining module for IJEPA.</p> <p>This class implements the IJEPA (Image Joint-Embedding Predictive Architecture) pretraining task using PyTorch Lightning. It trains an encoder and a predictor to reconstruct masked regions of an image based on its unmasked context.</p> <p>Parameters:</p> Name Type Description Default <code>encoder</code> <code>VisionTransformer</code> <p>Vision transformer encoder.</p> required <code>predictor</code> <code>VisionTransformerPredictor</code> <p>Vision transformer predictor.</p> required <code>optimizer</code> <code>Optional[partial[Optimizer]]</code> <p>The optimizer to use for training. This is expected to be a func:<code>~functools.partial</code> function that takes the model parameters as the only required argument. If not provided, training will continue without an optimizer.</p> <code>None</code> <code>lr_scheduler</code> <code>Optional[Union[dict[str, Union[partial[LRScheduler], Any]], partial[LRScheduler]]]</code> <p>The learning rate scheduler to use for training. This can be a func:<code>~functools.partial</code> function that takes the optimizer as the only required argument or a dictionary with a <code>'scheduler'</code> key that specifies the scheduler and an optional <code>'extras'</code> key that specifies additional arguments to pass to the scheduler. If not provided, the learning rate will not be adjusted during training.</p> <code>None</code> <code>ema_decay</code> <code>float</code> <p>Initial momentum for EMA of target encoder.</p> <code>0.996</code> <code>ema_decay_end</code> <code>float</code> <p>Final momentum for EMA of target encoder.</p> <code>1.0</code> <code>ema_anneal_end_step</code> <code>int</code> <p>Number of steps to anneal EMA momentum to <code>ema_decay_end</code>.</p> <code>1000</code> <code>loss_fn</code> <code>Optional[Callable[[Tensor, Tensor], Tensor]]</code> <p>Loss function to use. If not provided, defaults to func:<code>~torch.nn.functional.smooth_l1_loss</code>.</p> <code>None</code> <code>compute_validation_loss</code> <code>bool</code> <p>Whether to compute validation loss.</p> <code>True</code> <code>compute_test_loss</code> <code>bool</code> <p>Whether to compute test loss.</p> <code>True</code> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>@store(group=\"task\", provider=\"mmlearn\", zen_partial=False)\nclass IJEPA(TrainingTask):\n    \"\"\"Pretraining module for IJEPA.\n\n    This class implements the IJEPA (Image Joint-Embedding Predictive Architecture)\n    pretraining task using PyTorch Lightning. It trains an encoder and a predictor to\n    reconstruct masked regions of an image based on its unmasked context.\n\n    Parameters\n    ----------\n    encoder : VisionTransformer\n        Vision transformer encoder.\n    predictor : VisionTransformerPredictor\n        Vision transformer predictor.\n    optimizer : Optional[partial[torch.optim.Optimizer]], optional, default=None\n        The optimizer to use for training. This is expected to be a :py:func:`~functools.partial`\n        function that takes the model parameters as the only required argument.\n        If not provided, training will continue without an optimizer.\n    lr_scheduler : Optional[Union[dict[str, Union[partial[torch.optim.lr_scheduler.LRScheduler], Any]], partial[torch.optim.lr_scheduler.LRScheduler]]], optional, default=None\n        The learning rate scheduler to use for training. This can be a\n        :py:func:`~functools.partial` function that takes the optimizer as the only\n        required argument or a dictionary with a ``'scheduler'`` key that specifies\n        the scheduler and an optional ``'extras'`` key that specifies additional\n        arguments to pass to the scheduler. If not provided, the learning rate will\n        not be adjusted during training.\n    ema_decay : float, optional, default=0.996\n        Initial momentum for EMA of target encoder.\n    ema_decay_end : float, optional, default=1.0\n        Final momentum for EMA of target encoder.\n    ema_anneal_end_step : int, optional, default=1000\n        Number of steps to anneal EMA momentum to ``ema_decay_end``.\n    loss_fn : Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]], optional\n        Loss function to use. If not provided, defaults to\n        :py:func:`~torch.nn.functional.smooth_l1_loss`.\n    compute_validation_loss : bool, optional, default=True\n        Whether to compute validation loss.\n    compute_test_loss : bool, optional, default=True\n        Whether to compute test loss.\n    \"\"\"  # noqa: W505\n\n    def __init__(\n        self,\n        encoder: VisionTransformer,\n        predictor: VisionTransformerPredictor,\n        modality: str = \"RGB\",\n        optimizer: Optional[partial[torch.optim.Optimizer]] = None,\n        lr_scheduler: Optional[\n            Union[\n                dict[str, Union[partial[torch.optim.lr_scheduler.LRScheduler], Any]],\n                partial[torch.optim.lr_scheduler.LRScheduler],\n            ]\n        ] = None,\n        ema_decay: float = 0.996,\n        ema_decay_end: float = 1.0,\n        ema_anneal_end_step: int = 1000,\n        loss_fn: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None,\n        compute_validation_loss: bool = True,\n        compute_test_loss: bool = True,\n    ):\n        super().__init__(\n            optimizer=optimizer,\n            lr_scheduler=lr_scheduler,\n            loss_fn=loss_fn if loss_fn is not None else F.smooth_l1_loss,\n            compute_validation_loss=compute_validation_loss,\n            compute_test_loss=compute_test_loss,\n        )\n        self.modality = Modalities.get_modality(modality)\n        self.mask_generator = IJEPAMaskGenerator()\n\n        self.encoder = encoder\n        self.predictor = predictor\n\n        self.predictor.num_patches = encoder.patch_embed.num_patches\n        self.predictor.embed_dim = encoder.embed_dim\n        self.predictor.num_heads = encoder.num_heads\n\n        self.target_encoder = ExponentialMovingAverage(\n            self.encoder, ema_decay, ema_decay_end, ema_anneal_end_step\n        )\n\n    def configure_model(self) -&gt; None:\n        \"\"\"Configure the model.\"\"\"\n        self.target_encoder.configure_model(self.device)\n\n    def on_before_zero_grad(self, optimizer: torch.optim.Optimizer) -&gt; None:\n        \"\"\"Perform exponential moving average update of target encoder.\n\n        This is done right after the ``optimizer.step()`, which comes just before\n        ``optimizer.zero_grad()`` to account for gradient accumulation.\n        \"\"\"\n        if self.target_encoder is not None:\n            self.target_encoder.step(self.encoder)\n\n    def training_step(self, batch: dict[str, Any], batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"Perform a single training step.\n\n        Parameters\n        ----------\n        batch : dict[str, Any]\n            A batch of data.\n        batch_idx : int\n            Index of the batch.\n\n        Returns\n        -------\n        torch.Tensor\n            Loss value.\n        \"\"\"\n        return self._shared_step(batch, batch_idx, step_type=\"train\")\n\n    def validation_step(\n        self, batch: dict[str, Any], batch_idx: int\n    ) -&gt; Optional[torch.Tensor]:\n        \"\"\"Run a single validation step.\n\n        Parameters\n        ----------\n        batch : dict[str, Any]\n            A batch of data.\n        batch_idx : int\n            Index of the batch.\n\n        Returns\n        -------\n        Optional[torch.Tensor]\n            Loss value or ``None`` if no loss is computed.\n        \"\"\"\n        return self._shared_step(batch, batch_idx, step_type=\"val\")\n\n    def test_step(\n        self, batch: dict[str, Any], batch_idx: int\n    ) -&gt; Optional[torch.Tensor]:\n        \"\"\"Run a single test step.\n\n        Parameters\n        ----------\n        batch : dict[str, Any]\n            A batch of data.\n        batch_idx : int\n            Index of the batch.\n\n        Returns\n        -------\n        Optional[torch.Tensor]\n            Loss value or ``None`` if no loss is computed\n        \"\"\"\n        return self._shared_step(batch, batch_idx, step_type=\"test\")\n\n    def on_validation_epoch_start(self) -&gt; None:\n        \"\"\"Prepare for the validation epoch.\"\"\"\n        self._on_eval_epoch_start(\"val\")\n\n    def on_validation_epoch_end(self) -&gt; None:\n        \"\"\"Actions at the end of the validation epoch.\"\"\"\n        self._on_eval_epoch_end(\"val\")\n\n    def on_test_epoch_start(self) -&gt; None:\n        \"\"\"Prepare for the test epoch.\"\"\"\n        self._on_eval_epoch_start(\"test\")\n\n    def on_test_epoch_end(self) -&gt; None:\n        \"\"\"Actions at the end of the test epoch.\"\"\"\n        self._on_eval_epoch_end(\"test\")\n\n    def on_save_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n        \"\"\"Add relevant EMA state to the checkpoint.\n\n        Parameters\n        ----------\n        checkpoint : dict[str, Any]\n            The state dictionary to save the EMA state to.\n        \"\"\"\n        if self.target_encoder is not None:\n            checkpoint[\"ema_params\"] = {\n                \"decay\": self.target_encoder.decay,\n                \"num_updates\": self.target_encoder.num_updates,\n            }\n\n    def on_load_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n        \"\"\"Restore EMA state from the checkpoint.\n\n        Parameters\n        ----------\n        checkpoint : dict[str, Any]\n            The state dictionary to restore the EMA state from.\n        \"\"\"\n        if \"ema_params\" in checkpoint and self.target_encoder is not None:\n            ema_params = checkpoint.pop(\"ema_params\")\n            self.target_encoder.decay = ema_params[\"decay\"]\n            self.target_encoder.num_updates = ema_params[\"num_updates\"]\n\n            self.target_encoder.restore(self.encoder)\n\n    def _shared_step(\n        self, batch: dict[str, Any], batch_idx: int, step_type: str\n    ) -&gt; Optional[torch.Tensor]:\n        images = batch[self.modality.name]\n\n        # Generate masks\n        batch_size = images.size(0)\n        mask_info = self.mask_generator(batch_size=batch_size)\n\n        # Extract masks and move to device\n        device = images.device\n        encoder_masks = [mask.to(device) for mask in mask_info[\"encoder_masks\"]]\n        predictor_masks = [mask.to(device) for mask in mask_info[\"predictor_masks\"]]\n\n        # Forward pass through target encoder to get h\n        with torch.no_grad():\n            h = self.target_encoder.model(batch)[0]\n            h = F.layer_norm(h, h.size()[-1:])\n            h_masked = apply_masks(h, predictor_masks)\n            h_masked = repeat_interleave_batch(\n                h_masked, images.size(0), repeat=len(encoder_masks)\n            )\n\n        # Forward pass through encoder with encoder_masks\n        batch[self.modality.mask] = encoder_masks\n        z = self.encoder(batch)[0]\n\n        # Pass z through predictor with encoder_masks and predictor_masks\n        z_pred = self.predictor(z, encoder_masks, predictor_masks)\n\n        if step_type == \"train\":\n            self.log(\"train/ema_decay\", self.target_encoder.decay, prog_bar=True)\n\n        if self.loss_fn is not None and (\n            step_type == \"train\"\n            or (step_type == \"val\" and self.compute_validation_loss)\n            or (step_type == \"test\" and self.compute_test_loss)\n        ):\n            # Compute loss between z_pred and h_masked\n            loss = self.loss_fn(z_pred, h_masked)\n\n            # Log loss\n            self.log(f\"{step_type}/loss\", loss, prog_bar=True, sync_dist=True)\n\n            return loss\n\n        return None\n\n    def _on_eval_epoch_start(self, step_type: str) -&gt; None:\n        \"\"\"Initialize states or configurations at the start of an evaluation epoch.\n\n        Parameters\n        ----------\n        step_type : str\n            Type of the evaluation phase (\"val\" or \"test\").\n        \"\"\"\n        if (\n            step_type == \"val\"\n            and self.compute_validation_loss\n            or step_type == \"test\"\n            and self.compute_test_loss\n        ):\n            self.log(f\"{step_type}/start\", 1, prog_bar=True, sync_dist=True)\n\n    def _on_eval_epoch_end(self, step_type: str) -&gt; None:\n        \"\"\"Finalize states or logging at the end of an evaluation epoch.\n\n        Parameters\n        ----------\n        step_type : str\n            Type of the evaluation phase (\"val\" or \"test\").\n        \"\"\"\n        if (\n            step_type == \"val\"\n            and self.compute_validation_loss\n            or step_type == \"test\"\n            and self.compute_test_loss\n        ):\n            self.log(f\"{step_type}/end\", 1, prog_bar=True, sync_dist=True)\n</code></pre>"},{"location":"api/#mmlearn.tasks.ijepa.IJEPA.configure_model","title":"configure_model","text":"<pre><code>configure_model()\n</code></pre> <p>Configure the model.</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def configure_model(self) -&gt; None:\n    \"\"\"Configure the model.\"\"\"\n    self.target_encoder.configure_model(self.device)\n</code></pre>"},{"location":"api/#mmlearn.tasks.ijepa.IJEPA.on_before_zero_grad","title":"on_before_zero_grad","text":"<pre><code>on_before_zero_grad(optimizer)\n</code></pre> <p>Perform exponential moving average update of target encoder.</p> <p>This is done right after the <code>optimizer.step()`, which comes just before</code>optimizer.zero_grad()`` to account for gradient accumulation.</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def on_before_zero_grad(self, optimizer: torch.optim.Optimizer) -&gt; None:\n    \"\"\"Perform exponential moving average update of target encoder.\n\n    This is done right after the ``optimizer.step()`, which comes just before\n    ``optimizer.zero_grad()`` to account for gradient accumulation.\n    \"\"\"\n    if self.target_encoder is not None:\n        self.target_encoder.step(self.encoder)\n</code></pre>"},{"location":"api/#mmlearn.tasks.ijepa.IJEPA.training_step","title":"training_step","text":"<pre><code>training_step(batch, batch_idx)\n</code></pre> <p>Perform a single training step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict[str, Any]</code> <p>A batch of data.</p> required <code>batch_idx</code> <code>int</code> <p>Index of the batch.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Loss value.</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def training_step(self, batch: dict[str, Any], batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"Perform a single training step.\n\n    Parameters\n    ----------\n    batch : dict[str, Any]\n        A batch of data.\n    batch_idx : int\n        Index of the batch.\n\n    Returns\n    -------\n    torch.Tensor\n        Loss value.\n    \"\"\"\n    return self._shared_step(batch, batch_idx, step_type=\"train\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.ijepa.IJEPA.validation_step","title":"validation_step","text":"<pre><code>validation_step(batch, batch_idx)\n</code></pre> <p>Run a single validation step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict[str, Any]</code> <p>A batch of data.</p> required <code>batch_idx</code> <code>int</code> <p>Index of the batch.</p> required <p>Returns:</p> Type Description <code>Optional[Tensor]</code> <p>Loss value or <code>None</code> if no loss is computed.</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def validation_step(\n    self, batch: dict[str, Any], batch_idx: int\n) -&gt; Optional[torch.Tensor]:\n    \"\"\"Run a single validation step.\n\n    Parameters\n    ----------\n    batch : dict[str, Any]\n        A batch of data.\n    batch_idx : int\n        Index of the batch.\n\n    Returns\n    -------\n    Optional[torch.Tensor]\n        Loss value or ``None`` if no loss is computed.\n    \"\"\"\n    return self._shared_step(batch, batch_idx, step_type=\"val\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.ijepa.IJEPA.test_step","title":"test_step","text":"<pre><code>test_step(batch, batch_idx)\n</code></pre> <p>Run a single test step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict[str, Any]</code> <p>A batch of data.</p> required <code>batch_idx</code> <code>int</code> <p>Index of the batch.</p> required <p>Returns:</p> Type Description <code>Optional[Tensor]</code> <p>Loss value or <code>None</code> if no loss is computed</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def test_step(\n    self, batch: dict[str, Any], batch_idx: int\n) -&gt; Optional[torch.Tensor]:\n    \"\"\"Run a single test step.\n\n    Parameters\n    ----------\n    batch : dict[str, Any]\n        A batch of data.\n    batch_idx : int\n        Index of the batch.\n\n    Returns\n    -------\n    Optional[torch.Tensor]\n        Loss value or ``None`` if no loss is computed\n    \"\"\"\n    return self._shared_step(batch, batch_idx, step_type=\"test\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.ijepa.IJEPA.on_validation_epoch_start","title":"on_validation_epoch_start","text":"<pre><code>on_validation_epoch_start()\n</code></pre> <p>Prepare for the validation epoch.</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def on_validation_epoch_start(self) -&gt; None:\n    \"\"\"Prepare for the validation epoch.\"\"\"\n    self._on_eval_epoch_start(\"val\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.ijepa.IJEPA.on_validation_epoch_end","title":"on_validation_epoch_end","text":"<pre><code>on_validation_epoch_end()\n</code></pre> <p>Actions at the end of the validation epoch.</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def on_validation_epoch_end(self) -&gt; None:\n    \"\"\"Actions at the end of the validation epoch.\"\"\"\n    self._on_eval_epoch_end(\"val\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.ijepa.IJEPA.on_test_epoch_start","title":"on_test_epoch_start","text":"<pre><code>on_test_epoch_start()\n</code></pre> <p>Prepare for the test epoch.</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def on_test_epoch_start(self) -&gt; None:\n    \"\"\"Prepare for the test epoch.\"\"\"\n    self._on_eval_epoch_start(\"test\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.ijepa.IJEPA.on_test_epoch_end","title":"on_test_epoch_end","text":"<pre><code>on_test_epoch_end()\n</code></pre> <p>Actions at the end of the test epoch.</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def on_test_epoch_end(self) -&gt; None:\n    \"\"\"Actions at the end of the test epoch.\"\"\"\n    self._on_eval_epoch_end(\"test\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.ijepa.IJEPA.on_save_checkpoint","title":"on_save_checkpoint","text":"<pre><code>on_save_checkpoint(checkpoint)\n</code></pre> <p>Add relevant EMA state to the checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <code>dict[str, Any]</code> <p>The state dictionary to save the EMA state to.</p> required Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def on_save_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n    \"\"\"Add relevant EMA state to the checkpoint.\n\n    Parameters\n    ----------\n    checkpoint : dict[str, Any]\n        The state dictionary to save the EMA state to.\n    \"\"\"\n    if self.target_encoder is not None:\n        checkpoint[\"ema_params\"] = {\n            \"decay\": self.target_encoder.decay,\n            \"num_updates\": self.target_encoder.num_updates,\n        }\n</code></pre>"},{"location":"api/#mmlearn.tasks.ijepa.IJEPA.on_load_checkpoint","title":"on_load_checkpoint","text":"<pre><code>on_load_checkpoint(checkpoint)\n</code></pre> <p>Restore EMA state from the checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <code>dict[str, Any]</code> <p>The state dictionary to restore the EMA state from.</p> required Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def on_load_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n    \"\"\"Restore EMA state from the checkpoint.\n\n    Parameters\n    ----------\n    checkpoint : dict[str, Any]\n        The state dictionary to restore the EMA state from.\n    \"\"\"\n    if \"ema_params\" in checkpoint and self.target_encoder is not None:\n        ema_params = checkpoint.pop(\"ema_params\")\n        self.target_encoder.decay = ema_params[\"decay\"]\n        self.target_encoder.num_updates = ema_params[\"num_updates\"]\n\n        self.target_encoder.restore(self.encoder)\n</code></pre>"},{"location":"api/#mmlearn.tasks.zero_shot_classification","title":"zero_shot_classification","text":"<p>Zero-shot classification evaluation task.</p>"},{"location":"api/#mmlearn.tasks.zero_shot_classification.ClassificationTaskSpec","title":"ClassificationTaskSpec  <code>dataclass</code>","text":"<p>Specification for a classification task.</p> Source code in <code>mmlearn/tasks/zero_shot_classification.py</code> <pre><code>@dataclass\nclass ClassificationTaskSpec:\n    \"\"\"Specification for a classification task.\"\"\"\n\n    #: The modality of the query input.\n    query_modality: str\n\n    #: The top-k values for which to compute the classification metrics like accuracy.\n    top_k: list[int]\n</code></pre>"},{"location":"api/#mmlearn.tasks.zero_shot_classification.ZeroShotClassification","title":"ZeroShotClassification","text":"<p>               Bases: <code>EvaluationHooks</code></p> <p>Zero-shot classification evaluation task.</p> <p>This task evaluates the zero-shot classification performance.</p> <p>Parameters:</p> Name Type Description Default <code>task_specs</code> <code>list[ClassificationTaskSpec]</code> <p>A list of classification task specifications.</p> required <code>tokenizer</code> <code>Callable[[Union[str, list[str]]], Union[Tensor, dict[str, Tensor]]]</code> <p>A function to tokenize text inputs.</p> required Source code in <code>mmlearn/tasks/zero_shot_classification.py</code> <pre><code>@store(group=\"eval_task\", provider=\"mmlearn\")\nclass ZeroShotClassification(EvaluationHooks):\n    \"\"\"Zero-shot classification evaluation task.\n\n    This task evaluates the zero-shot classification performance.\n\n    Parameters\n    ----------\n    task_specs : list[ClassificationTaskSpec]\n        A list of classification task specifications.\n    tokenizer : Callable[[Union[str, list[str]]], Union[torch.Tensor, dict[str, torch.Tensor]]]\n        A function to tokenize text inputs.\n    \"\"\"  # noqa: W505\n\n    def __init__(\n        self,\n        task_specs: list[ClassificationTaskSpec],\n        tokenizer: Callable[\n            [Union[str, list[str]]], Union[torch.Tensor, dict[str, torch.Tensor]]\n        ],\n    ) -&gt; None:\n        super().__init__()\n        self.tokenizer = tokenizer\n        self.task_specs = task_specs\n        for spec in self.task_specs:\n            assert Modalities.has_modality(spec.query_modality)\n\n        self.metrics: dict[tuple[str, int], MetricCollection] = {}\n        self._embeddings_store: dict[int, torch.Tensor] = {}\n\n    def on_evaluation_epoch_start(self, pl_module: LightningModule) -&gt; None:\n        \"\"\"Set up the evaluation task.\n\n        Parameters\n        ----------\n        pl_module : pl.LightningModule\n            A reference to the Lightning module being evaluated.\n\n        Raises\n        ------\n        ValueError\n            - If the task is not being run for validation or testing.\n            - If the dataset does not have the required attributes to perform zero-shot\n              classification (i.e ``id2label`` and ``zero_shot_prompt_templates``).\n        \"\"\"\n        if pl_module.trainer.validating:\n            eval_dataset: CombinedDataset = pl_module.trainer.val_dataloaders.dataset\n        elif pl_module.trainer.testing:\n            eval_dataset = pl_module.trainer.test_dataloaders.dataset\n        else:\n            raise ValueError(\n                \"ZeroShotClassification task is only supported for validation and testing.\"\n            )\n\n        self.all_dataset_info = {}\n\n        # create metrics for each dataset/query_modality combination\n        if not self.metrics:\n            for dataset_index, dataset in enumerate(eval_dataset.datasets):\n                dataset_name = getattr(dataset, \"name\", dataset.__class__.__name__)\n                try:\n                    id2label: dict[int, str] = dataset.id2label\n                except AttributeError:\n                    raise ValueError(\n                        f\"Dataset '{dataset_name}' must have a `id2label` attribute \"\n                        \"to perform zero-shot classification.\"\n                    ) from None\n\n                try:\n                    zero_shot_prompt_templates: list[str] = (\n                        dataset.zero_shot_prompt_templates\n                    )\n                except AttributeError:\n                    raise ValueError(\n                        \"Dataset must have a `zero_shot_prompt_templates` attribute to perform zero-shot classification.\"\n                    ) from None\n\n                num_classes = len(id2label)\n\n                self.all_dataset_info[dataset_index] = {\n                    \"name\": dataset_name,\n                    \"id2label\": id2label,\n                    \"prompt_templates\": zero_shot_prompt_templates,\n                    \"num_classes\": num_classes,\n                }\n\n                for spec in self.task_specs:\n                    query_modality = Modalities.get_modality(spec.query_modality).name\n                    self.metrics[(query_modality, dataset_index)] = (\n                        self._create_metrics(\n                            num_classes,\n                            spec.top_k,\n                            prefix=f\"{dataset_name}/{query_modality}_\",\n                            postfix=\"\",\n                        )\n                    )\n\n        for metric in self.metrics.values():\n            metric.to(pl_module.device)\n\n        for dataset_index, dataset_info in self.all_dataset_info.items():\n            id2label = dataset_info[\"id2label\"]\n            prompt_templates: list[str] = dataset_info[\"prompt_templates\"]\n            labels = list(id2label.values())\n\n            with torch.no_grad():\n                chunk_size = 10\n                all_embeddings = []\n\n                for i in tqdm(\n                    range(0, len(labels), chunk_size),\n                    desc=\"Encoding class descriptions\",\n                ):\n                    batch_labels = labels[i : min(i + chunk_size, len(labels))]\n                    descriptions = [\n                        template.format(label)\n                        for label in batch_labels\n                        for template in prompt_templates\n                    ]\n                    tokenized_descriptions = move_data_to_device(\n                        self.tokenizer(descriptions),\n                        pl_module.device,\n                    )\n\n                    # Encode the chunk using the pl_module's encode method\n                    chunk_embeddings = pl_module.encode(\n                        tokenized_descriptions, Modalities.TEXT\n                    )  # shape: [chunk_size x len(prompt_templates), embed_dim]\n                    chunk_embeddings /= chunk_embeddings.norm(p=2, dim=-1, keepdim=True)\n                    chunk_embeddings = chunk_embeddings.reshape(\n                        len(batch_labels), len(prompt_templates), -1\n                    ).mean(dim=1)  # shape: [chunk_size, embed_dim]\n                    chunk_embeddings /= chunk_embeddings.norm(p=2, dim=-1, keepdim=True)\n\n                    # Append the chunk embeddings to the list\n                    all_embeddings.append(chunk_embeddings)\n\n                # Concatenate all chunk embeddings into a single tensor\n                class_embeddings = torch.cat(all_embeddings, dim=0)\n\n            self._embeddings_store[dataset_index] = class_embeddings\n\n    def evaluation_step(\n        self, pl_module: LightningModule, batch: dict[str, torch.Tensor], batch_idx: int\n    ) -&gt; None:\n        \"\"\"Compute logits and update metrics.\n\n        Parameters\n        ----------\n        pl_module : pl.LightningModule\n            A reference to the Lightning module being evaluated.\n        batch : dict[str, torch.Tensor]\n            A batch of data.\n        batch_idx : int\n            The index of the batch.\n        \"\"\"\n        if pl_module.trainer.sanity_checking:\n            return\n\n        for (query_modality, dataset_index), metric_collection in self.metrics.items():\n            matching_indices = torch.where(batch[\"dataset_index\"] == dataset_index)[0]\n\n            if not matching_indices.numel():\n                continue\n\n            class_embeddings = self._embeddings_store[dataset_index]\n            query_embeddings: torch.Tensor = pl_module.encode(\n                batch, Modalities.get_modality(query_modality)\n            )\n            query_embeddings /= query_embeddings.norm(p=2, dim=-1, keepdim=True)\n            query_embeddings = query_embeddings[matching_indices]\n\n            if self.all_dataset_info[dataset_index][\"num_classes\"] == 2:\n                softmax_output = _safe_matmul(\n                    query_embeddings, class_embeddings\n                ).softmax(dim=-1)\n                logits = softmax_output[:, 1] - softmax_output[:, 0]\n            else:\n                logits = 100.0 * _safe_matmul(query_embeddings, class_embeddings)\n            targets = batch[Modalities.get_modality(query_modality).target][\n                matching_indices\n            ]\n\n            metric_collection.update(logits, targets)\n\n    def on_evaluation_epoch_end(self, pl_module: LightningModule) -&gt; dict[str, Any]:\n        \"\"\"Compute and reset metrics.\n\n        Parameters\n        ----------\n        pl_module : pl.LightningModule\n            A reference to the Lightning module being evaluated.\n\n        Returns\n        -------\n        dict[str, Any]\n            The computed metrics.\n        \"\"\"\n        results = {}\n        for metric_collection in self.metrics.values():\n            results.update(metric_collection.compute())\n            metric_collection.reset()\n\n        self._embeddings_store.clear()\n\n        eval_type = \"val\" if pl_module.trainer.validating else \"test\"\n        for key, value in results.items():\n            pl_module.log(f\"{eval_type}/{key}\", value)\n\n        return results\n\n    @staticmethod\n    def _create_metrics(\n        num_classes: int, top_k: list[int], prefix: str, postfix: str\n    ) -&gt; MetricCollection:\n        \"\"\"Create a collection of classification metrics.\"\"\"\n        task_type = \"binary\" if num_classes == 2 else \"multiclass\"\n        acc_metrics = (\n            {\n                f\"top{k}_accuracy\": Accuracy(\n                    task=task_type, num_classes=num_classes, top_k=k, average=\"micro\"\n                )\n                for k in top_k\n            }\n            if num_classes &gt; 2\n            else {\"accuracy\": Accuracy(task=task_type, num_classes=num_classes)}\n        )\n        return MetricCollection(\n            {\n                \"precision\": Precision(\n                    task=task_type,\n                    num_classes=num_classes,\n                    average=\"macro\" if num_classes &gt; 2 else \"micro\",\n                ),\n                \"recall\": Recall(\n                    task=task_type,\n                    num_classes=num_classes,\n                    average=\"macro\" if num_classes &gt; 2 else \"micro\",\n                ),\n                \"f1_score_macro\": F1Score(\n                    task=task_type,\n                    num_classes=num_classes,\n                    average=\"macro\" if num_classes &gt; 2 else \"micro\",\n                ),\n                \"aucroc\": AUROC(task=task_type, num_classes=num_classes),\n                **acc_metrics,\n            },\n            prefix=prefix,\n            postfix=postfix,\n            compute_groups=True,\n        )\n</code></pre>"},{"location":"api/#mmlearn.tasks.zero_shot_classification.ZeroShotClassification.on_evaluation_epoch_start","title":"on_evaluation_epoch_start","text":"<pre><code>on_evaluation_epoch_start(pl_module)\n</code></pre> <p>Set up the evaluation task.</p> <p>Parameters:</p> Name Type Description Default <code>pl_module</code> <code>LightningModule</code> <p>A reference to the Lightning module being evaluated.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <ul> <li>If the task is not being run for validation or testing.</li> <li>If the dataset does not have the required attributes to perform zero-shot   classification (i.e <code>id2label</code> and <code>zero_shot_prompt_templates</code>).</li> </ul> Source code in <code>mmlearn/tasks/zero_shot_classification.py</code> <pre><code>def on_evaluation_epoch_start(self, pl_module: LightningModule) -&gt; None:\n    \"\"\"Set up the evaluation task.\n\n    Parameters\n    ----------\n    pl_module : pl.LightningModule\n        A reference to the Lightning module being evaluated.\n\n    Raises\n    ------\n    ValueError\n        - If the task is not being run for validation or testing.\n        - If the dataset does not have the required attributes to perform zero-shot\n          classification (i.e ``id2label`` and ``zero_shot_prompt_templates``).\n    \"\"\"\n    if pl_module.trainer.validating:\n        eval_dataset: CombinedDataset = pl_module.trainer.val_dataloaders.dataset\n    elif pl_module.trainer.testing:\n        eval_dataset = pl_module.trainer.test_dataloaders.dataset\n    else:\n        raise ValueError(\n            \"ZeroShotClassification task is only supported for validation and testing.\"\n        )\n\n    self.all_dataset_info = {}\n\n    # create metrics for each dataset/query_modality combination\n    if not self.metrics:\n        for dataset_index, dataset in enumerate(eval_dataset.datasets):\n            dataset_name = getattr(dataset, \"name\", dataset.__class__.__name__)\n            try:\n                id2label: dict[int, str] = dataset.id2label\n            except AttributeError:\n                raise ValueError(\n                    f\"Dataset '{dataset_name}' must have a `id2label` attribute \"\n                    \"to perform zero-shot classification.\"\n                ) from None\n\n            try:\n                zero_shot_prompt_templates: list[str] = (\n                    dataset.zero_shot_prompt_templates\n                )\n            except AttributeError:\n                raise ValueError(\n                    \"Dataset must have a `zero_shot_prompt_templates` attribute to perform zero-shot classification.\"\n                ) from None\n\n            num_classes = len(id2label)\n\n            self.all_dataset_info[dataset_index] = {\n                \"name\": dataset_name,\n                \"id2label\": id2label,\n                \"prompt_templates\": zero_shot_prompt_templates,\n                \"num_classes\": num_classes,\n            }\n\n            for spec in self.task_specs:\n                query_modality = Modalities.get_modality(spec.query_modality).name\n                self.metrics[(query_modality, dataset_index)] = (\n                    self._create_metrics(\n                        num_classes,\n                        spec.top_k,\n                        prefix=f\"{dataset_name}/{query_modality}_\",\n                        postfix=\"\",\n                    )\n                )\n\n    for metric in self.metrics.values():\n        metric.to(pl_module.device)\n\n    for dataset_index, dataset_info in self.all_dataset_info.items():\n        id2label = dataset_info[\"id2label\"]\n        prompt_templates: list[str] = dataset_info[\"prompt_templates\"]\n        labels = list(id2label.values())\n\n        with torch.no_grad():\n            chunk_size = 10\n            all_embeddings = []\n\n            for i in tqdm(\n                range(0, len(labels), chunk_size),\n                desc=\"Encoding class descriptions\",\n            ):\n                batch_labels = labels[i : min(i + chunk_size, len(labels))]\n                descriptions = [\n                    template.format(label)\n                    for label in batch_labels\n                    for template in prompt_templates\n                ]\n                tokenized_descriptions = move_data_to_device(\n                    self.tokenizer(descriptions),\n                    pl_module.device,\n                )\n\n                # Encode the chunk using the pl_module's encode method\n                chunk_embeddings = pl_module.encode(\n                    tokenized_descriptions, Modalities.TEXT\n                )  # shape: [chunk_size x len(prompt_templates), embed_dim]\n                chunk_embeddings /= chunk_embeddings.norm(p=2, dim=-1, keepdim=True)\n                chunk_embeddings = chunk_embeddings.reshape(\n                    len(batch_labels), len(prompt_templates), -1\n                ).mean(dim=1)  # shape: [chunk_size, embed_dim]\n                chunk_embeddings /= chunk_embeddings.norm(p=2, dim=-1, keepdim=True)\n\n                # Append the chunk embeddings to the list\n                all_embeddings.append(chunk_embeddings)\n\n            # Concatenate all chunk embeddings into a single tensor\n            class_embeddings = torch.cat(all_embeddings, dim=0)\n\n        self._embeddings_store[dataset_index] = class_embeddings\n</code></pre>"},{"location":"api/#mmlearn.tasks.zero_shot_classification.ZeroShotClassification.evaluation_step","title":"evaluation_step","text":"<pre><code>evaluation_step(pl_module, batch, batch_idx)\n</code></pre> <p>Compute logits and update metrics.</p> <p>Parameters:</p> Name Type Description Default <code>pl_module</code> <code>LightningModule</code> <p>A reference to the Lightning module being evaluated.</p> required <code>batch</code> <code>dict[str, Tensor]</code> <p>A batch of data.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required Source code in <code>mmlearn/tasks/zero_shot_classification.py</code> <pre><code>def evaluation_step(\n    self, pl_module: LightningModule, batch: dict[str, torch.Tensor], batch_idx: int\n) -&gt; None:\n    \"\"\"Compute logits and update metrics.\n\n    Parameters\n    ----------\n    pl_module : pl.LightningModule\n        A reference to the Lightning module being evaluated.\n    batch : dict[str, torch.Tensor]\n        A batch of data.\n    batch_idx : int\n        The index of the batch.\n    \"\"\"\n    if pl_module.trainer.sanity_checking:\n        return\n\n    for (query_modality, dataset_index), metric_collection in self.metrics.items():\n        matching_indices = torch.where(batch[\"dataset_index\"] == dataset_index)[0]\n\n        if not matching_indices.numel():\n            continue\n\n        class_embeddings = self._embeddings_store[dataset_index]\n        query_embeddings: torch.Tensor = pl_module.encode(\n            batch, Modalities.get_modality(query_modality)\n        )\n        query_embeddings /= query_embeddings.norm(p=2, dim=-1, keepdim=True)\n        query_embeddings = query_embeddings[matching_indices]\n\n        if self.all_dataset_info[dataset_index][\"num_classes\"] == 2:\n            softmax_output = _safe_matmul(\n                query_embeddings, class_embeddings\n            ).softmax(dim=-1)\n            logits = softmax_output[:, 1] - softmax_output[:, 0]\n        else:\n            logits = 100.0 * _safe_matmul(query_embeddings, class_embeddings)\n        targets = batch[Modalities.get_modality(query_modality).target][\n            matching_indices\n        ]\n\n        metric_collection.update(logits, targets)\n</code></pre>"},{"location":"api/#mmlearn.tasks.zero_shot_classification.ZeroShotClassification.on_evaluation_epoch_end","title":"on_evaluation_epoch_end","text":"<pre><code>on_evaluation_epoch_end(pl_module)\n</code></pre> <p>Compute and reset metrics.</p> <p>Parameters:</p> Name Type Description Default <code>pl_module</code> <code>LightningModule</code> <p>A reference to the Lightning module being evaluated.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>The computed metrics.</p> Source code in <code>mmlearn/tasks/zero_shot_classification.py</code> <pre><code>def on_evaluation_epoch_end(self, pl_module: LightningModule) -&gt; dict[str, Any]:\n    \"\"\"Compute and reset metrics.\n\n    Parameters\n    ----------\n    pl_module : pl.LightningModule\n        A reference to the Lightning module being evaluated.\n\n    Returns\n    -------\n    dict[str, Any]\n        The computed metrics.\n    \"\"\"\n    results = {}\n    for metric_collection in self.metrics.values():\n        results.update(metric_collection.compute())\n        metric_collection.reset()\n\n    self._embeddings_store.clear()\n\n    eval_type = \"val\" if pl_module.trainer.validating else \"test\"\n    for key, value in results.items():\n        pl_module.log(f\"{eval_type}/{key}\", value)\n\n    return results\n</code></pre>"},{"location":"api/#mmlearn.tasks.zero_shot_retrieval","title":"zero_shot_retrieval","text":"<p>Zero-shot cross-modal retrieval evaluation task.</p>"},{"location":"api/#mmlearn.tasks.zero_shot_retrieval.RetrievalTaskSpec","title":"RetrievalTaskSpec  <code>dataclass</code>","text":"<p>Specification for a retrieval task.</p> Source code in <code>mmlearn/tasks/zero_shot_retrieval.py</code> <pre><code>@dataclass\nclass RetrievalTaskSpec:\n    \"\"\"Specification for a retrieval task.\"\"\"\n\n    #: The query modality.\n    query_modality: str\n\n    #: The target modality.\n    target_modality: str\n\n    #: The top-k values for which to compute the retrieval recall metrics.\n    top_k: list[int]\n</code></pre>"},{"location":"api/#mmlearn.tasks.zero_shot_retrieval.ZeroShotCrossModalRetrieval","title":"ZeroShotCrossModalRetrieval","text":"<p>               Bases: <code>EvaluationHooks</code></p> <p>Zero-shot cross-modal retrieval evaluation task.</p> <p>This task evaluates the retrieval performance of a model on a set of query-target pairs. The model is expected to produce embeddings for both the query and target modalities. The task computes the retrieval recall at <code>k</code> for each pair of modalities.</p> <p>Parameters:</p> Name Type Description Default <code>task_specs</code> <code>list[RetrievalTaskSpec]</code> <p>A list of retrieval task specifications. Each specification defines the query and target modalities, as well as the top-k values for which to compute the retrieval recall metrics.</p> required Source code in <code>mmlearn/tasks/zero_shot_retrieval.py</code> <pre><code>@store(group=\"eval_task\", provider=\"mmlearn\")\nclass ZeroShotCrossModalRetrieval(EvaluationHooks):\n    \"\"\"Zero-shot cross-modal retrieval evaluation task.\n\n    This task evaluates the retrieval performance of a model on a set of query-target\n    pairs. The model is expected to produce embeddings for both the query and target\n    modalities. The task computes the retrieval recall at `k` for each pair of\n    modalities.\n\n    Parameters\n    ----------\n    task_specs : list[RetrievalTaskSpec]\n        A list of retrieval task specifications. Each specification defines the query\n        and target modalities, as well as the top-k values for which to compute the\n        retrieval recall metrics.\n\n    \"\"\"\n\n    def __init__(self, task_specs: list[RetrievalTaskSpec]) -&gt; None:\n        super().__init__()\n\n        self.task_specs = task_specs\n        self.metrics: dict[tuple[str, str], MetricCollection] = {}\n        self._available_modalities = set()\n\n        for spec in self.task_specs:\n            query_modality = spec.query_modality\n            target_modality = spec.target_modality\n            assert Modalities.has_modality(query_modality)\n            assert Modalities.has_modality(target_modality)\n\n            self.metrics[(query_modality, target_modality)] = MetricCollection(\n                {\n                    f\"{query_modality}_to_{target_modality}_R@{k}\": RetrievalRecallAtK(\n                        top_k=k, aggregation=\"mean\", reduction=\"none\"\n                    )\n                    for k in spec.top_k\n                }\n            )\n            self._available_modalities.add(query_modality)\n            self._available_modalities.add(target_modality)\n\n    def on_evaluation_epoch_start(self, pl_module: pl.LightningModule) -&gt; None:\n        \"\"\"Move the metrics to the device of the Lightning module.\"\"\"\n        for metric in self.metrics.values():\n            metric.to(pl_module.device)\n\n    def evaluation_step(\n        self,\n        pl_module: pl.LightningModule,\n        batch: dict[str, torch.Tensor],\n        batch_idx: int,\n    ) -&gt; None:\n        \"\"\"Run the forward pass and update retrieval recall metrics.\n\n        Parameters\n        ----------\n        pl_module : pl.LightningModule\n            A reference to the Lightning module being evaluated.\n        batch : dict[str, torch.Tensor]\n            A dictionary of batched input tensors.\n        batch_idx : int\n            The index of the batch.\n\n        \"\"\"\n        if pl_module.trainer.sanity_checking:\n            return\n\n        outputs: dict[str, Any] = {}\n        for modality_name in self._available_modalities:\n            if modality_name in batch:\n                outputs[modality_name] = pl_module.encode(\n                    batch, Modalities.get_modality(modality_name), normalize=False\n                )\n        for (query_modality, target_modality), metric in self.metrics.items():\n            if query_modality not in outputs or target_modality not in outputs:\n                continue\n            query_embeddings: torch.Tensor = outputs[query_modality]\n            target_embeddings: torch.Tensor = outputs[target_modality]\n            indexes = torch.arange(query_embeddings.size(0), device=pl_module.device)\n\n            metric.update(query_embeddings, target_embeddings, indexes)\n\n    def on_evaluation_epoch_end(\n        self, pl_module: pl.LightningModule\n    ) -&gt; Optional[dict[str, Any]]:\n        \"\"\"Compute the retrieval recall metrics.\n\n        Parameters\n        ----------\n        pl_module : pl.LightningModule\n            A reference to the Lightning module being evaluated.\n\n        Returns\n        -------\n        Optional[dict[str, Any]]\n            A dictionary of evaluation results or `None` if no results are available.\n        \"\"\"\n        if pl_module.trainer.sanity_checking:\n            return None\n\n        results = {}\n        for metric in self.metrics.values():\n            results.update(metric.compute())\n            metric.reset()\n\n        eval_type = \"val\" if pl_module.trainer.validating else \"test\"\n\n        for key, value in results.items():\n            pl_module.log(f\"{eval_type}/{key}\", value)\n\n        return results\n</code></pre>"},{"location":"api/#mmlearn.tasks.zero_shot_retrieval.ZeroShotCrossModalRetrieval.on_evaluation_epoch_start","title":"on_evaluation_epoch_start","text":"<pre><code>on_evaluation_epoch_start(pl_module)\n</code></pre> <p>Move the metrics to the device of the Lightning module.</p> Source code in <code>mmlearn/tasks/zero_shot_retrieval.py</code> <pre><code>def on_evaluation_epoch_start(self, pl_module: pl.LightningModule) -&gt; None:\n    \"\"\"Move the metrics to the device of the Lightning module.\"\"\"\n    for metric in self.metrics.values():\n        metric.to(pl_module.device)\n</code></pre>"},{"location":"api/#mmlearn.tasks.zero_shot_retrieval.ZeroShotCrossModalRetrieval.evaluation_step","title":"evaluation_step","text":"<pre><code>evaluation_step(pl_module, batch, batch_idx)\n</code></pre> <p>Run the forward pass and update retrieval recall metrics.</p> <p>Parameters:</p> Name Type Description Default <code>pl_module</code> <code>LightningModule</code> <p>A reference to the Lightning module being evaluated.</p> required <code>batch</code> <code>dict[str, Tensor]</code> <p>A dictionary of batched input tensors.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required Source code in <code>mmlearn/tasks/zero_shot_retrieval.py</code> <pre><code>def evaluation_step(\n    self,\n    pl_module: pl.LightningModule,\n    batch: dict[str, torch.Tensor],\n    batch_idx: int,\n) -&gt; None:\n    \"\"\"Run the forward pass and update retrieval recall metrics.\n\n    Parameters\n    ----------\n    pl_module : pl.LightningModule\n        A reference to the Lightning module being evaluated.\n    batch : dict[str, torch.Tensor]\n        A dictionary of batched input tensors.\n    batch_idx : int\n        The index of the batch.\n\n    \"\"\"\n    if pl_module.trainer.sanity_checking:\n        return\n\n    outputs: dict[str, Any] = {}\n    for modality_name in self._available_modalities:\n        if modality_name in batch:\n            outputs[modality_name] = pl_module.encode(\n                batch, Modalities.get_modality(modality_name), normalize=False\n            )\n    for (query_modality, target_modality), metric in self.metrics.items():\n        if query_modality not in outputs or target_modality not in outputs:\n            continue\n        query_embeddings: torch.Tensor = outputs[query_modality]\n        target_embeddings: torch.Tensor = outputs[target_modality]\n        indexes = torch.arange(query_embeddings.size(0), device=pl_module.device)\n\n        metric.update(query_embeddings, target_embeddings, indexes)\n</code></pre>"},{"location":"api/#mmlearn.tasks.zero_shot_retrieval.ZeroShotCrossModalRetrieval.on_evaluation_epoch_end","title":"on_evaluation_epoch_end","text":"<pre><code>on_evaluation_epoch_end(pl_module)\n</code></pre> <p>Compute the retrieval recall metrics.</p> <p>Parameters:</p> Name Type Description Default <code>pl_module</code> <code>LightningModule</code> <p>A reference to the Lightning module being evaluated.</p> required <p>Returns:</p> Type Description <code>Optional[dict[str, Any]]</code> <p>A dictionary of evaluation results or <code>None</code> if no results are available.</p> Source code in <code>mmlearn/tasks/zero_shot_retrieval.py</code> <pre><code>def on_evaluation_epoch_end(\n    self, pl_module: pl.LightningModule\n) -&gt; Optional[dict[str, Any]]:\n    \"\"\"Compute the retrieval recall metrics.\n\n    Parameters\n    ----------\n    pl_module : pl.LightningModule\n        A reference to the Lightning module being evaluated.\n\n    Returns\n    -------\n    Optional[dict[str, Any]]\n        A dictionary of evaluation results or `None` if no results are available.\n    \"\"\"\n    if pl_module.trainer.sanity_checking:\n        return None\n\n    results = {}\n    for metric in self.metrics.values():\n        results.update(metric.compute())\n        metric.reset()\n\n    eval_type = \"val\" if pl_module.trainer.validating else \"test\"\n\n    for key, value in results.items():\n        pl_module.log(f\"{eval_type}/{key}\", value)\n\n    return results\n</code></pre>"},{"location":"api/#cli-module","title":"CLI Module","text":""},{"location":"api/#mmlearn.cli","title":"mmlearn.cli","text":"<p>Command Line Interface for mmlearn.</p>"},{"location":"api/#mmlearn.cli.run","title":"run","text":"<p>Main entry point for training and evaluation.</p>"},{"location":"api/#mmlearn.cli.run.CombinedDataset","title":"CombinedDataset","text":"<p>               Bases: <code>Dataset[Example]</code></p> <p>Combine multiple datasets into one.</p> <p>This class is similar to class:<code>~torch.utils.data.ConcatDataset</code> but allows for combining iterable-style datasets with map-style datasets. The iterable-style datasets must implement the :meth:<code>__len__</code> method, which is used to determine the total length of the combined dataset. When an index is passed to the combined dataset, the dataset that contains the example at that index is determined and the example is retrieved from that dataset. Since iterable-style datasets do not support random access, the examples are retrieved sequentially from the iterable-style datasets. When the end of an iterable-style dataset is reached, the iterator is reset and the next example is retrieved from the beginning of the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>datasets</code> <code>Iterable[Union[Dataset, IterableDataset]]</code> <p>Iterable of datasets to combine.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the datasets in the input iterable are not instances of class:<code>~torch.utils.data.Dataset</code> or class:<code>~torch.utils.data.IterableDataset</code>.</p> <code>ValueError</code> <p>If the input iterable of datasets is empty.</p> Source code in <code>mmlearn/datasets/core/combined_dataset.py</code> <pre><code>class CombinedDataset(Dataset[Example]):\n    \"\"\"Combine multiple datasets into one.\n\n    This class is similar to :py:class:`~torch.utils.data.ConcatDataset` but allows\n    for combining iterable-style datasets with map-style datasets. The iterable-style\n    datasets must implement the :meth:`__len__` method, which is used to determine the\n    total length of the combined dataset. When an index is passed to the combined\n    dataset, the dataset that contains the example at that index is determined and\n    the example is retrieved from that dataset. Since iterable-style datasets do\n    not support random access, the examples are retrieved sequentially from the\n    iterable-style datasets. When the end of an iterable-style dataset is reached,\n    the iterator is reset and the next example is retrieved from the beginning of\n    the dataset.\n\n\n    Parameters\n    ----------\n    datasets : Iterable[Union[torch.utils.data.Dataset, torch.utils.data.IterableDataset]]\n        Iterable of datasets to combine.\n\n    Raises\n    ------\n    TypeError\n        If any of the datasets in the input iterable are not instances of\n        :py:class:`~torch.utils.data.Dataset` or :py:class:`~torch.utils.data.IterableDataset`.\n    ValueError\n        If the input iterable of datasets is empty.\n\n    \"\"\"  # noqa: W505\n\n    def __init__(\n        self, datasets: Iterable[Union[Dataset[Example], IterableDataset[Example]]]\n    ) -&gt; None:\n        self.datasets, _ = tree_flatten(datasets)\n        if not all(\n            isinstance(dataset, (Dataset, IterableDataset)) for dataset in self.datasets\n        ):\n            raise TypeError(\n                \"Expected argument `datasets` to be an iterable of `Dataset` or \"\n                f\"`IterableDataset` instances, but found: {self.datasets}\",\n            )\n        if len(self.datasets) == 0:\n            raise ValueError(\n                \"Expected a non-empty iterable of datasets but found an empty iterable\",\n            )\n\n        self._cumulative_sizes: list[int] = np.cumsum(\n            [len(dataset) for dataset in self.datasets]\n        ).tolist()\n        self._iterators: list[Iterator[Example]] = []\n        self._iter_dataset_mapping: dict[int, int] = {}\n\n        # create iterators for iterable datasets and map dataset index to iterator index\n        for idx, dataset in enumerate(self.datasets):\n            if isinstance(dataset, IterableDataset):\n                self._iterators.append(iter(dataset))\n                self._iter_dataset_mapping[idx] = len(self._iterators) - 1\n\n    def __getitem__(self, idx: int) -&gt; Example:\n        \"\"\"Return an example from the combined dataset.\"\"\"\n        if idx &lt; 0:  # handle negative indices\n            if -idx &gt; len(self):\n                raise IndexError(\n                    f\"Index {idx} is out of bounds for the combined dataset with \"\n                    f\"length {len(self)}\",\n                )\n            idx = len(self) + idx\n\n        dataset_idx = bisect.bisect_right(self._cumulative_sizes, idx)\n\n        curr_dataset = self.datasets[dataset_idx]\n        if isinstance(curr_dataset, IterableDataset):\n            iter_idx = self._iter_dataset_mapping[dataset_idx]\n            try:\n                example = next(self._iterators[iter_idx])\n            except StopIteration:\n                self._iterators[iter_idx] = iter(curr_dataset)\n                example = next(self._iterators[iter_idx])\n        else:\n            if dataset_idx == 0:\n                example_idx = idx\n            else:\n                example_idx = idx - self._cumulative_sizes[dataset_idx - 1]\n            example = curr_dataset[example_idx]\n\n        if not isinstance(example, Example):\n            raise TypeError(\n                \"Expected dataset examples to be instances of `Example` \"\n                f\"but found {type(example)}\",\n            )\n\n        if not hasattr(example, \"dataset_index\"):\n            example.dataset_index = dataset_idx\n        if not hasattr(example, \"example_ids\"):\n            example.create_ids()\n\n        return example\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the total number of examples in the combined dataset.\"\"\"\n        return self._cumulative_sizes[-1]\n</code></pre>"},{"location":"api/#mmlearn.cli.run.CombinedDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> <p>Return an example from the combined dataset.</p> Source code in <code>mmlearn/datasets/core/combined_dataset.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; Example:\n    \"\"\"Return an example from the combined dataset.\"\"\"\n    if idx &lt; 0:  # handle negative indices\n        if -idx &gt; len(self):\n            raise IndexError(\n                f\"Index {idx} is out of bounds for the combined dataset with \"\n                f\"length {len(self)}\",\n            )\n        idx = len(self) + idx\n\n    dataset_idx = bisect.bisect_right(self._cumulative_sizes, idx)\n\n    curr_dataset = self.datasets[dataset_idx]\n    if isinstance(curr_dataset, IterableDataset):\n        iter_idx = self._iter_dataset_mapping[dataset_idx]\n        try:\n            example = next(self._iterators[iter_idx])\n        except StopIteration:\n            self._iterators[iter_idx] = iter(curr_dataset)\n            example = next(self._iterators[iter_idx])\n    else:\n        if dataset_idx == 0:\n            example_idx = idx\n        else:\n            example_idx = idx - self._cumulative_sizes[dataset_idx - 1]\n        example = curr_dataset[example_idx]\n\n    if not isinstance(example, Example):\n        raise TypeError(\n            \"Expected dataset examples to be instances of `Example` \"\n            f\"but found {type(example)}\",\n        )\n\n    if not hasattr(example, \"dataset_index\"):\n        example.dataset_index = dataset_idx\n    if not hasattr(example, \"example_ids\"):\n        example.create_ids()\n\n    return example\n</code></pre>"},{"location":"api/#mmlearn.cli.run.CombinedDataset.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the total number of examples in the combined dataset.</p> Source code in <code>mmlearn/datasets/core/combined_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the total number of examples in the combined dataset.\"\"\"\n    return self._cumulative_sizes[-1]\n</code></pre>"},{"location":"api/#mmlearn.cli.run.DefaultDataCollator","title":"DefaultDataCollator  <code>dataclass</code>","text":"<p>Default data collator for batching examples.</p> <p>This data collator will collate a list of class:<code>~mmlearn.datasets.core.example.Example</code> objects into a batch. It can also apply processing functions to specified keys in the batch before returning it.</p> <p>Parameters:</p> Name Type Description Default <code>batch_processors</code> <code>Optional[dict[str, Callable[[Any], Any]]]</code> <p>Dictionary of callables to apply to the batch before returning it.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the batch processor for a key does not return a dictionary with the key in it.</p> Source code in <code>mmlearn/datasets/core/data_collator.py</code> <pre><code>@dataclass\nclass DefaultDataCollator:\n    \"\"\"Default data collator for batching examples.\n\n    This data collator will collate a list of :py:class:`~mmlearn.datasets.core.example.Example`\n    objects into a batch. It can also apply processing functions to specified keys\n    in the batch before returning it.\n\n    Parameters\n    ----------\n    batch_processors : Optional[dict[str, Callable[[Any], Any]]], optional, default=None\n        Dictionary of callables to apply to the batch before returning it.\n\n    Raises\n    ------\n    ValueError\n        If the batch processor for a key does not return a dictionary with the\n        key in it.\n    \"\"\"  # noqa: W505\n\n    #: Dictionary of callables to apply to the batch before returning it.\n    #: The key is the name of the key in the batch, and the value is the processing\n    #: function to apply to the key. The processing function must take a single\n    #: argument and return a single value. If the processing function returns\n    #: a dictionary, it must contain the key that was processed in it (all the\n    #: other keys will also be included in the batch).\n    batch_processors: Optional[dict[str, Callable[[Any], Any]]] = None\n\n    def __call__(self, examples: list[Example]) -&gt; dict[str, Any]:\n        \"\"\"Collate a list of `Example` objects and apply processing functions.\"\"\"\n        batch = collate_example_list(examples)\n\n        if self.batch_processors is not None:\n            for key, processor in self.batch_processors.items():\n                batch_key: str = key\n                if Modalities.has_modality(key):\n                    batch_key = Modalities.get_modality(key).name\n\n                if batch_key in batch:\n                    batch_processed = processor(batch[batch_key])\n                    if isinstance(batch_processed, Mapping):\n                        if batch_key not in batch_processed:\n                            raise ValueError(\n                                f\"Batch processor for '{key}' key must return a dictionary \"\n                                f\"with '{batch_key}' in it.\"\n                            )\n                        batch.update(batch_processed)\n                    else:\n                        batch[batch_key] = batch_processed\n\n        return batch\n</code></pre>"},{"location":"api/#mmlearn.cli.run.DefaultDataCollator.__call__","title":"__call__","text":"<pre><code>__call__(examples)\n</code></pre> <p>Collate a list of <code>Example</code> objects and apply processing functions.</p> Source code in <code>mmlearn/datasets/core/data_collator.py</code> <pre><code>def __call__(self, examples: list[Example]) -&gt; dict[str, Any]:\n    \"\"\"Collate a list of `Example` objects and apply processing functions.\"\"\"\n    batch = collate_example_list(examples)\n\n    if self.batch_processors is not None:\n        for key, processor in self.batch_processors.items():\n            batch_key: str = key\n            if Modalities.has_modality(key):\n                batch_key = Modalities.get_modality(key).name\n\n            if batch_key in batch:\n                batch_processed = processor(batch[batch_key])\n                if isinstance(batch_processed, Mapping):\n                    if batch_key not in batch_processed:\n                        raise ValueError(\n                            f\"Batch processor for '{key}' key must return a dictionary \"\n                            f\"with '{batch_key}' in it.\"\n                        )\n                    batch.update(batch_processed)\n                else:\n                    batch[batch_key] = batch_processed\n\n    return batch\n</code></pre>"},{"location":"api/#mmlearn.cli.run.Example","title":"Example","text":"<p>               Bases: <code>OrderedDict[Any, Any]</code></p> <p>A representation of a single example from a dataset.</p> <p>This class is a subclass of class:<code>~collections.OrderedDict</code> and provides attribute-style access. This means that <code>example[\"text\"]</code> and <code>example.text</code> are equivalent. All datasets in this library return examples as class:<code>~mmlearn.datasets.core.example.Example</code> objects.</p> <p>Parameters:</p> Name Type Description Default <code>init_dict</code> <code>Optional[MutableMapping[Hashable, Any]]</code> <p>Dictionary to init <code>Example</code> class with.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; example = Example({\"text\": torch.tensor(2)})\n&gt;&gt;&gt; example.text.zero_()\ntensor(0)\n&gt;&gt;&gt; example.context = torch.tensor(4)  # set custom attributes after initialization\n</code></pre> Source code in <code>mmlearn/datasets/core/example.py</code> <pre><code>class Example(OrderedDict[Any, Any]):\n    \"\"\"A representation of a single example from a dataset.\n\n    This class is a subclass of :py:class:`~collections.OrderedDict` and provides\n    attribute-style access. This means that `example[\"text\"]` and `example.text`\n    are equivalent. All datasets in this library return examples as\n    :py:class:`~mmlearn.datasets.core.example.Example` objects.\n\n\n    Parameters\n    ----------\n    init_dict : Optional[MutableMapping[Hashable, Any]], optional, default=None\n        Dictionary to init `Example` class with.\n\n    Examples\n    --------\n    &gt;&gt;&gt; example = Example({\"text\": torch.tensor(2)})\n    &gt;&gt;&gt; example.text.zero_()\n    tensor(0)\n    &gt;&gt;&gt; example.context = torch.tensor(4)  # set custom attributes after initialization\n    \"\"\"\n\n    def __init__(\n        self,\n        init_dict: Optional[MutableMapping[Hashable, Any]] = None,\n    ) -&gt; None:\n        if init_dict is None:\n            init_dict = {}\n        super().__init__(init_dict)\n\n    def create_ids(self) -&gt; None:\n        \"\"\"Create a unique id for the example from the dataset and example index.\n\n        This method combines the dataset index and example index to create an\n        attribute called `example_ids`, which is a dictionary of tensors. The\n        dictionary keys are all the keys in the example except for `example_ids`,\n        `example_index`, and `dataset_index`. The values are tensors of shape `(2,)`\n        containing the tuple `(dataset_index, example_index)` for each key.\n        The `example_ids` is used to (re-)identify pairs of examples from different\n        modalities after they have been combined into a batch.\n\n        Warns\n        -----\n        UserWarning\n            If the `example_index` and `dataset_index` attributes are not set.\n\n        Notes\n        -----\n        - The Example must have the following attributes set before calling this\n          this method: `example_index` (usually set/returned by the dataset) and\n          `dataset_index` (usually set by the :py:class:`~mmlearn.datasets.core.combined_dataset.CombinedDataset` object)\n        - The :py:func:`~mmlearn.datasets.core.example.find_matching_indices`\n          function can be used to find matching examples given two tensors of example ids.\n\n        \"\"\"  # noqa: W505\n        if hasattr(self, \"example_index\") and hasattr(self, \"dataset_index\"):\n            self.example_ids = {\n                key: torch.tensor([self.dataset_index, self.example_index])\n                for key in self.keys()\n                if key not in (\"example_ids\", \"example_index\", \"dataset_index\")\n            }\n        else:\n            rank_zero_warn(\n                \"Cannot create `example_ids` without `example_index` and `dataset_index` \"\n                \"attributes. Set these attributes before calling `create_ids`. \"\n                \"No `example_ids` was created.\",\n                stacklevel=2,\n                category=UserWarning,\n            )\n\n    def __getattr__(self, key: str) -&gt; Any:\n        \"\"\"Get attribute by key.\"\"\"\n        try:\n            return self[key]\n        except KeyError:\n            raise AttributeError(key) from None\n\n    def __setattr__(self, key: str, value: Any) -&gt; None:\n        \"\"\"Set attribute by key.\"\"\"\n        if isinstance(value, MutableMapping):\n            value = Example(value)\n        self[key] = value\n\n    def __setitem__(self, key: Hashable, value: Any) -&gt; None:\n        \"\"\"Set item by key.\"\"\"\n        if isinstance(value, MutableMapping):\n            value = Example(value)\n        super().__setitem__(key, value)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.Example.create_ids","title":"create_ids","text":"<pre><code>create_ids()\n</code></pre> <p>Create a unique id for the example from the dataset and example index.</p> <p>This method combines the dataset index and example index to create an attribute called <code>example_ids</code>, which is a dictionary of tensors. The dictionary keys are all the keys in the example except for <code>example_ids</code>, <code>example_index</code>, and <code>dataset_index</code>. The values are tensors of shape <code>(2,)</code> containing the tuple <code>(dataset_index, example_index)</code> for each key. The <code>example_ids</code> is used to (re-)identify pairs of examples from different modalities after they have been combined into a batch.</p> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If the <code>example_index</code> and <code>dataset_index</code> attributes are not set.</p> Notes <ul> <li>The Example must have the following attributes set before calling this   this method: <code>example_index</code> (usually set/returned by the dataset) and   <code>dataset_index</code> (usually set by the class:<code>~mmlearn.datasets.core.combined_dataset.CombinedDataset</code> object)</li> <li>The func:<code>~mmlearn.datasets.core.example.find_matching_indices</code>   function can be used to find matching examples given two tensors of example ids.</li> </ul> Source code in <code>mmlearn/datasets/core/example.py</code> <pre><code>def create_ids(self) -&gt; None:\n    \"\"\"Create a unique id for the example from the dataset and example index.\n\n    This method combines the dataset index and example index to create an\n    attribute called `example_ids`, which is a dictionary of tensors. The\n    dictionary keys are all the keys in the example except for `example_ids`,\n    `example_index`, and `dataset_index`. The values are tensors of shape `(2,)`\n    containing the tuple `(dataset_index, example_index)` for each key.\n    The `example_ids` is used to (re-)identify pairs of examples from different\n    modalities after they have been combined into a batch.\n\n    Warns\n    -----\n    UserWarning\n        If the `example_index` and `dataset_index` attributes are not set.\n\n    Notes\n    -----\n    - The Example must have the following attributes set before calling this\n      this method: `example_index` (usually set/returned by the dataset) and\n      `dataset_index` (usually set by the :py:class:`~mmlearn.datasets.core.combined_dataset.CombinedDataset` object)\n    - The :py:func:`~mmlearn.datasets.core.example.find_matching_indices`\n      function can be used to find matching examples given two tensors of example ids.\n\n    \"\"\"  # noqa: W505\n    if hasattr(self, \"example_index\") and hasattr(self, \"dataset_index\"):\n        self.example_ids = {\n            key: torch.tensor([self.dataset_index, self.example_index])\n            for key in self.keys()\n            if key not in (\"example_ids\", \"example_index\", \"dataset_index\")\n        }\n    else:\n        rank_zero_warn(\n            \"Cannot create `example_ids` without `example_index` and `dataset_index` \"\n            \"attributes. Set these attributes before calling `create_ids`. \"\n            \"No `example_ids` was created.\",\n            stacklevel=2,\n            category=UserWarning,\n        )\n</code></pre>"},{"location":"api/#mmlearn.cli.run.Example.__getattr__","title":"__getattr__","text":"<pre><code>__getattr__(key)\n</code></pre> <p>Get attribute by key.</p> Source code in <code>mmlearn/datasets/core/example.py</code> <pre><code>def __getattr__(self, key: str) -&gt; Any:\n    \"\"\"Get attribute by key.\"\"\"\n    try:\n        return self[key]\n    except KeyError:\n        raise AttributeError(key) from None\n</code></pre>"},{"location":"api/#mmlearn.cli.run.Example.__setattr__","title":"__setattr__","text":"<pre><code>__setattr__(key, value)\n</code></pre> <p>Set attribute by key.</p> Source code in <code>mmlearn/datasets/core/example.py</code> <pre><code>def __setattr__(self, key: str, value: Any) -&gt; None:\n    \"\"\"Set attribute by key.\"\"\"\n    if isinstance(value, MutableMapping):\n        value = Example(value)\n    self[key] = value\n</code></pre>"},{"location":"api/#mmlearn.cli.run.Example.__setitem__","title":"__setitem__","text":"<pre><code>__setitem__(key, value)\n</code></pre> <p>Set item by key.</p> Source code in <code>mmlearn/datasets/core/example.py</code> <pre><code>def __setitem__(self, key: Hashable, value: Any) -&gt; None:\n    \"\"\"Set item by key.\"\"\"\n    if isinstance(value, MutableMapping):\n        value = Example(value)\n    super().__setitem__(key, value)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.CombinedDatasetRatioSampler","title":"CombinedDatasetRatioSampler","text":"<p>               Bases: <code>Sampler[int]</code></p> <p>Sampler for weighted sampling from a class:<code>~mmlearn.datasets.core.combined_dataset.CombinedDataset</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>CombinedDataset</code> <p>An instance of class:<code>~mmlearn.datasets.core.combined_dataset.CombinedDataset</code> to sample from.</p> required <code>ratios</code> <code>Optional[Sequence[float]]</code> <p>A sequence of ratios for sampling from each dataset in the combined dataset. The length of the sequence must be equal to the number of datasets in the combined dataset (<code>dataset</code>). If <code>None</code>, the length of each dataset in the combined dataset is used as the ratio. The ratios are normalized to sum to 1.</p> <code>None</code> <code>num_samples</code> <code>Optional[int]</code> <p>The number of samples to draw from the combined dataset. If <code>None</code>, the sampler will draw as many samples as there are in the combined dataset. This number must yield at least one sample per dataset in the combined dataset, when multiplied by the corresponding ratio.</p> <code>None</code> <code>replacement</code> <code>bool</code> <p>Whether to sample with replacement or not.</p> <code>False</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the sampled indices or not. If <code>False</code>, the indices of each dataset will appear in the order they are stored in the combined dataset. This is similar to sequential sampling from each dataset. The datasets that make up the combined dataset are still sampled randomly.</p> <code>True</code> <code>rank</code> <code>Optional[int]</code> <p>Rank of the current process within :attr:<code>num_replicas</code>. By default, :attr:<code>rank</code> is retrieved from the current distributed group.</p> <code>None</code> <code>num_replicas</code> <code>Optional[int]</code> <p>Number of processes participating in distributed training. By default, :attr:<code>num_replicas</code> is retrieved from the current distributed group.</p> <code>None</code> <code>drop_last</code> <code>bool</code> <p>Whether to drop the last incomplete batch or not. If <code>True</code>, the sampler will drop samples to make the number of samples evenly divisible by the number of replicas in distributed mode.</p> <code>False</code> <code>seed</code> <code>int</code> <p>Random seed used to when sampling from the combined dataset and shuffling the sampled indices.</p> <code>0</code> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>CombinedDataset</code> <p>The dataset to sample from.</p> <code>num_samples</code> <code>int</code> <p>The number of samples to draw from the combined dataset.</p> <code>probs</code> <code>Tensor</code> <p>The probabilities for sampling from each dataset in the combined dataset. This is computed from the <code>ratios</code> argument and is normalized to sum to 1.</p> <code>replacement</code> <code>bool</code> <p>Whether to sample with replacement or not.</p> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the sampled indices or not.</p> <code>rank</code> <code>int</code> <p>Rank of the current process within :attr:<code>num_replicas</code>.</p> <code>num_replicas</code> <code>int</code> <p>Number of processes participating in distributed training.</p> <code>drop_last</code> <code>bool</code> <p>Whether to drop samples to make the number of samples evenly divisible by the number of replicas in distributed mode.</p> <code>seed</code> <code>int</code> <p>Random seed used to when sampling from the combined dataset and shuffling the sampled indices.</p> <code>epoch</code> <code>int</code> <p>Current epoch number. This is used to set the random seed. This is useful in distributed mode to ensure that each process receives a different random ordering of the samples.</p> <code>total_size</code> <code>int</code> <p>The total number of samples across all processes.</p> Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>@store(group=\"dataloader/sampler\", provider=\"mmlearn\")\nclass CombinedDatasetRatioSampler(Sampler[int]):\n    \"\"\"Sampler for weighted sampling from a :py:class:`~mmlearn.datasets.core.combined_dataset.CombinedDataset`.\n\n    Parameters\n    ----------\n    dataset : CombinedDataset\n        An instance of :py:class:`~mmlearn.datasets.core.combined_dataset.CombinedDataset`\n        to sample from.\n    ratios : Optional[Sequence[float]], optional, default=None\n        A sequence of ratios for sampling from each dataset in the combined dataset.\n        The length of the sequence must be equal to the number of datasets in the\n        combined dataset (`dataset`). If `None`, the length of each dataset in the\n        combined dataset is used as the ratio. The ratios are normalized to sum to 1.\n    num_samples : Optional[int], optional, default=None\n        The number of samples to draw from the combined dataset. If `None`, the\n        sampler will draw as many samples as there are in the combined dataset.\n        This number must yield at least one sample per dataset in the combined\n        dataset, when multiplied by the corresponding ratio.\n    replacement : bool, default=False\n        Whether to sample with replacement or not.\n    shuffle : bool, default=True\n        Whether to shuffle the sampled indices or not. If `False`, the indices of\n        each dataset will appear in the order they are stored in the combined dataset.\n        This is similar to sequential sampling from each dataset. The datasets\n        that make up the combined dataset are still sampled randomly.\n    rank : Optional[int], optional, default=None\n        Rank of the current process within :attr:`num_replicas`. By default,\n        :attr:`rank` is retrieved from the current distributed group.\n    num_replicas : Optional[int], optional, default=None\n        Number of processes participating in distributed training. By\n        default, :attr:`num_replicas` is retrieved from the current distributed group.\n    drop_last : bool, default=False\n        Whether to drop the last incomplete batch or not. If `True`, the sampler will\n        drop samples to make the number of samples evenly divisible by the number of\n        replicas in distributed mode.\n    seed : int, default=0\n        Random seed used to when sampling from the combined dataset and shuffling\n        the sampled indices.\n\n    Attributes\n    ----------\n    dataset : CombinedDataset\n        The dataset to sample from.\n    num_samples : int\n        The number of samples to draw from the combined dataset.\n    probs : torch.Tensor\n        The probabilities for sampling from each dataset in the combined dataset.\n        This is computed from the `ratios` argument and is normalized to sum to 1.\n    replacement : bool\n        Whether to sample with replacement or not.\n    shuffle : bool\n        Whether to shuffle the sampled indices or not.\n    rank : int\n        Rank of the current process within :attr:`num_replicas`.\n    num_replicas : int\n        Number of processes participating in distributed training.\n    drop_last : bool\n        Whether to drop samples to make the number of samples evenly divisible by the\n        number of replicas in distributed mode.\n    seed : int\n        Random seed used to when sampling from the combined dataset and shuffling\n        the sampled indices.\n    epoch : int\n        Current epoch number. This is used to set the random seed. This is useful\n        in distributed mode to ensure that each process receives a different random\n        ordering of the samples.\n    total_size : int\n        The total number of samples across all processes.\n    \"\"\"  # noqa: W505\n\n    def __init__(  # noqa: PLR0912\n        self,\n        dataset: CombinedDataset,\n        ratios: Optional[Sequence[float]] = None,\n        num_samples: Optional[int] = None,\n        replacement: bool = False,\n        shuffle: bool = True,\n        rank: Optional[int] = None,\n        num_replicas: Optional[int] = None,\n        drop_last: bool = False,\n        seed: int = 0,\n    ):\n        if not isinstance(dataset, CombinedDataset):\n            raise TypeError(\n                \"Expected argument `dataset` to be of type `CombinedDataset`, \"\n                f\"but got {type(dataset)}.\",\n            )\n        if not isinstance(seed, int):\n            raise TypeError(\n                f\"Expected argument `seed` to be an integer, but got {type(seed)}.\",\n            )\n        if num_replicas is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            num_replicas = dist.get_world_size()\n        if rank is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            rank = dist.get_rank()\n        if rank &gt;= num_replicas or rank &lt; 0:\n            raise ValueError(\n                f\"Invalid rank {rank}, rank should be in the interval [0, {num_replicas - 1}]\"\n            )\n\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.drop_last = drop_last\n        self.replacement = replacement\n        self.shuffle = shuffle\n        self.seed = seed\n        self.epoch = 0\n\n        self._num_samples = num_samples\n        if not isinstance(self.num_samples, int) or self.num_samples &lt;= 0:\n            raise ValueError(\n                \"Expected argument `num_samples` to be a positive integer, but got \"\n                f\"{self.num_samples}.\",\n            )\n\n        if ratios is None:\n            ratios = [len(subset) for subset in self.dataset.datasets]\n\n        num_datasets = len(self.dataset.datasets)\n        if len(ratios) != num_datasets:\n            raise ValueError(\n                f\"Expected argument `ratios` to be of length {num_datasets}, \"\n                f\"but got length {len(ratios)}.\",\n            )\n        prob_sum = sum(ratios)\n        if not all(ratio &gt;= 0 for ratio in ratios) and prob_sum &gt; 0:\n            raise ValueError(\n                \"Expected argument `ratios` to be a sequence of non-negative numbers. \"\n                f\"Got {ratios}.\",\n            )\n        self.probs = torch.tensor(\n            [ratio / prob_sum for ratio in ratios],\n            dtype=torch.double,\n        )\n        if any((prob * self.num_samples) &lt;= 0 for prob in self.probs):\n            raise ValueError(\n                \"Expected dataset ratio to result in at least one sample per dataset. \"\n                f\"Got dataset sizes {self.probs * self.num_samples}.\",\n            )\n\n    @property\n    def num_samples(self) -&gt; int:\n        \"\"\"Return the number of samples managed by the sampler.\"\"\"\n        # dataset size might change at runtime\n        if self._num_samples is None:\n            num_samples = len(self.dataset)\n        else:\n            num_samples = self._num_samples\n\n        if self.drop_last and num_samples % self.num_replicas != 0:\n            # split to nearest available length that is evenly divisible.\n            # This is to ensure each rank receives the same amount of data when\n            # using this Sampler.\n            num_samples = math.ceil(\n                (num_samples - self.num_replicas) / self.num_replicas,\n            )\n        else:\n            num_samples = math.ceil(num_samples / self.num_replicas)\n        return num_samples\n\n    @property\n    def total_size(self) -&gt; int:\n        \"\"\"Return the total size of the dataset.\"\"\"\n        return self.num_samples * self.num_replicas\n\n    def __iter__(self) -&gt; Iterator[int]:\n        \"\"\"Return an iterator that yields sample indices for the combined dataset.\"\"\"\n        generator = torch.Generator()\n        seed = self.seed + self.epoch\n        generator.manual_seed(seed)\n\n        cumulative_sizes = [0] + self.dataset._cumulative_sizes\n        num_samples_per_dataset = [int(prob * self.total_size) for prob in self.probs]\n        indices = []\n        for i in range(len(self.dataset.datasets)):\n            per_dataset_indices: torch.Tensor = torch.multinomial(\n                torch.ones(cumulative_sizes[i + 1] - cumulative_sizes[i]),\n                num_samples_per_dataset[i],\n                replacement=self.replacement,\n                generator=generator,\n            )\n            # adjust indices to reflect position in cumulative dataset\n            per_dataset_indices += cumulative_sizes[i]\n            assert per_dataset_indices.max() &lt; cumulative_sizes[i + 1], (\n                f\"Indices from dataset {i} exceed dataset size. \"\n                f\"Got indices {per_dataset_indices} and dataset size {cumulative_sizes[i + 1]}.\",\n            )\n            indices.append(per_dataset_indices)\n\n        indices = torch.cat(indices)\n        if self.shuffle:\n            rand_indices = torch.randperm(len(indices), generator=generator)\n            indices = indices[rand_indices]\n\n        indices = indices.tolist()  # type: ignore[attr-defined]\n        num_indices = len(indices)\n\n        if num_indices &lt; self.total_size:\n            padding_size = self.total_size - num_indices\n            if padding_size &lt;= num_indices:\n                indices += indices[:padding_size]\n            else:\n                indices += (indices * math.ceil(padding_size / num_indices))[\n                    :padding_size\n                ]\n        elif num_indices &gt; self.total_size:\n            indices = indices[: self.total_size]\n        assert len(indices) == self.total_size\n\n        # subsample\n        indices = indices[self.rank : self.total_size : self.num_replicas]\n        assert len(indices) == self.num_samples, (\n            f\"Expected {self.num_samples} samples, but got {len(indices)}.\",\n        )\n\n        yield from iter(indices)\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the total number of samples in the sampler.\"\"\"\n        return self.num_samples\n\n    def set_epoch(self, epoch: int) -&gt; None:\n        \"\"\"Set the epoch for this sampler.\n\n        When :attr:`shuffle=True`, this ensures all replicas use a different random\n        ordering for each epoch. Otherwise, the next iteration of this sampler\n        will yield the same ordering.\n\n        Parameters\n        ----------\n        epoch : int\n            Epoch number.\n\n        \"\"\"\n        self.epoch = epoch\n\n        # some iterable datasets (especially huggingface iterable datasets) might\n        # require setting the epoch to ensure shuffling works properly\n        for dataset in self.dataset.datasets:\n            if hasattr(dataset, \"set_epoch\"):\n                dataset.set_epoch(epoch)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.CombinedDatasetRatioSampler.num_samples","title":"num_samples  <code>property</code>","text":"<pre><code>num_samples\n</code></pre> <p>Return the number of samples managed by the sampler.</p>"},{"location":"api/#mmlearn.cli.run.CombinedDatasetRatioSampler.total_size","title":"total_size  <code>property</code>","text":"<pre><code>total_size\n</code></pre> <p>Return the total size of the dataset.</p>"},{"location":"api/#mmlearn.cli.run.CombinedDatasetRatioSampler.__iter__","title":"__iter__","text":"<pre><code>__iter__()\n</code></pre> <p>Return an iterator that yields sample indices for the combined dataset.</p> Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>def __iter__(self) -&gt; Iterator[int]:\n    \"\"\"Return an iterator that yields sample indices for the combined dataset.\"\"\"\n    generator = torch.Generator()\n    seed = self.seed + self.epoch\n    generator.manual_seed(seed)\n\n    cumulative_sizes = [0] + self.dataset._cumulative_sizes\n    num_samples_per_dataset = [int(prob * self.total_size) for prob in self.probs]\n    indices = []\n    for i in range(len(self.dataset.datasets)):\n        per_dataset_indices: torch.Tensor = torch.multinomial(\n            torch.ones(cumulative_sizes[i + 1] - cumulative_sizes[i]),\n            num_samples_per_dataset[i],\n            replacement=self.replacement,\n            generator=generator,\n        )\n        # adjust indices to reflect position in cumulative dataset\n        per_dataset_indices += cumulative_sizes[i]\n        assert per_dataset_indices.max() &lt; cumulative_sizes[i + 1], (\n            f\"Indices from dataset {i} exceed dataset size. \"\n            f\"Got indices {per_dataset_indices} and dataset size {cumulative_sizes[i + 1]}.\",\n        )\n        indices.append(per_dataset_indices)\n\n    indices = torch.cat(indices)\n    if self.shuffle:\n        rand_indices = torch.randperm(len(indices), generator=generator)\n        indices = indices[rand_indices]\n\n    indices = indices.tolist()  # type: ignore[attr-defined]\n    num_indices = len(indices)\n\n    if num_indices &lt; self.total_size:\n        padding_size = self.total_size - num_indices\n        if padding_size &lt;= num_indices:\n            indices += indices[:padding_size]\n        else:\n            indices += (indices * math.ceil(padding_size / num_indices))[\n                :padding_size\n            ]\n    elif num_indices &gt; self.total_size:\n        indices = indices[: self.total_size]\n    assert len(indices) == self.total_size\n\n    # subsample\n    indices = indices[self.rank : self.total_size : self.num_replicas]\n    assert len(indices) == self.num_samples, (\n        f\"Expected {self.num_samples} samples, but got {len(indices)}.\",\n    )\n\n    yield from iter(indices)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.CombinedDatasetRatioSampler.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the total number of samples in the sampler.</p> Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the total number of samples in the sampler.\"\"\"\n    return self.num_samples\n</code></pre>"},{"location":"api/#mmlearn.cli.run.CombinedDatasetRatioSampler.set_epoch","title":"set_epoch","text":"<pre><code>set_epoch(epoch)\n</code></pre> <p>Set the epoch for this sampler.</p> <p>When :attr:<code>shuffle=True</code>, this ensures all replicas use a different random ordering for each epoch. Otherwise, the next iteration of this sampler will yield the same ordering.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>Epoch number.</p> required Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>def set_epoch(self, epoch: int) -&gt; None:\n    \"\"\"Set the epoch for this sampler.\n\n    When :attr:`shuffle=True`, this ensures all replicas use a different random\n    ordering for each epoch. Otherwise, the next iteration of this sampler\n    will yield the same ordering.\n\n    Parameters\n    ----------\n    epoch : int\n        Epoch number.\n\n    \"\"\"\n    self.epoch = epoch\n\n    # some iterable datasets (especially huggingface iterable datasets) might\n    # require setting the epoch to ensure shuffling works properly\n    for dataset in self.dataset.datasets:\n        if hasattr(dataset, \"set_epoch\"):\n            dataset.set_epoch(epoch)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.DistributedEvalSampler","title":"DistributedEvalSampler","text":"<p>               Bases: <code>Sampler[int]</code></p> <p>Sampler for distributed evaluation.</p> <p>The main differences between this and class:<code>torch.utils.data.DistributedSampler</code> are that this sampler does not add extra samples to make it evenly divisible and shuffling is disabled by default.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>Dataset used for sampling.</p> required <code>num_replicas</code> <code>Optional[int]</code> <p>Number of processes participating in distributed training. By default, :attr:<code>rank</code> is retrieved from the current distributed group.</p> <code>None</code> <code>rank</code> <code>Optional[int]</code> <p>Rank of the current process within :attr:<code>num_replicas</code>. By default, :attr:<code>rank</code> is retrieved from the current distributed group.</p> <code>None</code> <code>shuffle</code> <code>bool</code> <p>If <code>True</code> (default), sampler will shuffle the indices.</p> <code>False</code> <code>seed</code> <code>int</code> <p>Random seed used to shuffle the sampler if :attr:<code>shuffle=True</code>. This number should be identical across all processes in the distributed group.</p> <code>0</code> Warnings <p>DistributedEvalSampler should NOT be used for training. The distributed processes could hang forever. See [1]_ for details</p> Notes <ul> <li>This sampler is for evaluation purpose where synchronization does not happen   every epoch. Synchronization should be done outside the dataloader loop.   It is especially useful in conjunction with   class:<code>torch.nn.parallel.DistributedDataParallel</code> [2]_.</li> <li>The input Dataset is assumed to be of constant size.</li> <li>This implementation is adapted from [3]_.</li> </ul> References <p>.. [1] https://github.com/pytorch/pytorch/issues/22584 .. [2] https://discuss.pytorch.org/t/how-to-validate-in-distributeddataparallel-correctly/94267/11 .. [3] https://github.com/SeungjunNah/DeepDeblur-PyTorch/blob/master/src/data/sampler.py</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; def example():\n...     start_epoch, n_epochs = 0, 2\n...     sampler = DistributedEvalSampler(dataset) if is_distributed else None\n...     loader = DataLoader(dataset, shuffle=(sampler is None), sampler=sampler)\n...     for epoch in range(start_epoch, n_epochs):\n...         if is_distributed:\n...             sampler.set_epoch(epoch)\n...         evaluate(loader)\n</code></pre> Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>@store(group=\"dataloader/sampler\", provider=\"mmlearn\")\nclass DistributedEvalSampler(Sampler[int]):\n    \"\"\"Sampler for distributed evaluation.\n\n    The main differences between this and :py:class:`torch.utils.data.DistributedSampler`\n    are that this sampler does not add extra samples to make it evenly divisible and\n    shuffling is disabled by default.\n\n    Parameters\n    ----------\n    dataset : torch.utils.data.Dataset\n        Dataset used for sampling.\n    num_replicas : Optional[int], optional, default=None\n        Number of processes participating in distributed training. By\n        default, :attr:`rank` is retrieved from the current distributed group.\n    rank : Optional[int], optional, default=None\n        Rank of the current process within :attr:`num_replicas`. By default,\n        :attr:`rank` is retrieved from the current distributed group.\n    shuffle : bool, optional, default=False\n        If `True` (default), sampler will shuffle the indices.\n    seed : int, optional, default=0\n        Random seed used to shuffle the sampler if :attr:`shuffle=True`.\n        This number should be identical across all processes in the\n        distributed group.\n\n    Warnings\n    --------\n    DistributedEvalSampler should NOT be used for training. The distributed processes\n    could hang forever. See [1]_ for details\n\n    Notes\n    -----\n    - This sampler is for evaluation purpose where synchronization does not happen\n      every epoch. Synchronization should be done outside the dataloader loop.\n      It is especially useful in conjunction with\n      :py:class:`torch.nn.parallel.DistributedDataParallel` [2]_.\n    - The input Dataset is assumed to be of constant size.\n    - This implementation is adapted from [3]_.\n\n    References\n    ----------\n    .. [1] https://github.com/pytorch/pytorch/issues/22584\n    .. [2] https://discuss.pytorch.org/t/how-to-validate-in-distributeddataparallel-correctly/94267/11\n    .. [3] https://github.com/SeungjunNah/DeepDeblur-PyTorch/blob/master/src/data/sampler.py\n\n\n    Examples\n    --------\n    &gt;&gt;&gt; def example():\n    ...     start_epoch, n_epochs = 0, 2\n    ...     sampler = DistributedEvalSampler(dataset) if is_distributed else None\n    ...     loader = DataLoader(dataset, shuffle=(sampler is None), sampler=sampler)\n    ...     for epoch in range(start_epoch, n_epochs):\n    ...         if is_distributed:\n    ...             sampler.set_epoch(epoch)\n    ...         evaluate(loader)\n\n    \"\"\"  # noqa: W505\n\n    def __init__(\n        self,\n        dataset: Dataset[Sized],\n        num_replicas: Optional[int] = None,\n        rank: Optional[int] = None,\n        shuffle: bool = False,\n        seed: int = 0,\n    ) -&gt; None:\n        if num_replicas is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            num_replicas = dist.get_world_size()\n        if rank is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            rank = dist.get_rank()\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.epoch = 0\n        self.shuffle = shuffle\n        self.seed = seed\n\n    @property\n    def total_size(self) -&gt; int:\n        \"\"\"Return the total size of the dataset.\"\"\"\n        return len(self.dataset)\n\n    @property\n    def num_samples(self) -&gt; int:\n        \"\"\"Return the number of samples managed by the sampler.\"\"\"\n        indices = list(range(self.total_size))[\n            self.rank : self.total_size : self.num_replicas\n        ]\n        return len(indices)  # true value without extra samples\n\n    def __iter__(self) -&gt; Iterator[int]:\n        \"\"\"Return an iterator that iterates over the indices of the dataset.\"\"\"\n        if self.shuffle:\n            # deterministically shuffle based on epoch and seed\n            g = torch.Generator()\n            g.manual_seed(self.seed + self.epoch)\n            indices = torch.randperm(self.total_size, generator=g).tolist()\n        else:\n            indices = list(range(self.total_size))\n\n        # subsample\n        indices = indices[self.rank : self.total_size : self.num_replicas]\n        assert len(indices) == self.num_samples\n\n        return iter(indices)\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the number of samples.\"\"\"\n        return self.num_samples\n\n    def set_epoch(self, epoch: int) -&gt; None:\n        \"\"\"Set the epoch for this sampler.\n\n        When :attr:`shuffle=True`, this ensures all replicas use a different random\n        ordering for each epoch. Otherwise, the next iteration of this sampler\n        will yield the same ordering.\n\n        Parameters\n        ----------\n        epoch : int\n            Epoch number.\n\n        \"\"\"\n        self.epoch = epoch\n</code></pre>"},{"location":"api/#mmlearn.cli.run.DistributedEvalSampler.total_size","title":"total_size  <code>property</code>","text":"<pre><code>total_size\n</code></pre> <p>Return the total size of the dataset.</p>"},{"location":"api/#mmlearn.cli.run.DistributedEvalSampler.num_samples","title":"num_samples  <code>property</code>","text":"<pre><code>num_samples\n</code></pre> <p>Return the number of samples managed by the sampler.</p>"},{"location":"api/#mmlearn.cli.run.DistributedEvalSampler.__iter__","title":"__iter__","text":"<pre><code>__iter__()\n</code></pre> <p>Return an iterator that iterates over the indices of the dataset.</p> Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>def __iter__(self) -&gt; Iterator[int]:\n    \"\"\"Return an iterator that iterates over the indices of the dataset.\"\"\"\n    if self.shuffle:\n        # deterministically shuffle based on epoch and seed\n        g = torch.Generator()\n        g.manual_seed(self.seed + self.epoch)\n        indices = torch.randperm(self.total_size, generator=g).tolist()\n    else:\n        indices = list(range(self.total_size))\n\n    # subsample\n    indices = indices[self.rank : self.total_size : self.num_replicas]\n    assert len(indices) == self.num_samples\n\n    return iter(indices)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.DistributedEvalSampler.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the number of samples.</p> Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the number of samples.\"\"\"\n    return self.num_samples\n</code></pre>"},{"location":"api/#mmlearn.cli.run.DistributedEvalSampler.set_epoch","title":"set_epoch","text":"<pre><code>set_epoch(epoch)\n</code></pre> <p>Set the epoch for this sampler.</p> <p>When :attr:<code>shuffle=True</code>, this ensures all replicas use a different random ordering for each epoch. Otherwise, the next iteration of this sampler will yield the same ordering.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>Epoch number.</p> required Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>def set_epoch(self, epoch: int) -&gt; None:\n    \"\"\"Set the epoch for this sampler.\n\n    When :attr:`shuffle=True`, this ensures all replicas use a different random\n    ordering for each epoch. Otherwise, the next iteration of this sampler\n    will yield the same ordering.\n\n    Parameters\n    ----------\n    epoch : int\n        Epoch number.\n\n    \"\"\"\n    self.epoch = epoch\n</code></pre>"},{"location":"api/#mmlearn.cli.run.BlockwiseImagePatchMaskGenerator","title":"BlockwiseImagePatchMaskGenerator","text":"<p>Blockwise image patch mask generator.</p> <p>This is primarily intended for the data2vec method.</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>Union[int, tuple[int, int]]</code> <p>The size of the input image. If an integer is provided, the image is assumed to be square.</p> required <code>num_masking_patches</code> <code>int</code> <p>The number of patches to mask.</p> required <code>min_num_patches</code> <code>int</code> <p>The minimum number of patches to mask.</p> <code>4</code> <code>max_num_patches</code> <code>int</code> <p>The maximum number of patches to mask.</p> <code>None</code> <code>min_aspect_ratio</code> <code>float</code> <p>The minimum aspect ratio of the patch.</p> <code>0.3</code> <code>max_aspect_ratio</code> <code>float</code> <p>The maximum aspect ratio of the patch.</p> <code>None</code> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>@store(group=\"datasets/masking\", provider=\"mmlearn\")\nclass BlockwiseImagePatchMaskGenerator:\n    \"\"\"Blockwise image patch mask generator.\n\n    This is primarily intended for the data2vec method.\n\n    Parameters\n    ----------\n    input_size : Union[int, tuple[int, int]]\n        The size of the input image. If an integer is provided, the image is assumed\n        to be square.\n    num_masking_patches : int\n        The number of patches to mask.\n    min_num_patches : int, default=4\n        The minimum number of patches to mask.\n    max_num_patches : int, default=None\n        The maximum number of patches to mask.\n    min_aspect_ratio : float, default=0.3\n        The minimum aspect ratio of the patch.\n    max_aspect_ratio : float, default=None\n        The maximum aspect ratio of the patch.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_size: Union[int, tuple[int, int]],\n        num_masking_patches: int,\n        min_num_patches: int = 4,\n        max_num_patches: Any = None,\n        min_aspect_ratio: float = 0.3,\n        max_aspect_ratio: Any = None,\n    ):\n        if not isinstance(input_size, tuple):\n            input_size = (input_size,) * 2\n        self.height, self.width = input_size\n\n        self.num_masking_patches = num_masking_patches\n\n        self.min_num_patches = min_num_patches\n        self.max_num_patches = (\n            num_masking_patches if max_num_patches is None else max_num_patches\n        )\n\n        max_aspect_ratio = max_aspect_ratio or 1 / min_aspect_ratio\n        self.log_aspect_ratio = (math.log(min_aspect_ratio), math.log(max_aspect_ratio))\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Generate a printable representation.\n\n        Returns\n        -------\n        str\n            A printable representation of the object.\n\n        \"\"\"\n        return \"Generator(%d, %d -&gt; [%d ~ %d], max = %d, %.3f ~ %.3f)\" % (\n            self.height,\n            self.width,\n            self.min_num_patches,\n            self.max_num_patches,\n            self.num_masking_patches,\n            self.log_aspect_ratio[0],\n            self.log_aspect_ratio[1],\n        )\n\n    def get_shape(self) -&gt; tuple[int, int]:\n        \"\"\"Get the shape of the input.\n\n        Returns\n        -------\n        tuple[int, int]\n            The shape of the input as a tuple `(height, width)`.\n        \"\"\"\n        return self.height, self.width\n\n    def _mask(self, mask: torch.Tensor, max_mask_patches: int) -&gt; int:\n        \"\"\"Masking function.\n\n        This function mask adjacent patches by first selecting a target area and aspect\n        ratio. Since, there might be overlap between selected areas  or the selected\n        area might already be masked, it runs for a  maximum of 10 attempts or until the\n        specified number of patches (max_mask_patches) is achieved.\n\n\n        Parameters\n        ----------\n        mask: torch.Tensor\n            Current mask. The mask to be updated.\n        max_mask_patches: int\n            The maximum number of patches to be masked.\n\n        Returns\n        -------\n        delta: int\n            The number of patches that were successfully masked.\n\n        Notes\n        -----\n        - `target_area`: Randomly chosen target area for the patch.\n        - `aspect_ratio`: Randomly chosen aspect ratio for the patch.\n        - `h`: Height of the patch based on the target area and aspect ratio.\n        - `w`: Width of the patch based on the target area and aspect ratio.\n        - `top`: Randomly chosen top position for the patch.\n        - `left`: Randomly chosen left position for the patch.\n        - `num_masked`: Number of masked pixels within the proposed patch area.\n        - `delta`: Accumulated count of modified pixels.\n        \"\"\"\n        delta = 0\n        for _ in range(10):\n            target_area = random.uniform(self.min_num_patches, max_mask_patches)\n            aspect_ratio = math.exp(random.uniform(*self.log_aspect_ratio))\n            h = int(round(math.sqrt(target_area * aspect_ratio)))\n            w = int(round(math.sqrt(target_area / aspect_ratio)))\n            if w &lt; self.width and h &lt; self.height:\n                top = random.randint(0, self.height - h)\n                left = random.randint(0, self.width - w)\n\n                num_masked = mask[top : top + h, left : left + w].sum()\n                # Overlap\n                if 0 &lt; h * w - num_masked &lt;= max_mask_patches:\n                    for i in range(top, top + h):\n                        for j in range(left, left + w):\n                            if mask[i, j] == 0:\n                                mask[i, j] = 1\n                                delta += 1\n\n                if delta &gt; 0:\n                    break\n        return delta\n\n    def __call__(self) -&gt; torch.Tensor:\n        \"\"\"Generate a random mask.\n\n        Returns a random mask of shape (nb_patches, nb_patches) based on the\n        configuration where the number of patches to be masked is num_masking_patches.\n\n        Returns\n        -------\n        mask: torch.Tensor\n            A mask of shape (nb_patches, nb_patches)\n\n        \"\"\"\n        mask = torch.zeros(self.get_shape(), dtype=torch.int)\n        mask_count = 0\n        while mask_count &lt; self.num_masking_patches:\n            max_mask_patches = self.num_masking_patches - mask_count\n            max_mask_patches = min(max_mask_patches, self.max_num_patches)\n\n            delta = self._mask(mask, max_mask_patches)\n            if delta == 0:\n                break\n            mask_count += delta\n\n        return mask\n</code></pre>"},{"location":"api/#mmlearn.cli.run.BlockwiseImagePatchMaskGenerator.__repr__","title":"__repr__","text":"<pre><code>__repr__()\n</code></pre> <p>Generate a printable representation.</p> <p>Returns:</p> Type Description <code>str</code> <p>A printable representation of the object.</p> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Generate a printable representation.\n\n    Returns\n    -------\n    str\n        A printable representation of the object.\n\n    \"\"\"\n    return \"Generator(%d, %d -&gt; [%d ~ %d], max = %d, %.3f ~ %.3f)\" % (\n        self.height,\n        self.width,\n        self.min_num_patches,\n        self.max_num_patches,\n        self.num_masking_patches,\n        self.log_aspect_ratio[0],\n        self.log_aspect_ratio[1],\n    )\n</code></pre>"},{"location":"api/#mmlearn.cli.run.BlockwiseImagePatchMaskGenerator.get_shape","title":"get_shape","text":"<pre><code>get_shape()\n</code></pre> <p>Get the shape of the input.</p> <p>Returns:</p> Type Description <code>tuple[int, int]</code> <p>The shape of the input as a tuple <code>(height, width)</code>.</p> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>def get_shape(self) -&gt; tuple[int, int]:\n    \"\"\"Get the shape of the input.\n\n    Returns\n    -------\n    tuple[int, int]\n        The shape of the input as a tuple `(height, width)`.\n    \"\"\"\n    return self.height, self.width\n</code></pre>"},{"location":"api/#mmlearn.cli.run.BlockwiseImagePatchMaskGenerator.__call__","title":"__call__","text":"<pre><code>__call__()\n</code></pre> <p>Generate a random mask.</p> <p>Returns a random mask of shape (nb_patches, nb_patches) based on the configuration where the number of patches to be masked is num_masking_patches.</p> <p>Returns:</p> Name Type Description <code>mask</code> <code>Tensor</code> <p>A mask of shape (nb_patches, nb_patches)</p> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>def __call__(self) -&gt; torch.Tensor:\n    \"\"\"Generate a random mask.\n\n    Returns a random mask of shape (nb_patches, nb_patches) based on the\n    configuration where the number of patches to be masked is num_masking_patches.\n\n    Returns\n    -------\n    mask: torch.Tensor\n        A mask of shape (nb_patches, nb_patches)\n\n    \"\"\"\n    mask = torch.zeros(self.get_shape(), dtype=torch.int)\n    mask_count = 0\n    while mask_count &lt; self.num_masking_patches:\n        max_mask_patches = self.num_masking_patches - mask_count\n        max_mask_patches = min(max_mask_patches, self.max_num_patches)\n\n        delta = self._mask(mask, max_mask_patches)\n        if delta == 0:\n            break\n        mask_count += delta\n\n    return mask\n</code></pre>"},{"location":"api/#mmlearn.cli.run.RandomMaskGenerator","title":"RandomMaskGenerator","text":"<p>Random mask generator.</p> <p>Returns a random mask of shape <code>(nb_patches, nb_patches)</code> based on the configuration where the number of patches to be masked is num_masking_patches. This is intended to be used for tasks like masked language modeling.</p> <p>Parameters:</p> Name Type Description Default <code>probability</code> <code>float</code> <p>Probability of masking a token.</p> required Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>@store(group=\"datasets/masking\", provider=\"mmlearn\", probability=0.15)\nclass RandomMaskGenerator:\n    \"\"\"Random mask generator.\n\n    Returns a random mask of shape `(nb_patches, nb_patches)` based on the\n    configuration where the number of patches to be masked is num_masking_patches.\n    **This is intended to be used for tasks like masked language modeling.**\n\n    Parameters\n    ----------\n    probability : float\n        Probability of masking a token.\n    \"\"\"\n\n    def __init__(self, probability: float):\n        self.probability = probability\n\n    def __call__(\n        self,\n        inputs: torch.Tensor,\n        tokenizer: PreTrainedTokenizerBase,\n        special_tokens_mask: Optional[torch.Tensor] = None,\n    ) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Generate a random mask.\n\n        Returns a random mask of shape (nb_patches, nb_patches) based on the\n        configuration where the number of patches to be masked is num_masking_patches.\n\n        Returns\n        -------\n        inputs : torch.Tensor\n            The encoded inputs.\n        tokenizer : PreTrainedTokenizer\n            The tokenizer.\n        special_tokens_mask : Optional[torch.Tensor], default=None\n            Mask for special tokens.\n        \"\"\"\n        inputs = tokenizer.pad(inputs, return_tensors=\"pt\")[\"input_ids\"]\n        labels = inputs.clone()\n        # We sample a few tokens in each sequence for MLM training\n        # (with probability `self.probability`)\n        probability_matrix = torch.full(labels.shape, self.probability)\n        if special_tokens_mask is None:\n            special_tokens_mask = tokenizer.get_special_tokens_mask(\n                labels, already_has_special_tokens=True\n            )\n            special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n        else:\n            special_tokens_mask = special_tokens_mask.bool()\n\n        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n        masked_indices = torch.bernoulli(probability_matrix).bool()\n        labels[~masked_indices] = tokenizer.pad_token_id\n        # 80% of the time, replace masked input tokens with tokenizer.mask_token([MASK])\n        indices_replaced = (\n            torch.bernoulli(torch.full(labels.shape, 0.8)).bool() &amp; masked_indices\n        )\n        inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n\n        # 10% of the time, we replace masked input tokens with random word\n        indices_random = (\n            torch.bernoulli(torch.full(labels.shape, 0.5)).bool()\n            &amp; masked_indices\n            &amp; ~indices_replaced\n        )\n        random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n        inputs[indices_random] = random_words[indices_random]\n\n        # Rest of the time (10% of the time) we keep the masked input tokens unchanged\n        return inputs, labels, masked_indices\n</code></pre>"},{"location":"api/#mmlearn.cli.run.RandomMaskGenerator.__call__","title":"__call__","text":"<pre><code>__call__(inputs, tokenizer, special_tokens_mask=None)\n</code></pre> <p>Generate a random mask.</p> <p>Returns a random mask of shape (nb_patches, nb_patches) based on the configuration where the number of patches to be masked is num_masking_patches.</p> <p>Returns:</p> Name Type Description <code>inputs</code> <code>Tensor</code> <p>The encoded inputs.</p> <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>The tokenizer.</p> <code>special_tokens_mask</code> <code>Optional[torch.Tensor], default=None</code> <p>Mask for special tokens.</p> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>def __call__(\n    self,\n    inputs: torch.Tensor,\n    tokenizer: PreTrainedTokenizerBase,\n    special_tokens_mask: Optional[torch.Tensor] = None,\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Generate a random mask.\n\n    Returns a random mask of shape (nb_patches, nb_patches) based on the\n    configuration where the number of patches to be masked is num_masking_patches.\n\n    Returns\n    -------\n    inputs : torch.Tensor\n        The encoded inputs.\n    tokenizer : PreTrainedTokenizer\n        The tokenizer.\n    special_tokens_mask : Optional[torch.Tensor], default=None\n        Mask for special tokens.\n    \"\"\"\n    inputs = tokenizer.pad(inputs, return_tensors=\"pt\")[\"input_ids\"]\n    labels = inputs.clone()\n    # We sample a few tokens in each sequence for MLM training\n    # (with probability `self.probability`)\n    probability_matrix = torch.full(labels.shape, self.probability)\n    if special_tokens_mask is None:\n        special_tokens_mask = tokenizer.get_special_tokens_mask(\n            labels, already_has_special_tokens=True\n        )\n        special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n    else:\n        special_tokens_mask = special_tokens_mask.bool()\n\n    probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n    masked_indices = torch.bernoulli(probability_matrix).bool()\n    labels[~masked_indices] = tokenizer.pad_token_id\n    # 80% of the time, replace masked input tokens with tokenizer.mask_token([MASK])\n    indices_replaced = (\n        torch.bernoulli(torch.full(labels.shape, 0.8)).bool() &amp; masked_indices\n    )\n    inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n\n    # 10% of the time, we replace masked input tokens with random word\n    indices_random = (\n        torch.bernoulli(torch.full(labels.shape, 0.5)).bool()\n        &amp; masked_indices\n        &amp; ~indices_replaced\n    )\n    random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n    inputs[indices_random] = random_words[indices_random]\n\n    # Rest of the time (10% of the time) we keep the masked input tokens unchanged\n    return inputs, labels, masked_indices\n</code></pre>"},{"location":"api/#mmlearn.cli.run.HFTokenizer","title":"HFTokenizer","text":"<p>A wrapper for loading HuggingFace tokenizers.</p> <p>This class wraps any huggingface tokenizer that can be initialized with meth:<code>transformers.AutoTokenizer.from_pretrained</code>. It preprocesses the input text and returns a dictionary with the tokenized text and other relevant information like attention masks.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>Pretrained model name or path - same as in meth:<code>transformers.AutoTokenizer.from_pretrained</code>.</p> required <code>max_length</code> <code>Optional[int]</code> <p>Maximum length of the tokenized sequence. This is passed to the tokenizer :meth:<code>__call__</code> method.</p> <code>None</code> <code>padding</code> <code>bool or str</code> <p>Padding strategy. Same as in meth:<code>transformers.AutoTokenizer.from_pretrained</code>; passed to the tokenizer :meth:<code>__call__</code> method.</p> <code>False</code> <code>truncation</code> <code>Optional[Union[bool, str]]</code> <p>Truncation strategy. Same as in meth:<code>transformers.AutoTokenizer.from_pretrained</code>; passed to the tokenizer :meth:<code>__call__</code> method.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to meth:<code>transformers.AutoTokenizer.from_pretrained</code>.</p> <code>{}</code> Source code in <code>mmlearn/datasets/processors/tokenizers.py</code> <pre><code>@store(group=\"datasets/tokenizers\", provider=\"mmlearn\")\nclass HFTokenizer:\n    \"\"\"A wrapper for loading HuggingFace tokenizers.\n\n    This class wraps any huggingface tokenizer that can be initialized with\n    :py:meth:`transformers.AutoTokenizer.from_pretrained`. It preprocesses the\n    input text and returns a dictionary with the tokenized text and other\n    relevant information like attention masks.\n\n    Parameters\n    ----------\n    model_name_or_path : str\n        Pretrained model name or path - same as in :py:meth:`transformers.AutoTokenizer.from_pretrained`.\n    max_length : Optional[int], optional, default=None\n        Maximum length of the tokenized sequence. This is passed to the tokenizer\n        :meth:`__call__` method.\n    padding : bool or str, default=False\n        Padding strategy. Same as in :py:meth:`transformers.AutoTokenizer.from_pretrained`;\n        passed to the tokenizer :meth:`__call__` method.\n    truncation : Optional[Union[bool, str]], optional, default=None\n        Truncation strategy. Same as in :py:meth:`transformers.AutoTokenizer.from_pretrained`;\n        passed to the tokenizer :meth:`__call__` method.\n    **kwargs : Any\n        Additional arguments passed to :py:meth:`transformers.AutoTokenizer.from_pretrained`.\n    \"\"\"  # noqa: W505\n\n    def __init__(\n        self,\n        model_name_or_path: str,\n        max_length: Optional[int] = None,\n        padding: Union[bool, str] = False,\n        truncation: Optional[Union[bool, str]] = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, **kwargs)\n        self.max_length = max_length\n        self.padding = padding\n        self.truncation = truncation\n\n    def __call__(\n        self, sentence: Union[str, list[str]], **kwargs: Any\n    ) -&gt; dict[str, torch.Tensor]:\n        \"\"\"Tokenize a text or a list of texts using the HuggingFace tokenizer.\n\n        Parameters\n        ----------\n        sentence : Union[str, list[str]]\n            Sentence(s) to be tokenized.\n        **kwargs : Any\n            Additional arguments passed to the tokenizer :meth:`__call__` method.\n\n        Returns\n        -------\n        dict[str, torch.Tensor]\n            Tokenized sentence(s).\n\n        Notes\n        -----\n        The ``input_ids`` key is replaced with ``Modalities.TEXT`` for consistency.\n        \"\"\"\n        batch_encoding = self.tokenizer(\n            sentence,\n            max_length=self.max_length,\n            padding=self.padding,\n            truncation=self.truncation,\n            return_tensors=\"pt\",\n            **kwargs,\n        )\n\n        if isinstance(\n            sentence, str\n        ):  # remove batch dimension if input is a single sentence\n            for key, value in batch_encoding.items():\n                if isinstance(value, torch.Tensor):\n                    batch_encoding[key] = torch.squeeze(value, 0)\n\n        # use 'Modalities.TEXT' key for input_ids for consistency\n        batch_encoding[Modalities.TEXT.name] = batch_encoding[\"input_ids\"]\n        return dict(batch_encoding)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.HFTokenizer.__call__","title":"__call__","text":"<pre><code>__call__(sentence, **kwargs)\n</code></pre> <p>Tokenize a text or a list of texts using the HuggingFace tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>sentence</code> <code>Union[str, list[str]]</code> <p>Sentence(s) to be tokenized.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to the tokenizer :meth:<code>__call__</code> method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>Tokenized sentence(s).</p> Notes <p>The <code>input_ids</code> key is replaced with <code>Modalities.TEXT</code> for consistency.</p> Source code in <code>mmlearn/datasets/processors/tokenizers.py</code> <pre><code>def __call__(\n    self, sentence: Union[str, list[str]], **kwargs: Any\n) -&gt; dict[str, torch.Tensor]:\n    \"\"\"Tokenize a text or a list of texts using the HuggingFace tokenizer.\n\n    Parameters\n    ----------\n    sentence : Union[str, list[str]]\n        Sentence(s) to be tokenized.\n    **kwargs : Any\n        Additional arguments passed to the tokenizer :meth:`__call__` method.\n\n    Returns\n    -------\n    dict[str, torch.Tensor]\n        Tokenized sentence(s).\n\n    Notes\n    -----\n    The ``input_ids`` key is replaced with ``Modalities.TEXT`` for consistency.\n    \"\"\"\n    batch_encoding = self.tokenizer(\n        sentence,\n        max_length=self.max_length,\n        padding=self.padding,\n        truncation=self.truncation,\n        return_tensors=\"pt\",\n        **kwargs,\n    )\n\n    if isinstance(\n        sentence, str\n    ):  # remove batch dimension if input is a single sentence\n        for key, value in batch_encoding.items():\n            if isinstance(value, torch.Tensor):\n                batch_encoding[key] = torch.squeeze(value, 0)\n\n    # use 'Modalities.TEXT' key for input_ids for consistency\n    batch_encoding[Modalities.TEXT.name] = batch_encoding[\"input_ids\"]\n    return dict(batch_encoding)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.TrimText","title":"TrimText","text":"<p>Trim text strings as a preprocessing step before tokenization.</p> <p>Parameters:</p> Name Type Description Default <code>trim_size</code> <code>int</code> <p>The maximum length of the trimmed text.</p> required Source code in <code>mmlearn/datasets/processors/transforms.py</code> <pre><code>@store(group=\"datasets/transforms\", provider=\"mmlearn\")\nclass TrimText:\n    \"\"\"Trim text strings as a preprocessing step before tokenization.\n\n    Parameters\n    ----------\n    trim_size : int\n        The maximum length of the trimmed text.\n    \"\"\"\n\n    def __init__(self, trim_size: int) -&gt; None:\n        self.trim_size = trim_size\n\n    def __call__(self, sentence: Union[str, list[str]]) -&gt; Union[str, list[str]]:\n        \"\"\"Trim the given sentence(s).\n\n        Parameters\n        ----------\n        sentence : Union[str, list[str]]\n            Sentence(s) to be trimmed.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            Trimmed sentence(s).\n\n        Raises\n        ------\n        TypeError\n            If the input sentence is not a string or list of strings.\n        \"\"\"\n        if not isinstance(sentence, (list, str)):\n            raise TypeError(\n                \"Expected argument `sentence` to be a string or list of strings, \"\n                f\"but got {type(sentence)}\"\n            )\n\n        if isinstance(sentence, str):\n            return sentence[: self.trim_size]\n\n        for i, s in enumerate(sentence):\n            sentence[i] = s[: self.trim_size]\n\n        return sentence\n</code></pre>"},{"location":"api/#mmlearn.cli.run.TrimText.__call__","title":"__call__","text":"<pre><code>__call__(sentence)\n</code></pre> <p>Trim the given sentence(s).</p> <p>Parameters:</p> Name Type Description Default <code>sentence</code> <code>Union[str, list[str]]</code> <p>Sentence(s) to be trimmed.</p> required <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>Trimmed sentence(s).</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input sentence is not a string or list of strings.</p> Source code in <code>mmlearn/datasets/processors/transforms.py</code> <pre><code>def __call__(self, sentence: Union[str, list[str]]) -&gt; Union[str, list[str]]:\n    \"\"\"Trim the given sentence(s).\n\n    Parameters\n    ----------\n    sentence : Union[str, list[str]]\n        Sentence(s) to be trimmed.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        Trimmed sentence(s).\n\n    Raises\n    ------\n    TypeError\n        If the input sentence is not a string or list of strings.\n    \"\"\"\n    if not isinstance(sentence, (list, str)):\n        raise TypeError(\n            \"Expected argument `sentence` to be a string or list of strings, \"\n            f\"but got {type(sentence)}\"\n        )\n\n    if isinstance(sentence, str):\n        return sentence[: self.trim_size]\n\n    for i, s in enumerate(sentence):\n        sentence[i] = s[: self.trim_size]\n\n    return sentence\n</code></pre>"},{"location":"api/#mmlearn.cli.run.HFCLIPTextEncoder","title":"HFCLIPTextEncoder","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper around the <code>CLIPTextModel</code> from HuggingFace.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>The huggingface model name or a local path from which to load the model.</p> required <code>pretrained</code> <code>bool</code> <p>Whether to load the pretrained weights or not.</p> <code>True</code> <code>pooling_layer</code> <code>Optional[Module]</code> <p>Pooling layer to apply to the last hidden state of the model.</p> <code>None</code> <code>freeze_layers</code> <code>Union[int, float, list[int], bool]</code> <p>Whether to freeze layers of the model and which layers to freeze. If <code>True</code>, all model layers are frozen. If it is an integer, the first <code>N</code> layers of the model are frozen. If it is a float, the first <code>N</code> percent of the layers are frozen. If it is a list of integers, the layers at the indices in the list are frozen.</p> <code>False</code> <code>freeze_layer_norm</code> <code>bool</code> <p>Whether to freeze the layer normalization layers of the model.</p> <code>True</code> <code>peft_config</code> <code>Optional[PeftConfig]</code> <p>The configuration from the <code>peft &lt;https://huggingface.co/docs/peft/index&gt;</code>_ library to use to wrap the model for parameter-efficient finetuning.</p> <code>None</code> <code>model_config_kwargs</code> <code>Optional[dict[str, Any]]</code> <p>Additional keyword arguments to pass to the model configuration.</p> <code>None</code> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If both <code>peft_config</code> and <code>freeze_layers</code> are set. The <code>peft_config</code> will override the <code>freeze_layers</code> setting.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>@store(\n    group=\"modules/encoders\",\n    provider=\"mmlearn\",\n    model_name_or_path=\"openai/clip-vit-base-patch16\",\n    hydra_convert=\"object\",  # required for `peft_config` to be converted to a `PeftConfig` object\n)\nclass HFCLIPTextEncoder(nn.Module):\n    \"\"\"Wrapper around the ``CLIPTextModel`` from HuggingFace.\n\n    Parameters\n    ----------\n    model_name_or_path : str\n        The huggingface model name or a local path from which to load the model.\n    pretrained : bool, default=True\n        Whether to load the pretrained weights or not.\n    pooling_layer : Optional[torch.nn.Module], optional, default=None\n        Pooling layer to apply to the last hidden state of the model.\n    freeze_layers : Union[int, float, list[int], bool], default=False\n        Whether to freeze layers of the model and which layers to freeze. If ``True``,\n        all model layers are frozen. If it is an integer, the first ``N`` layers of\n        the model are frozen. If it is a float, the first ``N`` percent of the layers\n        are frozen. If it is a list of integers, the layers at the indices in the\n        list are frozen.\n    freeze_layer_norm : bool, default=True\n        Whether to freeze the layer normalization layers of the model.\n    peft_config : Optional[PeftConfig], optional, default=None\n        The configuration from the `peft &lt;https://huggingface.co/docs/peft/index&gt;`_\n        library to use to wrap the model for parameter-efficient finetuning.\n    model_config_kwargs : Optional[dict[str, Any]], optional, default=None\n        Additional keyword arguments to pass to the model configuration.\n\n    Warns\n    -----\n    UserWarning\n        If both ``peft_config`` and ``freeze_layers`` are set. The ``peft_config``\n        will override the ``freeze_layers`` setting.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name_or_path: str,\n        pretrained: bool = True,\n        pooling_layer: Optional[nn.Module] = None,\n        freeze_layers: Union[int, float, list[int], bool] = False,\n        freeze_layer_norm: bool = True,\n        peft_config: Optional[\"PeftConfig\"] = None,\n        model_config_kwargs: Optional[dict[str, Any]] = None,\n    ) -&gt; None:\n        super().__init__()\n        _warn_freeze_with_peft(peft_config, freeze_layers)\n\n        model = hf_utils.load_huggingface_model(\n            transformers.CLIPTextModel,\n            model_name_or_path=model_name_or_path,\n            load_pretrained_weights=pretrained,\n            model_config_kwargs=model_config_kwargs,\n        )\n        model = _freeze_text_model(model, freeze_layers, freeze_layer_norm)\n        if peft_config is not None:\n            model = hf_utils._wrap_peft_model(model, peft_config)\n\n        self.model = model\n        self.pooling_layer = pooling_layer\n\n    def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The ``input_ids`` will be expected under the\n            ``Modalities.TEXT`` key.\n\n        Returns\n        -------\n        BaseModelOutput\n            The output of the model, including the last hidden state, all hidden\n            states, and the attention weights, if ``output_attentions`` is set\n            to ``True``.\n        \"\"\"\n        outputs = self.model(\n            input_ids=inputs[Modalities.TEXT.name],\n            attention_mask=inputs.get(\"attention_mask\")\n            or inputs.get(Modalities.TEXT.attention_mask),\n            position_ids=inputs.get(\"position_ids\"),\n            output_attentions=inputs.get(\"output_attentions\"),\n            return_dict=True,\n        )\n        last_hidden_state = outputs.hidden_states[-1]  # NOTE: no layer norm applied\n        if self.pooling_layer:\n            last_hidden_state = self.pooling_layer(last_hidden_state)\n\n        return BaseModelOutput(\n            last_hidden_state=last_hidden_state,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n</code></pre>"},{"location":"api/#mmlearn.cli.run.HFCLIPTextEncoder.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The <code>input_ids</code> will be expected under the <code>Modalities.TEXT</code> key.</p> required <p>Returns:</p> Type Description <code>BaseModelOutput</code> <p>The output of the model, including the last hidden state, all hidden states, and the attention weights, if <code>output_attentions</code> is set to <code>True</code>.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The ``input_ids`` will be expected under the\n        ``Modalities.TEXT`` key.\n\n    Returns\n    -------\n    BaseModelOutput\n        The output of the model, including the last hidden state, all hidden\n        states, and the attention weights, if ``output_attentions`` is set\n        to ``True``.\n    \"\"\"\n    outputs = self.model(\n        input_ids=inputs[Modalities.TEXT.name],\n        attention_mask=inputs.get(\"attention_mask\")\n        or inputs.get(Modalities.TEXT.attention_mask),\n        position_ids=inputs.get(\"position_ids\"),\n        output_attentions=inputs.get(\"output_attentions\"),\n        return_dict=True,\n    )\n    last_hidden_state = outputs.hidden_states[-1]  # NOTE: no layer norm applied\n    if self.pooling_layer:\n        last_hidden_state = self.pooling_layer(last_hidden_state)\n\n    return BaseModelOutput(\n        last_hidden_state=last_hidden_state,\n        hidden_states=outputs.hidden_states,\n        attentions=outputs.attentions,\n    )\n</code></pre>"},{"location":"api/#mmlearn.cli.run.HFCLIPTextEncoderWithProjection","title":"HFCLIPTextEncoderWithProjection","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper around the <code>CLIPTextModelWithProjection</code> from HuggingFace.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>The huggingface model name or a local path from which to load the model.</p> required <code>pretrained</code> <code>bool</code> <p>Whether to load the pretrained weights or not.</p> <code>True</code> <code>use_all_token_embeddings</code> <code>bool</code> <p>Whether to use all token embeddings for the text. If <code>False</code> the first token embedding will be used.</p> <code>False</code> <code>freeze_layers</code> <code>Union[int, float, list[int], bool]</code> <p>Whether to freeze layers of the model and which layers to freeze. If <code>True</code>, all model layers are frozen. If it is an integer, the first <code>N</code> layers of the model are frozen. If it is a float, the first <code>N</code> percent of the layers are frozen. If it is a list of integers, the layers at the indices in the list are frozen.</p> <code>False</code> <code>freeze_layer_norm</code> <code>bool</code> <p>Whether to freeze the layer normalization layers of the model.</p> <code>True</code> <code>peft_config</code> <code>Optional[PeftConfig]</code> <p>The configuration from the <code>peft &lt;https://huggingface.co/docs/peft/index&gt;</code>_ library to use to wrap the model for parameter-efficient finetuning.</p> <code>None</code> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If both <code>peft_config</code> and <code>freeze_layers</code> are set. The <code>peft_config</code> will override the <code>freeze_layers</code> setting.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>@store(\n    group=\"modules/encoders\",\n    provider=\"mmlearn\",\n    model_name_or_path=\"openai/clip-vit-base-patch16\",\n    hydra_convert=\"object\",\n)\nclass HFCLIPTextEncoderWithProjection(nn.Module):\n    \"\"\"Wrapper around the ``CLIPTextModelWithProjection`` from HuggingFace.\n\n    Parameters\n    ----------\n    model_name_or_path : str\n        The huggingface model name or a local path from which to load the model.\n    pretrained : bool, default=True\n        Whether to load the pretrained weights or not.\n    use_all_token_embeddings : bool, default=False\n        Whether to use all token embeddings for the text. If ``False`` the first token\n        embedding will be used.\n    freeze_layers : Union[int, float, list[int], bool], default=False\n        Whether to freeze layers of the model and which layers to freeze. If ``True``,\n        all model layers are frozen. If it is an integer, the first ``N`` layers of\n        the model are frozen. If it is a float, the first ``N`` percent of the layers\n        are frozen. If it is a list of integers, the layers at the indices in the\n        list are frozen.\n    freeze_layer_norm : bool, default=True\n        Whether to freeze the layer normalization layers of the model.\n    peft_config : Optional[PeftConfig], optional, default=None\n        The configuration from the `peft &lt;https://huggingface.co/docs/peft/index&gt;`_\n        library to use to wrap the model for parameter-efficient finetuning.\n\n    Warns\n    -----\n    UserWarning\n        If both ``peft_config`` and ``freeze_layers`` are set. The ``peft_config``\n        will override the ``freeze_layers`` setting.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name_or_path: str,\n        pretrained: bool = True,\n        use_all_token_embeddings: bool = False,\n        freeze_layers: Union[int, float, list[int], bool] = False,\n        freeze_layer_norm: bool = True,\n        peft_config: Optional[\"PeftConfig\"] = None,\n        model_config_kwargs: Optional[dict[str, Any]] = None,\n    ) -&gt; None:\n        super().__init__()\n        _warn_freeze_with_peft(peft_config, freeze_layers)\n\n        self.use_all_token_embeddings = use_all_token_embeddings\n\n        model = hf_utils.load_huggingface_model(\n            transformers.CLIPTextModelWithProjection,\n            model_name_or_path,\n            load_pretrained_weights=pretrained,\n            model_config_kwargs=model_config_kwargs,\n            config_type=CLIPTextConfig,\n        )\n\n        model = _freeze_text_model(model, freeze_layers, freeze_layer_norm)\n        if peft_config is not None:\n            model = hf_utils._wrap_peft_model(model, peft_config)\n\n        self.model = model\n\n    def forward(self, inputs: dict[str, Any]) -&gt; tuple[torch.Tensor]:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The ``input_ids`` will be expected under the\n            ``Modalities.TEXT`` key.\n\n        Returns\n        -------\n        tuple[torch.Tensor]\n            The text embeddings. Will be a tuple with a single element.\n        \"\"\"\n        input_ids = inputs[Modalities.TEXT.name]\n        attention_mask: Optional[torch.Tensor] = inputs.get(\n            \"attention_mask\", inputs.get(Modalities.TEXT.attention_mask, None)\n        )\n        position_ids = inputs.get(\"position_ids\")\n\n        if self.use_all_token_embeddings:\n            text_outputs = self.model.text_model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                return_dict=True,\n            )\n            # TODO: add more options for pooling before projection\n            text_embeds = self.model.text_projection(text_outputs.last_hidden_state)\n        else:\n            text_embeds = self.model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                return_dict=True,\n            ).text_embeds\n\n        return (text_embeds,)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.HFCLIPTextEncoderWithProjection.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The <code>input_ids</code> will be expected under the <code>Modalities.TEXT</code> key.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor]</code> <p>The text embeddings. Will be a tuple with a single element.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; tuple[torch.Tensor]:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The ``input_ids`` will be expected under the\n        ``Modalities.TEXT`` key.\n\n    Returns\n    -------\n    tuple[torch.Tensor]\n        The text embeddings. Will be a tuple with a single element.\n    \"\"\"\n    input_ids = inputs[Modalities.TEXT.name]\n    attention_mask: Optional[torch.Tensor] = inputs.get(\n        \"attention_mask\", inputs.get(Modalities.TEXT.attention_mask, None)\n    )\n    position_ids = inputs.get(\"position_ids\")\n\n    if self.use_all_token_embeddings:\n        text_outputs = self.model.text_model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            return_dict=True,\n        )\n        # TODO: add more options for pooling before projection\n        text_embeds = self.model.text_projection(text_outputs.last_hidden_state)\n    else:\n        text_embeds = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            return_dict=True,\n        ).text_embeds\n\n    return (text_embeds,)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.HFCLIPVisionEncoder","title":"HFCLIPVisionEncoder","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper around the <code>CLIPVisionModel</code> from HuggingFace.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>The huggingface model name or a local path from which to load the model.</p> required <code>pretrained</code> <code>bool</code> <p>Whether to load the pretrained weights or not.</p> <code>True</code> <code>pooling_layer</code> <code>Optional[Module]</code> <p>Pooling layer to apply to the last hidden state of the model.</p> <code>None</code> <code>freeze_layers</code> <code>Union[int, float, list[int], bool]</code> <p>Whether to freeze layers of the model and which layers to freeze. If <code>True</code>, all model layers are frozen. If it is an integer, the first <code>N</code> layers of the model are frozen. If it is a float, the first <code>N</code> percent of the layers are frozen. If it is a list of integers, the layers at the indices in the list are frozen.</p> <code>False</code> <code>freeze_layer_norm</code> <code>bool</code> <p>Whether to freeze the layer normalization layers of the model.</p> <code>True</code> <code>patch_dropout_rate</code> <code>float</code> <p>The proportion of patch embeddings to drop out.</p> <code>0.0</code> <code>patch_dropout_shuffle</code> <code>bool</code> <p>Whether to shuffle the patches while applying patch dropout.</p> <code>False</code> <code>patch_dropout_bias</code> <code>Optional[float]</code> <p>The bias to apply to the patch dropout mask.</p> <code>None</code> <code>peft_config</code> <code>Optional[PeftConfig]</code> <p>The configuration from the <code>peft &lt;https://huggingface.co/docs/peft/index&gt;</code>_ library to use to wrap the model for parameter-efficient finetuning.</p> <code>None</code> <code>model_config_kwargs</code> <code>Optional[dict[str, Any]]</code> <p>Additional keyword arguments to pass to the model configuration.</p> <code>None</code> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If both <code>peft_config</code> and <code>freeze_layers</code> are set. The <code>peft_config</code> will override the <code>freeze_layers</code> setting.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>@store(\n    group=\"modules/encoders\",\n    provider=\"mmlearn\",\n    model_name_or_path=\"openai/clip-vit-base-patch16\",\n    hydra_convert=\"object\",\n)\nclass HFCLIPVisionEncoder(nn.Module):\n    \"\"\"Wrapper around the ``CLIPVisionModel`` from HuggingFace.\n\n    Parameters\n    ----------\n    model_name_or_path : str\n        The huggingface model name or a local path from which to load the model.\n    pretrained : bool, default=True\n        Whether to load the pretrained weights or not.\n    pooling_layer : Optional[torch.nn.Module], optional, default=None\n        Pooling layer to apply to the last hidden state of the model.\n    freeze_layers : Union[int, float, list[int], bool], default=False\n        Whether to freeze layers of the model and which layers to freeze. If ``True``,\n        all model layers are frozen. If it is an integer, the first ``N`` layers of\n        the model are frozen. If it is a float, the first ``N`` percent of the layers\n        are frozen. If it is a list of integers, the layers at the indices in the\n        list are frozen.\n    freeze_layer_norm : bool, default=True\n        Whether to freeze the layer normalization layers of the model.\n    patch_dropout_rate : float, default=0.0\n        The proportion of patch embeddings to drop out.\n    patch_dropout_shuffle : bool, default=False\n        Whether to shuffle the patches while applying patch dropout.\n    patch_dropout_bias : Optional[float], optional, default=None\n        The bias to apply to the patch dropout mask.\n    peft_config : Optional[PeftConfig], optional, default=None\n        The configuration from the `peft &lt;https://huggingface.co/docs/peft/index&gt;`_\n        library to use to wrap the model for parameter-efficient finetuning.\n    model_config_kwargs : Optional[dict[str, Any]], optional, default=None\n        Additional keyword arguments to pass to the model configuration.\n\n    Warns\n    -----\n    UserWarning\n        If both ``peft_config`` and ``freeze_layers`` are set. The ``peft_config``\n        will override the ``freeze_layers`` setting.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name_or_path: str,\n        pretrained: bool = True,\n        pooling_layer: Optional[nn.Module] = None,\n        freeze_layers: Union[int, float, list[int], bool] = False,\n        freeze_layer_norm: bool = True,\n        patch_dropout_rate: float = 0.0,\n        patch_dropout_shuffle: bool = False,\n        patch_dropout_bias: Optional[float] = None,\n        peft_config: Optional[\"PeftConfig\"] = None,\n        model_config_kwargs: Optional[dict[str, Any]] = None,\n    ) -&gt; None:\n        super().__init__()\n        _warn_freeze_with_peft(peft_config, freeze_layers)\n\n        model = hf_utils.load_huggingface_model(\n            transformers.CLIPVisionModel,\n            model_name_or_path=model_name_or_path,\n            load_pretrained_weights=pretrained,\n            model_config_kwargs=model_config_kwargs,\n        )\n        model = _freeze_vision_model(model, freeze_layers, freeze_layer_norm)\n        if peft_config is not None:\n            model = hf_utils._wrap_peft_model(model, peft_config)\n\n        self.model = model.vision_model\n        self.pooling_layer = pooling_layer\n        self.patch_dropout = None\n        if patch_dropout_rate &gt; 0:\n            self.patch_dropout = PatchDropout(\n                keep_rate=1 - patch_dropout_rate,\n                token_shuffling=patch_dropout_shuffle,\n                bias=patch_dropout_bias,\n            )\n\n    def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The image tensor will be expected under the\n            ``Modalities.RGB`` key.\n\n        Returns\n        -------\n        BaseModelOutput\n            The output of the model, including the last hidden state, all hidden states,\n            and the attention weights, if ``output_attentions`` is set to ``True``.\n\n        \"\"\"\n        # FIXME: handle other vision modalities\n        pixel_values = inputs[Modalities.RGB.name]\n        hidden_states = self.model.embeddings(pixel_values)\n        if self.patch_dropout is not None:\n            hidden_states = self.patch_dropout(hidden_states)\n        hidden_states = self.model.pre_layrnorm(hidden_states)\n\n        encoder_outputs = self.model.encoder(\n            inputs_embeds=hidden_states,\n            output_attentions=inputs.get(\n                \"output_attentions\", self.model.config.output_attentions\n            ),\n            output_hidden_states=True,\n            return_dict=True,\n        )\n\n        last_hidden_state = encoder_outputs[0]\n        if self.pooling_layer is not None:\n            last_hidden_state = self.pooling_layer(last_hidden_state)\n\n        return BaseModelOutput(\n            last_hidden_state=last_hidden_state,\n            hidden_states=encoder_outputs.hidden_states,\n            attentions=encoder_outputs.attentions,\n        )\n</code></pre>"},{"location":"api/#mmlearn.cli.run.HFCLIPVisionEncoder.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The image tensor will be expected under the <code>Modalities.RGB</code> key.</p> required <p>Returns:</p> Type Description <code>BaseModelOutput</code> <p>The output of the model, including the last hidden state, all hidden states, and the attention weights, if <code>output_attentions</code> is set to <code>True</code>.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The image tensor will be expected under the\n        ``Modalities.RGB`` key.\n\n    Returns\n    -------\n    BaseModelOutput\n        The output of the model, including the last hidden state, all hidden states,\n        and the attention weights, if ``output_attentions`` is set to ``True``.\n\n    \"\"\"\n    # FIXME: handle other vision modalities\n    pixel_values = inputs[Modalities.RGB.name]\n    hidden_states = self.model.embeddings(pixel_values)\n    if self.patch_dropout is not None:\n        hidden_states = self.patch_dropout(hidden_states)\n    hidden_states = self.model.pre_layrnorm(hidden_states)\n\n    encoder_outputs = self.model.encoder(\n        inputs_embeds=hidden_states,\n        output_attentions=inputs.get(\n            \"output_attentions\", self.model.config.output_attentions\n        ),\n        output_hidden_states=True,\n        return_dict=True,\n    )\n\n    last_hidden_state = encoder_outputs[0]\n    if self.pooling_layer is not None:\n        last_hidden_state = self.pooling_layer(last_hidden_state)\n\n    return BaseModelOutput(\n        last_hidden_state=last_hidden_state,\n        hidden_states=encoder_outputs.hidden_states,\n        attentions=encoder_outputs.attentions,\n    )\n</code></pre>"},{"location":"api/#mmlearn.cli.run.HFCLIPVisionEncoderWithProjection","title":"HFCLIPVisionEncoderWithProjection","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper around the <code>CLIPVisionModelWithProjection</code> class from HuggingFace.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>The huggingface model name or a local path from which to load the model.</p> required <code>pretrained</code> <code>bool</code> <p>Whether to load the pretrained weights or not.</p> <code>True</code> <code>use_all_token_embeddings</code> <code>bool</code> <p>Whether to use all token embeddings for the text. If <code>False</code> the first token embedding will be used.</p> <code>False</code> <code>freeze_layers</code> <code>Union[int, float, list[int], bool]</code> <p>Whether to freeze layers of the model and which layers to freeze. If <code>True</code>, all model layers are frozen. If it is an integer, the first <code>N` layers of the model are frozen. If it is a float, the first</code>N`` percent of the layers are frozen. If it is a list of integers, the layers at the indices in the list are frozen.</p> <code>False</code> <code>freeze_layer_norm</code> <code>bool</code> <p>Whether to freeze the layer normalization layers of the model.</p> <code>True</code> <code>patch_dropout_rate</code> <code>float</code> <p>The proportion of patch embeddings to drop out.</p> <code>0.0</code> <code>patch_dropout_shuffle</code> <code>bool</code> <p>Whether to shuffle the patches while applying patch dropout.</p> <code>False</code> <code>patch_dropout_bias</code> <code>float</code> <p>The bias to apply to the patch dropout mask.</p> <code>None</code> <code>peft_config</code> <code>Optional[PeftConfig]</code> <p>The configuration from the <code>peft &lt;https://huggingface.co/docs/peft/index&gt;</code>_ library to use to wrap the model for parameter-efficient finetuning.</p> <code>None</code> <code>model_config_kwargs</code> <code>dict[str, Any]</code> <p>Additional keyword arguments to pass to the model configuration.</p> <code>None</code> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If both <code>peft_config</code> and <code>freeze_layers</code> are set. The <code>peft_config</code> will override the <code>freeze_layers</code> setting.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>@store(\n    group=\"modules/encoders\",\n    provider=\"mmlearn\",\n    model_name_or_path=\"openai/clip-vit-base-patch16\",\n    hydra_convert=\"object\",\n)\nclass HFCLIPVisionEncoderWithProjection(nn.Module):\n    \"\"\"Wrapper around the ``CLIPVisionModelWithProjection`` class from HuggingFace.\n\n    Parameters\n    ----------\n    model_name_or_path : str\n        The huggingface model name or a local path from which to load the model.\n    pretrained : bool, default=True\n        Whether to load the pretrained weights or not.\n    use_all_token_embeddings : bool, default=False\n        Whether to use all token embeddings for the text. If ``False`` the first token\n        embedding will be used.\n    freeze_layers : Union[int, float, list[int], bool], default=False\n        Whether to freeze layers of the model and which layers to freeze. If ``True``,\n        all model layers are frozen. If it is an integer, the first ``N` layers of\n        the model are frozen. If it is a float, the first ``N`` percent of the layers\n        are frozen. If it is a list of integers, the layers at the indices in the\n        list are frozen.\n    freeze_layer_norm : bool, default=True\n        Whether to freeze the layer normalization layers of the model.\n    patch_dropout_rate : float, default=0.0\n        The proportion of patch embeddings to drop out.\n    patch_dropout_shuffle : bool, default=False\n        Whether to shuffle the patches while applying patch dropout.\n    patch_dropout_bias : float, optional, default=None\n        The bias to apply to the patch dropout mask.\n    peft_config : Optional[PeftConfig], optional, default=None\n        The configuration from the `peft &lt;https://huggingface.co/docs/peft/index&gt;`_\n        library to use to wrap the model for parameter-efficient finetuning.\n    model_config_kwargs : dict[str, Any], optional, default=None\n        Additional keyword arguments to pass to the model configuration.\n\n    Warns\n    -----\n    UserWarning\n        If both ``peft_config`` and ``freeze_layers`` are set. The ``peft_config``\n        will override the ``freeze_layers`` setting.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name_or_path: str,\n        pretrained: bool = True,\n        use_all_token_embeddings: bool = False,\n        patch_dropout_rate: float = 0.0,\n        patch_dropout_shuffle: bool = False,\n        patch_dropout_bias: Optional[float] = None,\n        freeze_layers: Union[int, float, list[int], bool] = False,\n        freeze_layer_norm: bool = True,\n        peft_config: Optional[\"PeftConfig\"] = None,\n        model_config_kwargs: Optional[dict[str, Any]] = None,\n    ) -&gt; None:\n        super().__init__()\n        _warn_freeze_with_peft(peft_config, freeze_layers)\n\n        self.use_all_token_embeddings = use_all_token_embeddings\n\n        model = hf_utils.load_huggingface_model(\n            transformers.CLIPVisionModelWithProjection,\n            model_name_or_path,\n            load_pretrained_weights=pretrained,\n            model_config_kwargs=model_config_kwargs,\n            config_type=CLIPVisionConfig,\n        )\n\n        model = _freeze_vision_model(model, freeze_layers, freeze_layer_norm)\n        if peft_config is not None:\n            model = hf_utils._wrap_peft_model(model, peft_config)\n\n        self.model = model\n        self.patch_dropout = None\n        if patch_dropout_rate &gt; 0:\n            self.patch_dropout = PatchDropout(\n                keep_rate=1 - patch_dropout_rate,\n                token_shuffling=patch_dropout_shuffle,\n                bias=patch_dropout_bias,\n            )\n\n    def forward(self, inputs: dict[str, Any]) -&gt; tuple[torch.Tensor]:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The image tensor will be expected under the\n            ``Modalities.RGB`` key.\n\n        Returns\n        -------\n        tuple[torch.Tensor]\n            The image embeddings. Will be a tuple with a single element.\n        \"\"\"\n        pixel_values = inputs[Modalities.RGB.name]\n        hidden_states = self.model.vision_model.embeddings(pixel_values)\n        if self.patch_dropout is not None:\n            hidden_states = self.patch_dropout(hidden_states)\n        hidden_states = self.model.vision_model.pre_layrnorm(hidden_states)\n\n        encoder_outputs = self.model.vision_model.encoder(\n            inputs_embeds=hidden_states, return_dict=True\n        )\n\n        last_hidden_state = encoder_outputs.last_hidden_state\n        if self.use_all_token_embeddings:\n            pooled_output = last_hidden_state\n        else:\n            pooled_output = last_hidden_state[:, 0, :]\n        pooled_output = self.model.vision_model.post_layernorm(pooled_output)\n\n        return (self.model.visual_projection(pooled_output),)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.HFCLIPVisionEncoderWithProjection.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The image tensor will be expected under the <code>Modalities.RGB</code> key.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor]</code> <p>The image embeddings. Will be a tuple with a single element.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; tuple[torch.Tensor]:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The image tensor will be expected under the\n        ``Modalities.RGB`` key.\n\n    Returns\n    -------\n    tuple[torch.Tensor]\n        The image embeddings. Will be a tuple with a single element.\n    \"\"\"\n    pixel_values = inputs[Modalities.RGB.name]\n    hidden_states = self.model.vision_model.embeddings(pixel_values)\n    if self.patch_dropout is not None:\n        hidden_states = self.patch_dropout(hidden_states)\n    hidden_states = self.model.vision_model.pre_layrnorm(hidden_states)\n\n    encoder_outputs = self.model.vision_model.encoder(\n        inputs_embeds=hidden_states, return_dict=True\n    )\n\n    last_hidden_state = encoder_outputs.last_hidden_state\n    if self.use_all_token_embeddings:\n        pooled_output = last_hidden_state\n    else:\n        pooled_output = last_hidden_state[:, 0, :]\n    pooled_output = self.model.vision_model.post_layernorm(pooled_output)\n\n    return (self.model.visual_projection(pooled_output),)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.HFTextEncoder","title":"HFTextEncoder","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper around huggingface models in the <code>AutoModelForTextEncoding</code> class.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>The huggingface model name or a local path from which to load the model.</p> required <code>pretrained</code> <code>bool</code> <p>Whether to load the pretrained weights or not.</p> <code>True</code> <code>pooling_layer</code> <code>Optional[Module]</code> <p>Pooling layer to apply to the last hidden state of the model.</p> <code>None</code> <code>freeze_layers</code> <code>Union[int, float, list[int], bool]</code> <p>Whether to freeze layers of the model and which layers to freeze. If <code>True</code>, all model layers are frozen. If it is an integer, the first <code>N</code> layers of the model are frozen. If it is a float, the first <code>N</code> percent of the layers are frozen. If it is a list of integers, the layers at the indices in the list are frozen.</p> <code>False</code> <code>freeze_layer_norm</code> <code>bool</code> <p>Whether to freeze the layer normalization layers of the model.</p> <code>True</code> <code>peft_config</code> <code>Optional[PeftConfig]</code> <p>The configuration from the <code>peft &lt;https://huggingface.co/docs/peft/index&gt;</code>_ library to use to wrap the model for parameter-efficient finetuning.</p> <code>None</code> <code>model_config_kwargs</code> <code>Optional[dict[str, Any]]</code> <p>Additional keyword arguments to pass to the model configuration.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model is a decoder model or if freezing individual layers is not supported for the model type.</p> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If both <code>peft_config</code> and <code>freeze_layers</code> are set. The <code>peft_config</code> will override the <code>freeze_layers</code> setting.</p> Source code in <code>mmlearn/modules/encoders/text.py</code> <pre><code>@store(group=\"modules/encoders\", provider=\"mmlearn\", hydra_convert=\"object\")\nclass HFTextEncoder(nn.Module):\n    \"\"\"Wrapper around huggingface models in the ``AutoModelForTextEncoding`` class.\n\n    Parameters\n    ----------\n    model_name_or_path : str\n        The huggingface model name or a local path from which to load the model.\n    pretrained : bool, default=True\n        Whether to load the pretrained weights or not.\n    pooling_layer : Optional[torch.nn.Module], optional, default=None\n        Pooling layer to apply to the last hidden state of the model.\n    freeze_layers : Union[int, float, list[int], bool], default=False\n        Whether to freeze layers of the model and which layers to freeze. If ``True``,\n        all model layers are frozen. If it is an integer, the first ``N`` layers of\n        the model are frozen. If it is a float, the first ``N`` percent of the layers\n        are frozen. If it is a list of integers, the layers at the indices in the\n        list are frozen.\n    freeze_layer_norm : bool, default=True\n        Whether to freeze the layer normalization layers of the model.\n    peft_config : Optional[PeftConfig], optional, default=None\n        The configuration from the `peft &lt;https://huggingface.co/docs/peft/index&gt;`_\n        library to use to wrap the model for parameter-efficient finetuning.\n    model_config_kwargs : Optional[dict[str, Any]], optional, default=None\n        Additional keyword arguments to pass to the model configuration.\n\n    Raises\n    ------\n    ValueError\n        If the model is a decoder model or if freezing individual layers is not\n        supported for the model type.\n\n    Warns\n    -----\n    UserWarning\n        If both ``peft_config`` and ``freeze_layers`` are set. The ``peft_config``\n        will override the ``freeze_layers`` setting.\n\n\n    \"\"\"\n\n    def __init__(  # noqa: PLR0912\n        self,\n        model_name_or_path: str,\n        pretrained: bool = True,\n        pooling_layer: Optional[nn.Module] = None,\n        freeze_layers: Union[int, float, list[int], bool] = False,\n        freeze_layer_norm: bool = True,\n        peft_config: Optional[\"PeftConfig\"] = None,\n        model_config_kwargs: Optional[dict[str, Any]] = None,\n    ):\n        super().__init__()\n        if model_config_kwargs is None:\n            model_config_kwargs = {}\n        model_config_kwargs[\"output_hidden_states\"] = True\n        model_config_kwargs[\"add_pooling_layer\"] = False\n        model = hf_utils.load_huggingface_model(\n            AutoModelForTextEncoding,\n            model_name_or_path,\n            load_pretrained_weights=pretrained,\n            model_config_kwargs=model_config_kwargs,\n        )\n        if hasattr(model.config, \"is_decoder\") and model.config.is_decoder:\n            raise ValueError(\"Model is a decoder. Only encoder models are supported.\")\n\n        if not pretrained and freeze_layers:\n            rank_zero_warn(\n                \"Freezing layers when loading a model with random weights may lead to \"\n                \"unexpected behavior. Consider setting `freeze_layers=False` if \"\n                \"`pretrained=False`.\",\n            )\n\n        if isinstance(freeze_layers, bool) and freeze_layers:\n            for name, param in model.named_parameters():\n                param.requires_grad = (\n                    (not freeze_layer_norm) if \"LayerNorm\" in name else False\n                )\n\n        if isinstance(\n            freeze_layers, (float, int, list)\n        ) and model.config.model_type in [\"flaubert\", \"xlm\"]:\n            # flaubert and xlm models have a different architecture that does not\n            # support freezing individual layers in the same way as other models\n            raise ValueError(\n                f\"Freezing individual layers is not supported for {model.config.model_type} \"\n                \"models. Please use `freeze_layers=False` or `freeze_layers=True`.\"\n            )\n\n        # get list of layers\n        embeddings = model.embeddings\n        encoder = getattr(model, \"encoder\", None) or getattr(\n            model, \"transformer\", model\n        )\n        encoder_layers = (\n            getattr(encoder, \"layer\", None)\n            or getattr(encoder, \"layers\", None)\n            or getattr(encoder, \"block\", None)\n        )\n        if encoder_layers is None and hasattr(encoder, \"albert_layer_groups\"):\n            encoder_layers = [\n                layer\n                for group in encoder.albert_layer_groups\n                for layer in group.albert_layers\n            ]\n        modules = [embeddings]\n        if encoder_layers is not None and isinstance(encoder_layers, list):\n            modules.extend(encoder_layers)\n\n        if isinstance(freeze_layers, float):\n            freeze_layers = int(freeze_layers * len(modules))\n        if isinstance(freeze_layers, int):\n            freeze_layers = list(range(freeze_layers))\n\n        if isinstance(freeze_layers, list):\n            for idx, module in enumerate(modules):\n                if idx in freeze_layers:\n                    for name, param in module.named_parameters():\n                        param.requires_grad = (\n                            (not freeze_layer_norm) if \"LayerNorm\" in name else False\n                        )\n\n        if peft_config is not None:\n            model = hf_utils._wrap_peft_model(model, peft_config)\n\n        self.model = model\n        self.pooling_layer = pooling_layer\n\n    def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The ``input_ids`` will be expected under the\n            ``Modalities.TEXT`` key.\n\n        Returns\n        -------\n        BaseModelOutput\n            The output of the model, including the last hidden state, all hidden states,\n            and the attention weights, if ``output_attentions`` is set to ``True``.\n        \"\"\"\n        outputs = self.model(\n            input_ids=inputs[Modalities.TEXT.name],\n            attention_mask=inputs.get(\n                \"attention_mask\", inputs.get(Modalities.TEXT.attention_mask, None)\n            ),\n            position_ids=inputs.get(\"position_ids\"),\n            output_attentions=inputs.get(\"output_attentions\"),\n            return_dict=True,\n        )\n        last_hidden_state = outputs.hidden_states[-1]  # NOTE: no layer norm applied\n        if self.pooling_layer:\n            last_hidden_state = self.pooling_layer(last_hidden_state)\n\n        return BaseModelOutput(\n            last_hidden_state=last_hidden_state,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n</code></pre>"},{"location":"api/#mmlearn.cli.run.HFTextEncoder.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The <code>input_ids</code> will be expected under the <code>Modalities.TEXT</code> key.</p> required <p>Returns:</p> Type Description <code>BaseModelOutput</code> <p>The output of the model, including the last hidden state, all hidden states, and the attention weights, if <code>output_attentions</code> is set to <code>True</code>.</p> Source code in <code>mmlearn/modules/encoders/text.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The ``input_ids`` will be expected under the\n        ``Modalities.TEXT`` key.\n\n    Returns\n    -------\n    BaseModelOutput\n        The output of the model, including the last hidden state, all hidden states,\n        and the attention weights, if ``output_attentions`` is set to ``True``.\n    \"\"\"\n    outputs = self.model(\n        input_ids=inputs[Modalities.TEXT.name],\n        attention_mask=inputs.get(\n            \"attention_mask\", inputs.get(Modalities.TEXT.attention_mask, None)\n        ),\n        position_ids=inputs.get(\"position_ids\"),\n        output_attentions=inputs.get(\"output_attentions\"),\n        return_dict=True,\n    )\n    last_hidden_state = outputs.hidden_states[-1]  # NOTE: no layer norm applied\n    if self.pooling_layer:\n        last_hidden_state = self.pooling_layer(last_hidden_state)\n\n    return BaseModelOutput(\n        last_hidden_state=last_hidden_state,\n        hidden_states=outputs.hidden_states,\n        attentions=outputs.attentions,\n    )\n</code></pre>"},{"location":"api/#mmlearn.cli.run.TimmViT","title":"TimmViT","text":"<p>               Bases: <code>Module</code></p> <p>Vision Transformer model from timm.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the model to use.</p> required <code>modality</code> <code>str</code> <p>The modality of the input data. This allows this model to be used with different image modalities e.g. RGB, Depth, etc.</p> <code>\"RGB\"</code> <code>projection_dim</code> <code>int</code> <p>The dimension of the projection head.</p> <code>768</code> <code>pretrained</code> <code>bool</code> <p>Whether to use the pretrained weights.</p> <code>True</code> <code>freeze_layers</code> <code>Union[int, float, list[int], bool]</code> <p>Whether to freeze the layers.</p> <code>False</code> <code>freeze_layer_norm</code> <code>bool</code> <p>Whether to freeze the layer norm.</p> <code>True</code> <code>peft_config</code> <code>Optional[PeftConfig]</code> <p>The configuration from the <code>peft &lt;https://huggingface.co/docs/peft/index&gt;</code>_ library to use to wrap the model for parameter-efficient finetuning.</p> <code>None</code> <code>model_kwargs</code> <code>Optional[dict[str, Any]]</code> <p>Additional keyword arguments for the model.</p> <code>None</code> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>@store(\n    group=\"modules/encoders\",\n    provider=\"mmlearn\",\n    model_name=\"vit_base_patch16_224\",\n    hydra_convert=\"object\",\n)\nclass TimmViT(nn.Module):\n    \"\"\"Vision Transformer model from timm.\n\n    Parameters\n    ----------\n    model_name : str\n        The name of the model to use.\n    modality : str, default=\"RGB\"\n        The modality of the input data. This allows this model to be used with different\n        image modalities e.g. RGB, Depth, etc.\n    projection_dim : int, default=768\n        The dimension of the projection head.\n    pretrained : bool, default=True\n        Whether to use the pretrained weights.\n    freeze_layers : Union[int, float, list[int], bool], default=False\n        Whether to freeze the layers.\n    freeze_layer_norm : bool, default=True\n        Whether to freeze the layer norm.\n    peft_config : Optional[PeftConfig], optional, default=None\n        The configuration from the `peft &lt;https://huggingface.co/docs/peft/index&gt;`_\n        library to use to wrap the model for parameter-efficient finetuning.\n    model_kwargs : Optional[dict[str, Any]], default=None\n        Additional keyword arguments for the model.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str,\n        modality: str = \"RGB\",\n        projection_dim: int = 768,\n        pretrained: bool = True,\n        freeze_layers: Union[int, float, list[int], bool] = False,\n        freeze_layer_norm: bool = True,\n        peft_config: Optional[\"PeftConfig\"] = None,\n        model_kwargs: Optional[dict[str, Any]] = None,\n    ) -&gt; None:\n        super().__init__()\n        self.modality = Modalities.get_modality(modality)\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        self.model: TimmVisionTransformer = timm.create_model(\n            model_name,\n            pretrained=pretrained,\n            num_classes=projection_dim,\n            **model_kwargs,\n        )\n        assert isinstance(self.model, TimmVisionTransformer), (\n            f\"Model {model_name} is not a Vision Transformer. \"\n            \"Please provide a model name that corresponds to a Vision Transformer.\"\n        )\n\n        self._freeze_layers(freeze_layers, freeze_layer_norm)\n\n        if peft_config is not None:\n            self.model = hf_utils._wrap_peft_model(self.model, peft_config)\n\n    def _freeze_layers(\n        self, freeze_layers: Union[int, float, list[int], bool], freeze_layer_norm: bool\n    ) -&gt; None:\n        \"\"\"Freeze the layers of the model.\n\n        Parameters\n        ----------\n        freeze_layers : Union[int, float, list[int], bool]\n            Whether to freeze the layers.\n        freeze_layer_norm : bool\n            Whether to freeze the layer norm.\n        \"\"\"\n        if isinstance(freeze_layers, bool) and freeze_layers:\n            for name, param in self.model.named_parameters():\n                param.requires_grad = (\n                    (not freeze_layer_norm) if \"norm\" in name else False\n                )\n\n        modules = [self.model.patch_embed, *self.model.blocks, self.model.norm]\n        if isinstance(freeze_layers, float):\n            freeze_layers = int(freeze_layers * len(modules))\n        if isinstance(freeze_layers, int):\n            freeze_layers = list(range(freeze_layers))\n\n        if isinstance(freeze_layers, list):\n            for idx, module in enumerate(modules):\n                if idx in freeze_layers:\n                    for name, param in module.named_parameters():\n                        param.requires_grad = (\n                            (not freeze_layer_norm) if \"norm\" in name else False\n                        )\n\n    def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The ``image`` will be expected under the\n            ``Modalities.RGB`` key.\n\n        Returns\n        -------\n        BaseModelOutput\n            The output of the model.\n        \"\"\"\n        x = inputs[self.modality.name]\n        last_hidden_state, hidden_states = self.model.forward_intermediates(\n            x, output_fmt=\"NLC\"\n        )\n        last_hidden_state = self.model.forward_head(last_hidden_state)\n\n        return BaseModelOutput(\n            last_hidden_state=last_hidden_state, hidden_states=hidden_states\n        )\n\n    def get_intermediate_layers(\n        self, inputs: dict[str, Any], n: int = 1\n    ) -&gt; list[torch.Tensor]:\n        \"\"\"Get the output of the intermediate layers.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The ``image`` will be expected under the ``Modalities.RGB``\n            key.\n        n : int, default=1\n            The number of intermediate layers to return.\n\n        Returns\n        -------\n        list[torch.Tensor]\n            The outputs of the last n intermediate layers.\n        \"\"\"\n        return self.model.get_intermediate_layers(inputs[Modalities.RGB], n)  # type: ignore\n\n    def get_patch_info(self) -&gt; tuple[int, int]:\n        \"\"\"Get patch size and number of patches.\n\n        Returns\n        -------\n        tuple[int, int]\n            Patch size and number of patches.\n        \"\"\"\n        patch_size = self.model.patch_embed.patch_size[0]\n        num_patches = self.model.patch_embed.num_patches\n        return patch_size, num_patches\n</code></pre>"},{"location":"api/#mmlearn.cli.run.TimmViT.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The <code>image</code> will be expected under the <code>Modalities.RGB</code> key.</p> required <p>Returns:</p> Type Description <code>BaseModelOutput</code> <p>The output of the model.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The ``image`` will be expected under the\n        ``Modalities.RGB`` key.\n\n    Returns\n    -------\n    BaseModelOutput\n        The output of the model.\n    \"\"\"\n    x = inputs[self.modality.name]\n    last_hidden_state, hidden_states = self.model.forward_intermediates(\n        x, output_fmt=\"NLC\"\n    )\n    last_hidden_state = self.model.forward_head(last_hidden_state)\n\n    return BaseModelOutput(\n        last_hidden_state=last_hidden_state, hidden_states=hidden_states\n    )\n</code></pre>"},{"location":"api/#mmlearn.cli.run.TimmViT.get_intermediate_layers","title":"get_intermediate_layers","text":"<pre><code>get_intermediate_layers(inputs, n=1)\n</code></pre> <p>Get the output of the intermediate layers.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The <code>image</code> will be expected under the <code>Modalities.RGB</code> key.</p> required <code>n</code> <code>int</code> <p>The number of intermediate layers to return.</p> <code>1</code> <p>Returns:</p> Type Description <code>list[Tensor]</code> <p>The outputs of the last n intermediate layers.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>def get_intermediate_layers(\n    self, inputs: dict[str, Any], n: int = 1\n) -&gt; list[torch.Tensor]:\n    \"\"\"Get the output of the intermediate layers.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The ``image`` will be expected under the ``Modalities.RGB``\n        key.\n    n : int, default=1\n        The number of intermediate layers to return.\n\n    Returns\n    -------\n    list[torch.Tensor]\n        The outputs of the last n intermediate layers.\n    \"\"\"\n    return self.model.get_intermediate_layers(inputs[Modalities.RGB], n)  # type: ignore\n</code></pre>"},{"location":"api/#mmlearn.cli.run.TimmViT.get_patch_info","title":"get_patch_info","text":"<pre><code>get_patch_info()\n</code></pre> <p>Get patch size and number of patches.</p> <p>Returns:</p> Type Description <code>tuple[int, int]</code> <p>Patch size and number of patches.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>def get_patch_info(self) -&gt; tuple[int, int]:\n    \"\"\"Get patch size and number of patches.\n\n    Returns\n    -------\n    tuple[int, int]\n        Patch size and number of patches.\n    \"\"\"\n    patch_size = self.model.patch_embed.patch_size[0]\n    num_patches = self.model.patch_embed.num_patches\n    return patch_size, num_patches\n</code></pre>"},{"location":"api/#mmlearn.cli.run.LearnableLogitScaling","title":"LearnableLogitScaling","text":"<p>               Bases: <code>Module</code></p> <p>Logit scaling layer.</p> <p>Parameters:</p> Name Type Description Default <code>init_logit_scale</code> <code>float</code> <p>Initial value of the logit scale.</p> <code>1/0.07</code> <code>learnable</code> <code>bool</code> <p>If True, the logit scale is learnable. Otherwise, it is fixed.</p> <code>True</code> <code>max_logit_scale</code> <code>float</code> <p>Maximum value of the logit scale.</p> <code>100</code> Source code in <code>mmlearn/modules/layers/logit_scaling.py</code> <pre><code>@store(group=\"modules/layers\", provider=\"mmlearn\")\nclass LearnableLogitScaling(torch.nn.Module):\n    \"\"\"Logit scaling layer.\n\n    Parameters\n    ----------\n    init_logit_scale : float, optional, default=1/0.07\n        Initial value of the logit scale.\n    learnable : bool, optional, default=True\n        If True, the logit scale is learnable. Otherwise, it is fixed.\n    max_logit_scale : float, optional, default=100\n        Maximum value of the logit scale.\n    \"\"\"\n\n    def __init__(\n        self,\n        init_logit_scale: float = 1 / 0.07,\n        max_logit_scale: float = 100,\n        learnable: bool = True,\n    ) -&gt; None:\n        super().__init__()\n        self.max_logit_scale = max_logit_scale\n        self.init_logit_scale = init_logit_scale\n        self.learnable = learnable\n        log_logit_scale = torch.ones([]) * np.log(self.init_logit_scale)\n        if learnable:\n            self.log_logit_scale = torch.nn.Parameter(log_logit_scale)\n        else:\n            self.register_buffer(\"log_logit_scale\", log_logit_scale)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Apply the logit scaling to the input tensor.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor of shape ``(batch_sz, seq_len, dim)``.\n        \"\"\"\n        return torch.clip(self.log_logit_scale.exp(), max=self.max_logit_scale) * x\n\n    def extra_repr(self) -&gt; str:\n        \"\"\"Return the string representation of the layer.\"\"\"\n        return (\n            f\"logit_scale_init={self.init_logit_scale},learnable={self.learnable},\"\n            f\" max_logit_scale={self.max_logit_scale}\"\n        )\n</code></pre>"},{"location":"api/#mmlearn.cli.run.LearnableLogitScaling.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Apply the logit scaling to the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape <code>(batch_sz, seq_len, dim)</code>.</p> required Source code in <code>mmlearn/modules/layers/logit_scaling.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Apply the logit scaling to the input tensor.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape ``(batch_sz, seq_len, dim)``.\n    \"\"\"\n    return torch.clip(self.log_logit_scale.exp(), max=self.max_logit_scale) * x\n</code></pre>"},{"location":"api/#mmlearn.cli.run.LearnableLogitScaling.extra_repr","title":"extra_repr","text":"<pre><code>extra_repr()\n</code></pre> <p>Return the string representation of the layer.</p> Source code in <code>mmlearn/modules/layers/logit_scaling.py</code> <pre><code>def extra_repr(self) -&gt; str:\n    \"\"\"Return the string representation of the layer.\"\"\"\n    return (\n        f\"logit_scale_init={self.init_logit_scale},learnable={self.learnable},\"\n        f\" max_logit_scale={self.max_logit_scale}\"\n    )\n</code></pre>"},{"location":"api/#mmlearn.cli.run.MLP","title":"MLP","text":"<p>               Bases: <code>Sequential</code></p> <p>Multi-layer perceptron (MLP).</p> <p>This module will create a block of <code>Linear -&gt; Normalization -&gt; Activation -&gt; Dropout</code> layers.</p> <p>Parameters:</p> Name Type Description Default <code>in_dim</code> <code>int</code> <p>The input dimension.</p> required <code>out_dim</code> <code>Optional[int]</code> <p>The output dimension. If not specified, it is set to :attr:<code>in_dim</code>.</p> <code>None</code> <code>hidden_dims</code> <code>Optional[list]</code> <p>The dimensions of the hidden layers. The length of the list determines the number of hidden layers. This parameter is mutually exclusive with :attr:<code>hidden_dims_multiplier</code>.</p> <code>None</code> <code>hidden_dims_multiplier</code> <code>Optional[list]</code> <p>The multipliers to apply to the input dimension to get the dimensions of the hidden layers. The length of the list determines the number of hidden layers. The multipliers will be used to get the dimensions of the hidden layers. This parameter is mutually exclusive with <code>hidden_dims</code>.</p> <code>None</code> <code>apply_multiplier_to_in_dim</code> <code>bool</code> <p>Whether to apply the :attr:<code>hidden_dims_multiplier</code> to :attr:<code>in_dim</code> to get the dimensions of the hidden layers. If <code>False</code>, the multipliers will be applied to the dimensions of the previous hidden layer, starting from :attr:<code>in_dim</code>. This parameter is only relevant when :attr:<code>hidden_dims_multiplier</code> is specified.</p> <code>False</code> <code>norm_layer</code> <code>Optional[Callable[..., Module]]</code> <p>The normalization layer to use. If not specified, no normalization is used. Partial functions can be used to specify the normalization layer with specific parameters.</p> <code>None</code> <code>activation_layer</code> <code>Optional[Callable[..., Module]]</code> <p>The activation layer to use. If not specified, ReLU is used. Partial functions can be used to specify the activation layer with specific parameters.</p> <code>torch.nn.ReLU</code> <code>bias</code> <code>Union[bool, list[bool]]</code> <p>Whether to use bias in the linear layers.</p> <code>True</code> <code>dropout</code> <code>Union[float, list[float]]</code> <p>The dropout probability to use.</p> <code>0.0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If both :attr:<code>hidden_dims</code> and :attr:<code>hidden_dims_multiplier</code> are specified or if the lengths of :attr:<code>bias</code> and :attr:<code>hidden_dims</code> do not match or if the lengths of :attr:<code>dropout</code> and :attr:<code>hidden_dims</code> do not match.</p> Source code in <code>mmlearn/modules/layers/mlp.py</code> <pre><code>@store(group=\"modules/layers\", provider=\"mmlearn\")\nclass MLP(torch.nn.Sequential):\n    \"\"\"Multi-layer perceptron (MLP).\n\n    This module will create a block of ``Linear -&gt; Normalization -&gt; Activation -&gt; Dropout``\n    layers.\n\n    Parameters\n    ----------\n    in_dim : int\n        The input dimension.\n    out_dim : Optional[int], optional, default=None\n        The output dimension. If not specified, it is set to :attr:`in_dim`.\n    hidden_dims : Optional[list], optional, default=None\n        The dimensions of the hidden layers. The length of the list determines the\n        number of hidden layers. This parameter is mutually exclusive with\n        :attr:`hidden_dims_multiplier`.\n    hidden_dims_multiplier : Optional[list], optional, default=None\n        The multipliers to apply to the input dimension to get the dimensions of\n        the hidden layers. The length of the list determines the number of hidden\n        layers. The multipliers will be used to get the dimensions of the hidden\n        layers. This parameter is mutually exclusive with `hidden_dims`.\n    apply_multiplier_to_in_dim : bool, optional, default=False\n        Whether to apply the :attr:`hidden_dims_multiplier` to :attr:`in_dim` to get the\n        dimensions of the hidden layers. If ``False``, the multipliers will be applied\n        to the dimensions of the previous hidden layer, starting from :attr:`in_dim`.\n        This parameter is only relevant when :attr:`hidden_dims_multiplier` is\n        specified.\n    norm_layer : Optional[Callable[..., torch.nn.Module]], optional, default=None\n        The normalization layer to use. If not specified, no normalization is used.\n        Partial functions can be used to specify the normalization layer with specific\n        parameters.\n    activation_layer : Optional[Callable[..., torch.nn.Module]], optional, default=torch.nn.ReLU\n        The activation layer to use. If not specified, ReLU is used. Partial functions\n        can be used to specify the activation layer with specific parameters.\n    bias : Union[bool, list[bool]], optional, default=True\n        Whether to use bias in the linear layers.\n    dropout : Union[float, list[float]], optional, default=0.0\n        The dropout probability to use.\n\n    Raises\n    ------\n    ValueError\n        If both :attr:`hidden_dims` and :attr:`hidden_dims_multiplier` are specified\n        or if the lengths of :attr:`bias` and :attr:`hidden_dims` do not match or if\n        the lengths of :attr:`dropout` and :attr:`hidden_dims` do not match.\n\n    \"\"\"  # noqa: W505\n\n    def __init__(  # noqa: PLR0912\n        self,\n        in_dim: int,\n        out_dim: Optional[int] = None,\n        hidden_dims: Optional[list[int]] = None,\n        hidden_dims_multiplier: Optional[list[float]] = None,\n        apply_multiplier_to_in_dim: bool = False,\n        norm_layer: Optional[Callable[..., torch.nn.Module]] = None,\n        activation_layer: Optional[Callable[..., torch.nn.Module]] = torch.nn.ReLU,\n        bias: Union[bool, list[bool]] = True,\n        dropout: Union[float, list[float]] = 0.0,\n    ) -&gt; None:\n        if hidden_dims is None and hidden_dims_multiplier is None:\n            hidden_dims = []\n        if hidden_dims is not None and hidden_dims_multiplier is not None:\n            raise ValueError(\n                \"Only one of `hidden_dims` or `hidden_dims_multiplier` must be specified.\"\n            )\n\n        if hidden_dims is None and hidden_dims_multiplier is not None:\n            if apply_multiplier_to_in_dim:\n                hidden_dims = [\n                    int(in_dim * multiplier) for multiplier in hidden_dims_multiplier\n                ]\n            else:\n                hidden_dims = [int(in_dim * hidden_dims_multiplier[0])]\n                for multiplier in hidden_dims_multiplier[1:]:\n                    hidden_dims.append(int(hidden_dims[-1] * multiplier))\n\n        if isinstance(bias, bool):\n            bias_list: list[bool] = [bias] * (len(hidden_dims) + 1)  # type: ignore[arg-type]\n        else:\n            bias_list = bias\n        if len(bias_list) != len(hidden_dims) + 1:  # type: ignore[arg-type]\n            raise ValueError(\n                \"Expected `bias` to be a boolean or a list of booleans with length \"\n                \"equal to the number of linear layers in the MLP.\"\n            )\n\n        if isinstance(dropout, float):\n            dropout_list: list[float] = [dropout] * (len(hidden_dims) + 1)  # type: ignore[arg-type]\n        else:\n            dropout_list = dropout\n        if len(dropout_list) != len(hidden_dims) + 1:  # type: ignore[arg-type]\n            raise ValueError(\n                \"Expected `dropout` to be a float or a list of floats with length \"\n                \"equal to the number of linear layers in the MLP.\"\n            )\n\n        # construct list of dimensions for the layers\n        dims = [in_dim] + hidden_dims  # type: ignore[operator]\n        layers = []\n        for layer_idx, (in_features, hidden_features) in enumerate(\n            zip(dims[:-1], dims[1:], strict=False)\n        ):\n            layers.append(\n                torch.nn.Linear(in_features, hidden_features, bias=bias_list[layer_idx])\n            )\n            if norm_layer is not None:\n                layers.append(norm_layer(hidden_features))\n            if activation_layer is not None:\n                layers.append(activation_layer())\n            layers.append(torch.nn.Dropout(dropout_list[layer_idx]))\n\n        out_dim = out_dim or in_dim\n\n        layers.append(torch.nn.Linear(dims[-1], out_dim, bias=bias_list[-1]))\n        layers.append(torch.nn.Dropout(dropout_list[-1]))\n\n        super().__init__(*layers)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.L2Norm","title":"L2Norm","text":"<p>               Bases: <code>Module</code></p> <p>L2 normalization.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>The dimension along which to normalize.</p> required Source code in <code>mmlearn/modules/layers/normalization.py</code> <pre><code>@store(group=\"modules/layers\", provider=\"mmlearn\")\nclass L2Norm(torch.nn.Module):\n    \"\"\"L2 normalization.\n\n    Parameters\n    ----------\n    dim : int\n        The dimension along which to normalize.\n    \"\"\"\n\n    def __init__(self, dim: int) -&gt; None:\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Apply L2 normalization to the input tensor.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor of shape ``(batch_sz, seq_len, dim)``.\n\n        Returns\n        -------\n        torch.Tensor\n            Normalized tensor of shape ``(batch_sz, seq_len, dim)``.\n        \"\"\"\n        return torch.nn.functional.normalize(x, dim=self.dim, p=2)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.L2Norm.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Apply L2 normalization to the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape <code>(batch_sz, seq_len, dim)</code>.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized tensor of shape <code>(batch_sz, seq_len, dim)</code>.</p> Source code in <code>mmlearn/modules/layers/normalization.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Apply L2 normalization to the input tensor.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape ``(batch_sz, seq_len, dim)``.\n\n    Returns\n    -------\n    torch.Tensor\n        Normalized tensor of shape ``(batch_sz, seq_len, dim)``.\n    \"\"\"\n    return torch.nn.functional.normalize(x, dim=self.dim, p=2)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.PatchDropout","title":"PatchDropout","text":"<p>               Bases: <code>Module</code></p> <p>Patch dropout layer.</p> <p>Drops patch tokens (after embedding and adding CLS token) from the input tensor. Usually used in vision transformers to reduce the number of tokens. [1]_</p> <p>Parameters:</p> Name Type Description Default <code>keep_rate</code> <code>float</code> <p>The proportion of tokens to keep.</p> <code>0.5</code> <code>bias</code> <code>Optional[float]</code> <p>The bias to add to the random noise before sorting.</p> <code>None</code> <code>token_shuffling</code> <code>bool</code> <p>If True, the tokens are shuffled.</p> <code>False</code> References <p>.. [1] Liu, Y., Matsoukas, C., Strand, F., Azizpour, H., &amp; Smith, K. (2023).    Patchdropout: Economizing vision transformers using patch dropout. In Proceedings    of the IEEE/CVF Winter Conference on Applications of Computer Vision    (pp. 3953-3962).</p> Source code in <code>mmlearn/modules/layers/patch_dropout.py</code> <pre><code>class PatchDropout(torch.nn.Module):\n    \"\"\"Patch dropout layer.\n\n    Drops patch tokens (after embedding and adding CLS token) from the input tensor.\n    Usually used in vision transformers to reduce the number of tokens. [1]_\n\n    Parameters\n    ----------\n    keep_rate : float, optional, default=0.5\n        The proportion of tokens to keep.\n    bias : Optional[float], optional, default=None\n        The bias to add to the random noise before sorting.\n    token_shuffling : bool, optional, default=False\n        If True, the tokens are shuffled.\n\n    References\n    ----------\n    .. [1] Liu, Y., Matsoukas, C., Strand, F., Azizpour, H., &amp; Smith, K. (2023).\n       Patchdropout: Economizing vision transformers using patch dropout. In Proceedings\n       of the IEEE/CVF Winter Conference on Applications of Computer Vision\n       (pp. 3953-3962).\n    \"\"\"\n\n    def __init__(\n        self,\n        keep_rate: float = 0.5,\n        bias: Optional[float] = None,\n        token_shuffling: bool = False,\n    ):\n        super().__init__()\n        assert 0 &lt; keep_rate &lt;= 1, \"The keep_rate must be in (0,1]\"\n\n        self.bias = bias\n        self.keep_rate = keep_rate\n        self.token_shuffling = token_shuffling\n\n    def forward(self, x: torch.Tensor, force_drop: bool = False) -&gt; torch.Tensor:\n        \"\"\"Drop tokens from the input tensor.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor of shape ``(batch_sz, seq_len, dim)``.\n        force_drop : bool, optional, default=False\n            If True, the tokens are always dropped, even when the model is in\n            evaluation mode.\n\n        Returns\n        -------\n        torch.Tensor\n            Tensor of shape ``(batch_sz, keep_len, dim)`` containing the kept tokens.\n        \"\"\"\n        if (not self.training and not force_drop) or self.keep_rate == 1:\n            return x\n\n        batch_sz, _, dim = x.shape\n\n        cls_mask = torch.zeros(  # assumes that CLS is always the 1st element\n            batch_sz, 1, dtype=torch.int64, device=x.device\n        )\n        patch_mask = self.uniform_mask(x)\n        patch_mask = torch.hstack([cls_mask, patch_mask])\n\n        return torch.gather(x, dim=1, index=patch_mask.unsqueeze(-1).repeat(1, 1, dim))\n\n    def uniform_mask(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Generate token ids to keep from uniform random distribution.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor of shape ``(batch_sz, seq_len, dim)``.\n\n        Returns\n        -------\n        torch.Tensor\n            Tensor of shape ``(batch_sz, keep_len)`` containing the token ids to keep.\n\n        \"\"\"\n        batch_sz, seq_len, _ = x.shape\n        seq_len = seq_len - 1  # patch length (without CLS)\n\n        keep_len = int(seq_len * self.keep_rate)\n        noise = torch.rand(batch_sz, seq_len, device=x.device)\n        if self.bias is not None:\n            noise += self.bias\n        ids = torch.argsort(noise, dim=1)\n        keep_ids = ids[:, :keep_len]\n        if not self.token_shuffling:\n            keep_ids = keep_ids.sort(1)[0]\n        return keep_ids\n</code></pre>"},{"location":"api/#mmlearn.cli.run.PatchDropout.forward","title":"forward","text":"<pre><code>forward(x, force_drop=False)\n</code></pre> <p>Drop tokens from the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape <code>(batch_sz, seq_len, dim)</code>.</p> required <code>force_drop</code> <code>bool</code> <p>If True, the tokens are always dropped, even when the model is in evaluation mode.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of shape <code>(batch_sz, keep_len, dim)</code> containing the kept tokens.</p> Source code in <code>mmlearn/modules/layers/patch_dropout.py</code> <pre><code>def forward(self, x: torch.Tensor, force_drop: bool = False) -&gt; torch.Tensor:\n    \"\"\"Drop tokens from the input tensor.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape ``(batch_sz, seq_len, dim)``.\n    force_drop : bool, optional, default=False\n        If True, the tokens are always dropped, even when the model is in\n        evaluation mode.\n\n    Returns\n    -------\n    torch.Tensor\n        Tensor of shape ``(batch_sz, keep_len, dim)`` containing the kept tokens.\n    \"\"\"\n    if (not self.training and not force_drop) or self.keep_rate == 1:\n        return x\n\n    batch_sz, _, dim = x.shape\n\n    cls_mask = torch.zeros(  # assumes that CLS is always the 1st element\n        batch_sz, 1, dtype=torch.int64, device=x.device\n    )\n    patch_mask = self.uniform_mask(x)\n    patch_mask = torch.hstack([cls_mask, patch_mask])\n\n    return torch.gather(x, dim=1, index=patch_mask.unsqueeze(-1).repeat(1, 1, dim))\n</code></pre>"},{"location":"api/#mmlearn.cli.run.PatchDropout.uniform_mask","title":"uniform_mask","text":"<pre><code>uniform_mask(x)\n</code></pre> <p>Generate token ids to keep from uniform random distribution.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape <code>(batch_sz, seq_len, dim)</code>.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of shape <code>(batch_sz, keep_len)</code> containing the token ids to keep.</p> Source code in <code>mmlearn/modules/layers/patch_dropout.py</code> <pre><code>def uniform_mask(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Generate token ids to keep from uniform random distribution.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape ``(batch_sz, seq_len, dim)``.\n\n    Returns\n    -------\n    torch.Tensor\n        Tensor of shape ``(batch_sz, keep_len)`` containing the token ids to keep.\n\n    \"\"\"\n    batch_sz, seq_len, _ = x.shape\n    seq_len = seq_len - 1  # patch length (without CLS)\n\n    keep_len = int(seq_len * self.keep_rate)\n    noise = torch.rand(batch_sz, seq_len, device=x.device)\n    if self.bias is not None:\n        noise += self.bias\n    ids = torch.argsort(noise, dim=1)\n    keep_ids = ids[:, :keep_len]\n    if not self.token_shuffling:\n        keep_ids = keep_ids.sort(1)[0]\n    return keep_ids\n</code></pre>"},{"location":"api/#mmlearn.cli.run.ContrastiveLoss","title":"ContrastiveLoss","text":"<p>               Bases: <code>Module</code></p> <p>Contrastive Loss.</p> <p>Parameters:</p> Name Type Description Default <code>l2_normalize</code> <code>bool</code> <p>Whether to L2 normalize the features.</p> <code>False</code> <code>local_loss</code> <code>bool</code> <p>Whether to calculate the loss locally i.e. <code>local_features@global_features</code>.</p> <code>False</code> <code>gather_with_grad</code> <code>bool</code> <p>Whether to gather tensors with gradients.</p> <code>False</code> <code>modality_alignment</code> <code>bool</code> <p>Whether to include modality alignment loss. This loss considers all features from the same modality as positive pairs and all features from different modalities as negative pairs.</p> <code>False</code> <code>cache_labels</code> <code>bool</code> <p>Whether to cache the labels.</p> <code>False</code> Source code in <code>mmlearn/modules/losses/contrastive.py</code> <pre><code>@store(group=\"modules/losses\", provider=\"mmlearn\")\nclass ContrastiveLoss(nn.Module):\n    \"\"\"Contrastive Loss.\n\n    Parameters\n    ----------\n    l2_normalize : bool, optional, default=False\n        Whether to L2 normalize the features.\n    local_loss : bool, optional, default=False\n        Whether to calculate the loss locally i.e. ``local_features@global_features``.\n    gather_with_grad : bool, optional, default=False\n        Whether to gather tensors with gradients.\n    modality_alignment : bool, optional, default=False\n        Whether to include modality alignment loss. This loss considers all features\n        from the same modality as positive pairs and all features from different\n        modalities as negative pairs.\n    cache_labels : bool, optional, default=False\n        Whether to cache the labels.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        l2_normalize: bool = False,\n        local_loss: bool = False,\n        gather_with_grad: bool = False,\n        modality_alignment: bool = False,\n        cache_labels: bool = False,\n    ):\n        super().__init__()\n        self.local_loss = local_loss\n        self.gather_with_grad = gather_with_grad\n        self.cache_labels = cache_labels\n        self.l2_normalize = l2_normalize\n        self.modality_alignment = modality_alignment\n\n        # cache state\n        self._prev_num_logits = 0\n        self._labels: dict[torch.device, torch.Tensor] = {}\n\n    def forward(\n        self,\n        embeddings: dict[str, torch.Tensor],\n        example_ids: dict[str, torch.Tensor],\n        logit_scale: torch.Tensor,\n        modality_loss_pairs: list[LossPairSpec],\n    ) -&gt; torch.Tensor:\n        \"\"\"Calculate the contrastive loss.\n\n        Parameters\n        ----------\n        embeddings : dict[str, torch.Tensor]\n            Dictionary of embeddings, where the key is the modality name and the value\n            is the corresponding embedding tensor.\n        example_ids : dict[str, torch.Tensor]\n            Dictionary of example IDs, where the key is the modality name and the value\n            is a tensor tuple of the dataset index and the example index.\n        logit_scale : torch.Tensor\n            Scale factor for the logits.\n        modality_loss_pairs : list[LossPairSpec]\n            Specification of the modality pairs for which the loss should be calculated.\n\n        Returns\n        -------\n        torch.Tensor\n            The contrastive loss.\n        \"\"\"\n        world_size = dist.get_world_size() if dist.is_initialized() else 1\n        rank = dist.get_rank() if world_size &gt; 1 else 0\n\n        if self.l2_normalize:\n            embeddings = {k: F.normalize(v, p=2, dim=-1) for k, v in embeddings.items()}\n\n        if world_size &gt; 1:  # gather embeddings and example_ids across all processes\n            # NOTE: gathering dictionaries of tensors across all processes\n            # (keys + values, as opposed to just values) is especially important\n            # for the modality_alignment loss, which requires all embeddings\n            all_embeddings = _gather_dicts(\n                embeddings,\n                local_loss=self.local_loss,\n                gather_with_grad=self.gather_with_grad,\n                rank=rank,\n            )\n            all_example_ids = _gather_dicts(\n                example_ids,\n                local_loss=self.local_loss,\n                gather_with_grad=self.gather_with_grad,\n                rank=rank,\n            )\n        else:\n            all_embeddings = embeddings\n            all_example_ids = example_ids\n\n        losses = []\n        for loss_pairs in modality_loss_pairs:\n            logits_per_feature_a, logits_per_feature_b, skip_flag = self._get_logits(\n                loss_pairs.modalities,\n                per_device_embeddings=embeddings,\n                all_embeddings=all_embeddings,\n                per_device_example_ids=example_ids,\n                all_example_ids=all_example_ids,\n                logit_scale=logit_scale,\n                world_size=world_size,\n            )\n            if logits_per_feature_a is None or logits_per_feature_b is None:\n                continue\n\n            labels = self._get_ground_truth(\n                logits_per_feature_a.shape,\n                device=logits_per_feature_a.device,\n                rank=rank,\n                world_size=world_size,\n                skipped_process=skip_flag,\n            )\n\n            if labels.numel() != 0:\n                losses.append(\n                    (\n                        (\n                            F.cross_entropy(logits_per_feature_a, labels)\n                            + F.cross_entropy(logits_per_feature_b, labels)\n                        )\n                        / 2\n                    )\n                    * loss_pairs.weight\n                )\n\n        if self.modality_alignment:\n            losses.append(\n                self._compute_modality_alignment_loss(all_embeddings, logit_scale)\n            )\n\n        if not losses:  # no loss to compute (e.g. no paired data in batch)\n            losses.append(\n                torch.tensor(\n                    0.0,\n                    device=logit_scale.device,\n                    dtype=next(iter(embeddings.values())).dtype,\n                )\n            )\n\n        return torch.stack(losses).sum()\n\n    def _get_ground_truth(\n        self,\n        logits_shape: tuple[int, int],\n        device: torch.device,\n        rank: int,\n        world_size: int,\n        skipped_process: bool,\n    ) -&gt; torch.Tensor:\n        \"\"\"Return the ground-truth labels.\n\n        Parameters\n        ----------\n        logits_shape : tuple[int, int]\n            Shape of the logits tensor.\n        device : torch.device\n            Device on which the labels should be created.\n        rank : int\n            Rank of the current process.\n        world_size : int\n            Number of processes.\n        skipped_process : bool\n            Whether the current process skipped the computation due to lack of data.\n\n        Returns\n        -------\n        torch.Tensor\n            Ground-truth labels.\n        \"\"\"\n        num_logits = logits_shape[-1]\n\n        # calculate ground-truth and cache if enabled\n        if self._prev_num_logits != num_logits or device not in self._labels:\n            labels = torch.arange(num_logits, device=device, dtype=torch.long)\n\n            if world_size &gt; 1 and self.local_loss:\n                local_size = torch.tensor(\n                    0 if skipped_process else logits_shape[0], device=device\n                )\n                # NOTE: all processes must participate in the all_gather operation\n                # even if they have no data to contribute.\n                sizes = torch.stack(\n                    _simple_gather_all_tensors(\n                        local_size, group=dist.group.WORLD, world_size=world_size\n                    )\n                )\n                sizes = torch.cat(\n                    [torch.tensor([0], device=sizes.device), torch.cumsum(sizes, dim=0)]\n                )\n                labels = labels[\n                    sizes[rank] : sizes[rank + 1] if rank + 1 &lt; world_size else None\n                ]\n\n            if self.cache_labels:\n                self._labels[device] = labels\n                self._prev_num_logits = num_logits\n        else:\n            labels = self._labels[device]\n        return labels\n\n    def _get_logits(  # noqa: PLR0912\n        self,\n        modalities: tuple[str, str],\n        per_device_embeddings: dict[str, torch.Tensor],\n        all_embeddings: dict[str, torch.Tensor],\n        per_device_example_ids: dict[str, torch.Tensor],\n        all_example_ids: dict[str, torch.Tensor],\n        logit_scale: torch.Tensor,\n        world_size: int,\n    ) -&gt; tuple[Optional[torch.Tensor], Optional[torch.Tensor], bool]:\n        \"\"\"Calculate the logits for the given modalities.\n\n        Parameters\n        ----------\n        modalities : tuple[str, str]\n            Tuple of modality names.\n        per_device_embeddings : dict[str, torch.Tensor]\n            Dictionary of embeddings, where the key is the modality name and the value\n            is the corresponding embedding tensor.\n        all_embeddings : dict[str, torch.Tensor]\n            Dictionary of embeddings, where the key is the modality name and the value\n            is the corresponding embedding tensor. In distributed mode, this contains\n            embeddings from all processes.\n        per_device_example_ids : dict[str, torch.Tensor]\n            Dictionary of example IDs, where the key is the modality name and the value\n            is a tensor tuple of the dataset index and the example index.\n        all_example_ids : dict[str, torch.Tensor]\n            Dictionary of example IDs, where the key is the modality name and the value\n            is a tensor tuple of the dataset index and the example index. In distributed\n            mode, this contains example IDs from all processes.\n        logit_scale : torch.Tensor\n            Scale factor for the logits.\n        world_size : int\n            Number of processes.\n\n        Returns\n        -------\n        tuple[Optional[torch.Tensor], Optional[torch.Tensor], bool]\n            Tuple of logits for the given modalities. If embeddings for the given\n            modalities are not available, returns `None` for the logits. The last\n            element is a flag indicating whether the process skipped the computation\n            due to lack of data.\n        \"\"\"\n        modality_a = Modalities.get_modality(modalities[0])\n        modality_b = Modalities.get_modality(modalities[1])\n        skip_flag = False\n\n        if self.local_loss or world_size == 1:\n            if not (\n                modality_a.embedding in per_device_embeddings\n                and modality_b.embedding in per_device_embeddings\n            ):\n                if world_size &gt; 1:  # NOTE: not all processes exit here, hence skip_flag\n                    skip_flag = True\n                else:\n                    return None, None, skip_flag\n\n            if not skip_flag:\n                indices_a, indices_b = find_matching_indices(\n                    per_device_example_ids[modality_a.name],\n                    per_device_example_ids[modality_b.name],\n                )\n                if indices_a.numel() == 0 or indices_b.numel() == 0:\n                    if world_size &gt; 1:  # not all processes exit here\n                        skip_flag = True\n                    else:\n                        return None, None, skip_flag\n\n            if not skip_flag:\n                features_a = per_device_embeddings[modality_a.embedding][indices_a]\n                features_b = per_device_embeddings[modality_b.embedding][indices_b]\n            else:\n                # all processes must participate in the all_gather operation\n                # that follows, even if they have no data to contribute. So,\n                # we create empty tensors here.\n                features_a = torch.empty(\n                    0, device=list(per_device_embeddings.values())[0].device\n                )\n                features_b = torch.empty(\n                    0, device=list(per_device_embeddings.values())[0].device\n                )\n\n        if world_size &gt; 1:\n            if not (\n                modality_a.embedding in all_embeddings\n                and modality_b.embedding in all_embeddings\n            ):  # all processes exit here\n                return None, None, skip_flag\n\n            indices_a, indices_b = find_matching_indices(\n                all_example_ids[modality_a.name],\n                all_example_ids[modality_b.name],\n            )\n            if indices_a.numel() == 0 or indices_b.numel() == 0:\n                # all processes exit here\n                return None, None, skip_flag\n\n            all_features_a = all_embeddings[modality_a.embedding][indices_a]\n            all_features_b = all_embeddings[modality_b.embedding][indices_b]\n\n            if self.local_loss:\n                if features_a.numel() == 0:\n                    features_a = all_features_a\n                if features_b.numel() == 0:\n                    features_b = all_features_b\n\n                logits_per_feature_a = logit_scale * _safe_matmul(\n                    features_a, all_features_b\n                )\n                logits_per_feature_b = logit_scale * _safe_matmul(\n                    features_b, all_features_a\n                )\n            else:\n                logits_per_feature_a = logit_scale * _safe_matmul(\n                    all_features_a, all_features_b\n                )\n                logits_per_feature_b = logits_per_feature_a.T\n        else:\n            logits_per_feature_a = logit_scale * _safe_matmul(features_a, features_b)\n            logits_per_feature_b = logit_scale * _safe_matmul(features_b, features_a)\n\n        return logits_per_feature_a, logits_per_feature_b, skip_flag\n\n    def _compute_modality_alignment_loss(\n        self, all_embeddings: dict[str, torch.Tensor], logit_scale: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"Compute the modality alignment loss.\n\n        This loss considers all features from the same modality as positive pairs\n        and all features from different modalities as negative pairs.\n\n        Parameters\n        ----------\n        all_embeddings : dict[str, torch.Tensor]\n            Dictionary of embeddings, where the key is the modality name and the value\n            is the corresponding embedding tensor.\n        logit_scale : torch.Tensor\n            Scale factor for the logits.\n\n        Returns\n        -------\n        torch.Tensor\n            Modality alignment loss.\n\n        Notes\n        -----\n        This loss does not support `local_loss=True`.\n        \"\"\"\n        available_modalities = list(all_embeddings.keys())\n        # TODO: support local_loss for modality_alignment?\n        # if world_size == 1, all_embeddings == embeddings\n        all_features = torch.cat(list(all_embeddings.values()), dim=0)\n\n        positive_indices = torch.tensor(\n            [\n                (i, j)\n                if idx == 0\n                else (\n                    i + all_embeddings[available_modalities[idx - 1]].size(0),\n                    j + all_embeddings[available_modalities[idx - 1]].size(0),\n                )\n                for idx, k in enumerate(all_embeddings)\n                for i, j in itertools.combinations(range(all_embeddings[k].size(0)), 2)\n            ],\n            device=all_features.device,\n        )\n        logits = logit_scale * _safe_matmul(all_features, all_features)\n\n        target = torch.eye(all_features.size(0), device=all_features.device)\n        target[positive_indices[:, 0], positive_indices[:, 1]] = 1\n\n        modality_loss = torch.nn.functional.binary_cross_entropy_with_logits(\n            logits, target, reduction=\"none\"\n        )\n\n        target_pos = target.bool()\n        target_neg = ~target_pos\n\n        # loss_pos and loss_neg below contain non-zero values only for those\n        # elements that are positive pairs and negative pairs respectively.\n        loss_pos = torch.zeros(\n            logits.size(0), logits.size(0), device=target.device\n        ).masked_scatter(target_pos, modality_loss[target_pos])\n        loss_neg = torch.zeros(\n            logits.size(0), logits.size(0), device=target.device\n        ).masked_scatter(target_neg, modality_loss[target_neg])\n\n        loss_pos = loss_pos.sum(dim=1)\n        loss_neg = loss_neg.sum(dim=1)\n        num_pos = target.sum(dim=1)\n        num_neg = logits.size(0) - num_pos\n\n        return ((loss_pos / num_pos) + (loss_neg / num_neg)).mean()\n</code></pre>"},{"location":"api/#mmlearn.cli.run.ContrastiveLoss.forward","title":"forward","text":"<pre><code>forward(\n    embeddings,\n    example_ids,\n    logit_scale,\n    modality_loss_pairs,\n)\n</code></pre> <p>Calculate the contrastive loss.</p> <p>Parameters:</p> Name Type Description Default <code>embeddings</code> <code>dict[str, Tensor]</code> <p>Dictionary of embeddings, where the key is the modality name and the value is the corresponding embedding tensor.</p> required <code>example_ids</code> <code>dict[str, Tensor]</code> <p>Dictionary of example IDs, where the key is the modality name and the value is a tensor tuple of the dataset index and the example index.</p> required <code>logit_scale</code> <code>Tensor</code> <p>Scale factor for the logits.</p> required <code>modality_loss_pairs</code> <code>list[LossPairSpec]</code> <p>Specification of the modality pairs for which the loss should be calculated.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The contrastive loss.</p> Source code in <code>mmlearn/modules/losses/contrastive.py</code> <pre><code>def forward(\n    self,\n    embeddings: dict[str, torch.Tensor],\n    example_ids: dict[str, torch.Tensor],\n    logit_scale: torch.Tensor,\n    modality_loss_pairs: list[LossPairSpec],\n) -&gt; torch.Tensor:\n    \"\"\"Calculate the contrastive loss.\n\n    Parameters\n    ----------\n    embeddings : dict[str, torch.Tensor]\n        Dictionary of embeddings, where the key is the modality name and the value\n        is the corresponding embedding tensor.\n    example_ids : dict[str, torch.Tensor]\n        Dictionary of example IDs, where the key is the modality name and the value\n        is a tensor tuple of the dataset index and the example index.\n    logit_scale : torch.Tensor\n        Scale factor for the logits.\n    modality_loss_pairs : list[LossPairSpec]\n        Specification of the modality pairs for which the loss should be calculated.\n\n    Returns\n    -------\n    torch.Tensor\n        The contrastive loss.\n    \"\"\"\n    world_size = dist.get_world_size() if dist.is_initialized() else 1\n    rank = dist.get_rank() if world_size &gt; 1 else 0\n\n    if self.l2_normalize:\n        embeddings = {k: F.normalize(v, p=2, dim=-1) for k, v in embeddings.items()}\n\n    if world_size &gt; 1:  # gather embeddings and example_ids across all processes\n        # NOTE: gathering dictionaries of tensors across all processes\n        # (keys + values, as opposed to just values) is especially important\n        # for the modality_alignment loss, which requires all embeddings\n        all_embeddings = _gather_dicts(\n            embeddings,\n            local_loss=self.local_loss,\n            gather_with_grad=self.gather_with_grad,\n            rank=rank,\n        )\n        all_example_ids = _gather_dicts(\n            example_ids,\n            local_loss=self.local_loss,\n            gather_with_grad=self.gather_with_grad,\n            rank=rank,\n        )\n    else:\n        all_embeddings = embeddings\n        all_example_ids = example_ids\n\n    losses = []\n    for loss_pairs in modality_loss_pairs:\n        logits_per_feature_a, logits_per_feature_b, skip_flag = self._get_logits(\n            loss_pairs.modalities,\n            per_device_embeddings=embeddings,\n            all_embeddings=all_embeddings,\n            per_device_example_ids=example_ids,\n            all_example_ids=all_example_ids,\n            logit_scale=logit_scale,\n            world_size=world_size,\n        )\n        if logits_per_feature_a is None or logits_per_feature_b is None:\n            continue\n\n        labels = self._get_ground_truth(\n            logits_per_feature_a.shape,\n            device=logits_per_feature_a.device,\n            rank=rank,\n            world_size=world_size,\n            skipped_process=skip_flag,\n        )\n\n        if labels.numel() != 0:\n            losses.append(\n                (\n                    (\n                        F.cross_entropy(logits_per_feature_a, labels)\n                        + F.cross_entropy(logits_per_feature_b, labels)\n                    )\n                    / 2\n                )\n                * loss_pairs.weight\n            )\n\n    if self.modality_alignment:\n        losses.append(\n            self._compute_modality_alignment_loss(all_embeddings, logit_scale)\n        )\n\n    if not losses:  # no loss to compute (e.g. no paired data in batch)\n        losses.append(\n            torch.tensor(\n                0.0,\n                device=logit_scale.device,\n                dtype=next(iter(embeddings.values())).dtype,\n            )\n        )\n\n    return torch.stack(losses).sum()\n</code></pre>"},{"location":"api/#mmlearn.cli.run.Data2VecLoss","title":"Data2VecLoss","text":"<p>               Bases: <code>Module</code></p> <p>Data2Vec loss function.</p> <p>Parameters:</p> Name Type Description Default <code>beta</code> <code>float</code> <p>Specifies the beta parameter for smooth L1 loss. If <code>0</code>, MSE loss is used.</p> <code>0</code> <code>loss_scale</code> <code>Optional[float]</code> <p>Scaling factor for the loss. If None, uses <code>1 / sqrt(embedding_dim)</code>.</p> <code>None</code> <code>reduction</code> <code>str</code> <p>Specifies the reduction to apply to the output: <code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>.</p> <code>'none'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the reduction mode is not supported.</p> Source code in <code>mmlearn/modules/losses/data2vec.py</code> <pre><code>@store(group=\"modules/losses\", provider=\"mmlearn\")\nclass Data2VecLoss(nn.Module):\n    \"\"\"Data2Vec loss function.\n\n    Parameters\n    ----------\n    beta : float, optional, default=0\n        Specifies the beta parameter for smooth L1 loss. If ``0``, MSE loss is used.\n    loss_scale : Optional[float], optional, default=None\n        Scaling factor for the loss. If None, uses ``1 / sqrt(embedding_dim)``.\n    reduction : str, optional, default='none'\n        Specifies the reduction to apply to the output:\n        ``'none'`` | ``'mean'`` | ``'sum'``.\n\n    Raises\n    ------\n    ValueError\n        If the reduction mode is not supported.\n    \"\"\"\n\n    def __init__(\n        self,\n        beta: float = 0,\n        loss_scale: Optional[float] = None,\n        reduction: str = \"none\",\n    ) -&gt; None:\n        super().__init__()\n        self.beta = beta\n        self.loss_scale = loss_scale\n        if reduction not in [\"none\", \"mean\", \"sum\"]:\n            raise ValueError(f\"Unsupported reduction mode: {reduction}\")\n        self.reduction = reduction\n\n    def forward(self, x: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute the Data2Vec loss.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Predicted embeddings of shape ``(batch_size, num_patches, embedding_dim)``.\n        y : torch.Tensor\n            Target embeddings of shape ``(batch_size, num_patches, embedding_dim)``.\n\n        Returns\n        -------\n        torch.Tensor\n            Data2Vec loss value.\n\n        Raises\n        ------\n        ValueError\n            If the shapes of x and y do not match.\n        \"\"\"\n        if x.shape != y.shape:\n            raise ValueError(f\"Shape mismatch: x: {x.shape}, y: {y.shape}\")\n\n        x = x.view(-1, x.size(-1)).float()\n        y = y.view(-1, y.size(-1))\n\n        if self.beta == 0:\n            loss = mse_loss(x, y, reduction=\"none\")\n        else:\n            loss = smooth_l1_loss(x, y, reduction=\"none\", beta=self.beta)\n\n        if self.loss_scale is not None:\n            scale = self.loss_scale\n        else:\n            scale = 1 / math.sqrt(x.size(-1))\n\n        loss = loss * scale\n\n        if self.reduction == \"mean\":\n            return loss.mean()\n        if self.reduction == \"sum\":\n            return loss.sum()\n        # 'none'\n        return loss.view(x.size(0), -1).sum(1)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.Data2VecLoss.forward","title":"forward","text":"<pre><code>forward(x, y)\n</code></pre> <p>Compute the Data2Vec loss.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Predicted embeddings of shape <code>(batch_size, num_patches, embedding_dim)</code>.</p> required <code>y</code> <code>Tensor</code> <p>Target embeddings of shape <code>(batch_size, num_patches, embedding_dim)</code>.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Data2Vec loss value.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the shapes of x and y do not match.</p> Source code in <code>mmlearn/modules/losses/data2vec.py</code> <pre><code>def forward(self, x: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute the Data2Vec loss.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Predicted embeddings of shape ``(batch_size, num_patches, embedding_dim)``.\n    y : torch.Tensor\n        Target embeddings of shape ``(batch_size, num_patches, embedding_dim)``.\n\n    Returns\n    -------\n    torch.Tensor\n        Data2Vec loss value.\n\n    Raises\n    ------\n    ValueError\n        If the shapes of x and y do not match.\n    \"\"\"\n    if x.shape != y.shape:\n        raise ValueError(f\"Shape mismatch: x: {x.shape}, y: {y.shape}\")\n\n    x = x.view(-1, x.size(-1)).float()\n    y = y.view(-1, y.size(-1))\n\n    if self.beta == 0:\n        loss = mse_loss(x, y, reduction=\"none\")\n    else:\n        loss = smooth_l1_loss(x, y, reduction=\"none\", beta=self.beta)\n\n    if self.loss_scale is not None:\n        scale = self.loss_scale\n    else:\n        scale = 1 / math.sqrt(x.size(-1))\n\n    loss = loss * scale\n\n    if self.reduction == \"mean\":\n        return loss.mean()\n    if self.reduction == \"sum\":\n        return loss.sum()\n    # 'none'\n    return loss.view(x.size(0), -1).sum(1)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.RetrievalRecallAtK","title":"RetrievalRecallAtK","text":"<p>               Bases: <code>Metric</code></p> <p>Retrieval Recall@K metric.</p> <p>Computes the Recall@K for retrieval tasks. The metric is computed as follows:</p> <ol> <li>Compute the cosine similarity between the query and the database.</li> <li>For each query, sort the database in decreasing order of similarity.</li> <li>Compute the Recall@K as the number of true positives among the top K elements.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>top_k</code> <code>int</code> <p>The number of top elements to consider for computing the Recall@K.</p> required <code>reduction</code> <code>(mean, sum, none, None)</code> <p>Specifies the reduction to apply after computing the pairwise cosine similarity scores.</p> <code>\"mean\"</code> <code>aggregation</code> <code>(mean, median, min, max)</code> <p>Specifies the aggregation function to apply to the Recall@K values computed in batches. If a callable is provided, it should accept a tensor of values and a keyword argument <code>'dim'</code> and return a single scalar value.</p> <code>\"mean\"</code> <code>kwargs</code> <code>Any</code> <p>Additional arguments to be passed to the class:<code>torchmetrics.Metric</code> class.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <ul> <li>If the <code>top_k</code> is not a positive integer or None.</li> <li>If the <code>reduction</code> is not one of {\"mean\", \"sum\", \"none\", None}.</li> <li>If the <code>aggregation</code> is not one of {\"mean\", \"median\", \"min\", \"max\"} or a   custom callable function.</li> </ul> Source code in <code>mmlearn/modules/metrics/retrieval_recall.py</code> <pre><code>@store(group=\"modules/metrics\", provider=\"mmlearn\")\nclass RetrievalRecallAtK(Metric):\n    \"\"\"Retrieval Recall@K metric.\n\n    Computes the Recall@K for retrieval tasks. The metric is computed as follows:\n\n    1. Compute the cosine similarity between the query and the database.\n    2. For each query, sort the database in decreasing order of similarity.\n    3. Compute the Recall@K as the number of true positives among the top K elements.\n\n    Parameters\n    ----------\n    top_k : int\n        The number of top elements to consider for computing the Recall@K.\n    reduction : {\"mean\", \"sum\", \"none\", None}, optional, default=\"sum\"\n        Specifies the reduction to apply after computing the pairwise cosine similarity\n        scores.\n    aggregation : {\"mean\", \"median\", \"min\", \"max\"} or callable, default=\"mean\"\n        Specifies the aggregation function to apply to the Recall@K values computed\n        in batches. If a callable is provided, it should accept a tensor of values\n        and a keyword argument ``'dim'`` and return a single scalar value.\n    kwargs : Any\n        Additional arguments to be passed to the :py:class:`torchmetrics.Metric` class.\n\n    Raises\n    ------\n    ValueError\n\n        - If the `top_k` is not a positive integer or None.\n        - If the `reduction` is not one of {\"mean\", \"sum\", \"none\", None}.\n        - If the `aggregation` is not one of {\"mean\", \"median\", \"min\", \"max\"} or a\n          custom callable function.\n\n    \"\"\"\n\n    is_differentiable: bool = False\n    higher_is_better: bool = True\n    full_state_update: bool = False\n\n    indexes: list[torch.Tensor]\n    x: list[torch.Tensor]\n    y: list[torch.Tensor]\n    num_samples: torch.Tensor\n\n    def __init__(\n        self,\n        top_k: int,\n        reduction: Optional[Literal[\"mean\", \"sum\", \"none\"]] = \"sum\",\n        aggregation: Union[\n            Literal[\"mean\", \"median\", \"min\", \"max\"],\n            Callable[[torch.Tensor, int], torch.Tensor],\n        ] = \"mean\",\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Initialize the metric.\"\"\"\n        super().__init__(**kwargs)\n\n        if top_k is not None and not (isinstance(top_k, int) and top_k &gt; 0):\n            raise ValueError(\"`top_k` has to be a positive integer or None\")\n        self.top_k = top_k\n\n        allowed_reduction = (\"sum\", \"mean\", \"none\", None)\n        if reduction not in allowed_reduction:\n            raise ValueError(\n                f\"Expected argument `reduction` to be one of {allowed_reduction} but got {reduction}\"\n            )\n        self.reduction = reduction\n\n        if not (\n            aggregation in (\"mean\", \"median\", \"min\", \"max\") or callable(aggregation)\n        ):\n            raise ValueError(\n                \"Argument `aggregation` must be one of `mean`, `median`, `min`, `max` or a custom callable function\"\n                f\"which takes tensor of values, but got {aggregation}.\"\n            )\n        self.aggregation = aggregation\n\n        self.add_state(\"x\", default=[], dist_reduce_fx=\"cat\")\n        self.add_state(\"y\", default=[], dist_reduce_fx=\"cat\")\n        self.add_state(\"indexes\", default=[], dist_reduce_fx=\"cat\")\n        self.add_state(\"num_samples\", default=torch.tensor(0), dist_reduce_fx=\"cat\")\n\n        self._batch_size: int = -1\n\n        self.compute_on_cpu = True\n        self.sync_on_compute = False\n        self.dist_sync_on_step = False\n        self._to_sync = self.sync_on_compute\n        self._should_unsync = False\n\n    def update(self, x: torch.Tensor, y: torch.Tensor, indexes: torch.Tensor) -&gt; None:\n        \"\"\"Check shape, convert dtypes and add to accumulators.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Embeddings (unnormalized) of shape ``(N, D)`` where ``N`` is the number\n            of samples and `D` is the number of dimensions.\n        y : torch.Tensor\n            Embeddings (unnormalized) of shape ``(M, D)`` where ``M`` is the number\n            of samples and ``D`` is the number of dimensions.\n        indexes : torch.Tensor\n            Index tensor of shape ``(N,)`` where ``N`` is the number of samples.\n            This specifies which sample in ``y`` is the positive match for each\n            sample in ``x``.\n\n        Raises\n        ------\n        ValueError\n            If `indexes` is None.\n\n        \"\"\"\n        if indexes is None:\n            raise ValueError(\"Argument `indexes` cannot be None\")\n\n        x, y, indexes = _update_batch_inputs(x.clone(), y.clone(), indexes.clone())\n\n        # offset batch indexes by the number of samples seen so far\n        indexes += self.num_samples\n\n        local_batch_size = torch.zeros_like(self.num_samples) + x.size(0)\n        if self._is_distributed():\n            x = dim_zero_cat(gather_all_tensors(x, self.process_group))\n            y = dim_zero_cat(gather_all_tensors(y, self.process_group))\n            indexes = dim_zero_cat(\n                gather_all_tensors(indexes.clone(), self.process_group)\n            )\n\n            # offset indexes for each device\n            bsz_per_device = dim_zero_cat(\n                gather_all_tensors(local_batch_size, self.process_group)\n            )\n            cum_local_bsz = torch.cumsum(bsz_per_device, dim=0)\n            for device_idx in range(1, bsz_per_device.numel()):\n                indexes[cum_local_bsz[device_idx - 1] : cum_local_bsz[device_idx]] += (\n                    cum_local_bsz[device_idx - 1]\n                )\n\n            # update the global sample count\n            self.num_samples += cum_local_bsz[-1]\n\n            self._is_synced = True\n        else:\n            self.num_samples += x.size(0)\n\n        self.x.append(x)\n        self.y.append(y)\n        self.indexes.append(indexes)\n\n        if self._batch_size == -1:\n            self._batch_size = x.size(0)  # global batch size\n\n    def compute(self) -&gt; torch.Tensor:\n        \"\"\"Compute the metric.\n\n        Returns\n        -------\n        torch.Tensor\n            The computed metric.\n        \"\"\"\n        x = dim_zero_cat(self.x)\n        y = dim_zero_cat(self.y)\n\n        # normalize embeddings\n        x /= x.norm(dim=-1, p=2, keepdim=True)\n        y /= y.norm(dim=-1, p=2, keepdim=True)\n\n        # instantiate reduction function\n        reduction_mapping: Dict[\n            Optional[str], Callable[[torch.Tensor], torch.Tensor]\n        ] = {\n            \"sum\": partial(torch.sum, dim=-1),\n            \"mean\": partial(torch.mean, dim=-1),\n            \"none\": lambda x: x,\n            None: lambda x: x,\n        }\n\n        # concatenate indexes of true pairs\n        indexes = dim_zero_cat(self.indexes)\n\n        results: list[torch.Tensor] = []\n        with concurrent.futures.ThreadPoolExecutor(\n            max_workers=os.cpu_count() or 1  # use all available CPUs\n        ) as executor:\n            futures = [\n                executor.submit(\n                    self._process_batch,\n                    start,\n                    x,\n                    y,\n                    indexes,\n                    reduction_mapping,\n                    self.top_k,\n                )\n                for start in tqdm(\n                    range(0, len(x), self._batch_size), desc=f\"Recall@{self.top_k}\"\n                )\n            ]\n            for future in concurrent.futures.as_completed(futures):\n                results.append(future.result())\n\n        return _retrieval_aggregate(\n            (torch.cat([x.float() for x in results]) &gt; 0).float(), self.aggregation\n        )\n\n    def forward(self, *args: Any, **kwargs: Any) -&gt; Any:\n        \"\"\"Forward method is not supported.\n\n        Raises\n        ------\n        NotImplementedError\n            The forward method is not supported for this metric.\n        \"\"\"\n        raise NotImplementedError(\n            \"RetrievalRecallAtK metric does not support forward method\"\n        )\n\n    def _is_distributed(self) -&gt; bool:\n        if self.distributed_available_fn is not None:\n            distributed_available = self.distributed_available_fn\n\n        return distributed_available() if callable(distributed_available) else False\n\n    def _process_batch(\n        self,\n        start: int,\n        x_norm: torch.Tensor,\n        y_norm: torch.Tensor,\n        indexes: torch.Tensor,\n        reduction_mapping: Dict[Optional[str], Callable[[torch.Tensor], torch.Tensor]],\n        top_k: int,\n    ) -&gt; torch.Tensor:\n        \"\"\"Compute the Recall@K for a batch of samples.\"\"\"\n        end = start + self._batch_size\n        x_norm_batch = x_norm[start:end]\n        indexes_batch = indexes[start:end]\n\n        similarity = _safe_matmul(x_norm_batch, y_norm)\n        scores: torch.Tensor = reduction_mapping[self.reduction](similarity)\n\n        with torch.inference_mode():\n            positive_pairs = torch.zeros_like(scores, dtype=torch.bool)\n            positive_pairs[torch.arange(len(scores)), indexes_batch] = True\n\n        return _recall_at_k(scores, positive_pairs, top_k)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.RetrievalRecallAtK.__init__","title":"__init__","text":"<pre><code>__init__(\n    top_k, reduction=\"sum\", aggregation=\"mean\", **kwargs\n)\n</code></pre> <p>Initialize the metric.</p> Source code in <code>mmlearn/modules/metrics/retrieval_recall.py</code> <pre><code>def __init__(\n    self,\n    top_k: int,\n    reduction: Optional[Literal[\"mean\", \"sum\", \"none\"]] = \"sum\",\n    aggregation: Union[\n        Literal[\"mean\", \"median\", \"min\", \"max\"],\n        Callable[[torch.Tensor, int], torch.Tensor],\n    ] = \"mean\",\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Initialize the metric.\"\"\"\n    super().__init__(**kwargs)\n\n    if top_k is not None and not (isinstance(top_k, int) and top_k &gt; 0):\n        raise ValueError(\"`top_k` has to be a positive integer or None\")\n    self.top_k = top_k\n\n    allowed_reduction = (\"sum\", \"mean\", \"none\", None)\n    if reduction not in allowed_reduction:\n        raise ValueError(\n            f\"Expected argument `reduction` to be one of {allowed_reduction} but got {reduction}\"\n        )\n    self.reduction = reduction\n\n    if not (\n        aggregation in (\"mean\", \"median\", \"min\", \"max\") or callable(aggregation)\n    ):\n        raise ValueError(\n            \"Argument `aggregation` must be one of `mean`, `median`, `min`, `max` or a custom callable function\"\n            f\"which takes tensor of values, but got {aggregation}.\"\n        )\n    self.aggregation = aggregation\n\n    self.add_state(\"x\", default=[], dist_reduce_fx=\"cat\")\n    self.add_state(\"y\", default=[], dist_reduce_fx=\"cat\")\n    self.add_state(\"indexes\", default=[], dist_reduce_fx=\"cat\")\n    self.add_state(\"num_samples\", default=torch.tensor(0), dist_reduce_fx=\"cat\")\n\n    self._batch_size: int = -1\n\n    self.compute_on_cpu = True\n    self.sync_on_compute = False\n    self.dist_sync_on_step = False\n    self._to_sync = self.sync_on_compute\n    self._should_unsync = False\n</code></pre>"},{"location":"api/#mmlearn.cli.run.RetrievalRecallAtK.update","title":"update","text":"<pre><code>update(x, y, indexes)\n</code></pre> <p>Check shape, convert dtypes and add to accumulators.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Embeddings (unnormalized) of shape <code>(N, D)</code> where <code>N</code> is the number of samples and <code>D</code> is the number of dimensions.</p> required <code>y</code> <code>Tensor</code> <p>Embeddings (unnormalized) of shape <code>(M, D)</code> where <code>M</code> is the number of samples and <code>D</code> is the number of dimensions.</p> required <code>indexes</code> <code>Tensor</code> <p>Index tensor of shape <code>(N,)</code> where <code>N</code> is the number of samples. This specifies which sample in <code>y</code> is the positive match for each sample in <code>x</code>.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>indexes</code> is None.</p> Source code in <code>mmlearn/modules/metrics/retrieval_recall.py</code> <pre><code>def update(self, x: torch.Tensor, y: torch.Tensor, indexes: torch.Tensor) -&gt; None:\n    \"\"\"Check shape, convert dtypes and add to accumulators.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Embeddings (unnormalized) of shape ``(N, D)`` where ``N`` is the number\n        of samples and `D` is the number of dimensions.\n    y : torch.Tensor\n        Embeddings (unnormalized) of shape ``(M, D)`` where ``M`` is the number\n        of samples and ``D`` is the number of dimensions.\n    indexes : torch.Tensor\n        Index tensor of shape ``(N,)`` where ``N`` is the number of samples.\n        This specifies which sample in ``y`` is the positive match for each\n        sample in ``x``.\n\n    Raises\n    ------\n    ValueError\n        If `indexes` is None.\n\n    \"\"\"\n    if indexes is None:\n        raise ValueError(\"Argument `indexes` cannot be None\")\n\n    x, y, indexes = _update_batch_inputs(x.clone(), y.clone(), indexes.clone())\n\n    # offset batch indexes by the number of samples seen so far\n    indexes += self.num_samples\n\n    local_batch_size = torch.zeros_like(self.num_samples) + x.size(0)\n    if self._is_distributed():\n        x = dim_zero_cat(gather_all_tensors(x, self.process_group))\n        y = dim_zero_cat(gather_all_tensors(y, self.process_group))\n        indexes = dim_zero_cat(\n            gather_all_tensors(indexes.clone(), self.process_group)\n        )\n\n        # offset indexes for each device\n        bsz_per_device = dim_zero_cat(\n            gather_all_tensors(local_batch_size, self.process_group)\n        )\n        cum_local_bsz = torch.cumsum(bsz_per_device, dim=0)\n        for device_idx in range(1, bsz_per_device.numel()):\n            indexes[cum_local_bsz[device_idx - 1] : cum_local_bsz[device_idx]] += (\n                cum_local_bsz[device_idx - 1]\n            )\n\n        # update the global sample count\n        self.num_samples += cum_local_bsz[-1]\n\n        self._is_synced = True\n    else:\n        self.num_samples += x.size(0)\n\n    self.x.append(x)\n    self.y.append(y)\n    self.indexes.append(indexes)\n\n    if self._batch_size == -1:\n        self._batch_size = x.size(0)  # global batch size\n</code></pre>"},{"location":"api/#mmlearn.cli.run.RetrievalRecallAtK.compute","title":"compute","text":"<pre><code>compute()\n</code></pre> <p>Compute the metric.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>The computed metric.</p> Source code in <code>mmlearn/modules/metrics/retrieval_recall.py</code> <pre><code>def compute(self) -&gt; torch.Tensor:\n    \"\"\"Compute the metric.\n\n    Returns\n    -------\n    torch.Tensor\n        The computed metric.\n    \"\"\"\n    x = dim_zero_cat(self.x)\n    y = dim_zero_cat(self.y)\n\n    # normalize embeddings\n    x /= x.norm(dim=-1, p=2, keepdim=True)\n    y /= y.norm(dim=-1, p=2, keepdim=True)\n\n    # instantiate reduction function\n    reduction_mapping: Dict[\n        Optional[str], Callable[[torch.Tensor], torch.Tensor]\n    ] = {\n        \"sum\": partial(torch.sum, dim=-1),\n        \"mean\": partial(torch.mean, dim=-1),\n        \"none\": lambda x: x,\n        None: lambda x: x,\n    }\n\n    # concatenate indexes of true pairs\n    indexes = dim_zero_cat(self.indexes)\n\n    results: list[torch.Tensor] = []\n    with concurrent.futures.ThreadPoolExecutor(\n        max_workers=os.cpu_count() or 1  # use all available CPUs\n    ) as executor:\n        futures = [\n            executor.submit(\n                self._process_batch,\n                start,\n                x,\n                y,\n                indexes,\n                reduction_mapping,\n                self.top_k,\n            )\n            for start in tqdm(\n                range(0, len(x), self._batch_size), desc=f\"Recall@{self.top_k}\"\n            )\n        ]\n        for future in concurrent.futures.as_completed(futures):\n            results.append(future.result())\n\n    return _retrieval_aggregate(\n        (torch.cat([x.float() for x in results]) &gt; 0).float(), self.aggregation\n    )\n</code></pre>"},{"location":"api/#mmlearn.cli.run.RetrievalRecallAtK.forward","title":"forward","text":"<pre><code>forward(*args, **kwargs)\n</code></pre> <p>Forward method is not supported.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>The forward method is not supported for this metric.</p> Source code in <code>mmlearn/modules/metrics/retrieval_recall.py</code> <pre><code>def forward(self, *args: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"Forward method is not supported.\n\n    Raises\n    ------\n    NotImplementedError\n        The forward method is not supported for this metric.\n    \"\"\"\n    raise NotImplementedError(\n        \"RetrievalRecallAtK metric does not support forward method\"\n    )\n</code></pre>"},{"location":"api/#mmlearn.cli.run.ContrastivePretraining","title":"ContrastivePretraining","text":"<p>               Bases: <code>TrainingTask</code></p> <p>Contrastive pretraining task.</p> <p>This class supports contrastive pretraining with <code>N</code> modalities of data. It allows the sharing of encoders, heads, and postprocessors across modalities. It also supports computing the contrastive loss between specified pairs of modalities, as well as training auxiliary tasks alongside the main contrastive pretraining task.</p> <p>Parameters:</p> Name Type Description Default <code>encoders</code> <code>dict[str, Module]</code> <p>A dictionary of encoders. The keys can be any string, including the names of any supported modalities. If the keys are not supported modalities, the <code>modality_module_mapping</code> parameter must be provided to map the encoders to specific modalities. The encoders are expected to take a dictionary of input values and return a list-like object with the first element being the encoded values. This first element is passed on to the heads or postprocessors and the remaining elements are ignored.</p> required <code>heads</code> <code>Optional[dict[str, Union[Module, dict[str, Module]]]]</code> <p>A dictionary of modules that process the encoder outputs, usually projection heads. If the keys do not correspond to the name of a supported modality, the <code>modality_module_mapping</code> parameter must be provided. If any of the values are dictionaries, they will be wrapped in a class:<code>torch.nn.Sequential</code> module. All head modules are expected to take a single input tensor and return a single output tensor.</p> <code>None</code> <code>postprocessors</code> <code>Optional[dict[str, Union[Module, dict[str, Module]]]]</code> <p>A dictionary of modules that process the head outputs. If the keys do not correspond to the name of a supported modality, the <code>modality_module_mapping</code> parameter must be provided. If any of the values are dictionaries, they will be wrapped in a <code>nn.Sequential</code> module. All postprocessor modules are expected to take a single input tensor and return a single output tensor.</p> <code>None</code> <code>modality_module_mapping</code> <code>Optional[dict[str, ModuleKeySpec]]</code> <p>A dictionary mapping modalities to encoders, heads, and postprocessors. Useful for reusing the same instance of a module across multiple modalities.</p> <code>None</code> <code>optimizer</code> <code>Optional[partial[Optimizer]]</code> <p>The optimizer to use for training. This is expected to be a func:<code>~functools.partial</code> function that takes the model parameters as the only required argument. If not provided, training will continue without an optimizer.</p> <code>None</code> <code>lr_scheduler</code> <code>Optional[Union[dict[str, Union[partial[LRScheduler], Any]], partial[LRScheduler]]]</code> <p>The learning rate scheduler to use for training. This can be a func:<code>~functools.partial</code> function that takes the optimizer as the only required argument or a dictionary with a <code>'scheduler'</code> key that specifies the scheduler and an optional <code>'extras'</code> key that specifies additional arguments to pass to the scheduler. If not provided, the learning rate will not be adjusted during training.</p> <code>None</code> <code>init_logit_scale</code> <code>float</code> <p>The initial value of the logit scale parameter. This is the log of the scale factor applied to the logits before computing the contrastive loss.</p> <code>1 / 0.07</code> <code>max_logit_scale</code> <code>float</code> <p>The maximum value of the logit scale parameter. The logit scale parameter is clamped to the range <code>[0, log(max_logit_scale)]</code>.</p> <code>100</code> <code>learnable_logit_scale</code> <code>bool</code> <p>Whether the logit scale parameter is learnable. If set to False, the logit scale parameter is treated as a constant.</p> <code>True</code> <code>loss</code> <code>Optional[Module]</code> <p>The loss function to use.</p> <code>None</code> <code>modality_loss_pairs</code> <code>Optional[list[LossPairSpec]]</code> <p>A list of pairs of modalities to compute the contrastive loss between and the weight to apply to each pair.</p> <code>None</code> <code>auxiliary_tasks</code> <code>dict[str, AuxiliaryTaskSpec]</code> <p>Auxiliary tasks to run alongside the main contrastive pretraining task.</p> <ul> <li>The auxiliary task module is expected to be a partially-initialized instance   of a class:<code>~lightning.pytorch.core.LightningModule</code> created using   func:<code>functools.partial</code>, such that an initialized encoder can be   passed as the only argument.</li> <li>The <code>modality</code> parameter specifies the modality of the encoder to use   for the auxiliary task. The <code>loss_weight</code> parameter specifies the weight   to apply to the auxiliary task loss.</li> </ul> <code>None</code> <code>log_auxiliary_tasks_loss</code> <code>bool</code> <p>Whether to log the loss of auxiliary tasks to the main logger.</p> <code>False</code> <code>compute_validation_loss</code> <code>bool</code> <p>Whether to compute the validation loss if a validation dataloader is provided. The loss function must be provided to compute the validation loss.</p> <code>True</code> <code>compute_test_loss</code> <code>bool</code> <p>Whether to compute the test loss if a test dataloader is provided. The loss function must be provided to compute the test loss.</p> <code>True</code> <code>evaluation_tasks</code> <code>Optional[dict[str, EvaluationSpec]]</code> <p>Evaluation tasks to run during validation, while training, and during testing.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <ul> <li>If the loss function is not provided and either the validation or test loss   needs to be computed.</li> <li>If the given modality is not supported.</li> <li>If the encoder, head, or postprocessor is not mapped to a modality.</li> <li>If an unsupported modality is found in the loss pair specification.</li> <li>If an unsupported modality is found in the auxiliary tasks.</li> <li>If the auxiliary task is not a partial function.</li> <li>If the evaluation task is not an instance of class:<code>~mmlearn.tasks.hooks.EvaluationHooks</code>.</li> </ul> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>@store(group=\"task\", provider=\"mmlearn\")\nclass ContrastivePretraining(TrainingTask):\n    \"\"\"Contrastive pretraining task.\n\n    This class supports contrastive pretraining with ``N`` modalities of data. It\n    allows the sharing of encoders, heads, and postprocessors across modalities.\n    It also supports computing the contrastive loss between specified pairs of\n    modalities, as well as training auxiliary tasks alongside the main contrastive\n    pretraining task.\n\n    Parameters\n    ----------\n    encoders : dict[str, torch.nn.Module]\n        A dictionary of encoders. The keys can be any string, including the names of\n        any supported modalities. If the keys are not supported modalities, the\n        ``modality_module_mapping`` parameter must be provided to map the encoders to\n        specific modalities. The encoders are expected to take a dictionary of input\n        values and return a list-like object with the first element being the encoded\n        values. This first element is passed on to the heads or postprocessors and\n        the remaining elements are ignored.\n    heads : Optional[dict[str, Union[torch.nn.Module, dict[str, torch.nn.Module]]]], optional, default=None\n        A dictionary of modules that process the encoder outputs, usually projection\n        heads. If the keys do not correspond to the name of a supported modality,\n        the ``modality_module_mapping`` parameter must be provided. If any of the values\n        are dictionaries, they will be wrapped in a :py:class:`torch.nn.Sequential`\n        module. All head modules are expected to take a single input tensor and\n        return a single output tensor.\n    postprocessors : Optional[dict[str, Union[torch.nn.Module, dict[str, torch.nn.Module]]]], optional, default=None\n        A dictionary of modules that process the head outputs. If the keys do not\n        correspond to the name of a supported modality, the `modality_module_mapping`\n        parameter must be provided. If any of the values are dictionaries, they will\n        be wrapped in a `nn.Sequential` module. All postprocessor modules are expected\n        to take a single input tensor and return a single output tensor.\n    modality_module_mapping : Optional[dict[str, ModuleKeySpec]], optional, default=None\n        A dictionary mapping modalities to encoders, heads, and postprocessors.\n        Useful for reusing the same instance of a module across multiple modalities.\n    optimizer : Optional[partial[torch.optim.Optimizer]], optional, default=None\n        The optimizer to use for training. This is expected to be a :py:func:`~functools.partial`\n        function that takes the model parameters as the only required argument.\n        If not provided, training will continue without an optimizer.\n    lr_scheduler : Optional[Union[dict[str, Union[partial[torch.optim.lr_scheduler.LRScheduler], Any]], partial[torch.optim.lr_scheduler.LRScheduler]]], optional, default=None\n        The learning rate scheduler to use for training. This can be a\n        :py:func:`~functools.partial` function that takes the optimizer as the only\n        required argument or a dictionary with a ``'scheduler'`` key that specifies\n        the scheduler and an optional ``'extras'`` key that specifies additional\n        arguments to pass to the scheduler. If not provided, the learning rate will\n        not be adjusted during training.\n    init_logit_scale : float, optional, default=1 / 0.07\n        The initial value of the logit scale parameter. This is the log of the scale\n        factor applied to the logits before computing the contrastive loss.\n    max_logit_scale : float, optional, default=100\n        The maximum value of the logit scale parameter. The logit scale parameter\n        is clamped to the range ``[0, log(max_logit_scale)]``.\n    learnable_logit_scale : bool, optional, default=True\n        Whether the logit scale parameter is learnable. If set to False, the logit\n        scale parameter is treated as a constant.\n    loss : Optional[torch.nn.Module], optional, default=None\n        The loss function to use.\n    modality_loss_pairs : Optional[list[LossPairSpec]], optional, default=None\n        A list of pairs of modalities to compute the contrastive loss between and\n        the weight to apply to each pair.\n    auxiliary_tasks : dict[str, AuxiliaryTaskSpec], optional, default=None\n        Auxiliary tasks to run alongside the main contrastive pretraining task.\n\n        - The auxiliary task module is expected to be a partially-initialized instance\n          of a :py:class:`~lightning.pytorch.core.LightningModule` created using\n          :py:func:`functools.partial`, such that an initialized encoder can be\n          passed as the only argument.\n        - The ``modality`` parameter specifies the modality of the encoder to use\n          for the auxiliary task. The ``loss_weight`` parameter specifies the weight\n          to apply to the auxiliary task loss.\n    log_auxiliary_tasks_loss : bool, optional, default=False\n        Whether to log the loss of auxiliary tasks to the main logger.\n    compute_validation_loss : bool, optional, default=True\n        Whether to compute the validation loss if a validation dataloader is provided.\n        The loss function must be provided to compute the validation loss.\n    compute_test_loss : bool, optional, default=True\n        Whether to compute the test loss if a test dataloader is provided. The loss\n        function must be provided to compute the test loss.\n    evaluation_tasks : Optional[dict[str, EvaluationSpec]], optional, default=None\n        Evaluation tasks to run during validation, while training, and during testing.\n\n    Raises\n    ------\n    ValueError\n\n        - If the loss function is not provided and either the validation or test loss\n          needs to be computed.\n        - If the given modality is not supported.\n        - If the encoder, head, or postprocessor is not mapped to a modality.\n        - If an unsupported modality is found in the loss pair specification.\n        - If an unsupported modality is found in the auxiliary tasks.\n        - If the auxiliary task is not a partial function.\n        - If the evaluation task is not an instance of :py:class:`~mmlearn.tasks.hooks.EvaluationHooks`.\n\n    \"\"\"  # noqa: W505\n\n    def __init__(  # noqa: PLR0912, PLR0915\n        self,\n        encoders: dict[str, nn.Module],\n        heads: Optional[dict[str, Union[nn.Module, dict[str, nn.Module]]]] = None,\n        postprocessors: Optional[\n            dict[str, Union[nn.Module, dict[str, nn.Module]]]\n        ] = None,\n        modality_module_mapping: Optional[dict[str, ModuleKeySpec]] = None,\n        optimizer: Optional[partial[torch.optim.Optimizer]] = None,\n        lr_scheduler: Optional[\n            Union[\n                dict[str, Union[partial[torch.optim.lr_scheduler.LRScheduler], Any]],\n                partial[torch.optim.lr_scheduler.LRScheduler],\n            ]\n        ] = None,\n        init_logit_scale: float = 1 / 0.07,\n        max_logit_scale: float = 100,\n        learnable_logit_scale: bool = True,\n        loss: Optional[nn.Module] = None,\n        modality_loss_pairs: Optional[list[LossPairSpec]] = None,\n        auxiliary_tasks: Optional[dict[str, AuxiliaryTaskSpec]] = None,\n        log_auxiliary_tasks_loss: bool = False,\n        compute_validation_loss: bool = True,\n        compute_test_loss: bool = True,\n        evaluation_tasks: Optional[dict[str, EvaluationSpec]] = None,\n    ) -&gt; None:\n        super().__init__(\n            optimizer=optimizer,\n            lr_scheduler=lr_scheduler,\n            loss_fn=loss,\n            compute_validation_loss=compute_validation_loss,\n            compute_test_loss=compute_test_loss,\n        )\n\n        self.save_hyperparameters(\n            ignore=[\n                \"encoders\",\n                \"heads\",\n                \"postprocessors\",\n                \"modality_module_mapping\",\n                \"loss\",\n                \"auxiliary_tasks\",\n                \"evaluation_tasks\",\n                \"modality_loss_pairs\",\n            ]\n        )\n\n        if modality_module_mapping is None:\n            # assume all the module dictionaries use the same keys corresponding\n            # to modalities\n            modality_module_mapping = {}\n            for key in encoders:\n                modality_module_mapping[key] = ModuleKeySpec(\n                    encoder_key=key,\n                    head_key=key,\n                    postprocessor_key=key,\n                )\n\n        # match modalities to encoders, heads, and postprocessors\n        modality_encoder_mapping: dict[str, Optional[str]] = {}\n        modality_head_mapping: dict[str, Optional[str]] = {}\n        modality_postprocessor_mapping: dict[str, Optional[str]] = {}\n        for modality_key, module_mapping in modality_module_mapping.items():\n            if not Modalities.has_modality(modality_key):\n                raise ValueError(_unsupported_modality_error.format(modality_key))\n            modality_encoder_mapping[modality_key] = module_mapping.encoder_key\n            modality_head_mapping[modality_key] = module_mapping.head_key\n            modality_postprocessor_mapping[modality_key] = (\n                module_mapping.postprocessor_key\n            )\n\n        # ensure all modules are mapped to a modality\n        for key in encoders:\n            if key not in modality_encoder_mapping.values():\n                if not Modalities.has_modality(key):\n                    raise ValueError(_unsupported_modality_error.format(key))\n                modality_encoder_mapping[key] = key\n\n        if heads is not None:\n            for key in heads:\n                if key not in modality_head_mapping.values():\n                    if not Modalities.has_modality(key):\n                        raise ValueError(_unsupported_modality_error.format(key))\n                    modality_head_mapping[key] = key\n\n        if postprocessors is not None:\n            for key in postprocessors:\n                if key not in modality_postprocessor_mapping.values():\n                    if not Modalities.has_modality(key):\n                        raise ValueError(_unsupported_modality_error.format(key))\n                    modality_postprocessor_mapping[key] = key\n\n        self._available_modalities: list[Modality] = [\n            Modalities.get_modality(modality_key)\n            for modality_key in modality_encoder_mapping\n        ]\n        assert len(self._available_modalities) &gt;= 2, (\n            \"Expected at least two modalities to be available. \"\n        )\n\n        #: A :py:class:`~torch.nn.ModuleDict`, where the keys are the names of the\n        #: modalities and the values are the encoder modules.\n        self.encoders = nn.ModuleDict(\n            {\n                Modalities.get_modality(modality_key).name: encoders[encoder_key]\n                for modality_key, encoder_key in modality_encoder_mapping.items()\n                if encoder_key is not None\n            }\n        )\n\n        #: A :py:class:`~torch.nn.ModuleDict`, where the keys are the names of the\n        #: modalities and the values are the projection head modules. This can be\n        #: ``None`` if no heads modules are provided.\n        self.heads = None\n        if heads is not None:\n            self.heads = nn.ModuleDict(\n                {\n                    Modalities.get_modality(modality_key).name: heads[head_key]\n                    if isinstance(heads[head_key], nn.Module)\n                    else nn.Sequential(*heads[head_key].values())\n                    for modality_key, head_key in modality_head_mapping.items()\n                    if head_key is not None and head_key in heads\n                }\n            )\n\n        #: A :py:class:`~torch.nn.ModuleDict`, where the keys are the names of the\n        #: modalities and the values are the postprocessor modules. This can be\n        #: ``None`` if no postprocessor modules are provided.\n        self.postprocessors = None\n        if postprocessors is not None:\n            self.postprocessors = nn.ModuleDict(\n                {\n                    Modalities.get_modality(modality_key).name: postprocessors[\n                        postprocessor_key\n                    ]\n                    if isinstance(postprocessors[postprocessor_key], nn.Module)\n                    else nn.Sequential(*postprocessors[postprocessor_key].values())\n                    for modality_key, postprocessor_key in modality_postprocessor_mapping.items()\n                    if postprocessor_key is not None\n                    and postprocessor_key in postprocessors\n                }\n            )\n\n        # set up logit scaling\n        log_logit_scale = torch.ones([]) * np.log(init_logit_scale)\n        self.max_logit_scale = max_logit_scale\n        self.learnable_logit_scale = learnable_logit_scale\n\n        if self.learnable_logit_scale:\n            self.log_logit_scale = torch.nn.Parameter(\n                log_logit_scale, requires_grad=True\n            )\n        else:\n            self.register_buffer(\"log_logit_scale\", log_logit_scale)\n\n        # set up contrastive loss pairs\n        if modality_loss_pairs is None:\n            modality_loss_pairs = [\n                LossPairSpec(modalities=(m1.name, m2.name))\n                for m1, m2 in itertools.combinations(self._available_modalities, 2)\n            ]\n\n        for modality_pair in modality_loss_pairs:\n            if not all(\n                Modalities.get_modality(modality) in self._available_modalities\n                for modality in modality_pair.modalities\n            ):\n                raise ValueError(\n                    \"Found unspecified modality in the loss pair specification \"\n                    f\"{modality_pair.modalities}. Available modalities are \"\n                    f\"{self._available_modalities}.\"\n                )\n\n        #: A list :py:class:`LossPairSpec` instances specifying the pairs of\n        #: modalities to compute the contrastive loss between and the weight to\n        #: apply to each pair.\n        self.modality_loss_pairs = modality_loss_pairs\n\n        # set up auxiliary tasks\n        self.aux_task_specs = auxiliary_tasks or {}\n        self.auxiliary_tasks: nn.ModuleDict[str, L.LightningModule] = nn.ModuleDict()\n        for task_name, task_spec in self.aux_task_specs.items():\n            if not Modalities.has_modality(task_spec.modality):\n                raise ValueError(\n                    f\"Found unsupported modality `{task_spec.modality}` in the auxiliary tasks. \"\n                    f\"Available modalities are {self._available_modalities}.\"\n                )\n            if not isinstance(task_spec.task, partial):\n                raise TypeError(\n                    f\"Expected auxiliary task to be a partial function, but got {type(task_spec.task)}.\"\n                )\n\n            self.auxiliary_tasks[task_name] = task_spec.task(\n                self.encoders[Modalities.get_modality(task_spec.modality).name]\n            )\n\n        self.log_auxiliary_tasks_loss = log_auxiliary_tasks_loss\n\n        if evaluation_tasks is not None:\n            for eval_task_spec in evaluation_tasks.values():\n                if not isinstance(eval_task_spec.task, EvaluationHooks):\n                    raise TypeError(\n                        f\"Expected {eval_task_spec.task} to be an instance of `EvaluationHooks` \"\n                        f\"but got {type(eval_task_spec.task)}.\"\n                    )\n\n        #: A dictionary of evaluation tasks to run during validation, while training,\n        #: or during testing.\n        self.evaluation_tasks = evaluation_tasks\n\n    def configure_model(self) -&gt; None:\n        \"\"\"Configure the model.\"\"\"\n        if self.auxiliary_tasks:\n            for task_name in self.auxiliary_tasks:\n                self.auxiliary_tasks[task_name].configure_model()\n\n    def encode(\n        self, inputs: dict[str, Any], modality: Modality, normalize: bool = False\n    ) -&gt; torch.Tensor:\n        \"\"\"Encode the input values for the given modality.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            Input values.\n        modality : Modality\n            The modality to encode.\n        normalize : bool, optional, default=False\n            Whether to apply L2 normalization to the output (after the head and\n            postprocessor layers, if present).\n\n        Returns\n        -------\n        torch.Tensor\n            The encoded values for the specified modality.\n        \"\"\"\n        output = self.encoders[modality.name](inputs)[0]\n\n        if self.postprocessors and modality.name in self.postprocessors:\n            output = self.postprocessors[modality.name](output)\n\n        if self.heads and modality.name in self.heads:\n            output = self.heads[modality.name](output)\n\n        if normalize:\n            output = torch.nn.functional.normalize(output, p=2, dim=-1)\n\n        return output\n\n    def forward(self, inputs: dict[str, Any]) -&gt; dict[str, torch.Tensor]:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input tensors to encode.\n\n        Returns\n        -------\n        dict[str, torch.Tensor]\n            The encodings for each modality.\n        \"\"\"\n        outputs = {\n            modality.embedding: self.encode(inputs, modality, normalize=True)\n            for modality in self._available_modalities\n            if modality.name in inputs\n        }\n\n        if not all(\n            output.size(-1) == list(outputs.values())[0].size(-1)\n            for output in outputs.values()\n        ):\n            raise ValueError(\"Expected all model outputs to have the same dimension.\")\n\n        return outputs\n\n    def on_train_epoch_start(self) -&gt; None:\n        \"\"\"Prepare for the training epoch.\n\n        This method sets the modules to training mode.\n        \"\"\"\n        self.encoders.train()\n        if self.heads:\n            self.heads.train()\n        if self.postprocessors:\n            self.postprocessors.train()\n\n    def training_step(self, batch: dict[str, Any], batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"Compute the loss for the batch.\n\n        Parameters\n        ----------\n        batch : dict[str, Any]\n            The batch of data to process.\n        batch_idx : int\n            The index of the batch.\n\n        Returns\n        -------\n        torch.Tensor\n            The loss for the batch.\n        \"\"\"\n        outputs = self(batch)\n\n        with torch.no_grad():\n            self.log_logit_scale.clamp_(0, math.log(self.max_logit_scale))\n\n        loss = self._compute_loss(batch, batch_idx, outputs)\n\n        if loss is None:\n            raise ValueError(\"The loss function must be provided for training.\")\n\n        self.log(\"train/loss\", loss, prog_bar=True, sync_dist=True)\n        self.log(\n            \"train/logit_scale\",\n            self.log_logit_scale.exp(),\n            prog_bar=True,\n            on_step=True,\n            on_epoch=False,\n        )\n\n        return loss\n\n    def on_before_zero_grad(self, optimizer: torch.optim.Optimizer) -&gt; None:\n        \"\"\"Zero out the gradients of the model.\"\"\"\n        if self.auxiliary_tasks:\n            for task in self.auxiliary_tasks.values():\n                task.on_before_zero_grad(optimizer)\n\n    def on_validation_epoch_start(self) -&gt; None:\n        \"\"\"Prepare for the validation epoch.\n\n        This method sets the modules to evaluation mode and calls the\n        ``on_evaluation_epoch_start`` method of each evaluation task.\n        \"\"\"\n        self._on_eval_epoch_start(\"val\")\n\n    def validation_step(\n        self, batch: dict[str, torch.Tensor], batch_idx: int\n    ) -&gt; Optional[torch.Tensor]:\n        \"\"\"Run a single validation step.\n\n        Parameters\n        ----------\n        batch : dict[str, torch.Tensor]\n            The batch of data to process.\n        batch_idx : int\n            The index of the batch.\n\n        Returns\n        -------\n        Optional[torch.Tensor]\n            The loss for the batch or ``None`` if the loss function is not provided.\n        \"\"\"\n        return self._shared_eval_step(batch, batch_idx, \"val\")\n\n    def on_validation_epoch_end(self) -&gt; None:\n        \"\"\"Compute and log epoch-level metrics at the end of the validation epoch.\"\"\"\n        self._on_eval_epoch_end(\"val\")\n\n    def on_test_epoch_start(self) -&gt; None:\n        \"\"\"Prepare for the test epoch.\"\"\"\n        self._on_eval_epoch_start(\"test\")\n\n    def test_step(\n        self, batch: dict[str, torch.Tensor], batch_idx: int\n    ) -&gt; Optional[torch.Tensor]:\n        \"\"\"Run a single test step.\n\n        Parameters\n        ----------\n        batch : dict[str, torch.Tensor]\n            The batch of data to process.\n        batch_idx : int\n            The index of the batch.\n\n        Returns\n        -------\n        Optional[torch.Tensor]\n            The loss for the batch or ``None`` if the loss function is not provided.\n        \"\"\"\n        return self._shared_eval_step(batch, batch_idx, \"test\")\n\n    def on_test_epoch_end(self) -&gt; None:\n        \"\"\"Compute and log epoch-level metrics at the end of the test epoch.\"\"\"\n        self._on_eval_epoch_end(\"test\")\n\n    def on_load_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n        \"\"\"Modify the model checkpoint after loading.\n\n        The `on_load_checkpoint` method of auxiliary tasks is called here to allow\n        them to modify the checkpoint after loading.\n\n        Parameters\n        ----------\n        checkpoint : Dict[str, Any]\n            The loaded checkpoint.\n        \"\"\"\n        if self.auxiliary_tasks:\n            for task in self.auxiliary_tasks.values():\n                task.on_load_checkpoint(checkpoint)\n\n    def on_save_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n        \"\"\"Modify the checkpoint before saving.\n\n        The `on_save_checkpoint` method of auxiliary tasks is called here to allow\n        them to modify the checkpoint before saving.\n\n        Parameters\n        ----------\n        checkpoint : Dict[str, Any]\n            The checkpoint to save.\n        \"\"\"\n        if self.auxiliary_tasks:\n            for task in self.auxiliary_tasks.values():\n                task.on_save_checkpoint(checkpoint)\n\n    def _compute_loss(\n        self, batch: dict[str, Any], batch_idx: int, outputs: dict[str, torch.Tensor]\n    ) -&gt; Optional[torch.Tensor]:\n        if self.loss_fn is None:\n            return None\n\n        contrastive_loss = self.loss_fn(\n            outputs,\n            batch[\"example_ids\"],\n            self.log_logit_scale.exp(),\n            self.modality_loss_pairs,\n        )\n\n        auxiliary_losses: list[torch.Tensor] = []\n        if self.auxiliary_tasks:\n            for task_name, task_spec in self.aux_task_specs.items():\n                auxiliary_task_output = self.auxiliary_tasks[task_name].training_step(\n                    batch, batch_idx\n                )\n                if isinstance(auxiliary_task_output, torch.Tensor):\n                    auxiliary_task_loss = auxiliary_task_output\n                elif isinstance(auxiliary_task_output, Mapping):\n                    auxiliary_task_loss = auxiliary_task_output[\"loss\"]\n                else:\n                    raise ValueError(\n                        \"Expected auxiliary task output to be a tensor or a mapping \"\n                        f\"containing a 'loss' key, but got {type(auxiliary_task_output)}.\"\n                    )\n\n                auxiliary_task_loss *= task_spec.loss_weight\n                auxiliary_losses.append(auxiliary_task_loss)\n                if self.log_auxiliary_tasks_loss:\n                    self.log(\n                        f\"train/{task_name}_loss\", auxiliary_task_loss, sync_dist=True\n                    )\n\n        if not auxiliary_losses:\n            return contrastive_loss\n\n        return torch.stack(auxiliary_losses).sum() + contrastive_loss\n\n    def _on_eval_epoch_start(self, eval_type: Literal[\"val\", \"test\"]) -&gt; None:\n        \"\"\"Prepare for the evaluation epoch.\"\"\"\n        self.encoders.eval()\n        if self.heads:\n            self.heads.eval()\n        if self.postprocessors:\n            self.postprocessors.eval()\n        if self.evaluation_tasks:\n            for task_spec in self.evaluation_tasks.values():\n                if (eval_type == \"val\" and task_spec.run_on_validation) or (\n                    eval_type == \"test\" and task_spec.run_on_test\n                ):\n                    task_spec.task.on_evaluation_epoch_start(self)\n\n    def _shared_eval_step(\n        self,\n        batch: dict[str, torch.Tensor],\n        batch_idx: int,\n        eval_type: Literal[\"val\", \"test\"],\n    ) -&gt; Optional[torch.Tensor]:\n        \"\"\"Run a single evaluation step.\n\n        Parameters\n        ----------\n        batch : dict[str, torch.Tensor]\n            The batch of data to process.\n        batch_idx : int\n            The index of the batch.\n\n        Returns\n        -------\n        Optional[torch.Tensor]\n            The loss for the batch or ``None`` if the loss function is not provided.\n        \"\"\"\n        loss: Optional[torch.Tensor] = None\n        if (eval_type == \"val\" and self.compute_validation_loss) or (\n            eval_type == \"test\" and self.compute_test_loss\n        ):\n            outputs = self(batch)\n            loss = self._compute_loss(batch, batch_idx, outputs)\n            if loss is not None and not self.trainer.sanity_checking:\n                self.log(f\"{eval_type}/loss\", loss, prog_bar=True, sync_dist=True)\n\n        if self.evaluation_tasks:\n            for task_spec in self.evaluation_tasks.values():\n                if (eval_type == \"val\" and task_spec.run_on_validation) or (\n                    eval_type == \"test\" and task_spec.run_on_test\n                ):\n                    task_spec.task.evaluation_step(self, batch, batch_idx)\n\n        return loss\n\n    def _on_eval_epoch_end(self, eval_type: Literal[\"val\", \"test\"]) -&gt; None:\n        \"\"\"Compute and log epoch-level metrics at the end of the evaluation epoch.\"\"\"\n        if self.evaluation_tasks:\n            for task_spec in self.evaluation_tasks.values():\n                if (eval_type == \"val\" and task_spec.run_on_validation) or (\n                    eval_type == \"test\" and task_spec.run_on_test\n                ):\n                    task_spec.task.on_evaluation_epoch_end(self)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.ContrastivePretraining.configure_model","title":"configure_model","text":"<pre><code>configure_model()\n</code></pre> <p>Configure the model.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def configure_model(self) -&gt; None:\n    \"\"\"Configure the model.\"\"\"\n    if self.auxiliary_tasks:\n        for task_name in self.auxiliary_tasks:\n            self.auxiliary_tasks[task_name].configure_model()\n</code></pre>"},{"location":"api/#mmlearn.cli.run.ContrastivePretraining.encode","title":"encode","text":"<pre><code>encode(inputs, modality, normalize=False)\n</code></pre> <p>Encode the input values for the given modality.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>Input values.</p> required <code>modality</code> <code>Modality</code> <p>The modality to encode.</p> required <code>normalize</code> <code>bool</code> <p>Whether to apply L2 normalization to the output (after the head and postprocessor layers, if present).</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The encoded values for the specified modality.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def encode(\n    self, inputs: dict[str, Any], modality: Modality, normalize: bool = False\n) -&gt; torch.Tensor:\n    \"\"\"Encode the input values for the given modality.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        Input values.\n    modality : Modality\n        The modality to encode.\n    normalize : bool, optional, default=False\n        Whether to apply L2 normalization to the output (after the head and\n        postprocessor layers, if present).\n\n    Returns\n    -------\n    torch.Tensor\n        The encoded values for the specified modality.\n    \"\"\"\n    output = self.encoders[modality.name](inputs)[0]\n\n    if self.postprocessors and modality.name in self.postprocessors:\n        output = self.postprocessors[modality.name](output)\n\n    if self.heads and modality.name in self.heads:\n        output = self.heads[modality.name](output)\n\n    if normalize:\n        output = torch.nn.functional.normalize(output, p=2, dim=-1)\n\n    return output\n</code></pre>"},{"location":"api/#mmlearn.cli.run.ContrastivePretraining.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input tensors to encode.</p> required <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>The encodings for each modality.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; dict[str, torch.Tensor]:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input tensors to encode.\n\n    Returns\n    -------\n    dict[str, torch.Tensor]\n        The encodings for each modality.\n    \"\"\"\n    outputs = {\n        modality.embedding: self.encode(inputs, modality, normalize=True)\n        for modality in self._available_modalities\n        if modality.name in inputs\n    }\n\n    if not all(\n        output.size(-1) == list(outputs.values())[0].size(-1)\n        for output in outputs.values()\n    ):\n        raise ValueError(\"Expected all model outputs to have the same dimension.\")\n\n    return outputs\n</code></pre>"},{"location":"api/#mmlearn.cli.run.ContrastivePretraining.on_train_epoch_start","title":"on_train_epoch_start","text":"<pre><code>on_train_epoch_start()\n</code></pre> <p>Prepare for the training epoch.</p> <p>This method sets the modules to training mode.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def on_train_epoch_start(self) -&gt; None:\n    \"\"\"Prepare for the training epoch.\n\n    This method sets the modules to training mode.\n    \"\"\"\n    self.encoders.train()\n    if self.heads:\n        self.heads.train()\n    if self.postprocessors:\n        self.postprocessors.train()\n</code></pre>"},{"location":"api/#mmlearn.cli.run.ContrastivePretraining.training_step","title":"training_step","text":"<pre><code>training_step(batch, batch_idx)\n</code></pre> <p>Compute the loss for the batch.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict[str, Any]</code> <p>The batch of data to process.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The loss for the batch.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def training_step(self, batch: dict[str, Any], batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"Compute the loss for the batch.\n\n    Parameters\n    ----------\n    batch : dict[str, Any]\n        The batch of data to process.\n    batch_idx : int\n        The index of the batch.\n\n    Returns\n    -------\n    torch.Tensor\n        The loss for the batch.\n    \"\"\"\n    outputs = self(batch)\n\n    with torch.no_grad():\n        self.log_logit_scale.clamp_(0, math.log(self.max_logit_scale))\n\n    loss = self._compute_loss(batch, batch_idx, outputs)\n\n    if loss is None:\n        raise ValueError(\"The loss function must be provided for training.\")\n\n    self.log(\"train/loss\", loss, prog_bar=True, sync_dist=True)\n    self.log(\n        \"train/logit_scale\",\n        self.log_logit_scale.exp(),\n        prog_bar=True,\n        on_step=True,\n        on_epoch=False,\n    )\n\n    return loss\n</code></pre>"},{"location":"api/#mmlearn.cli.run.ContrastivePretraining.on_before_zero_grad","title":"on_before_zero_grad","text":"<pre><code>on_before_zero_grad(optimizer)\n</code></pre> <p>Zero out the gradients of the model.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def on_before_zero_grad(self, optimizer: torch.optim.Optimizer) -&gt; None:\n    \"\"\"Zero out the gradients of the model.\"\"\"\n    if self.auxiliary_tasks:\n        for task in self.auxiliary_tasks.values():\n            task.on_before_zero_grad(optimizer)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.ContrastivePretraining.on_validation_epoch_start","title":"on_validation_epoch_start","text":"<pre><code>on_validation_epoch_start()\n</code></pre> <p>Prepare for the validation epoch.</p> <p>This method sets the modules to evaluation mode and calls the <code>on_evaluation_epoch_start</code> method of each evaluation task.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def on_validation_epoch_start(self) -&gt; None:\n    \"\"\"Prepare for the validation epoch.\n\n    This method sets the modules to evaluation mode and calls the\n    ``on_evaluation_epoch_start`` method of each evaluation task.\n    \"\"\"\n    self._on_eval_epoch_start(\"val\")\n</code></pre>"},{"location":"api/#mmlearn.cli.run.ContrastivePretraining.validation_step","title":"validation_step","text":"<pre><code>validation_step(batch, batch_idx)\n</code></pre> <p>Run a single validation step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict[str, Tensor]</code> <p>The batch of data to process.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required <p>Returns:</p> Type Description <code>Optional[Tensor]</code> <p>The loss for the batch or <code>None</code> if the loss function is not provided.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def validation_step(\n    self, batch: dict[str, torch.Tensor], batch_idx: int\n) -&gt; Optional[torch.Tensor]:\n    \"\"\"Run a single validation step.\n\n    Parameters\n    ----------\n    batch : dict[str, torch.Tensor]\n        The batch of data to process.\n    batch_idx : int\n        The index of the batch.\n\n    Returns\n    -------\n    Optional[torch.Tensor]\n        The loss for the batch or ``None`` if the loss function is not provided.\n    \"\"\"\n    return self._shared_eval_step(batch, batch_idx, \"val\")\n</code></pre>"},{"location":"api/#mmlearn.cli.run.ContrastivePretraining.on_validation_epoch_end","title":"on_validation_epoch_end","text":"<pre><code>on_validation_epoch_end()\n</code></pre> <p>Compute and log epoch-level metrics at the end of the validation epoch.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def on_validation_epoch_end(self) -&gt; None:\n    \"\"\"Compute and log epoch-level metrics at the end of the validation epoch.\"\"\"\n    self._on_eval_epoch_end(\"val\")\n</code></pre>"},{"location":"api/#mmlearn.cli.run.ContrastivePretraining.on_test_epoch_start","title":"on_test_epoch_start","text":"<pre><code>on_test_epoch_start()\n</code></pre> <p>Prepare for the test epoch.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def on_test_epoch_start(self) -&gt; None:\n    \"\"\"Prepare for the test epoch.\"\"\"\n    self._on_eval_epoch_start(\"test\")\n</code></pre>"},{"location":"api/#mmlearn.cli.run.ContrastivePretraining.test_step","title":"test_step","text":"<pre><code>test_step(batch, batch_idx)\n</code></pre> <p>Run a single test step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict[str, Tensor]</code> <p>The batch of data to process.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required <p>Returns:</p> Type Description <code>Optional[Tensor]</code> <p>The loss for the batch or <code>None</code> if the loss function is not provided.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def test_step(\n    self, batch: dict[str, torch.Tensor], batch_idx: int\n) -&gt; Optional[torch.Tensor]:\n    \"\"\"Run a single test step.\n\n    Parameters\n    ----------\n    batch : dict[str, torch.Tensor]\n        The batch of data to process.\n    batch_idx : int\n        The index of the batch.\n\n    Returns\n    -------\n    Optional[torch.Tensor]\n        The loss for the batch or ``None`` if the loss function is not provided.\n    \"\"\"\n    return self._shared_eval_step(batch, batch_idx, \"test\")\n</code></pre>"},{"location":"api/#mmlearn.cli.run.ContrastivePretraining.on_test_epoch_end","title":"on_test_epoch_end","text":"<pre><code>on_test_epoch_end()\n</code></pre> <p>Compute and log epoch-level metrics at the end of the test epoch.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def on_test_epoch_end(self) -&gt; None:\n    \"\"\"Compute and log epoch-level metrics at the end of the test epoch.\"\"\"\n    self._on_eval_epoch_end(\"test\")\n</code></pre>"},{"location":"api/#mmlearn.cli.run.ContrastivePretraining.on_load_checkpoint","title":"on_load_checkpoint","text":"<pre><code>on_load_checkpoint(checkpoint)\n</code></pre> <p>Modify the model checkpoint after loading.</p> <p>The <code>on_load_checkpoint</code> method of auxiliary tasks is called here to allow them to modify the checkpoint after loading.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <code>Dict[str, Any]</code> <p>The loaded checkpoint.</p> required Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def on_load_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n    \"\"\"Modify the model checkpoint after loading.\n\n    The `on_load_checkpoint` method of auxiliary tasks is called here to allow\n    them to modify the checkpoint after loading.\n\n    Parameters\n    ----------\n    checkpoint : Dict[str, Any]\n        The loaded checkpoint.\n    \"\"\"\n    if self.auxiliary_tasks:\n        for task in self.auxiliary_tasks.values():\n            task.on_load_checkpoint(checkpoint)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.ContrastivePretraining.on_save_checkpoint","title":"on_save_checkpoint","text":"<pre><code>on_save_checkpoint(checkpoint)\n</code></pre> <p>Modify the checkpoint before saving.</p> <p>The <code>on_save_checkpoint</code> method of auxiliary tasks is called here to allow them to modify the checkpoint before saving.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <code>Dict[str, Any]</code> <p>The checkpoint to save.</p> required Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def on_save_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n    \"\"\"Modify the checkpoint before saving.\n\n    The `on_save_checkpoint` method of auxiliary tasks is called here to allow\n    them to modify the checkpoint before saving.\n\n    Parameters\n    ----------\n    checkpoint : Dict[str, Any]\n        The checkpoint to save.\n    \"\"\"\n    if self.auxiliary_tasks:\n        for task in self.auxiliary_tasks.values():\n            task.on_save_checkpoint(checkpoint)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.IJEPA","title":"IJEPA","text":"<p>               Bases: <code>TrainingTask</code></p> <p>Pretraining module for IJEPA.</p> <p>This class implements the IJEPA (Image Joint-Embedding Predictive Architecture) pretraining task using PyTorch Lightning. It trains an encoder and a predictor to reconstruct masked regions of an image based on its unmasked context.</p> <p>Parameters:</p> Name Type Description Default <code>encoder</code> <code>VisionTransformer</code> <p>Vision transformer encoder.</p> required <code>predictor</code> <code>VisionTransformerPredictor</code> <p>Vision transformer predictor.</p> required <code>optimizer</code> <code>Optional[partial[Optimizer]]</code> <p>The optimizer to use for training. This is expected to be a func:<code>~functools.partial</code> function that takes the model parameters as the only required argument. If not provided, training will continue without an optimizer.</p> <code>None</code> <code>lr_scheduler</code> <code>Optional[Union[dict[str, Union[partial[LRScheduler], Any]], partial[LRScheduler]]]</code> <p>The learning rate scheduler to use for training. This can be a func:<code>~functools.partial</code> function that takes the optimizer as the only required argument or a dictionary with a <code>'scheduler'</code> key that specifies the scheduler and an optional <code>'extras'</code> key that specifies additional arguments to pass to the scheduler. If not provided, the learning rate will not be adjusted during training.</p> <code>None</code> <code>ema_decay</code> <code>float</code> <p>Initial momentum for EMA of target encoder.</p> <code>0.996</code> <code>ema_decay_end</code> <code>float</code> <p>Final momentum for EMA of target encoder.</p> <code>1.0</code> <code>ema_anneal_end_step</code> <code>int</code> <p>Number of steps to anneal EMA momentum to <code>ema_decay_end</code>.</p> <code>1000</code> <code>loss_fn</code> <code>Optional[Callable[[Tensor, Tensor], Tensor]]</code> <p>Loss function to use. If not provided, defaults to func:<code>~torch.nn.functional.smooth_l1_loss</code>.</p> <code>None</code> <code>compute_validation_loss</code> <code>bool</code> <p>Whether to compute validation loss.</p> <code>True</code> <code>compute_test_loss</code> <code>bool</code> <p>Whether to compute test loss.</p> <code>True</code> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>@store(group=\"task\", provider=\"mmlearn\", zen_partial=False)\nclass IJEPA(TrainingTask):\n    \"\"\"Pretraining module for IJEPA.\n\n    This class implements the IJEPA (Image Joint-Embedding Predictive Architecture)\n    pretraining task using PyTorch Lightning. It trains an encoder and a predictor to\n    reconstruct masked regions of an image based on its unmasked context.\n\n    Parameters\n    ----------\n    encoder : VisionTransformer\n        Vision transformer encoder.\n    predictor : VisionTransformerPredictor\n        Vision transformer predictor.\n    optimizer : Optional[partial[torch.optim.Optimizer]], optional, default=None\n        The optimizer to use for training. This is expected to be a :py:func:`~functools.partial`\n        function that takes the model parameters as the only required argument.\n        If not provided, training will continue without an optimizer.\n    lr_scheduler : Optional[Union[dict[str, Union[partial[torch.optim.lr_scheduler.LRScheduler], Any]], partial[torch.optim.lr_scheduler.LRScheduler]]], optional, default=None\n        The learning rate scheduler to use for training. This can be a\n        :py:func:`~functools.partial` function that takes the optimizer as the only\n        required argument or a dictionary with a ``'scheduler'`` key that specifies\n        the scheduler and an optional ``'extras'`` key that specifies additional\n        arguments to pass to the scheduler. If not provided, the learning rate will\n        not be adjusted during training.\n    ema_decay : float, optional, default=0.996\n        Initial momentum for EMA of target encoder.\n    ema_decay_end : float, optional, default=1.0\n        Final momentum for EMA of target encoder.\n    ema_anneal_end_step : int, optional, default=1000\n        Number of steps to anneal EMA momentum to ``ema_decay_end``.\n    loss_fn : Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]], optional\n        Loss function to use. If not provided, defaults to\n        :py:func:`~torch.nn.functional.smooth_l1_loss`.\n    compute_validation_loss : bool, optional, default=True\n        Whether to compute validation loss.\n    compute_test_loss : bool, optional, default=True\n        Whether to compute test loss.\n    \"\"\"  # noqa: W505\n\n    def __init__(\n        self,\n        encoder: VisionTransformer,\n        predictor: VisionTransformerPredictor,\n        modality: str = \"RGB\",\n        optimizer: Optional[partial[torch.optim.Optimizer]] = None,\n        lr_scheduler: Optional[\n            Union[\n                dict[str, Union[partial[torch.optim.lr_scheduler.LRScheduler], Any]],\n                partial[torch.optim.lr_scheduler.LRScheduler],\n            ]\n        ] = None,\n        ema_decay: float = 0.996,\n        ema_decay_end: float = 1.0,\n        ema_anneal_end_step: int = 1000,\n        loss_fn: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None,\n        compute_validation_loss: bool = True,\n        compute_test_loss: bool = True,\n    ):\n        super().__init__(\n            optimizer=optimizer,\n            lr_scheduler=lr_scheduler,\n            loss_fn=loss_fn if loss_fn is not None else F.smooth_l1_loss,\n            compute_validation_loss=compute_validation_loss,\n            compute_test_loss=compute_test_loss,\n        )\n        self.modality = Modalities.get_modality(modality)\n        self.mask_generator = IJEPAMaskGenerator()\n\n        self.encoder = encoder\n        self.predictor = predictor\n\n        self.predictor.num_patches = encoder.patch_embed.num_patches\n        self.predictor.embed_dim = encoder.embed_dim\n        self.predictor.num_heads = encoder.num_heads\n\n        self.target_encoder = ExponentialMovingAverage(\n            self.encoder, ema_decay, ema_decay_end, ema_anneal_end_step\n        )\n\n    def configure_model(self) -&gt; None:\n        \"\"\"Configure the model.\"\"\"\n        self.target_encoder.configure_model(self.device)\n\n    def on_before_zero_grad(self, optimizer: torch.optim.Optimizer) -&gt; None:\n        \"\"\"Perform exponential moving average update of target encoder.\n\n        This is done right after the ``optimizer.step()`, which comes just before\n        ``optimizer.zero_grad()`` to account for gradient accumulation.\n        \"\"\"\n        if self.target_encoder is not None:\n            self.target_encoder.step(self.encoder)\n\n    def training_step(self, batch: dict[str, Any], batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"Perform a single training step.\n\n        Parameters\n        ----------\n        batch : dict[str, Any]\n            A batch of data.\n        batch_idx : int\n            Index of the batch.\n\n        Returns\n        -------\n        torch.Tensor\n            Loss value.\n        \"\"\"\n        return self._shared_step(batch, batch_idx, step_type=\"train\")\n\n    def validation_step(\n        self, batch: dict[str, Any], batch_idx: int\n    ) -&gt; Optional[torch.Tensor]:\n        \"\"\"Run a single validation step.\n\n        Parameters\n        ----------\n        batch : dict[str, Any]\n            A batch of data.\n        batch_idx : int\n            Index of the batch.\n\n        Returns\n        -------\n        Optional[torch.Tensor]\n            Loss value or ``None`` if no loss is computed.\n        \"\"\"\n        return self._shared_step(batch, batch_idx, step_type=\"val\")\n\n    def test_step(\n        self, batch: dict[str, Any], batch_idx: int\n    ) -&gt; Optional[torch.Tensor]:\n        \"\"\"Run a single test step.\n\n        Parameters\n        ----------\n        batch : dict[str, Any]\n            A batch of data.\n        batch_idx : int\n            Index of the batch.\n\n        Returns\n        -------\n        Optional[torch.Tensor]\n            Loss value or ``None`` if no loss is computed\n        \"\"\"\n        return self._shared_step(batch, batch_idx, step_type=\"test\")\n\n    def on_validation_epoch_start(self) -&gt; None:\n        \"\"\"Prepare for the validation epoch.\"\"\"\n        self._on_eval_epoch_start(\"val\")\n\n    def on_validation_epoch_end(self) -&gt; None:\n        \"\"\"Actions at the end of the validation epoch.\"\"\"\n        self._on_eval_epoch_end(\"val\")\n\n    def on_test_epoch_start(self) -&gt; None:\n        \"\"\"Prepare for the test epoch.\"\"\"\n        self._on_eval_epoch_start(\"test\")\n\n    def on_test_epoch_end(self) -&gt; None:\n        \"\"\"Actions at the end of the test epoch.\"\"\"\n        self._on_eval_epoch_end(\"test\")\n\n    def on_save_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n        \"\"\"Add relevant EMA state to the checkpoint.\n\n        Parameters\n        ----------\n        checkpoint : dict[str, Any]\n            The state dictionary to save the EMA state to.\n        \"\"\"\n        if self.target_encoder is not None:\n            checkpoint[\"ema_params\"] = {\n                \"decay\": self.target_encoder.decay,\n                \"num_updates\": self.target_encoder.num_updates,\n            }\n\n    def on_load_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n        \"\"\"Restore EMA state from the checkpoint.\n\n        Parameters\n        ----------\n        checkpoint : dict[str, Any]\n            The state dictionary to restore the EMA state from.\n        \"\"\"\n        if \"ema_params\" in checkpoint and self.target_encoder is not None:\n            ema_params = checkpoint.pop(\"ema_params\")\n            self.target_encoder.decay = ema_params[\"decay\"]\n            self.target_encoder.num_updates = ema_params[\"num_updates\"]\n\n            self.target_encoder.restore(self.encoder)\n\n    def _shared_step(\n        self, batch: dict[str, Any], batch_idx: int, step_type: str\n    ) -&gt; Optional[torch.Tensor]:\n        images = batch[self.modality.name]\n\n        # Generate masks\n        batch_size = images.size(0)\n        mask_info = self.mask_generator(batch_size=batch_size)\n\n        # Extract masks and move to device\n        device = images.device\n        encoder_masks = [mask.to(device) for mask in mask_info[\"encoder_masks\"]]\n        predictor_masks = [mask.to(device) for mask in mask_info[\"predictor_masks\"]]\n\n        # Forward pass through target encoder to get h\n        with torch.no_grad():\n            h = self.target_encoder.model(batch)[0]\n            h = F.layer_norm(h, h.size()[-1:])\n            h_masked = apply_masks(h, predictor_masks)\n            h_masked = repeat_interleave_batch(\n                h_masked, images.size(0), repeat=len(encoder_masks)\n            )\n\n        # Forward pass through encoder with encoder_masks\n        batch[self.modality.mask] = encoder_masks\n        z = self.encoder(batch)[0]\n\n        # Pass z through predictor with encoder_masks and predictor_masks\n        z_pred = self.predictor(z, encoder_masks, predictor_masks)\n\n        if step_type == \"train\":\n            self.log(\"train/ema_decay\", self.target_encoder.decay, prog_bar=True)\n\n        if self.loss_fn is not None and (\n            step_type == \"train\"\n            or (step_type == \"val\" and self.compute_validation_loss)\n            or (step_type == \"test\" and self.compute_test_loss)\n        ):\n            # Compute loss between z_pred and h_masked\n            loss = self.loss_fn(z_pred, h_masked)\n\n            # Log loss\n            self.log(f\"{step_type}/loss\", loss, prog_bar=True, sync_dist=True)\n\n            return loss\n\n        return None\n\n    def _on_eval_epoch_start(self, step_type: str) -&gt; None:\n        \"\"\"Initialize states or configurations at the start of an evaluation epoch.\n\n        Parameters\n        ----------\n        step_type : str\n            Type of the evaluation phase (\"val\" or \"test\").\n        \"\"\"\n        if (\n            step_type == \"val\"\n            and self.compute_validation_loss\n            or step_type == \"test\"\n            and self.compute_test_loss\n        ):\n            self.log(f\"{step_type}/start\", 1, prog_bar=True, sync_dist=True)\n\n    def _on_eval_epoch_end(self, step_type: str) -&gt; None:\n        \"\"\"Finalize states or logging at the end of an evaluation epoch.\n\n        Parameters\n        ----------\n        step_type : str\n            Type of the evaluation phase (\"val\" or \"test\").\n        \"\"\"\n        if (\n            step_type == \"val\"\n            and self.compute_validation_loss\n            or step_type == \"test\"\n            and self.compute_test_loss\n        ):\n            self.log(f\"{step_type}/end\", 1, prog_bar=True, sync_dist=True)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.IJEPA.configure_model","title":"configure_model","text":"<pre><code>configure_model()\n</code></pre> <p>Configure the model.</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def configure_model(self) -&gt; None:\n    \"\"\"Configure the model.\"\"\"\n    self.target_encoder.configure_model(self.device)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.IJEPA.on_before_zero_grad","title":"on_before_zero_grad","text":"<pre><code>on_before_zero_grad(optimizer)\n</code></pre> <p>Perform exponential moving average update of target encoder.</p> <p>This is done right after the <code>optimizer.step()`, which comes just before</code>optimizer.zero_grad()`` to account for gradient accumulation.</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def on_before_zero_grad(self, optimizer: torch.optim.Optimizer) -&gt; None:\n    \"\"\"Perform exponential moving average update of target encoder.\n\n    This is done right after the ``optimizer.step()`, which comes just before\n    ``optimizer.zero_grad()`` to account for gradient accumulation.\n    \"\"\"\n    if self.target_encoder is not None:\n        self.target_encoder.step(self.encoder)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.IJEPA.training_step","title":"training_step","text":"<pre><code>training_step(batch, batch_idx)\n</code></pre> <p>Perform a single training step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict[str, Any]</code> <p>A batch of data.</p> required <code>batch_idx</code> <code>int</code> <p>Index of the batch.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Loss value.</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def training_step(self, batch: dict[str, Any], batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"Perform a single training step.\n\n    Parameters\n    ----------\n    batch : dict[str, Any]\n        A batch of data.\n    batch_idx : int\n        Index of the batch.\n\n    Returns\n    -------\n    torch.Tensor\n        Loss value.\n    \"\"\"\n    return self._shared_step(batch, batch_idx, step_type=\"train\")\n</code></pre>"},{"location":"api/#mmlearn.cli.run.IJEPA.validation_step","title":"validation_step","text":"<pre><code>validation_step(batch, batch_idx)\n</code></pre> <p>Run a single validation step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict[str, Any]</code> <p>A batch of data.</p> required <code>batch_idx</code> <code>int</code> <p>Index of the batch.</p> required <p>Returns:</p> Type Description <code>Optional[Tensor]</code> <p>Loss value or <code>None</code> if no loss is computed.</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def validation_step(\n    self, batch: dict[str, Any], batch_idx: int\n) -&gt; Optional[torch.Tensor]:\n    \"\"\"Run a single validation step.\n\n    Parameters\n    ----------\n    batch : dict[str, Any]\n        A batch of data.\n    batch_idx : int\n        Index of the batch.\n\n    Returns\n    -------\n    Optional[torch.Tensor]\n        Loss value or ``None`` if no loss is computed.\n    \"\"\"\n    return self._shared_step(batch, batch_idx, step_type=\"val\")\n</code></pre>"},{"location":"api/#mmlearn.cli.run.IJEPA.test_step","title":"test_step","text":"<pre><code>test_step(batch, batch_idx)\n</code></pre> <p>Run a single test step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict[str, Any]</code> <p>A batch of data.</p> required <code>batch_idx</code> <code>int</code> <p>Index of the batch.</p> required <p>Returns:</p> Type Description <code>Optional[Tensor]</code> <p>Loss value or <code>None</code> if no loss is computed</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def test_step(\n    self, batch: dict[str, Any], batch_idx: int\n) -&gt; Optional[torch.Tensor]:\n    \"\"\"Run a single test step.\n\n    Parameters\n    ----------\n    batch : dict[str, Any]\n        A batch of data.\n    batch_idx : int\n        Index of the batch.\n\n    Returns\n    -------\n    Optional[torch.Tensor]\n        Loss value or ``None`` if no loss is computed\n    \"\"\"\n    return self._shared_step(batch, batch_idx, step_type=\"test\")\n</code></pre>"},{"location":"api/#mmlearn.cli.run.IJEPA.on_validation_epoch_start","title":"on_validation_epoch_start","text":"<pre><code>on_validation_epoch_start()\n</code></pre> <p>Prepare for the validation epoch.</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def on_validation_epoch_start(self) -&gt; None:\n    \"\"\"Prepare for the validation epoch.\"\"\"\n    self._on_eval_epoch_start(\"val\")\n</code></pre>"},{"location":"api/#mmlearn.cli.run.IJEPA.on_validation_epoch_end","title":"on_validation_epoch_end","text":"<pre><code>on_validation_epoch_end()\n</code></pre> <p>Actions at the end of the validation epoch.</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def on_validation_epoch_end(self) -&gt; None:\n    \"\"\"Actions at the end of the validation epoch.\"\"\"\n    self._on_eval_epoch_end(\"val\")\n</code></pre>"},{"location":"api/#mmlearn.cli.run.IJEPA.on_test_epoch_start","title":"on_test_epoch_start","text":"<pre><code>on_test_epoch_start()\n</code></pre> <p>Prepare for the test epoch.</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def on_test_epoch_start(self) -&gt; None:\n    \"\"\"Prepare for the test epoch.\"\"\"\n    self._on_eval_epoch_start(\"test\")\n</code></pre>"},{"location":"api/#mmlearn.cli.run.IJEPA.on_test_epoch_end","title":"on_test_epoch_end","text":"<pre><code>on_test_epoch_end()\n</code></pre> <p>Actions at the end of the test epoch.</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def on_test_epoch_end(self) -&gt; None:\n    \"\"\"Actions at the end of the test epoch.\"\"\"\n    self._on_eval_epoch_end(\"test\")\n</code></pre>"},{"location":"api/#mmlearn.cli.run.IJEPA.on_save_checkpoint","title":"on_save_checkpoint","text":"<pre><code>on_save_checkpoint(checkpoint)\n</code></pre> <p>Add relevant EMA state to the checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <code>dict[str, Any]</code> <p>The state dictionary to save the EMA state to.</p> required Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def on_save_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n    \"\"\"Add relevant EMA state to the checkpoint.\n\n    Parameters\n    ----------\n    checkpoint : dict[str, Any]\n        The state dictionary to save the EMA state to.\n    \"\"\"\n    if self.target_encoder is not None:\n        checkpoint[\"ema_params\"] = {\n            \"decay\": self.target_encoder.decay,\n            \"num_updates\": self.target_encoder.num_updates,\n        }\n</code></pre>"},{"location":"api/#mmlearn.cli.run.IJEPA.on_load_checkpoint","title":"on_load_checkpoint","text":"<pre><code>on_load_checkpoint(checkpoint)\n</code></pre> <p>Restore EMA state from the checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <code>dict[str, Any]</code> <p>The state dictionary to restore the EMA state from.</p> required Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def on_load_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n    \"\"\"Restore EMA state from the checkpoint.\n\n    Parameters\n    ----------\n    checkpoint : dict[str, Any]\n        The state dictionary to restore the EMA state from.\n    \"\"\"\n    if \"ema_params\" in checkpoint and self.target_encoder is not None:\n        ema_params = checkpoint.pop(\"ema_params\")\n        self.target_encoder.decay = ema_params[\"decay\"]\n        self.target_encoder.num_updates = ema_params[\"num_updates\"]\n\n        self.target_encoder.restore(self.encoder)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.ZeroShotClassification","title":"ZeroShotClassification","text":"<p>               Bases: <code>EvaluationHooks</code></p> <p>Zero-shot classification evaluation task.</p> <p>This task evaluates the zero-shot classification performance.</p> <p>Parameters:</p> Name Type Description Default <code>task_specs</code> <code>list[ClassificationTaskSpec]</code> <p>A list of classification task specifications.</p> required <code>tokenizer</code> <code>Callable[[Union[str, list[str]]], Union[Tensor, dict[str, Tensor]]]</code> <p>A function to tokenize text inputs.</p> required Source code in <code>mmlearn/tasks/zero_shot_classification.py</code> <pre><code>@store(group=\"eval_task\", provider=\"mmlearn\")\nclass ZeroShotClassification(EvaluationHooks):\n    \"\"\"Zero-shot classification evaluation task.\n\n    This task evaluates the zero-shot classification performance.\n\n    Parameters\n    ----------\n    task_specs : list[ClassificationTaskSpec]\n        A list of classification task specifications.\n    tokenizer : Callable[[Union[str, list[str]]], Union[torch.Tensor, dict[str, torch.Tensor]]]\n        A function to tokenize text inputs.\n    \"\"\"  # noqa: W505\n\n    def __init__(\n        self,\n        task_specs: list[ClassificationTaskSpec],\n        tokenizer: Callable[\n            [Union[str, list[str]]], Union[torch.Tensor, dict[str, torch.Tensor]]\n        ],\n    ) -&gt; None:\n        super().__init__()\n        self.tokenizer = tokenizer\n        self.task_specs = task_specs\n        for spec in self.task_specs:\n            assert Modalities.has_modality(spec.query_modality)\n\n        self.metrics: dict[tuple[str, int], MetricCollection] = {}\n        self._embeddings_store: dict[int, torch.Tensor] = {}\n\n    def on_evaluation_epoch_start(self, pl_module: LightningModule) -&gt; None:\n        \"\"\"Set up the evaluation task.\n\n        Parameters\n        ----------\n        pl_module : pl.LightningModule\n            A reference to the Lightning module being evaluated.\n\n        Raises\n        ------\n        ValueError\n            - If the task is not being run for validation or testing.\n            - If the dataset does not have the required attributes to perform zero-shot\n              classification (i.e ``id2label`` and ``zero_shot_prompt_templates``).\n        \"\"\"\n        if pl_module.trainer.validating:\n            eval_dataset: CombinedDataset = pl_module.trainer.val_dataloaders.dataset\n        elif pl_module.trainer.testing:\n            eval_dataset = pl_module.trainer.test_dataloaders.dataset\n        else:\n            raise ValueError(\n                \"ZeroShotClassification task is only supported for validation and testing.\"\n            )\n\n        self.all_dataset_info = {}\n\n        # create metrics for each dataset/query_modality combination\n        if not self.metrics:\n            for dataset_index, dataset in enumerate(eval_dataset.datasets):\n                dataset_name = getattr(dataset, \"name\", dataset.__class__.__name__)\n                try:\n                    id2label: dict[int, str] = dataset.id2label\n                except AttributeError:\n                    raise ValueError(\n                        f\"Dataset '{dataset_name}' must have a `id2label` attribute \"\n                        \"to perform zero-shot classification.\"\n                    ) from None\n\n                try:\n                    zero_shot_prompt_templates: list[str] = (\n                        dataset.zero_shot_prompt_templates\n                    )\n                except AttributeError:\n                    raise ValueError(\n                        \"Dataset must have a `zero_shot_prompt_templates` attribute to perform zero-shot classification.\"\n                    ) from None\n\n                num_classes = len(id2label)\n\n                self.all_dataset_info[dataset_index] = {\n                    \"name\": dataset_name,\n                    \"id2label\": id2label,\n                    \"prompt_templates\": zero_shot_prompt_templates,\n                    \"num_classes\": num_classes,\n                }\n\n                for spec in self.task_specs:\n                    query_modality = Modalities.get_modality(spec.query_modality).name\n                    self.metrics[(query_modality, dataset_index)] = (\n                        self._create_metrics(\n                            num_classes,\n                            spec.top_k,\n                            prefix=f\"{dataset_name}/{query_modality}_\",\n                            postfix=\"\",\n                        )\n                    )\n\n        for metric in self.metrics.values():\n            metric.to(pl_module.device)\n\n        for dataset_index, dataset_info in self.all_dataset_info.items():\n            id2label = dataset_info[\"id2label\"]\n            prompt_templates: list[str] = dataset_info[\"prompt_templates\"]\n            labels = list(id2label.values())\n\n            with torch.no_grad():\n                chunk_size = 10\n                all_embeddings = []\n\n                for i in tqdm(\n                    range(0, len(labels), chunk_size),\n                    desc=\"Encoding class descriptions\",\n                ):\n                    batch_labels = labels[i : min(i + chunk_size, len(labels))]\n                    descriptions = [\n                        template.format(label)\n                        for label in batch_labels\n                        for template in prompt_templates\n                    ]\n                    tokenized_descriptions = move_data_to_device(\n                        self.tokenizer(descriptions),\n                        pl_module.device,\n                    )\n\n                    # Encode the chunk using the pl_module's encode method\n                    chunk_embeddings = pl_module.encode(\n                        tokenized_descriptions, Modalities.TEXT\n                    )  # shape: [chunk_size x len(prompt_templates), embed_dim]\n                    chunk_embeddings /= chunk_embeddings.norm(p=2, dim=-1, keepdim=True)\n                    chunk_embeddings = chunk_embeddings.reshape(\n                        len(batch_labels), len(prompt_templates), -1\n                    ).mean(dim=1)  # shape: [chunk_size, embed_dim]\n                    chunk_embeddings /= chunk_embeddings.norm(p=2, dim=-1, keepdim=True)\n\n                    # Append the chunk embeddings to the list\n                    all_embeddings.append(chunk_embeddings)\n\n                # Concatenate all chunk embeddings into a single tensor\n                class_embeddings = torch.cat(all_embeddings, dim=0)\n\n            self._embeddings_store[dataset_index] = class_embeddings\n\n    def evaluation_step(\n        self, pl_module: LightningModule, batch: dict[str, torch.Tensor], batch_idx: int\n    ) -&gt; None:\n        \"\"\"Compute logits and update metrics.\n\n        Parameters\n        ----------\n        pl_module : pl.LightningModule\n            A reference to the Lightning module being evaluated.\n        batch : dict[str, torch.Tensor]\n            A batch of data.\n        batch_idx : int\n            The index of the batch.\n        \"\"\"\n        if pl_module.trainer.sanity_checking:\n            return\n\n        for (query_modality, dataset_index), metric_collection in self.metrics.items():\n            matching_indices = torch.where(batch[\"dataset_index\"] == dataset_index)[0]\n\n            if not matching_indices.numel():\n                continue\n\n            class_embeddings = self._embeddings_store[dataset_index]\n            query_embeddings: torch.Tensor = pl_module.encode(\n                batch, Modalities.get_modality(query_modality)\n            )\n            query_embeddings /= query_embeddings.norm(p=2, dim=-1, keepdim=True)\n            query_embeddings = query_embeddings[matching_indices]\n\n            if self.all_dataset_info[dataset_index][\"num_classes\"] == 2:\n                softmax_output = _safe_matmul(\n                    query_embeddings, class_embeddings\n                ).softmax(dim=-1)\n                logits = softmax_output[:, 1] - softmax_output[:, 0]\n            else:\n                logits = 100.0 * _safe_matmul(query_embeddings, class_embeddings)\n            targets = batch[Modalities.get_modality(query_modality).target][\n                matching_indices\n            ]\n\n            metric_collection.update(logits, targets)\n\n    def on_evaluation_epoch_end(self, pl_module: LightningModule) -&gt; dict[str, Any]:\n        \"\"\"Compute and reset metrics.\n\n        Parameters\n        ----------\n        pl_module : pl.LightningModule\n            A reference to the Lightning module being evaluated.\n\n        Returns\n        -------\n        dict[str, Any]\n            The computed metrics.\n        \"\"\"\n        results = {}\n        for metric_collection in self.metrics.values():\n            results.update(metric_collection.compute())\n            metric_collection.reset()\n\n        self._embeddings_store.clear()\n\n        eval_type = \"val\" if pl_module.trainer.validating else \"test\"\n        for key, value in results.items():\n            pl_module.log(f\"{eval_type}/{key}\", value)\n\n        return results\n\n    @staticmethod\n    def _create_metrics(\n        num_classes: int, top_k: list[int], prefix: str, postfix: str\n    ) -&gt; MetricCollection:\n        \"\"\"Create a collection of classification metrics.\"\"\"\n        task_type = \"binary\" if num_classes == 2 else \"multiclass\"\n        acc_metrics = (\n            {\n                f\"top{k}_accuracy\": Accuracy(\n                    task=task_type, num_classes=num_classes, top_k=k, average=\"micro\"\n                )\n                for k in top_k\n            }\n            if num_classes &gt; 2\n            else {\"accuracy\": Accuracy(task=task_type, num_classes=num_classes)}\n        )\n        return MetricCollection(\n            {\n                \"precision\": Precision(\n                    task=task_type,\n                    num_classes=num_classes,\n                    average=\"macro\" if num_classes &gt; 2 else \"micro\",\n                ),\n                \"recall\": Recall(\n                    task=task_type,\n                    num_classes=num_classes,\n                    average=\"macro\" if num_classes &gt; 2 else \"micro\",\n                ),\n                \"f1_score_macro\": F1Score(\n                    task=task_type,\n                    num_classes=num_classes,\n                    average=\"macro\" if num_classes &gt; 2 else \"micro\",\n                ),\n                \"aucroc\": AUROC(task=task_type, num_classes=num_classes),\n                **acc_metrics,\n            },\n            prefix=prefix,\n            postfix=postfix,\n            compute_groups=True,\n        )\n</code></pre>"},{"location":"api/#mmlearn.cli.run.ZeroShotClassification.on_evaluation_epoch_start","title":"on_evaluation_epoch_start","text":"<pre><code>on_evaluation_epoch_start(pl_module)\n</code></pre> <p>Set up the evaluation task.</p> <p>Parameters:</p> Name Type Description Default <code>pl_module</code> <code>LightningModule</code> <p>A reference to the Lightning module being evaluated.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <ul> <li>If the task is not being run for validation or testing.</li> <li>If the dataset does not have the required attributes to perform zero-shot   classification (i.e <code>id2label</code> and <code>zero_shot_prompt_templates</code>).</li> </ul> Source code in <code>mmlearn/tasks/zero_shot_classification.py</code> <pre><code>def on_evaluation_epoch_start(self, pl_module: LightningModule) -&gt; None:\n    \"\"\"Set up the evaluation task.\n\n    Parameters\n    ----------\n    pl_module : pl.LightningModule\n        A reference to the Lightning module being evaluated.\n\n    Raises\n    ------\n    ValueError\n        - If the task is not being run for validation or testing.\n        - If the dataset does not have the required attributes to perform zero-shot\n          classification (i.e ``id2label`` and ``zero_shot_prompt_templates``).\n    \"\"\"\n    if pl_module.trainer.validating:\n        eval_dataset: CombinedDataset = pl_module.trainer.val_dataloaders.dataset\n    elif pl_module.trainer.testing:\n        eval_dataset = pl_module.trainer.test_dataloaders.dataset\n    else:\n        raise ValueError(\n            \"ZeroShotClassification task is only supported for validation and testing.\"\n        )\n\n    self.all_dataset_info = {}\n\n    # create metrics for each dataset/query_modality combination\n    if not self.metrics:\n        for dataset_index, dataset in enumerate(eval_dataset.datasets):\n            dataset_name = getattr(dataset, \"name\", dataset.__class__.__name__)\n            try:\n                id2label: dict[int, str] = dataset.id2label\n            except AttributeError:\n                raise ValueError(\n                    f\"Dataset '{dataset_name}' must have a `id2label` attribute \"\n                    \"to perform zero-shot classification.\"\n                ) from None\n\n            try:\n                zero_shot_prompt_templates: list[str] = (\n                    dataset.zero_shot_prompt_templates\n                )\n            except AttributeError:\n                raise ValueError(\n                    \"Dataset must have a `zero_shot_prompt_templates` attribute to perform zero-shot classification.\"\n                ) from None\n\n            num_classes = len(id2label)\n\n            self.all_dataset_info[dataset_index] = {\n                \"name\": dataset_name,\n                \"id2label\": id2label,\n                \"prompt_templates\": zero_shot_prompt_templates,\n                \"num_classes\": num_classes,\n            }\n\n            for spec in self.task_specs:\n                query_modality = Modalities.get_modality(spec.query_modality).name\n                self.metrics[(query_modality, dataset_index)] = (\n                    self._create_metrics(\n                        num_classes,\n                        spec.top_k,\n                        prefix=f\"{dataset_name}/{query_modality}_\",\n                        postfix=\"\",\n                    )\n                )\n\n    for metric in self.metrics.values():\n        metric.to(pl_module.device)\n\n    for dataset_index, dataset_info in self.all_dataset_info.items():\n        id2label = dataset_info[\"id2label\"]\n        prompt_templates: list[str] = dataset_info[\"prompt_templates\"]\n        labels = list(id2label.values())\n\n        with torch.no_grad():\n            chunk_size = 10\n            all_embeddings = []\n\n            for i in tqdm(\n                range(0, len(labels), chunk_size),\n                desc=\"Encoding class descriptions\",\n            ):\n                batch_labels = labels[i : min(i + chunk_size, len(labels))]\n                descriptions = [\n                    template.format(label)\n                    for label in batch_labels\n                    for template in prompt_templates\n                ]\n                tokenized_descriptions = move_data_to_device(\n                    self.tokenizer(descriptions),\n                    pl_module.device,\n                )\n\n                # Encode the chunk using the pl_module's encode method\n                chunk_embeddings = pl_module.encode(\n                    tokenized_descriptions, Modalities.TEXT\n                )  # shape: [chunk_size x len(prompt_templates), embed_dim]\n                chunk_embeddings /= chunk_embeddings.norm(p=2, dim=-1, keepdim=True)\n                chunk_embeddings = chunk_embeddings.reshape(\n                    len(batch_labels), len(prompt_templates), -1\n                ).mean(dim=1)  # shape: [chunk_size, embed_dim]\n                chunk_embeddings /= chunk_embeddings.norm(p=2, dim=-1, keepdim=True)\n\n                # Append the chunk embeddings to the list\n                all_embeddings.append(chunk_embeddings)\n\n            # Concatenate all chunk embeddings into a single tensor\n            class_embeddings = torch.cat(all_embeddings, dim=0)\n\n        self._embeddings_store[dataset_index] = class_embeddings\n</code></pre>"},{"location":"api/#mmlearn.cli.run.ZeroShotClassification.evaluation_step","title":"evaluation_step","text":"<pre><code>evaluation_step(pl_module, batch, batch_idx)\n</code></pre> <p>Compute logits and update metrics.</p> <p>Parameters:</p> Name Type Description Default <code>pl_module</code> <code>LightningModule</code> <p>A reference to the Lightning module being evaluated.</p> required <code>batch</code> <code>dict[str, Tensor]</code> <p>A batch of data.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required Source code in <code>mmlearn/tasks/zero_shot_classification.py</code> <pre><code>def evaluation_step(\n    self, pl_module: LightningModule, batch: dict[str, torch.Tensor], batch_idx: int\n) -&gt; None:\n    \"\"\"Compute logits and update metrics.\n\n    Parameters\n    ----------\n    pl_module : pl.LightningModule\n        A reference to the Lightning module being evaluated.\n    batch : dict[str, torch.Tensor]\n        A batch of data.\n    batch_idx : int\n        The index of the batch.\n    \"\"\"\n    if pl_module.trainer.sanity_checking:\n        return\n\n    for (query_modality, dataset_index), metric_collection in self.metrics.items():\n        matching_indices = torch.where(batch[\"dataset_index\"] == dataset_index)[0]\n\n        if not matching_indices.numel():\n            continue\n\n        class_embeddings = self._embeddings_store[dataset_index]\n        query_embeddings: torch.Tensor = pl_module.encode(\n            batch, Modalities.get_modality(query_modality)\n        )\n        query_embeddings /= query_embeddings.norm(p=2, dim=-1, keepdim=True)\n        query_embeddings = query_embeddings[matching_indices]\n\n        if self.all_dataset_info[dataset_index][\"num_classes\"] == 2:\n            softmax_output = _safe_matmul(\n                query_embeddings, class_embeddings\n            ).softmax(dim=-1)\n            logits = softmax_output[:, 1] - softmax_output[:, 0]\n        else:\n            logits = 100.0 * _safe_matmul(query_embeddings, class_embeddings)\n        targets = batch[Modalities.get_modality(query_modality).target][\n            matching_indices\n        ]\n\n        metric_collection.update(logits, targets)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.ZeroShotClassification.on_evaluation_epoch_end","title":"on_evaluation_epoch_end","text":"<pre><code>on_evaluation_epoch_end(pl_module)\n</code></pre> <p>Compute and reset metrics.</p> <p>Parameters:</p> Name Type Description Default <code>pl_module</code> <code>LightningModule</code> <p>A reference to the Lightning module being evaluated.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>The computed metrics.</p> Source code in <code>mmlearn/tasks/zero_shot_classification.py</code> <pre><code>def on_evaluation_epoch_end(self, pl_module: LightningModule) -&gt; dict[str, Any]:\n    \"\"\"Compute and reset metrics.\n\n    Parameters\n    ----------\n    pl_module : pl.LightningModule\n        A reference to the Lightning module being evaluated.\n\n    Returns\n    -------\n    dict[str, Any]\n        The computed metrics.\n    \"\"\"\n    results = {}\n    for metric_collection in self.metrics.values():\n        results.update(metric_collection.compute())\n        metric_collection.reset()\n\n    self._embeddings_store.clear()\n\n    eval_type = \"val\" if pl_module.trainer.validating else \"test\"\n    for key, value in results.items():\n        pl_module.log(f\"{eval_type}/{key}\", value)\n\n    return results\n</code></pre>"},{"location":"api/#mmlearn.cli.run.ZeroShotCrossModalRetrieval","title":"ZeroShotCrossModalRetrieval","text":"<p>               Bases: <code>EvaluationHooks</code></p> <p>Zero-shot cross-modal retrieval evaluation task.</p> <p>This task evaluates the retrieval performance of a model on a set of query-target pairs. The model is expected to produce embeddings for both the query and target modalities. The task computes the retrieval recall at <code>k</code> for each pair of modalities.</p> <p>Parameters:</p> Name Type Description Default <code>task_specs</code> <code>list[RetrievalTaskSpec]</code> <p>A list of retrieval task specifications. Each specification defines the query and target modalities, as well as the top-k values for which to compute the retrieval recall metrics.</p> required Source code in <code>mmlearn/tasks/zero_shot_retrieval.py</code> <pre><code>@store(group=\"eval_task\", provider=\"mmlearn\")\nclass ZeroShotCrossModalRetrieval(EvaluationHooks):\n    \"\"\"Zero-shot cross-modal retrieval evaluation task.\n\n    This task evaluates the retrieval performance of a model on a set of query-target\n    pairs. The model is expected to produce embeddings for both the query and target\n    modalities. The task computes the retrieval recall at `k` for each pair of\n    modalities.\n\n    Parameters\n    ----------\n    task_specs : list[RetrievalTaskSpec]\n        A list of retrieval task specifications. Each specification defines the query\n        and target modalities, as well as the top-k values for which to compute the\n        retrieval recall metrics.\n\n    \"\"\"\n\n    def __init__(self, task_specs: list[RetrievalTaskSpec]) -&gt; None:\n        super().__init__()\n\n        self.task_specs = task_specs\n        self.metrics: dict[tuple[str, str], MetricCollection] = {}\n        self._available_modalities = set()\n\n        for spec in self.task_specs:\n            query_modality = spec.query_modality\n            target_modality = spec.target_modality\n            assert Modalities.has_modality(query_modality)\n            assert Modalities.has_modality(target_modality)\n\n            self.metrics[(query_modality, target_modality)] = MetricCollection(\n                {\n                    f\"{query_modality}_to_{target_modality}_R@{k}\": RetrievalRecallAtK(\n                        top_k=k, aggregation=\"mean\", reduction=\"none\"\n                    )\n                    for k in spec.top_k\n                }\n            )\n            self._available_modalities.add(query_modality)\n            self._available_modalities.add(target_modality)\n\n    def on_evaluation_epoch_start(self, pl_module: pl.LightningModule) -&gt; None:\n        \"\"\"Move the metrics to the device of the Lightning module.\"\"\"\n        for metric in self.metrics.values():\n            metric.to(pl_module.device)\n\n    def evaluation_step(\n        self,\n        pl_module: pl.LightningModule,\n        batch: dict[str, torch.Tensor],\n        batch_idx: int,\n    ) -&gt; None:\n        \"\"\"Run the forward pass and update retrieval recall metrics.\n\n        Parameters\n        ----------\n        pl_module : pl.LightningModule\n            A reference to the Lightning module being evaluated.\n        batch : dict[str, torch.Tensor]\n            A dictionary of batched input tensors.\n        batch_idx : int\n            The index of the batch.\n\n        \"\"\"\n        if pl_module.trainer.sanity_checking:\n            return\n\n        outputs: dict[str, Any] = {}\n        for modality_name in self._available_modalities:\n            if modality_name in batch:\n                outputs[modality_name] = pl_module.encode(\n                    batch, Modalities.get_modality(modality_name), normalize=False\n                )\n        for (query_modality, target_modality), metric in self.metrics.items():\n            if query_modality not in outputs or target_modality not in outputs:\n                continue\n            query_embeddings: torch.Tensor = outputs[query_modality]\n            target_embeddings: torch.Tensor = outputs[target_modality]\n            indexes = torch.arange(query_embeddings.size(0), device=pl_module.device)\n\n            metric.update(query_embeddings, target_embeddings, indexes)\n\n    def on_evaluation_epoch_end(\n        self, pl_module: pl.LightningModule\n    ) -&gt; Optional[dict[str, Any]]:\n        \"\"\"Compute the retrieval recall metrics.\n\n        Parameters\n        ----------\n        pl_module : pl.LightningModule\n            A reference to the Lightning module being evaluated.\n\n        Returns\n        -------\n        Optional[dict[str, Any]]\n            A dictionary of evaluation results or `None` if no results are available.\n        \"\"\"\n        if pl_module.trainer.sanity_checking:\n            return None\n\n        results = {}\n        for metric in self.metrics.values():\n            results.update(metric.compute())\n            metric.reset()\n\n        eval_type = \"val\" if pl_module.trainer.validating else \"test\"\n\n        for key, value in results.items():\n            pl_module.log(f\"{eval_type}/{key}\", value)\n\n        return results\n</code></pre>"},{"location":"api/#mmlearn.cli.run.ZeroShotCrossModalRetrieval.on_evaluation_epoch_start","title":"on_evaluation_epoch_start","text":"<pre><code>on_evaluation_epoch_start(pl_module)\n</code></pre> <p>Move the metrics to the device of the Lightning module.</p> Source code in <code>mmlearn/tasks/zero_shot_retrieval.py</code> <pre><code>def on_evaluation_epoch_start(self, pl_module: pl.LightningModule) -&gt; None:\n    \"\"\"Move the metrics to the device of the Lightning module.\"\"\"\n    for metric in self.metrics.values():\n        metric.to(pl_module.device)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.ZeroShotCrossModalRetrieval.evaluation_step","title":"evaluation_step","text":"<pre><code>evaluation_step(pl_module, batch, batch_idx)\n</code></pre> <p>Run the forward pass and update retrieval recall metrics.</p> <p>Parameters:</p> Name Type Description Default <code>pl_module</code> <code>LightningModule</code> <p>A reference to the Lightning module being evaluated.</p> required <code>batch</code> <code>dict[str, Tensor]</code> <p>A dictionary of batched input tensors.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required Source code in <code>mmlearn/tasks/zero_shot_retrieval.py</code> <pre><code>def evaluation_step(\n    self,\n    pl_module: pl.LightningModule,\n    batch: dict[str, torch.Tensor],\n    batch_idx: int,\n) -&gt; None:\n    \"\"\"Run the forward pass and update retrieval recall metrics.\n\n    Parameters\n    ----------\n    pl_module : pl.LightningModule\n        A reference to the Lightning module being evaluated.\n    batch : dict[str, torch.Tensor]\n        A dictionary of batched input tensors.\n    batch_idx : int\n        The index of the batch.\n\n    \"\"\"\n    if pl_module.trainer.sanity_checking:\n        return\n\n    outputs: dict[str, Any] = {}\n    for modality_name in self._available_modalities:\n        if modality_name in batch:\n            outputs[modality_name] = pl_module.encode(\n                batch, Modalities.get_modality(modality_name), normalize=False\n            )\n    for (query_modality, target_modality), metric in self.metrics.items():\n        if query_modality not in outputs or target_modality not in outputs:\n            continue\n        query_embeddings: torch.Tensor = outputs[query_modality]\n        target_embeddings: torch.Tensor = outputs[target_modality]\n        indexes = torch.arange(query_embeddings.size(0), device=pl_module.device)\n\n        metric.update(query_embeddings, target_embeddings, indexes)\n</code></pre>"},{"location":"api/#mmlearn.cli.run.ZeroShotCrossModalRetrieval.on_evaluation_epoch_end","title":"on_evaluation_epoch_end","text":"<pre><code>on_evaluation_epoch_end(pl_module)\n</code></pre> <p>Compute the retrieval recall metrics.</p> <p>Parameters:</p> Name Type Description Default <code>pl_module</code> <code>LightningModule</code> <p>A reference to the Lightning module being evaluated.</p> required <p>Returns:</p> Type Description <code>Optional[dict[str, Any]]</code> <p>A dictionary of evaluation results or <code>None</code> if no results are available.</p> Source code in <code>mmlearn/tasks/zero_shot_retrieval.py</code> <pre><code>def on_evaluation_epoch_end(\n    self, pl_module: pl.LightningModule\n) -&gt; Optional[dict[str, Any]]:\n    \"\"\"Compute the retrieval recall metrics.\n\n    Parameters\n    ----------\n    pl_module : pl.LightningModule\n        A reference to the Lightning module being evaluated.\n\n    Returns\n    -------\n    Optional[dict[str, Any]]\n        A dictionary of evaluation results or `None` if no results are available.\n    \"\"\"\n    if pl_module.trainer.sanity_checking:\n        return None\n\n    results = {}\n    for metric in self.metrics.values():\n        results.update(metric.compute())\n        metric.reset()\n\n    eval_type = \"val\" if pl_module.trainer.validating else \"test\"\n\n    for key, value in results.items():\n        pl_module.log(f\"{eval_type}/{key}\", value)\n\n    return results\n</code></pre>"},{"location":"api/#mmlearn.cli.run.find_matching_indices","title":"find_matching_indices","text":"<pre><code>find_matching_indices(\n    first_example_ids, second_example_ids\n)\n</code></pre> <p>Find the indices of matching examples given two tensors of example ids.</p> <p>Matching examples are defined as examples with the same value in both tensors. This method is useful for finding pairs of examples from different modalities that are related to each other in a batch.</p> <p>Parameters:</p> Name Type Description Default <code>first_example_ids</code> <code>Tensor</code> <p>A tensor of example ids of shape <code>(N, 2)</code>, where <code>N</code> is the number of examples.</p> required <code>second_example_ids</code> <code>Tensor</code> <p>A tensor of example ids of shape <code>(M, 2)</code>, where <code>M</code> is the number of examples.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>A tuple of tensors containing the indices of matching examples in the first and second tensor, respectively.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If either <code>first_example_ids</code> or <code>second_example_ids</code> is not a tensor.</p> <code>ValueError</code> <p>If either <code>first_example_ids</code> or <code>second_example_ids</code> is not a 2D tensor with the second dimension having a size of <code>2</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; img_example_ids = torch.tensor([(0, 0), (0, 1), (1, 0), (1, 1)])\n&gt;&gt;&gt; text_example_ids = torch.tensor([(1, 0), (1, 1), (2, 0), (2, 1), (2, 2)])\n&gt;&gt;&gt; find_matching_indices(img_example_ids, text_example_ids)\n(tensor([2, 3]), tensor([0, 1]))\n</code></pre> Source code in <code>mmlearn/datasets/core/example.py</code> <pre><code>def find_matching_indices(\n    first_example_ids: torch.Tensor, second_example_ids: torch.Tensor\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Find the indices of matching examples given two tensors of example ids.\n\n    Matching examples are defined as examples with the same value in both tensors.\n    This method is useful for finding pairs of examples from different modalities\n    that are related to each other in a batch.\n\n    Parameters\n    ----------\n    first_example_ids : torch.Tensor\n        A tensor of example ids of shape `(N, 2)`, where `N` is the number of examples.\n    second_example_ids : torch.Tensor\n        A tensor of example ids of shape `(M, 2)`, where `M` is the number of examples.\n\n    Returns\n    -------\n    tuple[torch.Tensor, torch.Tensor]\n        A tuple of tensors containing the indices of matching examples in the first and\n        second tensor, respectively.\n\n    Raises\n    ------\n    TypeError\n        If either `first_example_ids` or `second_example_ids` is not a tensor.\n    ValueError\n        If either `first_example_ids` or `second_example_ids` is not a 2D tensor\n        with the second dimension having a size of `2`.\n\n    Examples\n    --------\n    &gt;&gt;&gt; img_example_ids = torch.tensor([(0, 0), (0, 1), (1, 0), (1, 1)])\n    &gt;&gt;&gt; text_example_ids = torch.tensor([(1, 0), (1, 1), (2, 0), (2, 1), (2, 2)])\n    &gt;&gt;&gt; find_matching_indices(img_example_ids, text_example_ids)\n    (tensor([2, 3]), tensor([0, 1]))\n\n\n    \"\"\"\n    if not isinstance(first_example_ids, torch.Tensor) or not isinstance(\n        second_example_ids,\n        torch.Tensor,\n    ):\n        raise TypeError(\n            f\"Expected inputs to be tensors, but got {type(first_example_ids)} \"\n            f\"and {type(second_example_ids)}.\",\n        )\n    val = 2\n    if not (first_example_ids.ndim == val and first_example_ids.shape[1] == val):\n        raise ValueError(\n            \"Expected argument `first_example_ids` to be a tensor of shape (N, 2), \"\n            f\"but got shape {first_example_ids.shape}.\",\n        )\n    if not (second_example_ids.ndim == val and second_example_ids.shape[1] == val):\n        raise ValueError(\n            \"Expected argument `second_example_ids` to be a tensor of shape (N, 2), \"\n            f\"but got shape {second_example_ids.shape}.\",\n        )\n\n    first_example_ids = first_example_ids.unsqueeze(1)  # shape=(N, 1, 2)\n    second_example_ids = second_example_ids.unsqueeze(0)  # shape=(1, M, 2)\n\n    # compare all elements; results in a shape (N, M) tensor\n    matches = torch.all(first_example_ids == second_example_ids, dim=-1)\n    first_indices, second_indices = torch.where(matches)\n    return first_indices, second_indices\n</code></pre>"},{"location":"api/#mmlearn.cli.run.linear_warmup_cosine_annealing_lr","title":"linear_warmup_cosine_annealing_lr","text":"<pre><code>linear_warmup_cosine_annealing_lr(\n    optimizer,\n    warmup_steps,\n    max_steps,\n    start_factor=1 / 3,\n    eta_min=0.0,\n    last_epoch=-1,\n)\n</code></pre> <p>Create a linear warmup cosine annealing learning rate scheduler.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>Optimizer</code> <p>The optimizer for which to schedule the learning rate.</p> required <code>warmup_steps</code> <code>int</code> <p>Maximum number of iterations for linear warmup.</p> required <code>max_steps</code> <code>int</code> <p>Maximum number of iterations.</p> required <code>start_factor</code> <code>float</code> <p>Multiplicative factor for the learning rate at the start of the warmup phase.</p> <code>1/3</code> <code>eta_min</code> <code>float</code> <p>Minimum learning rate.</p> <code>0</code> <code>last_epoch</code> <code>int</code> <p>The index of last epoch. If set to <code>-1</code>, it initializes the learning rate as the base learning rate</p> <code>-1</code> <p>Returns:</p> Type Description <code>LRScheduler</code> <p>The learning rate scheduler.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>warmup_steps</code> is greater than or equal to <code>max_steps</code> or if <code>warmup_steps</code> is less than or equal to 0.</p> Source code in <code>mmlearn/modules/lr_schedulers/linear_warmup_cosine_lr.py</code> <pre><code>@store(  # type: ignore[misc]\n    group=\"modules/lr_schedulers\",\n    provider=\"mmlearn\",\n    zen_partial=True,\n    warmup_steps=MISSING,\n    max_steps=MISSING,\n)\ndef linear_warmup_cosine_annealing_lr(\n    optimizer: Optimizer,\n    warmup_steps: int,\n    max_steps: int,\n    start_factor: float = 1 / 3,\n    eta_min: float = 0.0,\n    last_epoch: int = -1,\n) -&gt; LRScheduler:\n    \"\"\"Create a linear warmup cosine annealing learning rate scheduler.\n\n    Parameters\n    ----------\n    optimizer : Optimizer\n        The optimizer for which to schedule the learning rate.\n    warmup_steps : int\n        Maximum number of iterations for linear warmup.\n    max_steps : int\n        Maximum number of iterations.\n    start_factor : float, optional, default=1/3\n        Multiplicative factor for the learning rate at the start of the warmup phase.\n    eta_min : float, optional, default=0\n        Minimum learning rate.\n    last_epoch : int, optional, default=-1\n        The index of last epoch. If set to ``-1``, it initializes the learning rate\n        as the base learning rate\n\n    Returns\n    -------\n    LRScheduler\n        The learning rate scheduler.\n\n    Raises\n    ------\n    ValueError\n        If `warmup_steps` is greater than or equal to `max_steps` or if `warmup_steps`\n        is less than or equal to 0.\n    \"\"\"\n    if warmup_steps &gt;= max_steps:\n        raise ValueError(\n            \"Expected `warmup_steps` to be less than `max_steps` but got \"\n            f\"`warmup_steps={warmup_steps}` and `max_steps={max_steps}`.\"\n        )\n    if warmup_steps &lt;= 0:\n        raise ValueError(\n            \"Expected `warmup_steps` to be positive but got \"\n            f\"`warmup_steps={warmup_steps}`.\"\n        )\n\n    linear_lr = LinearLR(\n        optimizer,\n        start_factor=start_factor,\n        total_iters=warmup_steps,\n        last_epoch=last_epoch,\n    )\n    cosine_lr = CosineAnnealingLR(\n        optimizer,\n        T_max=max_steps - warmup_steps,\n        eta_min=eta_min,\n        last_epoch=last_epoch,\n    )\n    return SequentialLR(\n        optimizer,\n        schedulers=[linear_lr, cosine_lr],\n        milestones=[warmup_steps],\n        last_epoch=last_epoch,\n    )\n</code></pre>"},{"location":"api/#mmlearn.cli.run.main","title":"main","text":"<pre><code>main(cfg)\n</code></pre> <p>Entry point for training or evaluation.</p> Source code in <code>mmlearn/cli/run.py</code> <pre><code>@_hydra_main(\n    config_path=\"pkg://mmlearn.conf\", config_name=\"base_config\", version_base=None\n)\ndef main(cfg: MMLearnConf) -&gt; None:  # noqa: PLR0912\n    \"\"\"Entry point for training or evaluation.\"\"\"\n    cfg_copy = copy.deepcopy(cfg)  # copy of the config for logging\n\n    L.seed_everything(cfg.seed, workers=True)\n\n    if is_torch_tf32_available():\n        torch.backends.cuda.matmul.allow_tf32 = True\n        if \"16-mixed\" in str(cfg.trainer.precision):\n            cfg.trainer.precision = \"bf16-mixed\"\n\n    # setup trainer first so that we can get some variables for distributed training\n    callbacks = instantiate_callbacks(cfg.trainer.get(\"callbacks\"))\n    cfg.trainer[\"callbacks\"] = None  # will be replaced with the instantiated object\n    loggers = instantiate_loggers(cfg.trainer.get(\"logger\"))\n    cfg.trainer[\"logger\"] = None\n    trainer: Trainer = hydra.utils.instantiate(\n        cfg.trainer, callbacks=callbacks, logger=loggers, _convert_=\"all\"\n    )\n    assert isinstance(trainer, Trainer), (\n        \"Trainer must be an instance of `lightning.pytorch.trainer.Trainer`\"\n    )\n\n    if rank_zero_only.rank == 0 and loggers is not None:  # update wandb config\n        for trainer_logger in loggers:\n            if isinstance(trainer_logger, WandbLogger):\n                trainer_logger.experiment.config.update(\n                    OmegaConf.to_container(cfg_copy, resolve=True, enum_to_str=True),\n                    allow_val_change=True,\n                )\n    trainer.print(OmegaConf.to_yaml(cfg_copy, resolve=True))\n\n    requires_distributed_sampler = (\n        trainer.distributed_sampler_kwargs is not None\n        and trainer._accelerator_connector.use_distributed_sampler\n    )\n    if requires_distributed_sampler:  # we handle distributed samplers\n        trainer._accelerator_connector.use_distributed_sampler = False\n\n    # prepare dataloaders\n    if cfg.job_type == JobType.train:\n        train_dataset = instantiate_datasets(cfg.datasets.train)\n        assert train_dataset is not None, (\n            \"Train dataset (`cfg.datasets.train`) is required for training.\"\n        )\n\n        train_sampler = instantiate_sampler(\n            cfg.dataloader.train.get(\"sampler\"),\n            train_dataset,\n            requires_distributed_sampler=requires_distributed_sampler,\n            distributed_sampler_kwargs=trainer.distributed_sampler_kwargs,\n        )\n        cfg.dataloader.train[\"sampler\"] = None  # replaced with the instantiated object\n        train_loader: DataLoader = hydra.utils.instantiate(\n            cfg.dataloader.train, dataset=train_dataset, sampler=train_sampler\n        )\n\n        val_loader: Optional[DataLoader] = None\n        val_dataset = instantiate_datasets(cfg.datasets.val)\n        if val_dataset is not None:\n            val_sampler = instantiate_sampler(\n                cfg.dataloader.val.get(\"sampler\"),\n                val_dataset,\n                requires_distributed_sampler=requires_distributed_sampler,\n                distributed_sampler_kwargs=trainer.distributed_sampler_kwargs,\n            )\n            cfg.dataloader.val[\"sampler\"] = None\n            val_loader = hydra.utils.instantiate(\n                cfg.dataloader.val, dataset=val_dataset, sampler=val_sampler\n            )\n    else:\n        test_dataset = instantiate_datasets(cfg.datasets.test)\n        assert test_dataset is not None, (\n            \"Test dataset (`cfg.datasets.test`) is required for evaluation.\"\n        )\n\n        test_sampler = instantiate_sampler(\n            cfg.dataloader.test.get(\"sampler\"),\n            test_dataset,\n            requires_distributed_sampler=requires_distributed_sampler,\n            distributed_sampler_kwargs=trainer.distributed_sampler_kwargs,\n        )\n        cfg.dataloader.test[\"sampler\"] = None\n        test_loader = hydra.utils.instantiate(\n            cfg.dataloader.test, dataset=test_dataset, sampler=test_sampler\n        )\n\n    # setup task module\n    if cfg.task is None or \"_target_\" not in cfg.task:\n        raise ValueError(\n            \"Expected a non-empty config for `cfg.task` with a `_target_` key. \"\n            f\"But got: {cfg.task}\"\n        )\n    logger.info(f\"Instantiating task module: {cfg.task['_target_']}\")\n    model: L.LightningModule = hydra.utils.instantiate(cfg.task, _convert_=\"partial\")\n    assert isinstance(model, L.LightningModule), \"Task must be a `LightningModule`\"\n    model.strict_loading = cfg.strict_loading\n\n    # compile model\n    model = torch.compile(model, **OmegaConf.to_object(cfg.torch_compile_kwargs))\n\n    if cfg.job_type == JobType.train:\n        trainer.fit(\n            model, train_loader, val_loader, ckpt_path=cfg.resume_from_checkpoint\n        )\n    elif cfg.job_type == JobType.eval:\n        trainer.test(model, test_loader, ckpt_path=cfg.resume_from_checkpoint)\n</code></pre>"},{"location":"api/#configuration-module","title":"Configuration Module","text":""},{"location":"api/#mmlearn.conf","title":"mmlearn.conf","text":"<p>Hydra/Hydra-zen-based configurations.</p>"},{"location":"api/#mmlearn.conf.JobType","title":"JobType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Type of the job.</p> Source code in <code>mmlearn/conf/__init__.py</code> <pre><code>class JobType(str, Enum):\n    \"\"\"Type of the job.\"\"\"\n\n    train = \"train\"\n    eval = \"eval\"\n</code></pre>"},{"location":"api/#mmlearn.conf.DatasetConf","title":"DatasetConf  <code>dataclass</code>","text":"<p>Configuration template for the datasets.</p> Source code in <code>mmlearn/conf/__init__.py</code> <pre><code>@dataclass\nclass DatasetConf:\n    \"\"\"Configuration template for the datasets.\"\"\"\n\n    #: Configuration for the training dataset.\n    train: Optional[Any] = field(\n        default=None,\n        metadata={\"help\": \"Configuration for the training dataset.\"},\n    )\n    #: Configuration for the validation dataset.\n    val: Optional[Any] = field(\n        default=None, metadata={\"help\": \"Configuration for the validation dataset.\"}\n    )\n    #: Configuration for the test dataset.\n    test: Optional[Any] = field(\n        default=None,\n        metadata={\"help\": \"Configuration for the test dataset.\"},\n    )\n</code></pre>"},{"location":"api/#mmlearn.conf.DataLoaderConf","title":"DataLoaderConf  <code>dataclass</code>","text":"<p>Configuration for the dataloader.</p> Source code in <code>mmlearn/conf/__init__.py</code> <pre><code>@dataclass\nclass DataLoaderConf:\n    \"\"\"Configuration for the dataloader.\"\"\"\n\n    #: Configuration for the training dataloader.\n    train: Any = field(\n        default_factory=_DataLoaderConf,\n        metadata={\"help\": \"Configuration for the training dataloader.\"},\n    )\n    #: Configuration for the validation dataloader.\n    val: Any = field(\n        default_factory=_DataLoaderConf,\n        metadata={\"help\": \"Configuration for the validation dataloader.\"},\n    )\n    #: Configuration for the test dataloader.\n    test: Any = field(\n        default_factory=_DataLoaderConf,\n        metadata={\"help\": \"Configuration for the test dataloader.\"},\n    )\n</code></pre>"},{"location":"api/#mmlearn.conf.MMLearnConf","title":"MMLearnConf  <code>dataclass</code>","text":"<p>Top-level configuration for mmlearn experiments.</p> Source code in <code>mmlearn/conf/__init__.py</code> <pre><code>@dataclass\nclass MMLearnConf:\n    \"\"\"Top-level configuration for mmlearn experiments.\"\"\"\n\n    defaults: list[Any] = field(\n        default_factory=lambda: [\n            \"_self_\",  # See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information\n            {\"task\": MISSING},\n            {\"override hydra/launcher\": \"submitit_slurm\"},\n        ]\n    )\n    #: Name of the experiment. This must be specified for any experiment to run.\n    experiment_name: str = field(default=MISSING)\n    #: Type of the job.\n    job_type: JobType = field(default=JobType.train)\n    #: Seed for the random number generators. This is set for Python, Numpy and PyTorch,\n    #: including the workers in PyTorch Dataloaders.\n    seed: Optional[int] = field(default=None)\n    #: Configuration for the datasets.\n    datasets: DatasetConf = field(default_factory=DatasetConf)\n    #: Configuration for the dataloaders.\n    dataloader: DataLoaderConf = field(default_factory=DataLoaderConf)\n    #: Configuration for the task. This is required to run any experiment.\n    task: Any = field(default=MISSING)\n    #: Configuration for the trainer. The options here are the same as in\n    #: :py:class:`~lightning.pytorch.trainer.trainer.Trainer`\n    trainer: Any = field(\n        default_factory=builds(\n            lightning_trainer.Trainer,\n            populate_full_signature=True,\n            enable_model_summary=True,\n            enable_progress_bar=True,\n            enable_checkpointing=True,\n            default_root_dir=_get_default_ckpt_dir(),\n        )\n    )\n    #: Tags for the experiment. This is useful for `wandb &lt;https://docs.wandb.ai/ref/python/init&gt;`_\n    #: logging.\n    tags: Optional[list[str]] = field(default_factory=lambda: [II(\"experiment_name\")])\n    #: Path to the checkpoint to resume training from.\n    resume_from_checkpoint: Optional[Path] = field(default=None)\n    #: Whether to strictly enforce loading of model weights i.e. `strict=True` in\n    #: :py:meth:`~lightning.pytorch.core.module.LightningModule.load_from_checkpoint`.\n    strict_loading: bool = field(default=True)\n    #: Configuration for torch.compile. These are essentially the same as the\n    #: arguments for :py:func:`torch.compile`.\n    torch_compile_kwargs: dict[str, Any] = field(\n        default_factory=lambda: {\n            \"disable\": True,\n            \"fullgraph\": False,\n            \"dynamic\": None,\n            \"backend\": \"inductor\",\n            \"mode\": None,\n            \"options\": None,\n        }\n    )\n    #: Hydra configuration.\n    hydra: HydraConf = field(\n        default_factory=lambda: HydraConf(\n            searchpath=[\"pkg://mmlearn.conf\"],\n            run=RunDir(\n                dir=SI(\"./outputs/${experiment_name}/${now:%Y-%m-%d}/${now:%H-%M-%S}\")\n            ),\n            sweep=SweepDir(\n                dir=SI(\"./outputs/${experiment_name}/${now:%Y-%m-%d}/${now:%H-%M-%S}\"),\n                subdir=SI(\"${hydra.job.num}_${hydra.job.id}\"),\n            ),\n            help=HelpConf(\n                app_name=\"mmlearn\",\n                header=\"mmlearn: A modular framework for research on multimodal representation learning.\",\n            ),\n            job=JobConf(\n                name=II(\"experiment_name\"),\n                env_set={\n                    \"TORCH_NCCL_ASYNC_ERROR_HANDLING\": \"1\",\n                    \"HYDRA_FULL_ERROR\": \"1\",\n                },\n            ),\n        )\n    )\n</code></pre>"},{"location":"api/#mmlearn.conf.register_external_modules","title":"register_external_modules","text":"<pre><code>register_external_modules(\n    module,\n    group,\n    name=None,\n    package=None,\n    provider=None,\n    base_cls=None,\n    ignore_cls=None,\n    ignore_prefix=None,\n    **kwargs_for_builds\n)\n</code></pre> <p>Add all classes in an external module to a ZenStore.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>ModuleType</code> <p>The module to add classes from.</p> required <code>group</code> <code>str</code> <p>The config group to add the classes to.</p> required <code>name</code> <code>Optional[str]</code> <p>The name to give to the dynamically-generated configs. If <code>None</code>, the class name is used.</p> <code>None</code> <code>package</code> <code>Optional[str]</code> <p>The package to add the configs to.</p> <code>None</code> <code>provider</code> <code>Optional[str]</code> <p>The provider to add the configs to.</p> <code>None</code> <code>base_cls</code> <code>Optional[type]</code> <p>The base class to filter classes by. The base class is also excluded from the configs.</p> <code>None</code> <code>ignore_cls</code> <code>Optional[list[type]]</code> <p>list of classes to ignore.</p> <code>None</code> <code>ignore_prefix</code> <code>Optional[str]</code> <p>Ignore classes whose names start with this prefix.</p> <code>None</code> <code>kwargs_for_builds</code> <code>Any</code> <p>Additional keyword arguments to pass to <code>hydra_zen.builds</code>.</p> <code>{}</code> Source code in <code>mmlearn/conf/__init__.py</code> <pre><code>def register_external_modules(\n    module: ModuleType,\n    group: str,\n    name: Optional[str] = None,\n    package: Optional[str] = None,\n    provider: Optional[str] = None,\n    base_cls: Optional[type] = None,\n    ignore_cls: Optional[list[type]] = None,\n    ignore_prefix: Optional[str] = None,\n    **kwargs_for_builds: Any,\n) -&gt; None:\n    \"\"\"Add all classes in an external module to a ZenStore.\n\n    Parameters\n    ----------\n    module : ModuleType\n        The module to add classes from.\n    group : str\n        The config group to add the classes to.\n    name : Optional[str], optional, default=None\n        The name to give to the dynamically-generated configs. If `None`, the\n        class name is used.\n    package : Optional[str], optional, default=None\n        The package to add the configs to.\n    provider : Optional[str], optional, default=None\n        The provider to add the configs to.\n    base_cls : Optional[type], optional, default=None\n        The base class to filter classes by. The base class is also excluded from\n        the configs.\n    ignore_cls : Optional[list[type]], optional, default=None\n        list of classes to ignore.\n    ignore_prefix : Optional[str], optional, default=None\n        Ignore classes whose names start with this prefix.\n    kwargs_for_builds : Any\n        Additional keyword arguments to pass to ``hydra_zen.builds``.\n\n    \"\"\"\n    for key, cls in module.__dict__.items():\n        if (\n            isinstance(cls, type)\n            and (base_cls is None or issubclass(cls, base_cls))\n            and cls != base_cls\n            and (ignore_cls is None or cls not in ignore_cls)\n            and (ignore_prefix is None or not key.startswith(ignore_prefix))\n        ):\n            external_store(\n                builds(cls, populate_full_signature=True, **kwargs_for_builds),\n                name=name or key,\n                group=group,\n                package=package,\n                provider=provider,\n            )\n</code></pre>"},{"location":"api/#datasets-module","title":"Datasets Module","text":""},{"location":"api/#mmlearn.datasets","title":"mmlearn.datasets","text":"<p>Datasets.</p>"},{"location":"api/#mmlearn.datasets.CheXpert","title":"CheXpert","text":"<p>               Bases: <code>Dataset[Example]</code></p> <p>CheXpert dataset.</p> <p>Each datapoint is a pair of <code>(image, target label)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>Directory which contains <code>.json</code> files stating all dataset entries.</p> required <code>split</code> <code>(train, valid)</code> <p>Dataset split.</p> <code>\"train\"</code> <code>labeler</code> <code>Optional[{chexpert, chexbert, vchexbert}]</code> <p>Labeler used to extract labels from the training images. \"valid\" split has no labeler, labeling for valid split was done by human radiologists.</p> <code>None</code> <code>transform</code> <code>Optional[Callable[[PIL.Image], torch.Tensor]</code> <p>A callable that takes in a PIL image and returns a transformed version of the image as a PyTorch tensor.</p> <code>None</code> Source code in <code>mmlearn/datasets/chexpert.py</code> <pre><code>@store(\n    group=\"datasets\",\n    provider=\"mmlearn\",\n    root_dir=os.getenv(\"CHEXPERT_ROOT_DIR\", MISSING),\n    split=\"train\",\n)\nclass CheXpert(Dataset[Example]):\n    \"\"\"CheXpert dataset.\n\n    Each datapoint is a pair of `(image, target label)`.\n\n    Parameters\n    ----------\n    root_dir : str\n        Directory which contains `.json` files stating all dataset entries.\n    split : {\"train\", \"valid\"}\n        Dataset split.\n    labeler : Optional[{\"chexpert\", \"chexbert\", \"vchexbert\"}], optional, default=None\n        Labeler used to extract labels from the training images. \"valid\" split\n        has no labeler, labeling for valid split was done by human radiologists.\n    transform : Optional[Callable[[PIL.Image], torch.Tensor], optional, default=None\n        A callable that takes in a PIL image and returns a transformed version\n        of the image as a PyTorch tensor.\n    \"\"\"\n\n    def __init__(\n        self,\n        root_dir: str,\n        split: Literal[\"train\", \"valid\"],\n        labeler: Optional[Literal[\"chexpert\", \"chexbert\", \"vchexbert\"]] = None,\n        transform: Optional[Callable[[Image.Image], torch.Tensor]] = None,\n    ) -&gt; None:\n        assert split in [\"train\", \"valid\"], f\"split {split} is not available.\"\n        assert labeler in [\"chexpert\", \"chexbert\", \"vchexbert\"] or labeler is None, (\n            f\"labeler {labeler} is not available.\"\n        )\n        assert callable(transform) or transform is None, (\n            \"transform is not callable or None.\"\n        )\n\n        if split == \"valid\":\n            data_file = f\"{split}_data.json\"\n        elif split == \"train\":\n            data_file = f\"{labeler}_{split}_data.json\"\n        data_path = os.path.join(root_dir, data_file)\n\n        assert os.path.isfile(data_path), f\"entries file does not exist: {data_path}.\"\n\n        with open(data_path, \"rb\") as file:\n            entries = json.load(file)\n        self.entries = entries\n\n        if transform is not None:\n            self.transform = transform\n        else:\n            self.transform = Compose([Resize(224), CenterCrop(224), ToTensor()])\n\n    def __getitem__(self, idx: int) -&gt; Example:\n        \"\"\"Return the idx'th datapoint.\"\"\"\n        entry = self.entries[idx]\n        image = Image.open(entry[\"image_path\"]).convert(\"RGB\")\n        image = self.transform(image)\n        label = torch.tensor(entry[\"label\"])\n\n        return Example(\n            {\n                Modalities.RGB.name: image,\n                Modalities.RGB.target: label,\n                \"qid\": entry[\"qid\"],\n                EXAMPLE_INDEX_KEY: idx,\n            }\n        )\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the length of the dataset.\"\"\"\n        return len(self.entries)\n</code></pre>"},{"location":"api/#mmlearn.datasets.CheXpert.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> <p>Return the idx'th datapoint.</p> Source code in <code>mmlearn/datasets/chexpert.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; Example:\n    \"\"\"Return the idx'th datapoint.\"\"\"\n    entry = self.entries[idx]\n    image = Image.open(entry[\"image_path\"]).convert(\"RGB\")\n    image = self.transform(image)\n    label = torch.tensor(entry[\"label\"])\n\n    return Example(\n        {\n            Modalities.RGB.name: image,\n            Modalities.RGB.target: label,\n            \"qid\": entry[\"qid\"],\n            EXAMPLE_INDEX_KEY: idx,\n        }\n    )\n</code></pre>"},{"location":"api/#mmlearn.datasets.CheXpert.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the length of the dataset.</p> Source code in <code>mmlearn/datasets/chexpert.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the length of the dataset.\"\"\"\n    return len(self.entries)\n</code></pre>"},{"location":"api/#mmlearn.datasets.ImageNet","title":"ImageNet","text":"<p>               Bases: <code>ImageFolder</code></p> <p>ImageNet dataset.</p> <p>This is a wrapper around the class:<code>~torchvision.datasets.ImageFolder</code> class that returns an class:<code>~mmlearn.datasets.core.example.Example</code> object.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>Path to the root directory of the dataset.</p> required <code>split</code> <code>(train, val)</code> <p>The split of the dataset to use.</p> <code>\"train\"</code> <code>transform</code> <code>Optional[Callable]</code> <p>A callable that takes in a PIL image and returns a transformed version of the image as a PyTorch tensor.</p> <code>None</code> <code>target_transform</code> <code>Optional[Callable]</code> <p>A callable that takes in the target and transforms it.</p> <code>None</code> <code>mask_generator</code> <code>Optional[Callable]</code> <p>A callable that generates a mask for the image.</p> <code>None</code> Source code in <code>mmlearn/datasets/imagenet.py</code> <pre><code>@store(\n    group=\"datasets\",\n    provider=\"mmlearn\",\n    root_dir=os.getenv(\"IMAGENET_ROOT_DIR\", MISSING),\n)\nclass ImageNet(ImageFolder):\n    \"\"\"ImageNet dataset.\n\n    This is a wrapper around the :py:class:`~torchvision.datasets.ImageFolder` class\n    that returns an :py:class:`~mmlearn.datasets.core.example.Example` object.\n\n    Parameters\n    ----------\n    root_dir : str\n        Path to the root directory of the dataset.\n    split : {\"train\", \"val\"}, default=\"train\"\n        The split of the dataset to use.\n    transform : Optional[Callable], optional, default=None\n        A callable that takes in a PIL image and returns a transformed version\n        of the image as a PyTorch tensor.\n    target_transform : Optional[Callable], optional, default=None\n        A callable that takes in the target and transforms it.\n    mask_generator : Optional[Callable], optional, default=None\n        A callable that generates a mask for the image.\n    \"\"\"\n\n    def __init__(\n        self,\n        root_dir: str,\n        split: Literal[\"train\", \"val\"] = \"train\",\n        transform: Optional[Callable[..., Any]] = None,\n        target_transform: Optional[Callable[..., Any]] = None,\n        mask_generator: Optional[Callable[..., Any]] = None,\n    ) -&gt; None:\n        split = \"train\" if split == \"train\" else \"val\"\n        root_dir = os.path.join(root_dir, split)\n        super().__init__(\n            root=root_dir, transform=transform, target_transform=target_transform\n        )\n        self.mask_generator = mask_generator\n\n    def __getitem__(self, index: int) -&gt; Example:\n        \"\"\"Get an example at the given index.\"\"\"\n        image, target = super().__getitem__(index)\n        example = Example(\n            {\n                Modalities.RGB.name: image,\n                Modalities.RGB.target: target,\n                EXAMPLE_INDEX_KEY: index,\n            }\n        )\n        mask = self.mask_generator() if self.mask_generator else None\n        if mask is not None:  # error will be raised during collation if `None`\n            example[Modalities.RGB.mask] = mask\n        return example\n\n    @property\n    def zero_shot_prompt_templates(self) -&gt; list[str]:\n        \"\"\"Return the zero-shot prompt templates.\"\"\"\n        return [\n            \"a bad photo of a {}.\",\n            \"a photo of many {}.\",\n            \"a sculpture of a {}.\",\n            \"a photo of the hard to see {}.\",\n            \"a low resolution photo of the {}.\",\n            \"a rendering of a {}.\",\n            \"graffiti of a {}.\",\n            \"a bad photo of the {}.\",\n            \"a cropped photo of the {}.\",\n            \"a tattoo of a {}.\",\n            \"the embroidered {}.\",\n            \"a photo of a hard to see {}.\",\n            \"a bright photo of a {}.\",\n            \"a photo of a clean {}.\",\n            \"a photo of a dirty {}.\",\n            \"a dark photo of the {}.\",\n            \"a drawing of a {}.\",\n            \"a photo of my {}.\",\n            \"the plastic {}.\",\n            \"a photo of the cool {}.\",\n            \"a close-up photo of a {}.\",\n            \"a black and white photo of the {}.\",\n            \"a painting of the {}.\",\n            \"a painting of a {}.\",\n            \"a pixelated photo of the {}.\",\n            \"a sculpture of the {}.\",\n            \"a bright photo of the {}.\",\n            \"a cropped photo of a {}.\",\n            \"a plastic {}.\",\n            \"a photo of the dirty {}.\",\n            \"a jpeg corrupted photo of a {}.\",\n            \"a blurry photo of the {}.\",\n            \"a photo of the {}.\",\n            \"a good photo of the {}.\",\n            \"a rendering of the {}.\",\n            \"a {} in a video game.\",\n            \"a photo of one {}.\",\n            \"a doodle of a {}.\",\n            \"a close-up photo of the {}.\",\n            \"a photo of a {}.\",\n            \"the origami {}.\",\n            \"the {} in a video game.\",\n            \"a sketch of a {}.\",\n            \"a doodle of the {}.\",\n            \"a origami {}.\",\n            \"a low resolution photo of a {}.\",\n            \"the toy {}.\",\n            \"a rendition of the {}.\",\n            \"a photo of the clean {}.\",\n            \"a photo of a large {}.\",\n            \"a rendition of a {}.\",\n            \"a photo of a nice {}.\",\n            \"a photo of a weird {}.\",\n            \"a blurry photo of a {}.\",\n            \"a cartoon {}.\",\n            \"art of a {}.\",\n            \"a sketch of the {}.\",\n            \"a embroidered {}.\",\n            \"a pixelated photo of a {}.\",\n            \"itap of the {}.\",\n            \"a jpeg corrupted photo of the {}.\",\n            \"a good photo of a {}.\",\n            \"a plushie {}.\",\n            \"a photo of the nice {}.\",\n            \"a photo of the small {}.\",\n            \"a photo of the weird {}.\",\n            \"the cartoon {}.\",\n            \"art of the {}.\",\n            \"a drawing of the {}.\",\n            \"a photo of the large {}.\",\n            \"a black and white photo of a {}.\",\n            \"the plushie {}.\",\n            \"a dark photo of a {}.\",\n            \"itap of a {}.\",\n            \"graffiti of the {}.\",\n            \"a toy {}.\",\n            \"itap of my {}.\",\n            \"a photo of a cool {}.\",\n            \"a photo of a small {}.\",\n            \"a tattoo of the {}.\",\n        ]\n\n    @property\n    def id2label(self) -&gt; dict[int, str]:\n        \"\"\"Return the label mapping.\"\"\"\n        return {\n            0: \"tench\",\n            1: \"goldfish\",\n            2: \"great white shark\",\n            3: \"tiger shark\",\n            4: \"hammerhead shark\",\n            5: \"electric ray\",\n            6: \"stingray\",\n            7: \"rooster\",\n            8: \"hen\",\n            9: \"ostrich\",\n            10: \"brambling\",\n            11: \"goldfinch\",\n            12: \"house finch\",\n            13: \"junco\",\n            14: \"indigo bunting\",\n            15: \"American robin\",\n            16: \"bulbul\",\n            17: \"jay\",\n            18: \"magpie\",\n            19: \"chickadee\",\n            20: \"American dipper\",\n            21: \"kite (bird of prey)\",\n            22: \"bald eagle\",\n            23: \"vulture\",\n            24: \"great grey owl\",\n            25: \"fire salamander\",\n            26: \"smooth newt\",\n            27: \"newt\",\n            28: \"spotted salamander\",\n            29: \"axolotl\",\n            30: \"American bullfrog\",\n            31: \"tree frog\",\n            32: \"tailed frog\",\n            33: \"loggerhead sea turtle\",\n            34: \"leatherback sea turtle\",\n            35: \"mud turtle\",\n            36: \"terrapin\",\n            37: \"box turtle\",\n            38: \"banded gecko\",\n            39: \"green iguana\",\n            40: \"Carolina anole\",\n            41: \"desert grassland whiptail lizard\",\n            42: \"agama\",\n            43: \"frilled-necked lizard\",\n            44: \"alligator lizard\",\n            45: \"Gila monster\",\n            46: \"European green lizard\",\n            47: \"chameleon\",\n            48: \"Komodo dragon\",\n            49: \"Nile crocodile\",\n            50: \"American alligator\",\n            51: \"triceratops\",\n            52: \"worm snake\",\n            53: \"ring-necked snake\",\n            54: \"eastern hog-nosed snake\",\n            55: \"smooth green snake\",\n            56: \"kingsnake\",\n            57: \"garter snake\",\n            58: \"water snake\",\n            59: \"vine snake\",\n            60: \"night snake\",\n            61: \"boa constrictor\",\n            62: \"African rock python\",\n            63: \"Indian cobra\",\n            64: \"green mamba\",\n            65: \"sea snake\",\n            66: \"Saharan horned viper\",\n            67: \"eastern diamondback rattlesnake\",\n            68: \"sidewinder rattlesnake\",\n            69: \"trilobite\",\n            70: \"harvestman\",\n            71: \"scorpion\",\n            72: \"yellow garden spider\",\n            73: \"barn spider\",\n            74: \"European garden spider\",\n            75: \"southern black widow\",\n            76: \"tarantula\",\n            77: \"wolf spider\",\n            78: \"tick\",\n            79: \"centipede\",\n            80: \"black grouse\",\n            81: \"ptarmigan\",\n            82: \"ruffed grouse\",\n            83: \"prairie grouse\",\n            84: \"peafowl\",\n            85: \"quail\",\n            86: \"partridge\",\n            87: \"african grey parrot\",\n            88: \"macaw\",\n            89: \"sulphur-crested cockatoo\",\n            90: \"lorikeet\",\n            91: \"coucal\",\n            92: \"bee eater\",\n            93: \"hornbill\",\n            94: \"hummingbird\",\n            95: \"jacamar\",\n            96: \"toucan\",\n            97: \"duck\",\n            98: \"red-breasted merganser\",\n            99: \"goose\",\n            100: \"black swan\",\n            101: \"tusker\",\n            102: \"echidna\",\n            103: \"platypus\",\n            104: \"wallaby\",\n            105: \"koala\",\n            106: \"wombat\",\n            107: \"jellyfish\",\n            108: \"sea anemone\",\n            109: \"brain coral\",\n            110: \"flatworm\",\n            111: \"nematode\",\n            112: \"conch\",\n            113: \"snail\",\n            114: \"slug\",\n            115: \"sea slug\",\n            116: \"chiton\",\n            117: \"chambered nautilus\",\n            118: \"Dungeness crab\",\n            119: \"rock crab\",\n            120: \"fiddler crab\",\n            121: \"red king crab\",\n            122: \"American lobster\",\n            123: \"spiny lobster\",\n            124: \"crayfish\",\n            125: \"hermit crab\",\n            126: \"isopod\",\n            127: \"white stork\",\n            128: \"black stork\",\n            129: \"spoonbill\",\n            130: \"flamingo\",\n            131: \"little blue heron\",\n            132: \"great egret\",\n            133: \"bittern bird\",\n            134: \"crane bird\",\n            135: \"limpkin\",\n            136: \"common gallinule\",\n            137: \"American coot\",\n            138: \"bustard\",\n            139: \"ruddy turnstone\",\n            140: \"dunlin\",\n            141: \"common redshank\",\n            142: \"dowitcher\",\n            143: \"oystercatcher\",\n            144: \"pelican\",\n            145: \"king penguin\",\n            146: \"albatross\",\n            147: \"grey whale\",\n            148: \"killer whale\",\n            149: \"dugong\",\n            150: \"sea lion\",\n            151: \"Chihuahua\",\n            152: \"Japanese Chin\",\n            153: \"Maltese\",\n            154: \"Pekingese\",\n            155: \"Shih Tzu\",\n            156: \"King Charles Spaniel\",\n            157: \"Papillon\",\n            158: \"toy terrier\",\n            159: \"Rhodesian Ridgeback\",\n            160: \"Afghan Hound\",\n            161: \"Basset Hound\",\n            162: \"Beagle\",\n            163: \"Bloodhound\",\n            164: \"Bluetick Coonhound\",\n            165: \"Black and Tan Coonhound\",\n            166: \"Treeing Walker Coonhound\",\n            167: \"English foxhound\",\n            168: \"Redbone Coonhound\",\n            169: \"borzoi\",\n            170: \"Irish Wolfhound\",\n            171: \"Italian Greyhound\",\n            172: \"Whippet\",\n            173: \"Ibizan Hound\",\n            174: \"Norwegian Elkhound\",\n            175: \"Otterhound\",\n            176: \"Saluki\",\n            177: \"Scottish Deerhound\",\n            178: \"Weimaraner\",\n            179: \"Staffordshire Bull Terrier\",\n            180: \"American Staffordshire Terrier\",\n            181: \"Bedlington Terrier\",\n            182: \"Border Terrier\",\n            183: \"Kerry Blue Terrier\",\n            184: \"Irish Terrier\",\n            185: \"Norfolk Terrier\",\n            186: \"Norwich Terrier\",\n            187: \"Yorkshire Terrier\",\n            188: \"Wire Fox Terrier\",\n            189: \"Lakeland Terrier\",\n            190: \"Sealyham Terrier\",\n            191: \"Airedale Terrier\",\n            192: \"Cairn Terrier\",\n            193: \"Australian Terrier\",\n            194: \"Dandie Dinmont Terrier\",\n            195: \"Boston Terrier\",\n            196: \"Miniature Schnauzer\",\n            197: \"Giant Schnauzer\",\n            198: \"Standard Schnauzer\",\n            199: \"Scottish Terrier\",\n            200: \"Tibetan Terrier\",\n            201: \"Australian Silky Terrier\",\n            202: \"Soft-coated Wheaten Terrier\",\n            203: \"West Highland White Terrier\",\n            204: \"Lhasa Apso\",\n            205: \"Flat-Coated Retriever\",\n            206: \"Curly-coated Retriever\",\n            207: \"Golden Retriever\",\n            208: \"Labrador Retriever\",\n            209: \"Chesapeake Bay Retriever\",\n            210: \"German Shorthaired Pointer\",\n            211: \"Vizsla\",\n            212: \"English Setter\",\n            213: \"Irish Setter\",\n            214: \"Gordon Setter\",\n            215: \"Brittany dog\",\n            216: \"Clumber Spaniel\",\n            217: \"English Springer Spaniel\",\n            218: \"Welsh Springer Spaniel\",\n            219: \"Cocker Spaniel\",\n            220: \"Sussex Spaniel\",\n            221: \"Irish Water Spaniel\",\n            222: \"Kuvasz\",\n            223: \"Schipperke\",\n            224: \"Groenendael dog\",\n            225: \"Malinois\",\n            226: \"Briard\",\n            227: \"Australian Kelpie\",\n            228: \"Komondor\",\n            229: \"Old English Sheepdog\",\n            230: \"Shetland Sheepdog\",\n            231: \"collie\",\n            232: \"Border Collie\",\n            233: \"Bouvier des Flandres dog\",\n            234: \"Rottweiler\",\n            235: \"German Shepherd Dog\",\n            236: \"Dobermann\",\n            237: \"Miniature Pinscher\",\n            238: \"Greater Swiss Mountain Dog\",\n            239: \"Bernese Mountain Dog\",\n            240: \"Appenzeller Sennenhund\",\n            241: \"Entlebucher Sennenhund\",\n            242: \"Boxer\",\n            243: \"Bullmastiff\",\n            244: \"Tibetan Mastiff\",\n            245: \"French Bulldog\",\n            246: \"Great Dane\",\n            247: \"St. Bernard\",\n            248: \"husky\",\n            249: \"Alaskan Malamute\",\n            250: \"Siberian Husky\",\n            251: \"Dalmatian\",\n            252: \"Affenpinscher\",\n            253: \"Basenji\",\n            254: \"pug\",\n            255: \"Leonberger\",\n            256: \"Newfoundland dog\",\n            257: \"Great Pyrenees dog\",\n            258: \"Samoyed\",\n            259: \"Pomeranian\",\n            260: \"Chow Chow\",\n            261: \"Keeshond\",\n            262: \"brussels griffon\",\n            263: \"Pembroke Welsh Corgi\",\n            264: \"Cardigan Welsh Corgi\",\n            265: \"Toy Poodle\",\n            266: \"Miniature Poodle\",\n            267: \"Standard Poodle\",\n            268: \"Mexican hairless dog (xoloitzcuintli)\",\n            269: \"grey wolf\",\n            270: \"Alaskan tundra wolf\",\n            271: \"red wolf or maned wolf\",\n            272: \"coyote\",\n            273: \"dingo\",\n            274: \"dhole\",\n            275: \"African wild dog\",\n            276: \"hyena\",\n            277: \"red fox\",\n            278: \"kit fox\",\n            279: \"Arctic fox\",\n            280: \"grey fox\",\n            281: \"tabby cat\",\n            282: \"tiger cat\",\n            283: \"Persian cat\",\n            284: \"Siamese cat\",\n            285: \"Egyptian Mau\",\n            286: \"cougar\",\n            287: \"lynx\",\n            288: \"leopard\",\n            289: \"snow leopard\",\n            290: \"jaguar\",\n            291: \"lion\",\n            292: \"tiger\",\n            293: \"cheetah\",\n            294: \"brown bear\",\n            295: \"American black bear\",\n            296: \"polar bear\",\n            297: \"sloth bear\",\n            298: \"mongoose\",\n            299: \"meerkat\",\n            300: \"tiger beetle\",\n            301: \"ladybug\",\n            302: \"ground beetle\",\n            303: \"longhorn beetle\",\n            304: \"leaf beetle\",\n            305: \"dung beetle\",\n            306: \"rhinoceros beetle\",\n            307: \"weevil\",\n            308: \"fly\",\n            309: \"bee\",\n            310: \"ant\",\n            311: \"grasshopper\",\n            312: \"cricket insect\",\n            313: \"stick insect\",\n            314: \"cockroach\",\n            315: \"praying mantis\",\n            316: \"cicada\",\n            317: \"leafhopper\",\n            318: \"lacewing\",\n            319: \"dragonfly\",\n            320: \"damselfly\",\n            321: \"red admiral butterfly\",\n            322: \"ringlet butterfly\",\n            323: \"monarch butterfly\",\n            324: \"small white butterfly\",\n            325: \"sulphur butterfly\",\n            326: \"gossamer-winged butterfly\",\n            327: \"starfish\",\n            328: \"sea urchin\",\n            329: \"sea cucumber\",\n            330: \"cottontail rabbit\",\n            331: \"hare\",\n            332: \"Angora rabbit\",\n            333: \"hamster\",\n            334: \"porcupine\",\n            335: \"fox squirrel\",\n            336: \"marmot\",\n            337: \"beaver\",\n            338: \"guinea pig\",\n            339: \"common sorrel horse\",\n            340: \"zebra\",\n            341: \"pig\",\n            342: \"wild boar\",\n            343: \"warthog\",\n            344: \"hippopotamus\",\n            345: \"ox\",\n            346: \"water buffalo\",\n            347: \"bison\",\n            348: \"ram (adult male sheep)\",\n            349: \"bighorn sheep\",\n            350: \"Alpine ibex\",\n            351: \"hartebeest\",\n            352: \"impala (antelope)\",\n            353: \"gazelle\",\n            354: \"arabian camel\",\n            355: \"llama\",\n            356: \"weasel\",\n            357: \"mink\",\n            358: \"European polecat\",\n            359: \"black-footed ferret\",\n            360: \"otter\",\n            361: \"skunk\",\n            362: \"badger\",\n            363: \"armadillo\",\n            364: \"three-toed sloth\",\n            365: \"orangutan\",\n            366: \"gorilla\",\n            367: \"chimpanzee\",\n            368: \"gibbon\",\n            369: \"siamang\",\n            370: \"guenon\",\n            371: \"patas monkey\",\n            372: \"baboon\",\n            373: \"macaque\",\n            374: \"langur\",\n            375: \"black-and-white colobus\",\n            376: \"proboscis monkey\",\n            377: \"marmoset\",\n            378: \"white-headed capuchin\",\n            379: \"howler monkey\",\n            380: \"titi monkey\",\n            381: \"Geoffroy's spider monkey\",\n            382: \"common squirrel monkey\",\n            383: \"ring-tailed lemur\",\n            384: \"indri\",\n            385: \"Asian elephant\",\n            386: \"African bush elephant\",\n            387: \"red panda\",\n            388: \"giant panda\",\n            389: \"snoek fish\",\n            390: \"eel\",\n            391: \"silver salmon\",\n            392: \"rock beauty fish\",\n            393: \"clownfish\",\n            394: \"sturgeon\",\n            395: \"gar fish\",\n            396: \"lionfish\",\n            397: \"pufferfish\",\n            398: \"abacus\",\n            399: \"abaya\",\n            400: \"academic gown\",\n            401: \"accordion\",\n            402: \"acoustic guitar\",\n            403: \"aircraft carrier\",\n            404: \"airliner\",\n            405: \"airship\",\n            406: \"altar\",\n            407: \"ambulance\",\n            408: \"amphibious vehicle\",\n            409: \"analog clock\",\n            410: \"apiary\",\n            411: \"apron\",\n            412: \"trash can\",\n            413: \"assault rifle\",\n            414: \"backpack\",\n            415: \"bakery\",\n            416: \"balance beam\",\n            417: \"balloon\",\n            418: \"ballpoint pen\",\n            419: \"Band-Aid\",\n            420: \"banjo\",\n            421: \"baluster / handrail\",\n            422: \"barbell\",\n            423: \"barber chair\",\n            424: \"barbershop\",\n            425: \"barn\",\n            426: \"barometer\",\n            427: \"barrel\",\n            428: \"wheelbarrow\",\n            429: \"baseball\",\n            430: \"basketball\",\n            431: \"bassinet\",\n            432: \"bassoon\",\n            433: \"swimming cap\",\n            434: \"bath towel\",\n            435: \"bathtub\",\n            436: \"station wagon\",\n            437: \"lighthouse\",\n            438: \"beaker\",\n            439: \"military hat (bearskin or shako)\",\n            440: \"beer bottle\",\n            441: \"beer glass\",\n            442: \"bell tower\",\n            443: \"baby bib\",\n            444: \"tandem bicycle\",\n            445: \"bikini\",\n            446: \"ring binder\",\n            447: \"binoculars\",\n            448: \"birdhouse\",\n            449: \"boathouse\",\n            450: \"bobsleigh\",\n            451: \"bolo tie\",\n            452: \"poke bonnet\",\n            453: \"bookcase\",\n            454: \"bookstore\",\n            455: \"bottle cap\",\n            456: \"hunting bow\",\n            457: \"bow tie\",\n            458: \"brass memorial plaque\",\n            459: \"bra\",\n            460: \"breakwater\",\n            461: \"breastplate\",\n            462: \"broom\",\n            463: \"bucket\",\n            464: \"buckle\",\n            465: \"bulletproof vest\",\n            466: \"high-speed train\",\n            467: \"butcher shop\",\n            468: \"taxicab\",\n            469: \"cauldron\",\n            470: \"candle\",\n            471: \"cannon\",\n            472: \"canoe\",\n            473: \"can opener\",\n            474: \"cardigan\",\n            475: \"car mirror\",\n            476: \"carousel\",\n            477: \"tool kit\",\n            478: \"cardboard box / carton\",\n            479: \"car wheel\",\n            480: \"automated teller machine\",\n            481: \"cassette\",\n            482: \"cassette player\",\n            483: \"castle\",\n            484: \"catamaran\",\n            485: \"CD player\",\n            486: \"cello\",\n            487: \"mobile phone\",\n            488: \"chain\",\n            489: \"chain-link fence\",\n            490: \"chain mail\",\n            491: \"chainsaw\",\n            492: \"storage chest\",\n            493: \"chiffonier\",\n            494: \"bell or wind chime\",\n            495: \"china cabinet\",\n            496: \"Christmas stocking\",\n            497: \"church\",\n            498: \"movie theater\",\n            499: \"cleaver\",\n            500: \"cliff dwelling\",\n            501: \"cloak\",\n            502: \"clogs\",\n            503: \"cocktail shaker\",\n            504: \"coffee mug\",\n            505: \"coffeemaker\",\n            506: \"spiral or coil\",\n            507: \"combination lock\",\n            508: \"computer keyboard\",\n            509: \"candy store\",\n            510: \"container ship\",\n            511: \"convertible\",\n            512: \"corkscrew\",\n            513: \"cornet\",\n            514: \"cowboy boot\",\n            515: \"cowboy hat\",\n            516: \"cradle\",\n            517: \"construction crane\",\n            518: \"crash helmet\",\n            519: \"crate\",\n            520: \"infant bed\",\n            521: \"Crock Pot\",\n            522: \"croquet ball\",\n            523: \"crutch\",\n            524: \"cuirass\",\n            525: \"dam\",\n            526: \"desk\",\n            527: \"desktop computer\",\n            528: \"rotary dial telephone\",\n            529: \"diaper\",\n            530: \"digital clock\",\n            531: \"digital watch\",\n            532: \"dining table\",\n            533: \"dishcloth\",\n            534: \"dishwasher\",\n            535: \"disc brake\",\n            536: \"dock\",\n            537: \"dog sled\",\n            538: \"dome\",\n            539: \"doormat\",\n            540: \"drilling rig\",\n            541: \"drum\",\n            542: \"drumstick\",\n            543: \"dumbbell\",\n            544: \"Dutch oven\",\n            545: \"electric fan\",\n            546: \"electric guitar\",\n            547: \"electric locomotive\",\n            548: \"entertainment center\",\n            549: \"envelope\",\n            550: \"espresso machine\",\n            551: \"face powder\",\n            552: \"feather boa\",\n            553: \"filing cabinet\",\n            554: \"fireboat\",\n            555: \"fire truck\",\n            556: \"fire screen\",\n            557: \"flagpole\",\n            558: \"flute\",\n            559: \"folding chair\",\n            560: \"football helmet\",\n            561: \"forklift\",\n            562: \"fountain\",\n            563: \"fountain pen\",\n            564: \"four-poster bed\",\n            565: \"freight car\",\n            566: \"French horn\",\n            567: \"frying pan\",\n            568: \"fur coat\",\n            569: \"garbage truck\",\n            570: \"gas mask or respirator\",\n            571: \"gas pump\",\n            572: \"goblet\",\n            573: \"go-kart\",\n            574: \"golf ball\",\n            575: \"golf cart\",\n            576: \"gondola\",\n            577: \"gong\",\n            578: \"gown\",\n            579: \"grand piano\",\n            580: \"greenhouse\",\n            581: \"radiator grille\",\n            582: \"grocery store\",\n            583: \"guillotine\",\n            584: \"hair clip\",\n            585: \"hair spray\",\n            586: \"half-track\",\n            587: \"hammer\",\n            588: \"hamper\",\n            589: \"hair dryer\",\n            590: \"hand-held computer\",\n            591: \"handkerchief\",\n            592: \"hard disk drive\",\n            593: \"harmonica\",\n            594: \"harp\",\n            595: \"combine harvester\",\n            596: \"hatchet\",\n            597: \"holster\",\n            598: \"home theater\",\n            599: \"honeycomb\",\n            600: \"hook\",\n            601: \"hoop skirt\",\n            602: \"gymnastic horizontal bar\",\n            603: \"horse-drawn vehicle\",\n            604: \"hourglass\",\n            605: \"iPod\",\n            606: \"clothes iron\",\n            607: \"carved pumpkin\",\n            608: \"jeans\",\n            609: \"jeep\",\n            610: \"T-shirt\",\n            611: \"jigsaw puzzle\",\n            612: \"rickshaw\",\n            613: \"joystick\",\n            614: \"kimono\",\n            615: \"knee pad\",\n            616: \"knot\",\n            617: \"lab coat\",\n            618: \"ladle\",\n            619: \"lampshade\",\n            620: \"laptop computer\",\n            621: \"lawn mower\",\n            622: \"lens cap\",\n            623: \"letter opener\",\n            624: \"library\",\n            625: \"lifeboat\",\n            626: \"lighter\",\n            627: \"limousine\",\n            628: \"ocean liner\",\n            629: \"lipstick\",\n            630: \"slip-on shoe\",\n            631: \"lotion\",\n            632: \"music speaker\",\n            633: \"loupe magnifying glass\",\n            634: \"sawmill\",\n            635: \"magnetic compass\",\n            636: \"messenger bag\",\n            637: \"mailbox\",\n            638: \"tights\",\n            639: \"one-piece bathing suit\",\n            640: \"manhole cover\",\n            641: \"maraca\",\n            642: \"marimba\",\n            643: \"mask\",\n            644: \"matchstick\",\n            645: \"maypole\",\n            646: \"maze\",\n            647: \"measuring cup\",\n            648: \"medicine cabinet\",\n            649: \"megalith\",\n            650: \"microphone\",\n            651: \"microwave oven\",\n            652: \"military uniform\",\n            653: \"milk can\",\n            654: \"minibus\",\n            655: \"miniskirt\",\n            656: \"minivan\",\n            657: \"missile\",\n            658: \"mitten\",\n            659: \"mixing bowl\",\n            660: \"mobile home\",\n            661: \"ford model t\",\n            662: \"modem\",\n            663: \"monastery\",\n            664: \"monitor\",\n            665: \"moped\",\n            666: \"mortar and pestle\",\n            667: \"graduation cap\",\n            668: \"mosque\",\n            669: \"mosquito net\",\n            670: \"vespa\",\n            671: \"mountain bike\",\n            672: \"tent\",\n            673: \"computer mouse\",\n            674: \"mousetrap\",\n            675: \"moving van\",\n            676: \"muzzle\",\n            677: \"metal nail\",\n            678: \"neck brace\",\n            679: \"necklace\",\n            680: \"baby pacifier\",\n            681: \"notebook computer\",\n            682: \"obelisk\",\n            683: \"oboe\",\n            684: \"ocarina\",\n            685: \"odometer\",\n            686: \"oil filter\",\n            687: \"pipe organ\",\n            688: \"oscilloscope\",\n            689: \"overskirt\",\n            690: \"bullock cart\",\n            691: \"oxygen mask\",\n            692: \"product packet / packaging\",\n            693: \"paddle\",\n            694: \"paddle wheel\",\n            695: \"padlock\",\n            696: \"paintbrush\",\n            697: \"pajamas\",\n            698: \"palace\",\n            699: \"pan flute\",\n            700: \"paper towel\",\n            701: \"parachute\",\n            702: \"parallel bars\",\n            703: \"park bench\",\n            704: \"parking meter\",\n            705: \"railroad car\",\n            706: \"patio\",\n            707: \"payphone\",\n            708: \"pedestal\",\n            709: \"pencil case\",\n            710: \"pencil sharpener\",\n            711: \"perfume\",\n            712: \"Petri dish\",\n            713: \"photocopier\",\n            714: \"plectrum\",\n            715: \"Pickelhaube\",\n            716: \"picket fence\",\n            717: \"pickup truck\",\n            718: \"pier\",\n            719: \"piggy bank\",\n            720: \"pill bottle\",\n            721: \"pillow\",\n            722: \"ping-pong ball\",\n            723: \"pinwheel\",\n            724: \"pirate ship\",\n            725: \"drink pitcher\",\n            726: \"block plane\",\n            727: \"planetarium\",\n            728: \"plastic bag\",\n            729: \"plate rack\",\n            730: \"farm plow\",\n            731: \"plunger\",\n            732: \"Polaroid camera\",\n            733: \"pole\",\n            734: \"police van\",\n            735: \"poncho\",\n            736: \"pool table\",\n            737: \"soda bottle\",\n            738: \"plant pot\",\n            739: \"potter's wheel\",\n            740: \"power drill\",\n            741: \"prayer rug\",\n            742: \"printer\",\n            743: \"prison\",\n            744: \"missile\",\n            745: \"projector\",\n            746: \"hockey puck\",\n            747: \"punching bag\",\n            748: \"purse\",\n            749: \"quill\",\n            750: \"quilt\",\n            751: \"race car\",\n            752: \"racket\",\n            753: \"radiator\",\n            754: \"radio\",\n            755: \"radio telescope\",\n            756: \"rain barrel\",\n            757: \"recreational vehicle\",\n            758: \"fishing casting reel\",\n            759: \"reflex camera\",\n            760: \"refrigerator\",\n            761: \"remote control\",\n            762: \"restaurant\",\n            763: \"revolver\",\n            764: \"rifle\",\n            765: \"rocking chair\",\n            766: \"rotisserie\",\n            767: \"eraser\",\n            768: \"rugby ball\",\n            769: \"ruler measuring stick\",\n            770: \"sneaker\",\n            771: \"safe\",\n            772: \"safety pin\",\n            773: \"salt shaker\",\n            774: \"sandal\",\n            775: \"sarong\",\n            776: \"saxophone\",\n            777: \"scabbard\",\n            778: \"weighing scale\",\n            779: \"school bus\",\n            780: \"schooner\",\n            781: \"scoreboard\",\n            782: \"CRT monitor\",\n            783: \"screw\",\n            784: \"screwdriver\",\n            785: \"seat belt\",\n            786: \"sewing machine\",\n            787: \"shield\",\n            788: \"shoe store\",\n            789: \"shoji screen / room divider\",\n            790: \"shopping basket\",\n            791: \"shopping cart\",\n            792: \"shovel\",\n            793: \"shower cap\",\n            794: \"shower curtain\",\n            795: \"ski\",\n            796: \"balaclava ski mask\",\n            797: \"sleeping bag\",\n            798: \"slide rule\",\n            799: \"sliding door\",\n            800: \"slot machine\",\n            801: \"snorkel\",\n            802: \"snowmobile\",\n            803: \"snowplow\",\n            804: \"soap dispenser\",\n            805: \"soccer ball\",\n            806: \"sock\",\n            807: \"solar thermal collector\",\n            808: \"sombrero\",\n            809: \"soup bowl\",\n            810: \"keyboard space bar\",\n            811: \"space heater\",\n            812: \"space shuttle\",\n            813: \"spatula\",\n            814: \"motorboat\",\n            815: \"spider web\",\n            816: \"spindle\",\n            817: \"sports car\",\n            818: \"spotlight\",\n            819: \"stage\",\n            820: \"steam locomotive\",\n            821: \"through arch bridge\",\n            822: \"steel drum\",\n            823: \"stethoscope\",\n            824: \"scarf\",\n            825: \"stone wall\",\n            826: \"stopwatch\",\n            827: \"stove\",\n            828: \"strainer\",\n            829: \"tram\",\n            830: \"stretcher\",\n            831: \"couch\",\n            832: \"stupa\",\n            833: \"submarine\",\n            834: \"suit\",\n            835: \"sundial\",\n            836: \"sunglasses\",\n            837: \"sunglasses\",\n            838: \"sunscreen\",\n            839: \"suspension bridge\",\n            840: \"mop\",\n            841: \"sweatshirt\",\n            842: \"swim trunks / shorts\",\n            843: \"swing\",\n            844: \"electrical switch\",\n            845: \"syringe\",\n            846: \"table lamp\",\n            847: \"tank\",\n            848: \"tape player\",\n            849: \"teapot\",\n            850: \"teddy bear\",\n            851: \"television\",\n            852: \"tennis ball\",\n            853: \"thatched roof\",\n            854: \"front curtain\",\n            855: \"thimble\",\n            856: \"threshing machine\",\n            857: \"throne\",\n            858: \"tile roof\",\n            859: \"toaster\",\n            860: \"tobacco shop\",\n            861: \"toilet seat\",\n            862: \"torch\",\n            863: \"totem pole\",\n            864: \"tow truck\",\n            865: \"toy store\",\n            866: \"tractor\",\n            867: \"semi-trailer truck\",\n            868: \"tray\",\n            869: \"trench coat\",\n            870: \"tricycle\",\n            871: \"trimaran\",\n            872: \"tripod\",\n            873: \"triumphal arch\",\n            874: \"trolleybus\",\n            875: \"trombone\",\n            876: \"hot tub\",\n            877: \"turnstile\",\n            878: \"typewriter keyboard\",\n            879: \"umbrella\",\n            880: \"unicycle\",\n            881: \"upright piano\",\n            882: \"vacuum cleaner\",\n            883: \"vase\",\n            884: \"vaulted or arched ceiling\",\n            885: \"velvet fabric\",\n            886: \"vending machine\",\n            887: \"vestment\",\n            888: \"viaduct\",\n            889: \"violin\",\n            890: \"volleyball\",\n            891: \"waffle iron\",\n            892: \"wall clock\",\n            893: \"wallet\",\n            894: \"wardrobe\",\n            895: \"military aircraft\",\n            896: \"sink\",\n            897: \"washing machine\",\n            898: \"water bottle\",\n            899: \"water jug\",\n            900: \"water tower\",\n            901: \"whiskey jug\",\n            902: \"whistle\",\n            903: \"hair wig\",\n            904: \"window screen\",\n            905: \"window shade\",\n            906: \"Windsor tie\",\n            907: \"wine bottle\",\n            908: \"airplane wing\",\n            909: \"wok\",\n            910: \"wooden spoon\",\n            911: \"wool\",\n            912: \"split-rail fence\",\n            913: \"shipwreck\",\n            914: \"sailboat\",\n            915: \"yurt\",\n            916: \"website\",\n            917: \"comic book\",\n            918: \"crossword\",\n            919: \"traffic or street sign\",\n            920: \"traffic light\",\n            921: \"dust jacket\",\n            922: \"menu\",\n            923: \"plate\",\n            924: \"guacamole\",\n            925: \"consomme\",\n            926: \"hot pot\",\n            927: \"trifle\",\n            928: \"ice cream\",\n            929: \"popsicle\",\n            930: \"baguette\",\n            931: \"bagel\",\n            932: \"pretzel\",\n            933: \"cheeseburger\",\n            934: \"hot dog\",\n            935: \"mashed potatoes\",\n            936: \"cabbage\",\n            937: \"broccoli\",\n            938: \"cauliflower\",\n            939: \"zucchini\",\n            940: \"spaghetti squash\",\n            941: \"acorn squash\",\n            942: \"butternut squash\",\n            943: \"cucumber\",\n            944: \"artichoke\",\n            945: \"bell pepper\",\n            946: \"cardoon\",\n            947: \"mushroom\",\n            948: \"Granny Smith apple\",\n            949: \"strawberry\",\n            950: \"orange\",\n            951: \"lemon\",\n            952: \"fig\",\n            953: \"pineapple\",\n            954: \"banana\",\n            955: \"jackfruit\",\n            956: \"cherimoya (custard apple)\",\n            957: \"pomegranate\",\n            958: \"hay\",\n            959: \"carbonara\",\n            960: \"chocolate syrup\",\n            961: \"dough\",\n            962: \"meatloaf\",\n            963: \"pizza\",\n            964: \"pot pie\",\n            965: \"burrito\",\n            966: \"red wine\",\n            967: \"espresso\",\n            968: \"tea cup\",\n            969: \"eggnog\",\n            970: \"mountain\",\n            971: \"bubble\",\n            972: \"cliff\",\n            973: \"coral reef\",\n            974: \"geyser\",\n            975: \"lakeshore\",\n            976: \"promontory\",\n            977: \"sandbar\",\n            978: \"beach\",\n            979: \"valley\",\n            980: \"volcano\",\n            981: \"baseball player\",\n            982: \"bridegroom\",\n            983: \"scuba diver\",\n            984: \"rapeseed\",\n            985: \"daisy\",\n            986: \"yellow lady's slipper\",\n            987: \"corn\",\n            988: \"acorn\",\n            989: \"rose hip\",\n            990: \"horse chestnut seed\",\n            991: \"coral fungus\",\n            992: \"agaric\",\n            993: \"gyromitra\",\n            994: \"stinkhorn mushroom\",\n            995: \"earth star fungus\",\n            996: \"hen of the woods mushroom\",\n            997: \"bolete\",\n            998: \"corn cob\",\n            999: \"toilet paper\",\n        }\n</code></pre>"},{"location":"api/#mmlearn.datasets.ImageNet.zero_shot_prompt_templates","title":"zero_shot_prompt_templates  <code>property</code>","text":"<pre><code>zero_shot_prompt_templates\n</code></pre> <p>Return the zero-shot prompt templates.</p>"},{"location":"api/#mmlearn.datasets.ImageNet.id2label","title":"id2label  <code>property</code>","text":"<pre><code>id2label\n</code></pre> <p>Return the label mapping.</p>"},{"location":"api/#mmlearn.datasets.ImageNet.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(index)\n</code></pre> <p>Get an example at the given index.</p> Source code in <code>mmlearn/datasets/imagenet.py</code> <pre><code>def __getitem__(self, index: int) -&gt; Example:\n    \"\"\"Get an example at the given index.\"\"\"\n    image, target = super().__getitem__(index)\n    example = Example(\n        {\n            Modalities.RGB.name: image,\n            Modalities.RGB.target: target,\n            EXAMPLE_INDEX_KEY: index,\n        }\n    )\n    mask = self.mask_generator() if self.mask_generator else None\n    if mask is not None:  # error will be raised during collation if `None`\n        example[Modalities.RGB.mask] = mask\n    return example\n</code></pre>"},{"location":"api/#mmlearn.datasets.LibriSpeech","title":"LibriSpeech","text":"<p>               Bases: <code>Dataset[Example]</code></p> <p>LibriSpeech dataset.</p> <p>This is a wrapper around class:<code>torchaudio.datasets.LIBRISPEECH</code> that assumes that the dataset is already downloaded and the top-level directory of the dataset in the root directory is <code>librispeech</code>.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>Root directory of dataset.</p> required <code>split</code> <code>(train - clean - 100, train - clean - 360, train - other - 500, dev - clean, dev - other, test - clean, test - other)</code> <p>Split of the dataset to use.</p> <code>\"train-clean-100\"</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If <code>torchaudio</code> is not installed.</p> Notes <p>This dataset only returns the audio and transcript from the dataset.</p> Source code in <code>mmlearn/datasets/librispeech.py</code> <pre><code>@store(\n    group=\"datasets\",\n    provider=\"mmlearn\",\n    root_dir=os.getenv(\"LIBRISPEECH_ROOT_DIR\", MISSING),\n)\nclass LibriSpeech(Dataset[Example]):\n    \"\"\"LibriSpeech dataset.\n\n    This is a wrapper around :py:class:`torchaudio.datasets.LIBRISPEECH` that assumes\n    that the dataset is already downloaded and the top-level directory of the dataset\n    in the root directory is `librispeech`.\n\n    Parameters\n    ----------\n    root_dir : str\n        Root directory of dataset.\n    split : {\"train-clean-100\", \"train-clean-360\", \"train-other-500\", \"dev-clean\", \"dev-other\", \"test-clean\", \"test-other\"}, default=\"train-clean-100\"\n        Split of the dataset to use.\n\n    Raises\n    ------\n    ImportError\n        If ``torchaudio`` is not installed.\n\n    Notes\n    -----\n    This dataset only returns the audio and transcript from the dataset.\n\n    \"\"\"  # noqa: W505\n\n    def __init__(self, root_dir: str, split: str = \"train-clean-100\") -&gt; None:\n        super().__init__()\n        if not _TORCHAUDIO_AVAILABLE:\n            raise ImportError(\n                \"LibriSpeech dataset requires `torchaudio`, which is not installed.\"\n            )\n        from torchaudio.datasets import LIBRISPEECH\n\n        self.dataset = LIBRISPEECH(\n            root=root_dir,\n            url=split,\n            download=False,\n            folder_in_archive=\"librispeech\",\n        )\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the length of the dataset.\"\"\"\n        return len(self.dataset)\n\n    def __getitem__(self, idx: int) -&gt; Example:\n        \"\"\"Return an example from the dataset.\"\"\"\n        waveform, sample_rate, transcript, _, _, _ = self.dataset[idx]\n        assert sample_rate == SAMPLE_RATE, (\n            f\"Expected sample rate to be `16000`, got {sample_rate}.\"\n        )\n        waveform = pad_or_trim(waveform.flatten())\n\n        return Example(\n            {\n                Modalities.AUDIO.name: waveform,\n                Modalities.TEXT.name: transcript,\n                EXAMPLE_INDEX_KEY: idx,\n            },\n        )\n</code></pre>"},{"location":"api/#mmlearn.datasets.LibriSpeech.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the length of the dataset.</p> Source code in <code>mmlearn/datasets/librispeech.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the length of the dataset.\"\"\"\n    return len(self.dataset)\n</code></pre>"},{"location":"api/#mmlearn.datasets.LibriSpeech.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> <p>Return an example from the dataset.</p> Source code in <code>mmlearn/datasets/librispeech.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; Example:\n    \"\"\"Return an example from the dataset.\"\"\"\n    waveform, sample_rate, transcript, _, _, _ = self.dataset[idx]\n    assert sample_rate == SAMPLE_RATE, (\n        f\"Expected sample rate to be `16000`, got {sample_rate}.\"\n    )\n    waveform = pad_or_trim(waveform.flatten())\n\n    return Example(\n        {\n            Modalities.AUDIO.name: waveform,\n            Modalities.TEXT.name: transcript,\n            EXAMPLE_INDEX_KEY: idx,\n        },\n    )\n</code></pre>"},{"location":"api/#mmlearn.datasets.LLVIPDataset","title":"LLVIPDataset","text":"<p>               Bases: <code>Dataset[Example]</code></p> <p>Low-Light Visible-Infrared Pair (LLVIP) dataset.</p> <p>Loads pairs of <code>RGB</code> and <code>THERMAL</code> images from the LLVIP dataset.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>Path to the root directory of the dataset. The directory should contain 'visible' and 'infrared' subdirectories.</p> required <code>train</code> <code>bool</code> <p>Flag to indicate whether to load the training or test set.</p> <code>True</code> <code>transform</code> <code>Optional[Callable[[Image], Tensor]]</code> <p>A callable that takes in a PIL image and returns a transformed version of the image as a PyTorch tensor. This is applied to both RGB and thermal images.</p> <code>None</code> Source code in <code>mmlearn/datasets/llvip.py</code> <pre><code>@store(\n    name=\"LLVIP\",\n    group=\"datasets\",\n    provider=\"mmlearn\",\n    root_dir=os.getenv(\"LLVIP_ROOT_DIR\", MISSING),\n)\nclass LLVIPDataset(Dataset[Example]):\n    \"\"\"Low-Light Visible-Infrared Pair (LLVIP) dataset.\n\n    Loads pairs of `RGB` and `THERMAL` images from the LLVIP dataset.\n\n    Parameters\n    ----------\n    root_dir : str\n        Path to the root directory of the dataset. The directory should contain\n        'visible' and 'infrared' subdirectories.\n    train : bool, default=True\n        Flag to indicate whether to load the training or test set.\n    transform : Optional[Callable[[PIL.Image], torch.Tensor]], optional, default=None\n        A callable that takes in a PIL image and returns a transformed version\n        of the image as a PyTorch tensor. This is applied to both RGB and thermal\n        images.\n    \"\"\"\n\n    def __init__(\n        self,\n        root_dir: str,\n        train: bool = True,\n        transform: Optional[Callable[[PILImage], torch.Tensor]] = None,\n    ):\n        self.path_images_rgb = os.path.join(\n            root_dir,\n            \"visible\",\n            \"train\" if train else \"test\",\n        )\n        self.path_images_ir = os.path.join(\n            root_dir, \"infrared\", \"train\" if train else \"test\"\n        )\n        self.train = train\n        self.transform = transform or transforms.ToTensor()\n\n        self.rgb_images = sorted(glob.glob(os.path.join(self.path_images_rgb, \"*.jpg\")))\n        self.ir_images = sorted(glob.glob(os.path.join(self.path_images_ir, \"*.jpg\")))\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the length of the dataset.\"\"\"\n        return len(self.rgb_images)\n\n    def __getitem__(self, idx: int) -&gt; Example:\n        \"\"\"Return an example from the dataset.\"\"\"\n        rgb_image_path = self.rgb_images[idx]\n        ir_image_path = self.ir_images[idx]\n\n        rgb_image = PILImage.open(rgb_image_path).convert(\"RGB\")\n        ir_image = PILImage.open(ir_image_path).convert(\"L\")\n\n        example = Example(\n            {\n                Modalities.RGB.name: self.transform(rgb_image),\n                Modalities.THERMAL.name: self.transform(ir_image),\n                EXAMPLE_INDEX_KEY: idx,\n            },\n        )\n\n        if self.train:\n            annot_path = (\n                rgb_image_path.replace(\"visible\", \"Annotations\")\n                .replace(\".jpg\", \".xml\")\n                .replace(\"train\", \"\")\n            )\n            annot = self._get_bbox(annot_path)\n            example[\"annotation\"] = {\n                \"bboxes\": torch.from_numpy(annot[\"bboxes\"]),\n                \"labels\": torch.from_numpy(annot[\"labels\"]),\n            }\n        return example\n\n    def _get_bbox(self, filename: str) -&gt; dict[str, np.ndarray]:\n        \"\"\"Parse the XML file to get bounding boxes and labels.\n\n        Parameters\n        ----------\n        filename : str\n            Path to the annotation XML file.\n\n        Returns\n        -------\n        dict\n            A dictionary containing bounding boxes and labels.\n        \"\"\"\n        try:\n            root = ET.parse(filename).getroot()\n\n            bboxes, labels = [], []\n            for obj in root.findall(\"object\"):\n                bbox_obj = obj.find(\"bndbox\")\n                bbox = [\n                    int(bbox_obj.find(dim).text)  # type: ignore[union-attr,arg-type]\n                    for dim in [\"xmin\", \"ymin\", \"xmax\", \"ymax\"]\n                ]\n                bboxes.append(bbox)\n                labels.append(1)  # Assuming 'person' is the only label\n            return {\n                \"bboxes\": np.array(bboxes).astype(\"float\"),\n                \"labels\": np.array(labels).astype(\"int\"),\n            }\n        except ET.ParseError as e:\n            raise ValueError(f\"Error parsing XML: {e}\") from None\n        except Exception as e:\n            raise RuntimeError(\n                f\"Error processing annotation file {filename}: {e}\",\n            ) from None\n</code></pre>"},{"location":"api/#mmlearn.datasets.LLVIPDataset.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the length of the dataset.</p> Source code in <code>mmlearn/datasets/llvip.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the length of the dataset.\"\"\"\n    return len(self.rgb_images)\n</code></pre>"},{"location":"api/#mmlearn.datasets.LLVIPDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> <p>Return an example from the dataset.</p> Source code in <code>mmlearn/datasets/llvip.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; Example:\n    \"\"\"Return an example from the dataset.\"\"\"\n    rgb_image_path = self.rgb_images[idx]\n    ir_image_path = self.ir_images[idx]\n\n    rgb_image = PILImage.open(rgb_image_path).convert(\"RGB\")\n    ir_image = PILImage.open(ir_image_path).convert(\"L\")\n\n    example = Example(\n        {\n            Modalities.RGB.name: self.transform(rgb_image),\n            Modalities.THERMAL.name: self.transform(ir_image),\n            EXAMPLE_INDEX_KEY: idx,\n        },\n    )\n\n    if self.train:\n        annot_path = (\n            rgb_image_path.replace(\"visible\", \"Annotations\")\n            .replace(\".jpg\", \".xml\")\n            .replace(\"train\", \"\")\n        )\n        annot = self._get_bbox(annot_path)\n        example[\"annotation\"] = {\n            \"bboxes\": torch.from_numpy(annot[\"bboxes\"]),\n            \"labels\": torch.from_numpy(annot[\"labels\"]),\n        }\n    return example\n</code></pre>"},{"location":"api/#mmlearn.datasets.NIHCXR","title":"NIHCXR","text":"<p>               Bases: <code>Dataset[Example]</code></p> <p>NIH Chest X-ray dataset.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>Directory which contains <code>.json</code> files stating all dataset entries.</p> required <code>split</code> <code>(train, test, bbox)</code> <p>Dataset split. \"bbox\" is a subset of \"test\" which contains bounding box info.</p> <code>\"train\"</code> <code>transform</code> <code>Optional[Callable[[Image], Tensor]]</code> <p>A callable that takes in a PIL image and returns a transformed version of the image as a PyTorch tensor.</p> <code>None</code> Source code in <code>mmlearn/datasets/nihcxr.py</code> <pre><code>@store(\n    group=\"datasets\",\n    provider=\"mmlearn\",\n    root_dir=os.getenv(\"NIH_CXR_DIR\", MISSING),\n    split=\"train\",\n)\nclass NIHCXR(Dataset[Example]):\n    \"\"\"NIH Chest X-ray dataset.\n\n    Parameters\n    ----------\n    root_dir : str\n        Directory which contains `.json` files stating all dataset entries.\n    split : {\"train\", \"test\", \"bbox\"}\n        Dataset split. \"bbox\" is a subset of \"test\" which contains bounding box info.\n    transform : Optional[Callable[[PIL.Image], torch.Tensor]], optional, default=None\n        A callable that takes in a PIL image and returns a transformed version\n        of the image as a PyTorch tensor.\n    \"\"\"\n\n    def __init__(\n        self,\n        root_dir: str,\n        split: Literal[\"train\", \"test\", \"bbox\"],\n        transform: Optional[Callable[[Image.Image], torch.Tensor]] = None,\n    ) -&gt; None:\n        assert split in [\"train\", \"test\", \"bbox\"], f\"split {split} is not available.\"\n        assert callable(transform) or transform is None, (\n            \"transform is not callable or None.\"\n        )\n\n        data_path = os.path.join(root_dir, split + \"_data.json\")\n\n        assert os.path.isfile(data_path), f\"entries file does not exist: {data_path}.\"\n\n        with open(data_path, \"rb\") as file:\n            entries = json.load(file)\n        self.entries = entries\n\n        if transform is not None:\n            self.transform = transform\n        else:\n            self.transform = Compose([Resize(224), CenterCrop(224), ToTensor()])\n\n        self.bbox = split == \"bbox\"\n\n    def __getitem__(self, idx: int) -&gt; Example:\n        \"\"\"Return image-label or image-label-tabular(bbox).\"\"\"\n        entry = self.entries[idx]\n        image = Image.open(entry[\"image_path\"]).convert(\"RGB\")\n        image = self.transform(image)\n        label = torch.tensor(entry[\"label\"])\n\n        example = Example(\n            {\n                Modalities.RGB.name: image,\n                Modalities.RGB.target: label,\n                \"qid\": entry[\"qid\"],\n                EXAMPLE_INDEX_KEY: idx,\n            }\n        )\n\n        if self.bbox:\n            example[\"bbox\"] = entry[\"bbox\"]\n\n        return example\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the length of the dataset.\"\"\"\n        return len(self.entries)\n</code></pre>"},{"location":"api/#mmlearn.datasets.NIHCXR.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> <p>Return image-label or image-label-tabular(bbox).</p> Source code in <code>mmlearn/datasets/nihcxr.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; Example:\n    \"\"\"Return image-label or image-label-tabular(bbox).\"\"\"\n    entry = self.entries[idx]\n    image = Image.open(entry[\"image_path\"]).convert(\"RGB\")\n    image = self.transform(image)\n    label = torch.tensor(entry[\"label\"])\n\n    example = Example(\n        {\n            Modalities.RGB.name: image,\n            Modalities.RGB.target: label,\n            \"qid\": entry[\"qid\"],\n            EXAMPLE_INDEX_KEY: idx,\n        }\n    )\n\n    if self.bbox:\n        example[\"bbox\"] = entry[\"bbox\"]\n\n    return example\n</code></pre>"},{"location":"api/#mmlearn.datasets.NIHCXR.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the length of the dataset.</p> Source code in <code>mmlearn/datasets/nihcxr.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the length of the dataset.\"\"\"\n    return len(self.entries)\n</code></pre>"},{"location":"api/#mmlearn.datasets.NYUv2Dataset","title":"NYUv2Dataset","text":"<p>               Bases: <code>Dataset[Example]</code></p> <p>NYUv2 dataset.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>Path to the root directory of the dataset.</p> required <code>split</code> <code>(train, test)</code> <p>Split of the dataset to use.</p> <code>\"train\"</code> <code>return_type</code> <code>(disparity, image)</code> <p>Return type of the depth images.</p> <ul> <li><code>\"disparity\"</code>: Return the depth image as disparity map.</li> <li><code>\"image\"</code>: Return the depth image as a 3-channel image.</li> </ul> <code>\"disparity\"</code> <code>rgb_transform</code> <code>Optional[Callable[[Image], Tensor]]</code> <p>A callable that takes in an RGB PIL image and returns a transformed version of the image as a PyTorch tensor.</p> <code>None</code> <code>depth_transform</code> <code>Optional[Callable[[Image], Tensor]]</code> <p>A callable that takes in a depth PIL image and returns a transformed version of the image as a PyTorch tensor.</p> <code>None</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If <code>opencv-python</code> is not installed.</p> Source code in <code>mmlearn/datasets/nyuv2.py</code> <pre><code>@store(\n    name=\"NYUv2\",\n    group=\"datasets\",\n    provider=\"mmlearn\",\n    root_dir=os.getenv(\"NYUV2_ROOT_DIR\", MISSING),\n)\nclass NYUv2Dataset(Dataset[Example]):\n    \"\"\"NYUv2 dataset.\n\n    Parameters\n    ----------\n    root_dir : str\n        Path to the root directory of the dataset.\n    split : {\"train\", \"test\"}, default=\"train\"\n        Split of the dataset to use.\n    return_type : {\"disparity\", \"image\"}, default=\"disparity\"\n        Return type of the depth images.\n\n        - `\"disparity\"`: Return the depth image as disparity map.\n        - `\"image\"`: Return the depth image as a 3-channel image.\n    rgb_transform: Callable[[PIL.Image], torch.Tensor], default=None\n        A callable that takes in an RGB PIL image and returns a transformed version\n        of the image as a PyTorch tensor.\n    depth_transform: Callable[[PIL.Image], torch.Tensor], default=None\n        A callable that takes in a depth PIL image and returns a transformed version\n        of the image as a PyTorch tensor.\n\n    Raises\n    ------\n    ImportError\n        If `opencv-python` is not installed.\n    \"\"\"\n\n    def __init__(\n        self,\n        root_dir: str,\n        split: Literal[\"train\", \"test\"] = \"train\",\n        return_type: Literal[\"disparity\", \"image\"] = \"disparity\",\n        rgb_transform: Optional[Callable[[PILImage], torch.Tensor]] = None,\n        depth_transform: Optional[Callable[[PILImage], torch.Tensor]] = None,\n    ) -&gt; None:\n        super().__init__()\n        if not _OPENCV_AVAILABLE:\n            raise ImportError(\n                \"NYUv2 dataset requires `opencv-python` which is not installed.\",\n            )\n        self._validate_args(root_dir, split, rgb_transform, depth_transform)\n        self.return_type = return_type\n\n        self.root_dir = root_dir\n        with open(os.path.join(root_dir, f\"{split}.txt\"), \"r\") as f:\n            file_ids = f.readlines()\n        file_ids = [f.strip() for f in file_ids]\n\n        root_dir = os.path.join(root_dir, split)\n        depth_files = [os.path.join(root_dir, \"depth\", f\"{f}.png\") for f in file_ids]\n        rgb_files = [os.path.join(root_dir, \"rgb\", f\"{f}.png\") for f in file_ids]\n\n        label_files = [\n            os.path.join(root_dir, \"scene_class\", f\"{f}.txt\") for f in file_ids\n        ]\n        labels = [str(open(f).read().strip()) for f in label_files]  # noqa: SIM115\n        labels = [label.replace(\"_\", \" \") for label in labels]\n        labels = [\n            _LABELS.index(label) if label in _LABELS else len(_LABELS)  # type: ignore\n            for label in labels\n        ]\n\n        # remove the samples with classes not in _LABELS\n        # this is to follow the same classes used in ImageBind\n        if split == \"test\":\n            valid_indices = [\n                i\n                for i, label in enumerate(labels)\n                if label &lt; len(_LABELS)  # type: ignore\n            ]\n            rgb_files = [rgb_files[i] for i in valid_indices]\n            depth_files = [depth_files[i] for i in valid_indices]\n            labels = [labels[i] for i in valid_indices]\n\n        self.samples = list(zip(rgb_files, depth_files, labels, strict=False))\n\n        self.rgb_transform = rgb_transform\n        self.depth_transform = depth_transform\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the length of the dataset.\"\"\"\n        return len(self.samples)\n\n    def _validate_args(\n        self,\n        root_dir: str,\n        split: str,\n        rgb_transform: Optional[Callable[[PILImage], torch.Tensor]],\n        depth_transform: Optional[Callable[[PILImage], torch.Tensor]],\n    ) -&gt; None:\n        \"\"\"Validate arguments.\"\"\"\n        if not os.path.isdir(root_dir):\n            raise NotADirectoryError(\n                f\"The given `root_dir` {root_dir} is not a directory\",\n            )\n        if split not in [\"train\", \"test\"]:\n            raise ValueError(\n                f\"Expected `split` to be one of `'train'` or `'test'`, but got {split}\",\n            )\n        if rgb_transform is not None and not callable(rgb_transform):\n            raise TypeError(\n                f\"Expected argument `rgb_transform` to be callable, but got {type(rgb_transform)}\",\n            )\n        if depth_transform is not None and not callable(depth_transform):\n            raise TypeError(\n                f\"Expected `depth_transform` to be callable, but got {type(depth_transform)}\",\n            )\n\n    def __getitem__(self, idx: int) -&gt; Example:\n        \"\"\"Return RGB and depth images at index `idx`.\"\"\"\n        # Read images\n        rgb_image = cv2.imread(self.samples[idx][0], cv2.IMREAD_UNCHANGED)\n        if self.rgb_transform is not None:\n            rgb_image = self.rgb_transform(to_pil_image(rgb_image))\n\n        if self.return_type == \"disparity\":\n            depth_image = depth_normalize(\n                self.samples[idx][1],\n            )\n        else:\n            # Using cv2 instead of PIL Image since we use PNG grayscale images.\n            depth_image = cv2.imread(\n                self.samples[idx][1],\n                cv2.IMREAD_GRAYSCALE,\n            )\n            # Make a 3-channel depth image to enable passing to a pretrained ViT.\n            depth_image = np.repeat(depth_image[:, :, np.newaxis], 3, axis=-1)\n\n        if self.depth_transform is not None:\n            depth_image = self.depth_transform(to_pil_image(depth_image))\n\n        return Example(\n            {\n                Modalities.RGB.name: rgb_image,\n                Modalities.DEPTH.name: depth_image,\n                EXAMPLE_INDEX_KEY: idx,\n                Modalities.DEPTH.target: self.samples[idx][2],\n            }\n        )\n</code></pre>"},{"location":"api/#mmlearn.datasets.NYUv2Dataset.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the length of the dataset.</p> Source code in <code>mmlearn/datasets/nyuv2.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the length of the dataset.\"\"\"\n    return len(self.samples)\n</code></pre>"},{"location":"api/#mmlearn.datasets.NYUv2Dataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> <p>Return RGB and depth images at index <code>idx</code>.</p> Source code in <code>mmlearn/datasets/nyuv2.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; Example:\n    \"\"\"Return RGB and depth images at index `idx`.\"\"\"\n    # Read images\n    rgb_image = cv2.imread(self.samples[idx][0], cv2.IMREAD_UNCHANGED)\n    if self.rgb_transform is not None:\n        rgb_image = self.rgb_transform(to_pil_image(rgb_image))\n\n    if self.return_type == \"disparity\":\n        depth_image = depth_normalize(\n            self.samples[idx][1],\n        )\n    else:\n        # Using cv2 instead of PIL Image since we use PNG grayscale images.\n        depth_image = cv2.imread(\n            self.samples[idx][1],\n            cv2.IMREAD_GRAYSCALE,\n        )\n        # Make a 3-channel depth image to enable passing to a pretrained ViT.\n        depth_image = np.repeat(depth_image[:, :, np.newaxis], 3, axis=-1)\n\n    if self.depth_transform is not None:\n        depth_image = self.depth_transform(to_pil_image(depth_image))\n\n    return Example(\n        {\n            Modalities.RGB.name: rgb_image,\n            Modalities.DEPTH.name: depth_image,\n            EXAMPLE_INDEX_KEY: idx,\n            Modalities.DEPTH.target: self.samples[idx][2],\n        }\n    )\n</code></pre>"},{"location":"api/#mmlearn.datasets.SUNRGBDDataset","title":"SUNRGBDDataset","text":"<p>               Bases: <code>Dataset[Example]</code></p> <p>SUN RGB-D dataset.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>Path to the root directory of the dataset.</p> required <code>split</code> <code>(train, test)</code> <p>Split of the dataset to use.</p> <code>\"train\"</code> <code>return_type</code> <code>(disparity, image)</code> <p>Return type of the depth images. If \"disparity\", the depth images are converted to disparity similar to the ImageBind implementation. Otherwise, return the depth image as a 3-channel image.</p> <code>\"disparity\"</code> <code>rgb_transform</code> <code>Optional[Callable[[Image], Tensor]]</code> <p>A callable that takes in an RGB PIL image and returns a transformed version of the image as a PyTorch tensor.</p> <code>None</code> <code>depth_transform</code> <code>Optional[Callable[[Image], Tensor]]</code> <p>A callable that takes in a depth PIL image and returns a transformed version of the image as a PyTorch tensor.</p> <code>None</code> References <p>.. [1] Repo followed to extract the dataset: https://github.com/TUI-NICR/nicr-scene-analysis-datasets</p> Source code in <code>mmlearn/datasets/sunrgbd.py</code> <pre><code>@store(\n    name=\"SUNRGBD\",\n    group=\"datasets\",\n    provider=\"mmlearn\",\n    root_dir=os.getenv(\"SUNRGBD_ROOT_DIR\", MISSING),\n)\nclass SUNRGBDDataset(Dataset[Example]):\n    \"\"\"SUN RGB-D dataset.\n\n    Parameters\n    ----------\n    root_dir : str\n        Path to the root directory of the dataset.\n    split : {\"train\", \"test\"}, default=\"train\"\n        Split of the dataset to use.\n    return_type : {\"disparity\", \"image\"}, default=\"disparity\"\n        Return type of the depth images. If \"disparity\", the depth images are\n        converted to disparity similar to the ImageBind implementation.\n        Otherwise, return the depth image as a 3-channel image.\n    rgb_transform: Callable[[PIL.Image], torch.Tensor], default=None\n        A callable that takes in an RGB PIL image and returns a transformed version\n        of the image as a PyTorch tensor.\n    depth_transform: Callable[[PIL.Image], torch.Tensor], default=None\n        A callable that takes in a depth PIL image and returns a transformed version\n        of the image as a PyTorch tensor.\n\n    References\n    ----------\n    .. [1] Repo followed to extract the dataset: https://github.com/TUI-NICR/nicr-scene-analysis-datasets\n    \"\"\"\n\n    def __init__(\n        self,\n        root_dir: str,\n        split: Literal[\"train\", \"test\"] = \"train\",\n        return_type: Literal[\"disparity\", \"image\"] = \"disparity\",\n        rgb_transform: Optional[Callable[[PILImage], torch.Tensor]] = None,\n        depth_transform: Optional[Callable[[PILImage], torch.Tensor]] = None,\n    ) -&gt; None:\n        super().__init__()\n        if not _OPENCV_AVAILABLE:\n            raise ImportError(\n                \"SUN RGB-D dataset requires `opencv-python` which is not installed.\",\n            )\n\n        self._validate_args(root_dir, split, rgb_transform, depth_transform)\n        self.return_type = return_type\n\n        self.root_dir = root_dir\n        with open(os.path.join(root_dir, f\"{split}.txt\"), \"r\") as f:\n            file_ids = f.readlines()\n        file_ids = [f.strip() for f in file_ids]\n\n        root_dir = os.path.join(root_dir, split)\n        depth_files = [os.path.join(root_dir, \"depth\", f\"{f}.png\") for f in file_ids]\n        rgb_files = [os.path.join(root_dir, \"rgb\", f\"{f}.jpg\") for f in file_ids]\n        intrinsic_files = [\n            os.path.join(root_dir, \"intrinsics\", f\"{f}.txt\") for f in file_ids\n        ]\n\n        sensor_types = [\n            file.removeprefix(os.path.join(root_dir, \"depth\")).split(os.sep)[1]\n            for file in depth_files\n        ]\n\n        label_files = [\n            os.path.join(root_dir, \"scene_class\", f\"{f}.txt\") for f in file_ids\n        ]\n        labels = []\n        for label_file in label_files:\n            with open(label_file, \"r\") as file:  # noqa: SIM115\n                labels.append(file.read().strip())\n        labels = [label.replace(\"_\", \" \") for label in labels]\n        labels = [\n            _LABELS.index(label) if label in _LABELS else len(_LABELS)  # type: ignore\n            for label in labels\n        ]\n\n        # remove the samples with classes not in _LABELS\n        # this is to follow the same classes used in ImageBind\n        if split == \"test\":\n            valid_indices = [\n                i\n                for i, label in enumerate(labels)\n                if label &lt; len(_LABELS)  # type: ignore\n            ]\n            rgb_files = [rgb_files[i] for i in valid_indices]\n            depth_files = [depth_files[i] for i in valid_indices]\n            labels = [labels[i] for i in valid_indices]\n            intrinsic_files = [intrinsic_files[i] for i in valid_indices]\n            sensor_types = [sensor_types[i] for i in valid_indices]\n\n        self.samples = list(\n            zip(\n                rgb_files,\n                depth_files,\n                labels,\n                intrinsic_files,\n                sensor_types,\n                strict=False,\n            )\n        )\n\n        self.rgb_transform = rgb_transform\n        self.depth_transform = depth_transform\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the length of the dataset.\"\"\"\n        return len(self.samples)\n\n    def _validate_args(\n        self,\n        root_dir: str,\n        split: str,\n        rgb_transform: Optional[Callable[[PILImage], torch.Tensor]],\n        depth_transform: Optional[Callable[[PILImage], torch.Tensor]],\n    ) -&gt; None:\n        \"\"\"Validate arguments.\"\"\"\n        if not os.path.isdir(root_dir):\n            raise NotADirectoryError(\n                f\"The given `root_dir` {root_dir} is not a directory\",\n            )\n        if split not in [\"train\", \"test\"]:\n            raise ValueError(\n                f\"Expected `split` to be one of `'train'` or `'test'`, but got {split}\",\n            )\n        if rgb_transform is not None and not callable(rgb_transform):\n            raise TypeError(\n                f\"Expected argument `rgb_transform` to be callable, but got {type(rgb_transform)}\",\n            )\n        if depth_transform is not None and not callable(depth_transform):\n            raise TypeError(\n                f\"Expected `depth_transform` to be callable, but got {type(depth_transform)}\",\n            )\n\n    def __getitem__(self, idx: int) -&gt; Example:\n        \"\"\"Return RGB and depth images at index `idx`.\"\"\"\n        # Read images\n        rgb_image = cv2.imread(self.samples[idx][0], cv2.IMREAD_UNCHANGED)\n        if self.rgb_transform is not None:\n            rgb_image = self.rgb_transform(to_pil_image(rgb_image))\n\n        if self.return_type == \"disparity\":\n            depth_image = convert_depth_to_disparity(\n                self.samples[idx][1],\n                self.samples[idx][3],\n                self.samples[idx][4],\n            )\n        else:\n            # Using cv2 instead of PIL Image since we use PNG grayscale images.\n            depth_image = cv2.imread(\n                self.samples[idx][1],\n                cv2.IMREAD_GRAYSCALE,\n            )\n            # Make a 3-channel depth image to enable passing to a pretrained ViT.\n            depth_image = np.repeat(depth_image[:, :, np.newaxis], 3, axis=-1)\n\n        if self.depth_transform is not None:\n            depth_image = self.depth_transform(to_pil_image(depth_image))\n\n        return Example(\n            {\n                Modalities.RGB.name: rgb_image,\n                Modalities.DEPTH.name: depth_image,\n                EXAMPLE_INDEX_KEY: idx,\n                Modalities.DEPTH.target: self.samples[idx][2],\n            }\n        )\n</code></pre>"},{"location":"api/#mmlearn.datasets.SUNRGBDDataset.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the length of the dataset.</p> Source code in <code>mmlearn/datasets/sunrgbd.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the length of the dataset.\"\"\"\n    return len(self.samples)\n</code></pre>"},{"location":"api/#mmlearn.datasets.SUNRGBDDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> <p>Return RGB and depth images at index <code>idx</code>.</p> Source code in <code>mmlearn/datasets/sunrgbd.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; Example:\n    \"\"\"Return RGB and depth images at index `idx`.\"\"\"\n    # Read images\n    rgb_image = cv2.imread(self.samples[idx][0], cv2.IMREAD_UNCHANGED)\n    if self.rgb_transform is not None:\n        rgb_image = self.rgb_transform(to_pil_image(rgb_image))\n\n    if self.return_type == \"disparity\":\n        depth_image = convert_depth_to_disparity(\n            self.samples[idx][1],\n            self.samples[idx][3],\n            self.samples[idx][4],\n        )\n    else:\n        # Using cv2 instead of PIL Image since we use PNG grayscale images.\n        depth_image = cv2.imread(\n            self.samples[idx][1],\n            cv2.IMREAD_GRAYSCALE,\n        )\n        # Make a 3-channel depth image to enable passing to a pretrained ViT.\n        depth_image = np.repeat(depth_image[:, :, np.newaxis], 3, axis=-1)\n\n    if self.depth_transform is not None:\n        depth_image = self.depth_transform(to_pil_image(depth_image))\n\n    return Example(\n        {\n            Modalities.RGB.name: rgb_image,\n            Modalities.DEPTH.name: depth_image,\n            EXAMPLE_INDEX_KEY: idx,\n            Modalities.DEPTH.target: self.samples[idx][2],\n        }\n    )\n</code></pre>"},{"location":"api/#mmlearn.datasets.chexpert","title":"chexpert","text":"<p>CheXpert Dataset.</p>"},{"location":"api/#mmlearn.datasets.chexpert.CheXpert","title":"CheXpert","text":"<p>               Bases: <code>Dataset[Example]</code></p> <p>CheXpert dataset.</p> <p>Each datapoint is a pair of <code>(image, target label)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>Directory which contains <code>.json</code> files stating all dataset entries.</p> required <code>split</code> <code>(train, valid)</code> <p>Dataset split.</p> <code>\"train\"</code> <code>labeler</code> <code>Optional[{chexpert, chexbert, vchexbert}]</code> <p>Labeler used to extract labels from the training images. \"valid\" split has no labeler, labeling for valid split was done by human radiologists.</p> <code>None</code> <code>transform</code> <code>Optional[Callable[[PIL.Image], torch.Tensor]</code> <p>A callable that takes in a PIL image and returns a transformed version of the image as a PyTorch tensor.</p> <code>None</code> Source code in <code>mmlearn/datasets/chexpert.py</code> <pre><code>@store(\n    group=\"datasets\",\n    provider=\"mmlearn\",\n    root_dir=os.getenv(\"CHEXPERT_ROOT_DIR\", MISSING),\n    split=\"train\",\n)\nclass CheXpert(Dataset[Example]):\n    \"\"\"CheXpert dataset.\n\n    Each datapoint is a pair of `(image, target label)`.\n\n    Parameters\n    ----------\n    root_dir : str\n        Directory which contains `.json` files stating all dataset entries.\n    split : {\"train\", \"valid\"}\n        Dataset split.\n    labeler : Optional[{\"chexpert\", \"chexbert\", \"vchexbert\"}], optional, default=None\n        Labeler used to extract labels from the training images. \"valid\" split\n        has no labeler, labeling for valid split was done by human radiologists.\n    transform : Optional[Callable[[PIL.Image], torch.Tensor], optional, default=None\n        A callable that takes in a PIL image and returns a transformed version\n        of the image as a PyTorch tensor.\n    \"\"\"\n\n    def __init__(\n        self,\n        root_dir: str,\n        split: Literal[\"train\", \"valid\"],\n        labeler: Optional[Literal[\"chexpert\", \"chexbert\", \"vchexbert\"]] = None,\n        transform: Optional[Callable[[Image.Image], torch.Tensor]] = None,\n    ) -&gt; None:\n        assert split in [\"train\", \"valid\"], f\"split {split} is not available.\"\n        assert labeler in [\"chexpert\", \"chexbert\", \"vchexbert\"] or labeler is None, (\n            f\"labeler {labeler} is not available.\"\n        )\n        assert callable(transform) or transform is None, (\n            \"transform is not callable or None.\"\n        )\n\n        if split == \"valid\":\n            data_file = f\"{split}_data.json\"\n        elif split == \"train\":\n            data_file = f\"{labeler}_{split}_data.json\"\n        data_path = os.path.join(root_dir, data_file)\n\n        assert os.path.isfile(data_path), f\"entries file does not exist: {data_path}.\"\n\n        with open(data_path, \"rb\") as file:\n            entries = json.load(file)\n        self.entries = entries\n\n        if transform is not None:\n            self.transform = transform\n        else:\n            self.transform = Compose([Resize(224), CenterCrop(224), ToTensor()])\n\n    def __getitem__(self, idx: int) -&gt; Example:\n        \"\"\"Return the idx'th datapoint.\"\"\"\n        entry = self.entries[idx]\n        image = Image.open(entry[\"image_path\"]).convert(\"RGB\")\n        image = self.transform(image)\n        label = torch.tensor(entry[\"label\"])\n\n        return Example(\n            {\n                Modalities.RGB.name: image,\n                Modalities.RGB.target: label,\n                \"qid\": entry[\"qid\"],\n                EXAMPLE_INDEX_KEY: idx,\n            }\n        )\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the length of the dataset.\"\"\"\n        return len(self.entries)\n</code></pre>"},{"location":"api/#mmlearn.datasets.chexpert.CheXpert.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> <p>Return the idx'th datapoint.</p> Source code in <code>mmlearn/datasets/chexpert.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; Example:\n    \"\"\"Return the idx'th datapoint.\"\"\"\n    entry = self.entries[idx]\n    image = Image.open(entry[\"image_path\"]).convert(\"RGB\")\n    image = self.transform(image)\n    label = torch.tensor(entry[\"label\"])\n\n    return Example(\n        {\n            Modalities.RGB.name: image,\n            Modalities.RGB.target: label,\n            \"qid\": entry[\"qid\"],\n            EXAMPLE_INDEX_KEY: idx,\n        }\n    )\n</code></pre>"},{"location":"api/#mmlearn.datasets.chexpert.CheXpert.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the length of the dataset.</p> Source code in <code>mmlearn/datasets/chexpert.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the length of the dataset.\"\"\"\n    return len(self.entries)\n</code></pre>"},{"location":"api/#mmlearn.datasets.core","title":"core","text":"<p>Modules for core dataloading functionality.</p>"},{"location":"api/#mmlearn.datasets.core.CombinedDataset","title":"CombinedDataset","text":"<p>               Bases: <code>Dataset[Example]</code></p> <p>Combine multiple datasets into one.</p> <p>This class is similar to class:<code>~torch.utils.data.ConcatDataset</code> but allows for combining iterable-style datasets with map-style datasets. The iterable-style datasets must implement the :meth:<code>__len__</code> method, which is used to determine the total length of the combined dataset. When an index is passed to the combined dataset, the dataset that contains the example at that index is determined and the example is retrieved from that dataset. Since iterable-style datasets do not support random access, the examples are retrieved sequentially from the iterable-style datasets. When the end of an iterable-style dataset is reached, the iterator is reset and the next example is retrieved from the beginning of the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>datasets</code> <code>Iterable[Union[Dataset, IterableDataset]]</code> <p>Iterable of datasets to combine.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the datasets in the input iterable are not instances of class:<code>~torch.utils.data.Dataset</code> or class:<code>~torch.utils.data.IterableDataset</code>.</p> <code>ValueError</code> <p>If the input iterable of datasets is empty.</p> Source code in <code>mmlearn/datasets/core/combined_dataset.py</code> <pre><code>class CombinedDataset(Dataset[Example]):\n    \"\"\"Combine multiple datasets into one.\n\n    This class is similar to :py:class:`~torch.utils.data.ConcatDataset` but allows\n    for combining iterable-style datasets with map-style datasets. The iterable-style\n    datasets must implement the :meth:`__len__` method, which is used to determine the\n    total length of the combined dataset. When an index is passed to the combined\n    dataset, the dataset that contains the example at that index is determined and\n    the example is retrieved from that dataset. Since iterable-style datasets do\n    not support random access, the examples are retrieved sequentially from the\n    iterable-style datasets. When the end of an iterable-style dataset is reached,\n    the iterator is reset and the next example is retrieved from the beginning of\n    the dataset.\n\n\n    Parameters\n    ----------\n    datasets : Iterable[Union[torch.utils.data.Dataset, torch.utils.data.IterableDataset]]\n        Iterable of datasets to combine.\n\n    Raises\n    ------\n    TypeError\n        If any of the datasets in the input iterable are not instances of\n        :py:class:`~torch.utils.data.Dataset` or :py:class:`~torch.utils.data.IterableDataset`.\n    ValueError\n        If the input iterable of datasets is empty.\n\n    \"\"\"  # noqa: W505\n\n    def __init__(\n        self, datasets: Iterable[Union[Dataset[Example], IterableDataset[Example]]]\n    ) -&gt; None:\n        self.datasets, _ = tree_flatten(datasets)\n        if not all(\n            isinstance(dataset, (Dataset, IterableDataset)) for dataset in self.datasets\n        ):\n            raise TypeError(\n                \"Expected argument `datasets` to be an iterable of `Dataset` or \"\n                f\"`IterableDataset` instances, but found: {self.datasets}\",\n            )\n        if len(self.datasets) == 0:\n            raise ValueError(\n                \"Expected a non-empty iterable of datasets but found an empty iterable\",\n            )\n\n        self._cumulative_sizes: list[int] = np.cumsum(\n            [len(dataset) for dataset in self.datasets]\n        ).tolist()\n        self._iterators: list[Iterator[Example]] = []\n        self._iter_dataset_mapping: dict[int, int] = {}\n\n        # create iterators for iterable datasets and map dataset index to iterator index\n        for idx, dataset in enumerate(self.datasets):\n            if isinstance(dataset, IterableDataset):\n                self._iterators.append(iter(dataset))\n                self._iter_dataset_mapping[idx] = len(self._iterators) - 1\n\n    def __getitem__(self, idx: int) -&gt; Example:\n        \"\"\"Return an example from the combined dataset.\"\"\"\n        if idx &lt; 0:  # handle negative indices\n            if -idx &gt; len(self):\n                raise IndexError(\n                    f\"Index {idx} is out of bounds for the combined dataset with \"\n                    f\"length {len(self)}\",\n                )\n            idx = len(self) + idx\n\n        dataset_idx = bisect.bisect_right(self._cumulative_sizes, idx)\n\n        curr_dataset = self.datasets[dataset_idx]\n        if isinstance(curr_dataset, IterableDataset):\n            iter_idx = self._iter_dataset_mapping[dataset_idx]\n            try:\n                example = next(self._iterators[iter_idx])\n            except StopIteration:\n                self._iterators[iter_idx] = iter(curr_dataset)\n                example = next(self._iterators[iter_idx])\n        else:\n            if dataset_idx == 0:\n                example_idx = idx\n            else:\n                example_idx = idx - self._cumulative_sizes[dataset_idx - 1]\n            example = curr_dataset[example_idx]\n\n        if not isinstance(example, Example):\n            raise TypeError(\n                \"Expected dataset examples to be instances of `Example` \"\n                f\"but found {type(example)}\",\n            )\n\n        if not hasattr(example, \"dataset_index\"):\n            example.dataset_index = dataset_idx\n        if not hasattr(example, \"example_ids\"):\n            example.create_ids()\n\n        return example\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the total number of examples in the combined dataset.\"\"\"\n        return self._cumulative_sizes[-1]\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.CombinedDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> <p>Return an example from the combined dataset.</p> Source code in <code>mmlearn/datasets/core/combined_dataset.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; Example:\n    \"\"\"Return an example from the combined dataset.\"\"\"\n    if idx &lt; 0:  # handle negative indices\n        if -idx &gt; len(self):\n            raise IndexError(\n                f\"Index {idx} is out of bounds for the combined dataset with \"\n                f\"length {len(self)}\",\n            )\n        idx = len(self) + idx\n\n    dataset_idx = bisect.bisect_right(self._cumulative_sizes, idx)\n\n    curr_dataset = self.datasets[dataset_idx]\n    if isinstance(curr_dataset, IterableDataset):\n        iter_idx = self._iter_dataset_mapping[dataset_idx]\n        try:\n            example = next(self._iterators[iter_idx])\n        except StopIteration:\n            self._iterators[iter_idx] = iter(curr_dataset)\n            example = next(self._iterators[iter_idx])\n    else:\n        if dataset_idx == 0:\n            example_idx = idx\n        else:\n            example_idx = idx - self._cumulative_sizes[dataset_idx - 1]\n        example = curr_dataset[example_idx]\n\n    if not isinstance(example, Example):\n        raise TypeError(\n            \"Expected dataset examples to be instances of `Example` \"\n            f\"but found {type(example)}\",\n        )\n\n    if not hasattr(example, \"dataset_index\"):\n        example.dataset_index = dataset_idx\n    if not hasattr(example, \"example_ids\"):\n        example.create_ids()\n\n    return example\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.CombinedDataset.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the total number of examples in the combined dataset.</p> Source code in <code>mmlearn/datasets/core/combined_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the total number of examples in the combined dataset.\"\"\"\n    return self._cumulative_sizes[-1]\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.DefaultDataCollator","title":"DefaultDataCollator  <code>dataclass</code>","text":"<p>Default data collator for batching examples.</p> <p>This data collator will collate a list of class:<code>~mmlearn.datasets.core.example.Example</code> objects into a batch. It can also apply processing functions to specified keys in the batch before returning it.</p> <p>Parameters:</p> Name Type Description Default <code>batch_processors</code> <code>Optional[dict[str, Callable[[Any], Any]]]</code> <p>Dictionary of callables to apply to the batch before returning it.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the batch processor for a key does not return a dictionary with the key in it.</p> Source code in <code>mmlearn/datasets/core/data_collator.py</code> <pre><code>@dataclass\nclass DefaultDataCollator:\n    \"\"\"Default data collator for batching examples.\n\n    This data collator will collate a list of :py:class:`~mmlearn.datasets.core.example.Example`\n    objects into a batch. It can also apply processing functions to specified keys\n    in the batch before returning it.\n\n    Parameters\n    ----------\n    batch_processors : Optional[dict[str, Callable[[Any], Any]]], optional, default=None\n        Dictionary of callables to apply to the batch before returning it.\n\n    Raises\n    ------\n    ValueError\n        If the batch processor for a key does not return a dictionary with the\n        key in it.\n    \"\"\"  # noqa: W505\n\n    #: Dictionary of callables to apply to the batch before returning it.\n    #: The key is the name of the key in the batch, and the value is the processing\n    #: function to apply to the key. The processing function must take a single\n    #: argument and return a single value. If the processing function returns\n    #: a dictionary, it must contain the key that was processed in it (all the\n    #: other keys will also be included in the batch).\n    batch_processors: Optional[dict[str, Callable[[Any], Any]]] = None\n\n    def __call__(self, examples: list[Example]) -&gt; dict[str, Any]:\n        \"\"\"Collate a list of `Example` objects and apply processing functions.\"\"\"\n        batch = collate_example_list(examples)\n\n        if self.batch_processors is not None:\n            for key, processor in self.batch_processors.items():\n                batch_key: str = key\n                if Modalities.has_modality(key):\n                    batch_key = Modalities.get_modality(key).name\n\n                if batch_key in batch:\n                    batch_processed = processor(batch[batch_key])\n                    if isinstance(batch_processed, Mapping):\n                        if batch_key not in batch_processed:\n                            raise ValueError(\n                                f\"Batch processor for '{key}' key must return a dictionary \"\n                                f\"with '{batch_key}' in it.\"\n                            )\n                        batch.update(batch_processed)\n                    else:\n                        batch[batch_key] = batch_processed\n\n        return batch\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.DefaultDataCollator.__call__","title":"__call__","text":"<pre><code>__call__(examples)\n</code></pre> <p>Collate a list of <code>Example</code> objects and apply processing functions.</p> Source code in <code>mmlearn/datasets/core/data_collator.py</code> <pre><code>def __call__(self, examples: list[Example]) -&gt; dict[str, Any]:\n    \"\"\"Collate a list of `Example` objects and apply processing functions.\"\"\"\n    batch = collate_example_list(examples)\n\n    if self.batch_processors is not None:\n        for key, processor in self.batch_processors.items():\n            batch_key: str = key\n            if Modalities.has_modality(key):\n                batch_key = Modalities.get_modality(key).name\n\n            if batch_key in batch:\n                batch_processed = processor(batch[batch_key])\n                if isinstance(batch_processed, Mapping):\n                    if batch_key not in batch_processed:\n                        raise ValueError(\n                            f\"Batch processor for '{key}' key must return a dictionary \"\n                            f\"with '{batch_key}' in it.\"\n                        )\n                    batch.update(batch_processed)\n                else:\n                    batch[batch_key] = batch_processed\n\n    return batch\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.Example","title":"Example","text":"<p>               Bases: <code>OrderedDict[Any, Any]</code></p> <p>A representation of a single example from a dataset.</p> <p>This class is a subclass of class:<code>~collections.OrderedDict</code> and provides attribute-style access. This means that <code>example[\"text\"]</code> and <code>example.text</code> are equivalent. All datasets in this library return examples as class:<code>~mmlearn.datasets.core.example.Example</code> objects.</p> <p>Parameters:</p> Name Type Description Default <code>init_dict</code> <code>Optional[MutableMapping[Hashable, Any]]</code> <p>Dictionary to init <code>Example</code> class with.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; example = Example({\"text\": torch.tensor(2)})\n&gt;&gt;&gt; example.text.zero_()\ntensor(0)\n&gt;&gt;&gt; example.context = torch.tensor(4)  # set custom attributes after initialization\n</code></pre> Source code in <code>mmlearn/datasets/core/example.py</code> <pre><code>class Example(OrderedDict[Any, Any]):\n    \"\"\"A representation of a single example from a dataset.\n\n    This class is a subclass of :py:class:`~collections.OrderedDict` and provides\n    attribute-style access. This means that `example[\"text\"]` and `example.text`\n    are equivalent. All datasets in this library return examples as\n    :py:class:`~mmlearn.datasets.core.example.Example` objects.\n\n\n    Parameters\n    ----------\n    init_dict : Optional[MutableMapping[Hashable, Any]], optional, default=None\n        Dictionary to init `Example` class with.\n\n    Examples\n    --------\n    &gt;&gt;&gt; example = Example({\"text\": torch.tensor(2)})\n    &gt;&gt;&gt; example.text.zero_()\n    tensor(0)\n    &gt;&gt;&gt; example.context = torch.tensor(4)  # set custom attributes after initialization\n    \"\"\"\n\n    def __init__(\n        self,\n        init_dict: Optional[MutableMapping[Hashable, Any]] = None,\n    ) -&gt; None:\n        if init_dict is None:\n            init_dict = {}\n        super().__init__(init_dict)\n\n    def create_ids(self) -&gt; None:\n        \"\"\"Create a unique id for the example from the dataset and example index.\n\n        This method combines the dataset index and example index to create an\n        attribute called `example_ids`, which is a dictionary of tensors. The\n        dictionary keys are all the keys in the example except for `example_ids`,\n        `example_index`, and `dataset_index`. The values are tensors of shape `(2,)`\n        containing the tuple `(dataset_index, example_index)` for each key.\n        The `example_ids` is used to (re-)identify pairs of examples from different\n        modalities after they have been combined into a batch.\n\n        Warns\n        -----\n        UserWarning\n            If the `example_index` and `dataset_index` attributes are not set.\n\n        Notes\n        -----\n        - The Example must have the following attributes set before calling this\n          this method: `example_index` (usually set/returned by the dataset) and\n          `dataset_index` (usually set by the :py:class:`~mmlearn.datasets.core.combined_dataset.CombinedDataset` object)\n        - The :py:func:`~mmlearn.datasets.core.example.find_matching_indices`\n          function can be used to find matching examples given two tensors of example ids.\n\n        \"\"\"  # noqa: W505\n        if hasattr(self, \"example_index\") and hasattr(self, \"dataset_index\"):\n            self.example_ids = {\n                key: torch.tensor([self.dataset_index, self.example_index])\n                for key in self.keys()\n                if key not in (\"example_ids\", \"example_index\", \"dataset_index\")\n            }\n        else:\n            rank_zero_warn(\n                \"Cannot create `example_ids` without `example_index` and `dataset_index` \"\n                \"attributes. Set these attributes before calling `create_ids`. \"\n                \"No `example_ids` was created.\",\n                stacklevel=2,\n                category=UserWarning,\n            )\n\n    def __getattr__(self, key: str) -&gt; Any:\n        \"\"\"Get attribute by key.\"\"\"\n        try:\n            return self[key]\n        except KeyError:\n            raise AttributeError(key) from None\n\n    def __setattr__(self, key: str, value: Any) -&gt; None:\n        \"\"\"Set attribute by key.\"\"\"\n        if isinstance(value, MutableMapping):\n            value = Example(value)\n        self[key] = value\n\n    def __setitem__(self, key: Hashable, value: Any) -&gt; None:\n        \"\"\"Set item by key.\"\"\"\n        if isinstance(value, MutableMapping):\n            value = Example(value)\n        super().__setitem__(key, value)\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.Example.create_ids","title":"create_ids","text":"<pre><code>create_ids()\n</code></pre> <p>Create a unique id for the example from the dataset and example index.</p> <p>This method combines the dataset index and example index to create an attribute called <code>example_ids</code>, which is a dictionary of tensors. The dictionary keys are all the keys in the example except for <code>example_ids</code>, <code>example_index</code>, and <code>dataset_index</code>. The values are tensors of shape <code>(2,)</code> containing the tuple <code>(dataset_index, example_index)</code> for each key. The <code>example_ids</code> is used to (re-)identify pairs of examples from different modalities after they have been combined into a batch.</p> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If the <code>example_index</code> and <code>dataset_index</code> attributes are not set.</p> Notes <ul> <li>The Example must have the following attributes set before calling this   this method: <code>example_index</code> (usually set/returned by the dataset) and   <code>dataset_index</code> (usually set by the class:<code>~mmlearn.datasets.core.combined_dataset.CombinedDataset</code> object)</li> <li>The func:<code>~mmlearn.datasets.core.example.find_matching_indices</code>   function can be used to find matching examples given two tensors of example ids.</li> </ul> Source code in <code>mmlearn/datasets/core/example.py</code> <pre><code>def create_ids(self) -&gt; None:\n    \"\"\"Create a unique id for the example from the dataset and example index.\n\n    This method combines the dataset index and example index to create an\n    attribute called `example_ids`, which is a dictionary of tensors. The\n    dictionary keys are all the keys in the example except for `example_ids`,\n    `example_index`, and `dataset_index`. The values are tensors of shape `(2,)`\n    containing the tuple `(dataset_index, example_index)` for each key.\n    The `example_ids` is used to (re-)identify pairs of examples from different\n    modalities after they have been combined into a batch.\n\n    Warns\n    -----\n    UserWarning\n        If the `example_index` and `dataset_index` attributes are not set.\n\n    Notes\n    -----\n    - The Example must have the following attributes set before calling this\n      this method: `example_index` (usually set/returned by the dataset) and\n      `dataset_index` (usually set by the :py:class:`~mmlearn.datasets.core.combined_dataset.CombinedDataset` object)\n    - The :py:func:`~mmlearn.datasets.core.example.find_matching_indices`\n      function can be used to find matching examples given two tensors of example ids.\n\n    \"\"\"  # noqa: W505\n    if hasattr(self, \"example_index\") and hasattr(self, \"dataset_index\"):\n        self.example_ids = {\n            key: torch.tensor([self.dataset_index, self.example_index])\n            for key in self.keys()\n            if key not in (\"example_ids\", \"example_index\", \"dataset_index\")\n        }\n    else:\n        rank_zero_warn(\n            \"Cannot create `example_ids` without `example_index` and `dataset_index` \"\n            \"attributes. Set these attributes before calling `create_ids`. \"\n            \"No `example_ids` was created.\",\n            stacklevel=2,\n            category=UserWarning,\n        )\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.Example.__getattr__","title":"__getattr__","text":"<pre><code>__getattr__(key)\n</code></pre> <p>Get attribute by key.</p> Source code in <code>mmlearn/datasets/core/example.py</code> <pre><code>def __getattr__(self, key: str) -&gt; Any:\n    \"\"\"Get attribute by key.\"\"\"\n    try:\n        return self[key]\n    except KeyError:\n        raise AttributeError(key) from None\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.Example.__setattr__","title":"__setattr__","text":"<pre><code>__setattr__(key, value)\n</code></pre> <p>Set attribute by key.</p> Source code in <code>mmlearn/datasets/core/example.py</code> <pre><code>def __setattr__(self, key: str, value: Any) -&gt; None:\n    \"\"\"Set attribute by key.\"\"\"\n    if isinstance(value, MutableMapping):\n        value = Example(value)\n    self[key] = value\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.Example.__setitem__","title":"__setitem__","text":"<pre><code>__setitem__(key, value)\n</code></pre> <p>Set item by key.</p> Source code in <code>mmlearn/datasets/core/example.py</code> <pre><code>def __setitem__(self, key: Hashable, value: Any) -&gt; None:\n    \"\"\"Set item by key.\"\"\"\n    if isinstance(value, MutableMapping):\n        value = Example(value)\n    super().__setitem__(key, value)\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.CombinedDatasetRatioSampler","title":"CombinedDatasetRatioSampler","text":"<p>               Bases: <code>Sampler[int]</code></p> <p>Sampler for weighted sampling from a class:<code>~mmlearn.datasets.core.combined_dataset.CombinedDataset</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>CombinedDataset</code> <p>An instance of class:<code>~mmlearn.datasets.core.combined_dataset.CombinedDataset</code> to sample from.</p> required <code>ratios</code> <code>Optional[Sequence[float]]</code> <p>A sequence of ratios for sampling from each dataset in the combined dataset. The length of the sequence must be equal to the number of datasets in the combined dataset (<code>dataset</code>). If <code>None</code>, the length of each dataset in the combined dataset is used as the ratio. The ratios are normalized to sum to 1.</p> <code>None</code> <code>num_samples</code> <code>Optional[int]</code> <p>The number of samples to draw from the combined dataset. If <code>None</code>, the sampler will draw as many samples as there are in the combined dataset. This number must yield at least one sample per dataset in the combined dataset, when multiplied by the corresponding ratio.</p> <code>None</code> <code>replacement</code> <code>bool</code> <p>Whether to sample with replacement or not.</p> <code>False</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the sampled indices or not. If <code>False</code>, the indices of each dataset will appear in the order they are stored in the combined dataset. This is similar to sequential sampling from each dataset. The datasets that make up the combined dataset are still sampled randomly.</p> <code>True</code> <code>rank</code> <code>Optional[int]</code> <p>Rank of the current process within :attr:<code>num_replicas</code>. By default, :attr:<code>rank</code> is retrieved from the current distributed group.</p> <code>None</code> <code>num_replicas</code> <code>Optional[int]</code> <p>Number of processes participating in distributed training. By default, :attr:<code>num_replicas</code> is retrieved from the current distributed group.</p> <code>None</code> <code>drop_last</code> <code>bool</code> <p>Whether to drop the last incomplete batch or not. If <code>True</code>, the sampler will drop samples to make the number of samples evenly divisible by the number of replicas in distributed mode.</p> <code>False</code> <code>seed</code> <code>int</code> <p>Random seed used to when sampling from the combined dataset and shuffling the sampled indices.</p> <code>0</code> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>CombinedDataset</code> <p>The dataset to sample from.</p> <code>num_samples</code> <code>int</code> <p>The number of samples to draw from the combined dataset.</p> <code>probs</code> <code>Tensor</code> <p>The probabilities for sampling from each dataset in the combined dataset. This is computed from the <code>ratios</code> argument and is normalized to sum to 1.</p> <code>replacement</code> <code>bool</code> <p>Whether to sample with replacement or not.</p> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the sampled indices or not.</p> <code>rank</code> <code>int</code> <p>Rank of the current process within :attr:<code>num_replicas</code>.</p> <code>num_replicas</code> <code>int</code> <p>Number of processes participating in distributed training.</p> <code>drop_last</code> <code>bool</code> <p>Whether to drop samples to make the number of samples evenly divisible by the number of replicas in distributed mode.</p> <code>seed</code> <code>int</code> <p>Random seed used to when sampling from the combined dataset and shuffling the sampled indices.</p> <code>epoch</code> <code>int</code> <p>Current epoch number. This is used to set the random seed. This is useful in distributed mode to ensure that each process receives a different random ordering of the samples.</p> <code>total_size</code> <code>int</code> <p>The total number of samples across all processes.</p> Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>@store(group=\"dataloader/sampler\", provider=\"mmlearn\")\nclass CombinedDatasetRatioSampler(Sampler[int]):\n    \"\"\"Sampler for weighted sampling from a :py:class:`~mmlearn.datasets.core.combined_dataset.CombinedDataset`.\n\n    Parameters\n    ----------\n    dataset : CombinedDataset\n        An instance of :py:class:`~mmlearn.datasets.core.combined_dataset.CombinedDataset`\n        to sample from.\n    ratios : Optional[Sequence[float]], optional, default=None\n        A sequence of ratios for sampling from each dataset in the combined dataset.\n        The length of the sequence must be equal to the number of datasets in the\n        combined dataset (`dataset`). If `None`, the length of each dataset in the\n        combined dataset is used as the ratio. The ratios are normalized to sum to 1.\n    num_samples : Optional[int], optional, default=None\n        The number of samples to draw from the combined dataset. If `None`, the\n        sampler will draw as many samples as there are in the combined dataset.\n        This number must yield at least one sample per dataset in the combined\n        dataset, when multiplied by the corresponding ratio.\n    replacement : bool, default=False\n        Whether to sample with replacement or not.\n    shuffle : bool, default=True\n        Whether to shuffle the sampled indices or not. If `False`, the indices of\n        each dataset will appear in the order they are stored in the combined dataset.\n        This is similar to sequential sampling from each dataset. The datasets\n        that make up the combined dataset are still sampled randomly.\n    rank : Optional[int], optional, default=None\n        Rank of the current process within :attr:`num_replicas`. By default,\n        :attr:`rank` is retrieved from the current distributed group.\n    num_replicas : Optional[int], optional, default=None\n        Number of processes participating in distributed training. By\n        default, :attr:`num_replicas` is retrieved from the current distributed group.\n    drop_last : bool, default=False\n        Whether to drop the last incomplete batch or not. If `True`, the sampler will\n        drop samples to make the number of samples evenly divisible by the number of\n        replicas in distributed mode.\n    seed : int, default=0\n        Random seed used to when sampling from the combined dataset and shuffling\n        the sampled indices.\n\n    Attributes\n    ----------\n    dataset : CombinedDataset\n        The dataset to sample from.\n    num_samples : int\n        The number of samples to draw from the combined dataset.\n    probs : torch.Tensor\n        The probabilities for sampling from each dataset in the combined dataset.\n        This is computed from the `ratios` argument and is normalized to sum to 1.\n    replacement : bool\n        Whether to sample with replacement or not.\n    shuffle : bool\n        Whether to shuffle the sampled indices or not.\n    rank : int\n        Rank of the current process within :attr:`num_replicas`.\n    num_replicas : int\n        Number of processes participating in distributed training.\n    drop_last : bool\n        Whether to drop samples to make the number of samples evenly divisible by the\n        number of replicas in distributed mode.\n    seed : int\n        Random seed used to when sampling from the combined dataset and shuffling\n        the sampled indices.\n    epoch : int\n        Current epoch number. This is used to set the random seed. This is useful\n        in distributed mode to ensure that each process receives a different random\n        ordering of the samples.\n    total_size : int\n        The total number of samples across all processes.\n    \"\"\"  # noqa: W505\n\n    def __init__(  # noqa: PLR0912\n        self,\n        dataset: CombinedDataset,\n        ratios: Optional[Sequence[float]] = None,\n        num_samples: Optional[int] = None,\n        replacement: bool = False,\n        shuffle: bool = True,\n        rank: Optional[int] = None,\n        num_replicas: Optional[int] = None,\n        drop_last: bool = False,\n        seed: int = 0,\n    ):\n        if not isinstance(dataset, CombinedDataset):\n            raise TypeError(\n                \"Expected argument `dataset` to be of type `CombinedDataset`, \"\n                f\"but got {type(dataset)}.\",\n            )\n        if not isinstance(seed, int):\n            raise TypeError(\n                f\"Expected argument `seed` to be an integer, but got {type(seed)}.\",\n            )\n        if num_replicas is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            num_replicas = dist.get_world_size()\n        if rank is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            rank = dist.get_rank()\n        if rank &gt;= num_replicas or rank &lt; 0:\n            raise ValueError(\n                f\"Invalid rank {rank}, rank should be in the interval [0, {num_replicas - 1}]\"\n            )\n\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.drop_last = drop_last\n        self.replacement = replacement\n        self.shuffle = shuffle\n        self.seed = seed\n        self.epoch = 0\n\n        self._num_samples = num_samples\n        if not isinstance(self.num_samples, int) or self.num_samples &lt;= 0:\n            raise ValueError(\n                \"Expected argument `num_samples` to be a positive integer, but got \"\n                f\"{self.num_samples}.\",\n            )\n\n        if ratios is None:\n            ratios = [len(subset) for subset in self.dataset.datasets]\n\n        num_datasets = len(self.dataset.datasets)\n        if len(ratios) != num_datasets:\n            raise ValueError(\n                f\"Expected argument `ratios` to be of length {num_datasets}, \"\n                f\"but got length {len(ratios)}.\",\n            )\n        prob_sum = sum(ratios)\n        if not all(ratio &gt;= 0 for ratio in ratios) and prob_sum &gt; 0:\n            raise ValueError(\n                \"Expected argument `ratios` to be a sequence of non-negative numbers. \"\n                f\"Got {ratios}.\",\n            )\n        self.probs = torch.tensor(\n            [ratio / prob_sum for ratio in ratios],\n            dtype=torch.double,\n        )\n        if any((prob * self.num_samples) &lt;= 0 for prob in self.probs):\n            raise ValueError(\n                \"Expected dataset ratio to result in at least one sample per dataset. \"\n                f\"Got dataset sizes {self.probs * self.num_samples}.\",\n            )\n\n    @property\n    def num_samples(self) -&gt; int:\n        \"\"\"Return the number of samples managed by the sampler.\"\"\"\n        # dataset size might change at runtime\n        if self._num_samples is None:\n            num_samples = len(self.dataset)\n        else:\n            num_samples = self._num_samples\n\n        if self.drop_last and num_samples % self.num_replicas != 0:\n            # split to nearest available length that is evenly divisible.\n            # This is to ensure each rank receives the same amount of data when\n            # using this Sampler.\n            num_samples = math.ceil(\n                (num_samples - self.num_replicas) / self.num_replicas,\n            )\n        else:\n            num_samples = math.ceil(num_samples / self.num_replicas)\n        return num_samples\n\n    @property\n    def total_size(self) -&gt; int:\n        \"\"\"Return the total size of the dataset.\"\"\"\n        return self.num_samples * self.num_replicas\n\n    def __iter__(self) -&gt; Iterator[int]:\n        \"\"\"Return an iterator that yields sample indices for the combined dataset.\"\"\"\n        generator = torch.Generator()\n        seed = self.seed + self.epoch\n        generator.manual_seed(seed)\n\n        cumulative_sizes = [0] + self.dataset._cumulative_sizes\n        num_samples_per_dataset = [int(prob * self.total_size) for prob in self.probs]\n        indices = []\n        for i in range(len(self.dataset.datasets)):\n            per_dataset_indices: torch.Tensor = torch.multinomial(\n                torch.ones(cumulative_sizes[i + 1] - cumulative_sizes[i]),\n                num_samples_per_dataset[i],\n                replacement=self.replacement,\n                generator=generator,\n            )\n            # adjust indices to reflect position in cumulative dataset\n            per_dataset_indices += cumulative_sizes[i]\n            assert per_dataset_indices.max() &lt; cumulative_sizes[i + 1], (\n                f\"Indices from dataset {i} exceed dataset size. \"\n                f\"Got indices {per_dataset_indices} and dataset size {cumulative_sizes[i + 1]}.\",\n            )\n            indices.append(per_dataset_indices)\n\n        indices = torch.cat(indices)\n        if self.shuffle:\n            rand_indices = torch.randperm(len(indices), generator=generator)\n            indices = indices[rand_indices]\n\n        indices = indices.tolist()  # type: ignore[attr-defined]\n        num_indices = len(indices)\n\n        if num_indices &lt; self.total_size:\n            padding_size = self.total_size - num_indices\n            if padding_size &lt;= num_indices:\n                indices += indices[:padding_size]\n            else:\n                indices += (indices * math.ceil(padding_size / num_indices))[\n                    :padding_size\n                ]\n        elif num_indices &gt; self.total_size:\n            indices = indices[: self.total_size]\n        assert len(indices) == self.total_size\n\n        # subsample\n        indices = indices[self.rank : self.total_size : self.num_replicas]\n        assert len(indices) == self.num_samples, (\n            f\"Expected {self.num_samples} samples, but got {len(indices)}.\",\n        )\n\n        yield from iter(indices)\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the total number of samples in the sampler.\"\"\"\n        return self.num_samples\n\n    def set_epoch(self, epoch: int) -&gt; None:\n        \"\"\"Set the epoch for this sampler.\n\n        When :attr:`shuffle=True`, this ensures all replicas use a different random\n        ordering for each epoch. Otherwise, the next iteration of this sampler\n        will yield the same ordering.\n\n        Parameters\n        ----------\n        epoch : int\n            Epoch number.\n\n        \"\"\"\n        self.epoch = epoch\n\n        # some iterable datasets (especially huggingface iterable datasets) might\n        # require setting the epoch to ensure shuffling works properly\n        for dataset in self.dataset.datasets:\n            if hasattr(dataset, \"set_epoch\"):\n                dataset.set_epoch(epoch)\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.CombinedDatasetRatioSampler.num_samples","title":"num_samples  <code>property</code>","text":"<pre><code>num_samples\n</code></pre> <p>Return the number of samples managed by the sampler.</p>"},{"location":"api/#mmlearn.datasets.core.CombinedDatasetRatioSampler.total_size","title":"total_size  <code>property</code>","text":"<pre><code>total_size\n</code></pre> <p>Return the total size of the dataset.</p>"},{"location":"api/#mmlearn.datasets.core.CombinedDatasetRatioSampler.__iter__","title":"__iter__","text":"<pre><code>__iter__()\n</code></pre> <p>Return an iterator that yields sample indices for the combined dataset.</p> Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>def __iter__(self) -&gt; Iterator[int]:\n    \"\"\"Return an iterator that yields sample indices for the combined dataset.\"\"\"\n    generator = torch.Generator()\n    seed = self.seed + self.epoch\n    generator.manual_seed(seed)\n\n    cumulative_sizes = [0] + self.dataset._cumulative_sizes\n    num_samples_per_dataset = [int(prob * self.total_size) for prob in self.probs]\n    indices = []\n    for i in range(len(self.dataset.datasets)):\n        per_dataset_indices: torch.Tensor = torch.multinomial(\n            torch.ones(cumulative_sizes[i + 1] - cumulative_sizes[i]),\n            num_samples_per_dataset[i],\n            replacement=self.replacement,\n            generator=generator,\n        )\n        # adjust indices to reflect position in cumulative dataset\n        per_dataset_indices += cumulative_sizes[i]\n        assert per_dataset_indices.max() &lt; cumulative_sizes[i + 1], (\n            f\"Indices from dataset {i} exceed dataset size. \"\n            f\"Got indices {per_dataset_indices} and dataset size {cumulative_sizes[i + 1]}.\",\n        )\n        indices.append(per_dataset_indices)\n\n    indices = torch.cat(indices)\n    if self.shuffle:\n        rand_indices = torch.randperm(len(indices), generator=generator)\n        indices = indices[rand_indices]\n\n    indices = indices.tolist()  # type: ignore[attr-defined]\n    num_indices = len(indices)\n\n    if num_indices &lt; self.total_size:\n        padding_size = self.total_size - num_indices\n        if padding_size &lt;= num_indices:\n            indices += indices[:padding_size]\n        else:\n            indices += (indices * math.ceil(padding_size / num_indices))[\n                :padding_size\n            ]\n    elif num_indices &gt; self.total_size:\n        indices = indices[: self.total_size]\n    assert len(indices) == self.total_size\n\n    # subsample\n    indices = indices[self.rank : self.total_size : self.num_replicas]\n    assert len(indices) == self.num_samples, (\n        f\"Expected {self.num_samples} samples, but got {len(indices)}.\",\n    )\n\n    yield from iter(indices)\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.CombinedDatasetRatioSampler.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the total number of samples in the sampler.</p> Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the total number of samples in the sampler.\"\"\"\n    return self.num_samples\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.CombinedDatasetRatioSampler.set_epoch","title":"set_epoch","text":"<pre><code>set_epoch(epoch)\n</code></pre> <p>Set the epoch for this sampler.</p> <p>When :attr:<code>shuffle=True</code>, this ensures all replicas use a different random ordering for each epoch. Otherwise, the next iteration of this sampler will yield the same ordering.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>Epoch number.</p> required Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>def set_epoch(self, epoch: int) -&gt; None:\n    \"\"\"Set the epoch for this sampler.\n\n    When :attr:`shuffle=True`, this ensures all replicas use a different random\n    ordering for each epoch. Otherwise, the next iteration of this sampler\n    will yield the same ordering.\n\n    Parameters\n    ----------\n    epoch : int\n        Epoch number.\n\n    \"\"\"\n    self.epoch = epoch\n\n    # some iterable datasets (especially huggingface iterable datasets) might\n    # require setting the epoch to ensure shuffling works properly\n    for dataset in self.dataset.datasets:\n        if hasattr(dataset, \"set_epoch\"):\n            dataset.set_epoch(epoch)\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.DistributedEvalSampler","title":"DistributedEvalSampler","text":"<p>               Bases: <code>Sampler[int]</code></p> <p>Sampler for distributed evaluation.</p> <p>The main differences between this and class:<code>torch.utils.data.DistributedSampler</code> are that this sampler does not add extra samples to make it evenly divisible and shuffling is disabled by default.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>Dataset used for sampling.</p> required <code>num_replicas</code> <code>Optional[int]</code> <p>Number of processes participating in distributed training. By default, :attr:<code>rank</code> is retrieved from the current distributed group.</p> <code>None</code> <code>rank</code> <code>Optional[int]</code> <p>Rank of the current process within :attr:<code>num_replicas</code>. By default, :attr:<code>rank</code> is retrieved from the current distributed group.</p> <code>None</code> <code>shuffle</code> <code>bool</code> <p>If <code>True</code> (default), sampler will shuffle the indices.</p> <code>False</code> <code>seed</code> <code>int</code> <p>Random seed used to shuffle the sampler if :attr:<code>shuffle=True</code>. This number should be identical across all processes in the distributed group.</p> <code>0</code> Warnings <p>DistributedEvalSampler should NOT be used for training. The distributed processes could hang forever. See [1]_ for details</p> Notes <ul> <li>This sampler is for evaluation purpose where synchronization does not happen   every epoch. Synchronization should be done outside the dataloader loop.   It is especially useful in conjunction with   class:<code>torch.nn.parallel.DistributedDataParallel</code> [2]_.</li> <li>The input Dataset is assumed to be of constant size.</li> <li>This implementation is adapted from [3]_.</li> </ul> References <p>.. [1] https://github.com/pytorch/pytorch/issues/22584 .. [2] https://discuss.pytorch.org/t/how-to-validate-in-distributeddataparallel-correctly/94267/11 .. [3] https://github.com/SeungjunNah/DeepDeblur-PyTorch/blob/master/src/data/sampler.py</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; def example():\n...     start_epoch, n_epochs = 0, 2\n...     sampler = DistributedEvalSampler(dataset) if is_distributed else None\n...     loader = DataLoader(dataset, shuffle=(sampler is None), sampler=sampler)\n...     for epoch in range(start_epoch, n_epochs):\n...         if is_distributed:\n...             sampler.set_epoch(epoch)\n...         evaluate(loader)\n</code></pre> Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>@store(group=\"dataloader/sampler\", provider=\"mmlearn\")\nclass DistributedEvalSampler(Sampler[int]):\n    \"\"\"Sampler for distributed evaluation.\n\n    The main differences between this and :py:class:`torch.utils.data.DistributedSampler`\n    are that this sampler does not add extra samples to make it evenly divisible and\n    shuffling is disabled by default.\n\n    Parameters\n    ----------\n    dataset : torch.utils.data.Dataset\n        Dataset used for sampling.\n    num_replicas : Optional[int], optional, default=None\n        Number of processes participating in distributed training. By\n        default, :attr:`rank` is retrieved from the current distributed group.\n    rank : Optional[int], optional, default=None\n        Rank of the current process within :attr:`num_replicas`. By default,\n        :attr:`rank` is retrieved from the current distributed group.\n    shuffle : bool, optional, default=False\n        If `True` (default), sampler will shuffle the indices.\n    seed : int, optional, default=0\n        Random seed used to shuffle the sampler if :attr:`shuffle=True`.\n        This number should be identical across all processes in the\n        distributed group.\n\n    Warnings\n    --------\n    DistributedEvalSampler should NOT be used for training. The distributed processes\n    could hang forever. See [1]_ for details\n\n    Notes\n    -----\n    - This sampler is for evaluation purpose where synchronization does not happen\n      every epoch. Synchronization should be done outside the dataloader loop.\n      It is especially useful in conjunction with\n      :py:class:`torch.nn.parallel.DistributedDataParallel` [2]_.\n    - The input Dataset is assumed to be of constant size.\n    - This implementation is adapted from [3]_.\n\n    References\n    ----------\n    .. [1] https://github.com/pytorch/pytorch/issues/22584\n    .. [2] https://discuss.pytorch.org/t/how-to-validate-in-distributeddataparallel-correctly/94267/11\n    .. [3] https://github.com/SeungjunNah/DeepDeblur-PyTorch/blob/master/src/data/sampler.py\n\n\n    Examples\n    --------\n    &gt;&gt;&gt; def example():\n    ...     start_epoch, n_epochs = 0, 2\n    ...     sampler = DistributedEvalSampler(dataset) if is_distributed else None\n    ...     loader = DataLoader(dataset, shuffle=(sampler is None), sampler=sampler)\n    ...     for epoch in range(start_epoch, n_epochs):\n    ...         if is_distributed:\n    ...             sampler.set_epoch(epoch)\n    ...         evaluate(loader)\n\n    \"\"\"  # noqa: W505\n\n    def __init__(\n        self,\n        dataset: Dataset[Sized],\n        num_replicas: Optional[int] = None,\n        rank: Optional[int] = None,\n        shuffle: bool = False,\n        seed: int = 0,\n    ) -&gt; None:\n        if num_replicas is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            num_replicas = dist.get_world_size()\n        if rank is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            rank = dist.get_rank()\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.epoch = 0\n        self.shuffle = shuffle\n        self.seed = seed\n\n    @property\n    def total_size(self) -&gt; int:\n        \"\"\"Return the total size of the dataset.\"\"\"\n        return len(self.dataset)\n\n    @property\n    def num_samples(self) -&gt; int:\n        \"\"\"Return the number of samples managed by the sampler.\"\"\"\n        indices = list(range(self.total_size))[\n            self.rank : self.total_size : self.num_replicas\n        ]\n        return len(indices)  # true value without extra samples\n\n    def __iter__(self) -&gt; Iterator[int]:\n        \"\"\"Return an iterator that iterates over the indices of the dataset.\"\"\"\n        if self.shuffle:\n            # deterministically shuffle based on epoch and seed\n            g = torch.Generator()\n            g.manual_seed(self.seed + self.epoch)\n            indices = torch.randperm(self.total_size, generator=g).tolist()\n        else:\n            indices = list(range(self.total_size))\n\n        # subsample\n        indices = indices[self.rank : self.total_size : self.num_replicas]\n        assert len(indices) == self.num_samples\n\n        return iter(indices)\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the number of samples.\"\"\"\n        return self.num_samples\n\n    def set_epoch(self, epoch: int) -&gt; None:\n        \"\"\"Set the epoch for this sampler.\n\n        When :attr:`shuffle=True`, this ensures all replicas use a different random\n        ordering for each epoch. Otherwise, the next iteration of this sampler\n        will yield the same ordering.\n\n        Parameters\n        ----------\n        epoch : int\n            Epoch number.\n\n        \"\"\"\n        self.epoch = epoch\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.DistributedEvalSampler.total_size","title":"total_size  <code>property</code>","text":"<pre><code>total_size\n</code></pre> <p>Return the total size of the dataset.</p>"},{"location":"api/#mmlearn.datasets.core.DistributedEvalSampler.num_samples","title":"num_samples  <code>property</code>","text":"<pre><code>num_samples\n</code></pre> <p>Return the number of samples managed by the sampler.</p>"},{"location":"api/#mmlearn.datasets.core.DistributedEvalSampler.__iter__","title":"__iter__","text":"<pre><code>__iter__()\n</code></pre> <p>Return an iterator that iterates over the indices of the dataset.</p> Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>def __iter__(self) -&gt; Iterator[int]:\n    \"\"\"Return an iterator that iterates over the indices of the dataset.\"\"\"\n    if self.shuffle:\n        # deterministically shuffle based on epoch and seed\n        g = torch.Generator()\n        g.manual_seed(self.seed + self.epoch)\n        indices = torch.randperm(self.total_size, generator=g).tolist()\n    else:\n        indices = list(range(self.total_size))\n\n    # subsample\n    indices = indices[self.rank : self.total_size : self.num_replicas]\n    assert len(indices) == self.num_samples\n\n    return iter(indices)\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.DistributedEvalSampler.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the number of samples.</p> Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the number of samples.\"\"\"\n    return self.num_samples\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.DistributedEvalSampler.set_epoch","title":"set_epoch","text":"<pre><code>set_epoch(epoch)\n</code></pre> <p>Set the epoch for this sampler.</p> <p>When :attr:<code>shuffle=True</code>, this ensures all replicas use a different random ordering for each epoch. Otherwise, the next iteration of this sampler will yield the same ordering.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>Epoch number.</p> required Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>def set_epoch(self, epoch: int) -&gt; None:\n    \"\"\"Set the epoch for this sampler.\n\n    When :attr:`shuffle=True`, this ensures all replicas use a different random\n    ordering for each epoch. Otherwise, the next iteration of this sampler\n    will yield the same ordering.\n\n    Parameters\n    ----------\n    epoch : int\n        Epoch number.\n\n    \"\"\"\n    self.epoch = epoch\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.find_matching_indices","title":"find_matching_indices","text":"<pre><code>find_matching_indices(\n    first_example_ids, second_example_ids\n)\n</code></pre> <p>Find the indices of matching examples given two tensors of example ids.</p> <p>Matching examples are defined as examples with the same value in both tensors. This method is useful for finding pairs of examples from different modalities that are related to each other in a batch.</p> <p>Parameters:</p> Name Type Description Default <code>first_example_ids</code> <code>Tensor</code> <p>A tensor of example ids of shape <code>(N, 2)</code>, where <code>N</code> is the number of examples.</p> required <code>second_example_ids</code> <code>Tensor</code> <p>A tensor of example ids of shape <code>(M, 2)</code>, where <code>M</code> is the number of examples.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>A tuple of tensors containing the indices of matching examples in the first and second tensor, respectively.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If either <code>first_example_ids</code> or <code>second_example_ids</code> is not a tensor.</p> <code>ValueError</code> <p>If either <code>first_example_ids</code> or <code>second_example_ids</code> is not a 2D tensor with the second dimension having a size of <code>2</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; img_example_ids = torch.tensor([(0, 0), (0, 1), (1, 0), (1, 1)])\n&gt;&gt;&gt; text_example_ids = torch.tensor([(1, 0), (1, 1), (2, 0), (2, 1), (2, 2)])\n&gt;&gt;&gt; find_matching_indices(img_example_ids, text_example_ids)\n(tensor([2, 3]), tensor([0, 1]))\n</code></pre> Source code in <code>mmlearn/datasets/core/example.py</code> <pre><code>def find_matching_indices(\n    first_example_ids: torch.Tensor, second_example_ids: torch.Tensor\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Find the indices of matching examples given two tensors of example ids.\n\n    Matching examples are defined as examples with the same value in both tensors.\n    This method is useful for finding pairs of examples from different modalities\n    that are related to each other in a batch.\n\n    Parameters\n    ----------\n    first_example_ids : torch.Tensor\n        A tensor of example ids of shape `(N, 2)`, where `N` is the number of examples.\n    second_example_ids : torch.Tensor\n        A tensor of example ids of shape `(M, 2)`, where `M` is the number of examples.\n\n    Returns\n    -------\n    tuple[torch.Tensor, torch.Tensor]\n        A tuple of tensors containing the indices of matching examples in the first and\n        second tensor, respectively.\n\n    Raises\n    ------\n    TypeError\n        If either `first_example_ids` or `second_example_ids` is not a tensor.\n    ValueError\n        If either `first_example_ids` or `second_example_ids` is not a 2D tensor\n        with the second dimension having a size of `2`.\n\n    Examples\n    --------\n    &gt;&gt;&gt; img_example_ids = torch.tensor([(0, 0), (0, 1), (1, 0), (1, 1)])\n    &gt;&gt;&gt; text_example_ids = torch.tensor([(1, 0), (1, 1), (2, 0), (2, 1), (2, 2)])\n    &gt;&gt;&gt; find_matching_indices(img_example_ids, text_example_ids)\n    (tensor([2, 3]), tensor([0, 1]))\n\n\n    \"\"\"\n    if not isinstance(first_example_ids, torch.Tensor) or not isinstance(\n        second_example_ids,\n        torch.Tensor,\n    ):\n        raise TypeError(\n            f\"Expected inputs to be tensors, but got {type(first_example_ids)} \"\n            f\"and {type(second_example_ids)}.\",\n        )\n    val = 2\n    if not (first_example_ids.ndim == val and first_example_ids.shape[1] == val):\n        raise ValueError(\n            \"Expected argument `first_example_ids` to be a tensor of shape (N, 2), \"\n            f\"but got shape {first_example_ids.shape}.\",\n        )\n    if not (second_example_ids.ndim == val and second_example_ids.shape[1] == val):\n        raise ValueError(\n            \"Expected argument `second_example_ids` to be a tensor of shape (N, 2), \"\n            f\"but got shape {second_example_ids.shape}.\",\n        )\n\n    first_example_ids = first_example_ids.unsqueeze(1)  # shape=(N, 1, 2)\n    second_example_ids = second_example_ids.unsqueeze(0)  # shape=(1, M, 2)\n\n    # compare all elements; results in a shape (N, M) tensor\n    matches = torch.all(first_example_ids == second_example_ids, dim=-1)\n    first_indices, second_indices = torch.where(matches)\n    return first_indices, second_indices\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.combined_dataset","title":"combined_dataset","text":"<p>Wrapper for combining multiple datasets into one.</p>"},{"location":"api/#mmlearn.datasets.core.combined_dataset.CombinedDataset","title":"CombinedDataset","text":"<p>               Bases: <code>Dataset[Example]</code></p> <p>Combine multiple datasets into one.</p> <p>This class is similar to class:<code>~torch.utils.data.ConcatDataset</code> but allows for combining iterable-style datasets with map-style datasets. The iterable-style datasets must implement the :meth:<code>__len__</code> method, which is used to determine the total length of the combined dataset. When an index is passed to the combined dataset, the dataset that contains the example at that index is determined and the example is retrieved from that dataset. Since iterable-style datasets do not support random access, the examples are retrieved sequentially from the iterable-style datasets. When the end of an iterable-style dataset is reached, the iterator is reset and the next example is retrieved from the beginning of the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>datasets</code> <code>Iterable[Union[Dataset, IterableDataset]]</code> <p>Iterable of datasets to combine.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the datasets in the input iterable are not instances of class:<code>~torch.utils.data.Dataset</code> or class:<code>~torch.utils.data.IterableDataset</code>.</p> <code>ValueError</code> <p>If the input iterable of datasets is empty.</p> Source code in <code>mmlearn/datasets/core/combined_dataset.py</code> <pre><code>class CombinedDataset(Dataset[Example]):\n    \"\"\"Combine multiple datasets into one.\n\n    This class is similar to :py:class:`~torch.utils.data.ConcatDataset` but allows\n    for combining iterable-style datasets with map-style datasets. The iterable-style\n    datasets must implement the :meth:`__len__` method, which is used to determine the\n    total length of the combined dataset. When an index is passed to the combined\n    dataset, the dataset that contains the example at that index is determined and\n    the example is retrieved from that dataset. Since iterable-style datasets do\n    not support random access, the examples are retrieved sequentially from the\n    iterable-style datasets. When the end of an iterable-style dataset is reached,\n    the iterator is reset and the next example is retrieved from the beginning of\n    the dataset.\n\n\n    Parameters\n    ----------\n    datasets : Iterable[Union[torch.utils.data.Dataset, torch.utils.data.IterableDataset]]\n        Iterable of datasets to combine.\n\n    Raises\n    ------\n    TypeError\n        If any of the datasets in the input iterable are not instances of\n        :py:class:`~torch.utils.data.Dataset` or :py:class:`~torch.utils.data.IterableDataset`.\n    ValueError\n        If the input iterable of datasets is empty.\n\n    \"\"\"  # noqa: W505\n\n    def __init__(\n        self, datasets: Iterable[Union[Dataset[Example], IterableDataset[Example]]]\n    ) -&gt; None:\n        self.datasets, _ = tree_flatten(datasets)\n        if not all(\n            isinstance(dataset, (Dataset, IterableDataset)) for dataset in self.datasets\n        ):\n            raise TypeError(\n                \"Expected argument `datasets` to be an iterable of `Dataset` or \"\n                f\"`IterableDataset` instances, but found: {self.datasets}\",\n            )\n        if len(self.datasets) == 0:\n            raise ValueError(\n                \"Expected a non-empty iterable of datasets but found an empty iterable\",\n            )\n\n        self._cumulative_sizes: list[int] = np.cumsum(\n            [len(dataset) for dataset in self.datasets]\n        ).tolist()\n        self._iterators: list[Iterator[Example]] = []\n        self._iter_dataset_mapping: dict[int, int] = {}\n\n        # create iterators for iterable datasets and map dataset index to iterator index\n        for idx, dataset in enumerate(self.datasets):\n            if isinstance(dataset, IterableDataset):\n                self._iterators.append(iter(dataset))\n                self._iter_dataset_mapping[idx] = len(self._iterators) - 1\n\n    def __getitem__(self, idx: int) -&gt; Example:\n        \"\"\"Return an example from the combined dataset.\"\"\"\n        if idx &lt; 0:  # handle negative indices\n            if -idx &gt; len(self):\n                raise IndexError(\n                    f\"Index {idx} is out of bounds for the combined dataset with \"\n                    f\"length {len(self)}\",\n                )\n            idx = len(self) + idx\n\n        dataset_idx = bisect.bisect_right(self._cumulative_sizes, idx)\n\n        curr_dataset = self.datasets[dataset_idx]\n        if isinstance(curr_dataset, IterableDataset):\n            iter_idx = self._iter_dataset_mapping[dataset_idx]\n            try:\n                example = next(self._iterators[iter_idx])\n            except StopIteration:\n                self._iterators[iter_idx] = iter(curr_dataset)\n                example = next(self._iterators[iter_idx])\n        else:\n            if dataset_idx == 0:\n                example_idx = idx\n            else:\n                example_idx = idx - self._cumulative_sizes[dataset_idx - 1]\n            example = curr_dataset[example_idx]\n\n        if not isinstance(example, Example):\n            raise TypeError(\n                \"Expected dataset examples to be instances of `Example` \"\n                f\"but found {type(example)}\",\n            )\n\n        if not hasattr(example, \"dataset_index\"):\n            example.dataset_index = dataset_idx\n        if not hasattr(example, \"example_ids\"):\n            example.create_ids()\n\n        return example\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the total number of examples in the combined dataset.\"\"\"\n        return self._cumulative_sizes[-1]\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.combined_dataset.CombinedDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> <p>Return an example from the combined dataset.</p> Source code in <code>mmlearn/datasets/core/combined_dataset.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; Example:\n    \"\"\"Return an example from the combined dataset.\"\"\"\n    if idx &lt; 0:  # handle negative indices\n        if -idx &gt; len(self):\n            raise IndexError(\n                f\"Index {idx} is out of bounds for the combined dataset with \"\n                f\"length {len(self)}\",\n            )\n        idx = len(self) + idx\n\n    dataset_idx = bisect.bisect_right(self._cumulative_sizes, idx)\n\n    curr_dataset = self.datasets[dataset_idx]\n    if isinstance(curr_dataset, IterableDataset):\n        iter_idx = self._iter_dataset_mapping[dataset_idx]\n        try:\n            example = next(self._iterators[iter_idx])\n        except StopIteration:\n            self._iterators[iter_idx] = iter(curr_dataset)\n            example = next(self._iterators[iter_idx])\n    else:\n        if dataset_idx == 0:\n            example_idx = idx\n        else:\n            example_idx = idx - self._cumulative_sizes[dataset_idx - 1]\n        example = curr_dataset[example_idx]\n\n    if not isinstance(example, Example):\n        raise TypeError(\n            \"Expected dataset examples to be instances of `Example` \"\n            f\"but found {type(example)}\",\n        )\n\n    if not hasattr(example, \"dataset_index\"):\n        example.dataset_index = dataset_idx\n    if not hasattr(example, \"example_ids\"):\n        example.create_ids()\n\n    return example\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.combined_dataset.CombinedDataset.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the total number of examples in the combined dataset.</p> Source code in <code>mmlearn/datasets/core/combined_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the total number of examples in the combined dataset.\"\"\"\n    return self._cumulative_sizes[-1]\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.data_collator","title":"data_collator","text":"<p>Data collators for batching examples.</p>"},{"location":"api/#mmlearn.datasets.core.data_collator.DefaultDataCollator","title":"DefaultDataCollator  <code>dataclass</code>","text":"<p>Default data collator for batching examples.</p> <p>This data collator will collate a list of class:<code>~mmlearn.datasets.core.example.Example</code> objects into a batch. It can also apply processing functions to specified keys in the batch before returning it.</p> <p>Parameters:</p> Name Type Description Default <code>batch_processors</code> <code>Optional[dict[str, Callable[[Any], Any]]]</code> <p>Dictionary of callables to apply to the batch before returning it.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the batch processor for a key does not return a dictionary with the key in it.</p> Source code in <code>mmlearn/datasets/core/data_collator.py</code> <pre><code>@dataclass\nclass DefaultDataCollator:\n    \"\"\"Default data collator for batching examples.\n\n    This data collator will collate a list of :py:class:`~mmlearn.datasets.core.example.Example`\n    objects into a batch. It can also apply processing functions to specified keys\n    in the batch before returning it.\n\n    Parameters\n    ----------\n    batch_processors : Optional[dict[str, Callable[[Any], Any]]], optional, default=None\n        Dictionary of callables to apply to the batch before returning it.\n\n    Raises\n    ------\n    ValueError\n        If the batch processor for a key does not return a dictionary with the\n        key in it.\n    \"\"\"  # noqa: W505\n\n    #: Dictionary of callables to apply to the batch before returning it.\n    #: The key is the name of the key in the batch, and the value is the processing\n    #: function to apply to the key. The processing function must take a single\n    #: argument and return a single value. If the processing function returns\n    #: a dictionary, it must contain the key that was processed in it (all the\n    #: other keys will also be included in the batch).\n    batch_processors: Optional[dict[str, Callable[[Any], Any]]] = None\n\n    def __call__(self, examples: list[Example]) -&gt; dict[str, Any]:\n        \"\"\"Collate a list of `Example` objects and apply processing functions.\"\"\"\n        batch = collate_example_list(examples)\n\n        if self.batch_processors is not None:\n            for key, processor in self.batch_processors.items():\n                batch_key: str = key\n                if Modalities.has_modality(key):\n                    batch_key = Modalities.get_modality(key).name\n\n                if batch_key in batch:\n                    batch_processed = processor(batch[batch_key])\n                    if isinstance(batch_processed, Mapping):\n                        if batch_key not in batch_processed:\n                            raise ValueError(\n                                f\"Batch processor for '{key}' key must return a dictionary \"\n                                f\"with '{batch_key}' in it.\"\n                            )\n                        batch.update(batch_processed)\n                    else:\n                        batch[batch_key] = batch_processed\n\n        return batch\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.data_collator.DefaultDataCollator.__call__","title":"__call__","text":"<pre><code>__call__(examples)\n</code></pre> <p>Collate a list of <code>Example</code> objects and apply processing functions.</p> Source code in <code>mmlearn/datasets/core/data_collator.py</code> <pre><code>def __call__(self, examples: list[Example]) -&gt; dict[str, Any]:\n    \"\"\"Collate a list of `Example` objects and apply processing functions.\"\"\"\n    batch = collate_example_list(examples)\n\n    if self.batch_processors is not None:\n        for key, processor in self.batch_processors.items():\n            batch_key: str = key\n            if Modalities.has_modality(key):\n                batch_key = Modalities.get_modality(key).name\n\n            if batch_key in batch:\n                batch_processed = processor(batch[batch_key])\n                if isinstance(batch_processed, Mapping):\n                    if batch_key not in batch_processed:\n                        raise ValueError(\n                            f\"Batch processor for '{key}' key must return a dictionary \"\n                            f\"with '{batch_key}' in it.\"\n                        )\n                    batch.update(batch_processed)\n                else:\n                    batch[batch_key] = batch_processed\n\n    return batch\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.data_collator.collate_example_list","title":"collate_example_list","text":"<pre><code>collate_example_list(examples)\n</code></pre> <p>Collate a list of class:<code>~mmlearn.datasets.core.example.Example</code> objects into a batch.</p> <p>Parameters:</p> Name Type Description Default <code>examples</code> <code>list[Example]</code> <p>list of examples to collate.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary of batched examples.</p> Source code in <code>mmlearn/datasets/core/data_collator.py</code> <pre><code>def collate_example_list(examples: list[Example]) -&gt; dict[str, Any]:\n    \"\"\"Collate a list of :py:class:`~mmlearn.datasets.core.example.Example` objects into a batch.\n\n    Parameters\n    ----------\n    examples : list[Example]\n        list of examples to collate.\n\n    Returns\n    -------\n    dict[str, Any]\n        Dictionary of batched examples.\n\n    \"\"\"  # noqa: W505\n    return _collate_example_dict(_merge_examples(examples))\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.example","title":"example","text":"<p>Module for example-related classes and functions.</p>"},{"location":"api/#mmlearn.datasets.core.example.Example","title":"Example","text":"<p>               Bases: <code>OrderedDict[Any, Any]</code></p> <p>A representation of a single example from a dataset.</p> <p>This class is a subclass of class:<code>~collections.OrderedDict</code> and provides attribute-style access. This means that <code>example[\"text\"]</code> and <code>example.text</code> are equivalent. All datasets in this library return examples as class:<code>~mmlearn.datasets.core.example.Example</code> objects.</p> <p>Parameters:</p> Name Type Description Default <code>init_dict</code> <code>Optional[MutableMapping[Hashable, Any]]</code> <p>Dictionary to init <code>Example</code> class with.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; example = Example({\"text\": torch.tensor(2)})\n&gt;&gt;&gt; example.text.zero_()\ntensor(0)\n&gt;&gt;&gt; example.context = torch.tensor(4)  # set custom attributes after initialization\n</code></pre> Source code in <code>mmlearn/datasets/core/example.py</code> <pre><code>class Example(OrderedDict[Any, Any]):\n    \"\"\"A representation of a single example from a dataset.\n\n    This class is a subclass of :py:class:`~collections.OrderedDict` and provides\n    attribute-style access. This means that `example[\"text\"]` and `example.text`\n    are equivalent. All datasets in this library return examples as\n    :py:class:`~mmlearn.datasets.core.example.Example` objects.\n\n\n    Parameters\n    ----------\n    init_dict : Optional[MutableMapping[Hashable, Any]], optional, default=None\n        Dictionary to init `Example` class with.\n\n    Examples\n    --------\n    &gt;&gt;&gt; example = Example({\"text\": torch.tensor(2)})\n    &gt;&gt;&gt; example.text.zero_()\n    tensor(0)\n    &gt;&gt;&gt; example.context = torch.tensor(4)  # set custom attributes after initialization\n    \"\"\"\n\n    def __init__(\n        self,\n        init_dict: Optional[MutableMapping[Hashable, Any]] = None,\n    ) -&gt; None:\n        if init_dict is None:\n            init_dict = {}\n        super().__init__(init_dict)\n\n    def create_ids(self) -&gt; None:\n        \"\"\"Create a unique id for the example from the dataset and example index.\n\n        This method combines the dataset index and example index to create an\n        attribute called `example_ids`, which is a dictionary of tensors. The\n        dictionary keys are all the keys in the example except for `example_ids`,\n        `example_index`, and `dataset_index`. The values are tensors of shape `(2,)`\n        containing the tuple `(dataset_index, example_index)` for each key.\n        The `example_ids` is used to (re-)identify pairs of examples from different\n        modalities after they have been combined into a batch.\n\n        Warns\n        -----\n        UserWarning\n            If the `example_index` and `dataset_index` attributes are not set.\n\n        Notes\n        -----\n        - The Example must have the following attributes set before calling this\n          this method: `example_index` (usually set/returned by the dataset) and\n          `dataset_index` (usually set by the :py:class:`~mmlearn.datasets.core.combined_dataset.CombinedDataset` object)\n        - The :py:func:`~mmlearn.datasets.core.example.find_matching_indices`\n          function can be used to find matching examples given two tensors of example ids.\n\n        \"\"\"  # noqa: W505\n        if hasattr(self, \"example_index\") and hasattr(self, \"dataset_index\"):\n            self.example_ids = {\n                key: torch.tensor([self.dataset_index, self.example_index])\n                for key in self.keys()\n                if key not in (\"example_ids\", \"example_index\", \"dataset_index\")\n            }\n        else:\n            rank_zero_warn(\n                \"Cannot create `example_ids` without `example_index` and `dataset_index` \"\n                \"attributes. Set these attributes before calling `create_ids`. \"\n                \"No `example_ids` was created.\",\n                stacklevel=2,\n                category=UserWarning,\n            )\n\n    def __getattr__(self, key: str) -&gt; Any:\n        \"\"\"Get attribute by key.\"\"\"\n        try:\n            return self[key]\n        except KeyError:\n            raise AttributeError(key) from None\n\n    def __setattr__(self, key: str, value: Any) -&gt; None:\n        \"\"\"Set attribute by key.\"\"\"\n        if isinstance(value, MutableMapping):\n            value = Example(value)\n        self[key] = value\n\n    def __setitem__(self, key: Hashable, value: Any) -&gt; None:\n        \"\"\"Set item by key.\"\"\"\n        if isinstance(value, MutableMapping):\n            value = Example(value)\n        super().__setitem__(key, value)\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.example.Example.create_ids","title":"create_ids","text":"<pre><code>create_ids()\n</code></pre> <p>Create a unique id for the example from the dataset and example index.</p> <p>This method combines the dataset index and example index to create an attribute called <code>example_ids</code>, which is a dictionary of tensors. The dictionary keys are all the keys in the example except for <code>example_ids</code>, <code>example_index</code>, and <code>dataset_index</code>. The values are tensors of shape <code>(2,)</code> containing the tuple <code>(dataset_index, example_index)</code> for each key. The <code>example_ids</code> is used to (re-)identify pairs of examples from different modalities after they have been combined into a batch.</p> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If the <code>example_index</code> and <code>dataset_index</code> attributes are not set.</p> Notes <ul> <li>The Example must have the following attributes set before calling this   this method: <code>example_index</code> (usually set/returned by the dataset) and   <code>dataset_index</code> (usually set by the class:<code>~mmlearn.datasets.core.combined_dataset.CombinedDataset</code> object)</li> <li>The func:<code>~mmlearn.datasets.core.example.find_matching_indices</code>   function can be used to find matching examples given two tensors of example ids.</li> </ul> Source code in <code>mmlearn/datasets/core/example.py</code> <pre><code>def create_ids(self) -&gt; None:\n    \"\"\"Create a unique id for the example from the dataset and example index.\n\n    This method combines the dataset index and example index to create an\n    attribute called `example_ids`, which is a dictionary of tensors. The\n    dictionary keys are all the keys in the example except for `example_ids`,\n    `example_index`, and `dataset_index`. The values are tensors of shape `(2,)`\n    containing the tuple `(dataset_index, example_index)` for each key.\n    The `example_ids` is used to (re-)identify pairs of examples from different\n    modalities after they have been combined into a batch.\n\n    Warns\n    -----\n    UserWarning\n        If the `example_index` and `dataset_index` attributes are not set.\n\n    Notes\n    -----\n    - The Example must have the following attributes set before calling this\n      this method: `example_index` (usually set/returned by the dataset) and\n      `dataset_index` (usually set by the :py:class:`~mmlearn.datasets.core.combined_dataset.CombinedDataset` object)\n    - The :py:func:`~mmlearn.datasets.core.example.find_matching_indices`\n      function can be used to find matching examples given two tensors of example ids.\n\n    \"\"\"  # noqa: W505\n    if hasattr(self, \"example_index\") and hasattr(self, \"dataset_index\"):\n        self.example_ids = {\n            key: torch.tensor([self.dataset_index, self.example_index])\n            for key in self.keys()\n            if key not in (\"example_ids\", \"example_index\", \"dataset_index\")\n        }\n    else:\n        rank_zero_warn(\n            \"Cannot create `example_ids` without `example_index` and `dataset_index` \"\n            \"attributes. Set these attributes before calling `create_ids`. \"\n            \"No `example_ids` was created.\",\n            stacklevel=2,\n            category=UserWarning,\n        )\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.example.Example.__getattr__","title":"__getattr__","text":"<pre><code>__getattr__(key)\n</code></pre> <p>Get attribute by key.</p> Source code in <code>mmlearn/datasets/core/example.py</code> <pre><code>def __getattr__(self, key: str) -&gt; Any:\n    \"\"\"Get attribute by key.\"\"\"\n    try:\n        return self[key]\n    except KeyError:\n        raise AttributeError(key) from None\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.example.Example.__setattr__","title":"__setattr__","text":"<pre><code>__setattr__(key, value)\n</code></pre> <p>Set attribute by key.</p> Source code in <code>mmlearn/datasets/core/example.py</code> <pre><code>def __setattr__(self, key: str, value: Any) -&gt; None:\n    \"\"\"Set attribute by key.\"\"\"\n    if isinstance(value, MutableMapping):\n        value = Example(value)\n    self[key] = value\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.example.Example.__setitem__","title":"__setitem__","text":"<pre><code>__setitem__(key, value)\n</code></pre> <p>Set item by key.</p> Source code in <code>mmlearn/datasets/core/example.py</code> <pre><code>def __setitem__(self, key: Hashable, value: Any) -&gt; None:\n    \"\"\"Set item by key.\"\"\"\n    if isinstance(value, MutableMapping):\n        value = Example(value)\n    super().__setitem__(key, value)\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.example.find_matching_indices","title":"find_matching_indices","text":"<pre><code>find_matching_indices(\n    first_example_ids, second_example_ids\n)\n</code></pre> <p>Find the indices of matching examples given two tensors of example ids.</p> <p>Matching examples are defined as examples with the same value in both tensors. This method is useful for finding pairs of examples from different modalities that are related to each other in a batch.</p> <p>Parameters:</p> Name Type Description Default <code>first_example_ids</code> <code>Tensor</code> <p>A tensor of example ids of shape <code>(N, 2)</code>, where <code>N</code> is the number of examples.</p> required <code>second_example_ids</code> <code>Tensor</code> <p>A tensor of example ids of shape <code>(M, 2)</code>, where <code>M</code> is the number of examples.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>A tuple of tensors containing the indices of matching examples in the first and second tensor, respectively.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If either <code>first_example_ids</code> or <code>second_example_ids</code> is not a tensor.</p> <code>ValueError</code> <p>If either <code>first_example_ids</code> or <code>second_example_ids</code> is not a 2D tensor with the second dimension having a size of <code>2</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; img_example_ids = torch.tensor([(0, 0), (0, 1), (1, 0), (1, 1)])\n&gt;&gt;&gt; text_example_ids = torch.tensor([(1, 0), (1, 1), (2, 0), (2, 1), (2, 2)])\n&gt;&gt;&gt; find_matching_indices(img_example_ids, text_example_ids)\n(tensor([2, 3]), tensor([0, 1]))\n</code></pre> Source code in <code>mmlearn/datasets/core/example.py</code> <pre><code>def find_matching_indices(\n    first_example_ids: torch.Tensor, second_example_ids: torch.Tensor\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Find the indices of matching examples given two tensors of example ids.\n\n    Matching examples are defined as examples with the same value in both tensors.\n    This method is useful for finding pairs of examples from different modalities\n    that are related to each other in a batch.\n\n    Parameters\n    ----------\n    first_example_ids : torch.Tensor\n        A tensor of example ids of shape `(N, 2)`, where `N` is the number of examples.\n    second_example_ids : torch.Tensor\n        A tensor of example ids of shape `(M, 2)`, where `M` is the number of examples.\n\n    Returns\n    -------\n    tuple[torch.Tensor, torch.Tensor]\n        A tuple of tensors containing the indices of matching examples in the first and\n        second tensor, respectively.\n\n    Raises\n    ------\n    TypeError\n        If either `first_example_ids` or `second_example_ids` is not a tensor.\n    ValueError\n        If either `first_example_ids` or `second_example_ids` is not a 2D tensor\n        with the second dimension having a size of `2`.\n\n    Examples\n    --------\n    &gt;&gt;&gt; img_example_ids = torch.tensor([(0, 0), (0, 1), (1, 0), (1, 1)])\n    &gt;&gt;&gt; text_example_ids = torch.tensor([(1, 0), (1, 1), (2, 0), (2, 1), (2, 2)])\n    &gt;&gt;&gt; find_matching_indices(img_example_ids, text_example_ids)\n    (tensor([2, 3]), tensor([0, 1]))\n\n\n    \"\"\"\n    if not isinstance(first_example_ids, torch.Tensor) or not isinstance(\n        second_example_ids,\n        torch.Tensor,\n    ):\n        raise TypeError(\n            f\"Expected inputs to be tensors, but got {type(first_example_ids)} \"\n            f\"and {type(second_example_ids)}.\",\n        )\n    val = 2\n    if not (first_example_ids.ndim == val and first_example_ids.shape[1] == val):\n        raise ValueError(\n            \"Expected argument `first_example_ids` to be a tensor of shape (N, 2), \"\n            f\"but got shape {first_example_ids.shape}.\",\n        )\n    if not (second_example_ids.ndim == val and second_example_ids.shape[1] == val):\n        raise ValueError(\n            \"Expected argument `second_example_ids` to be a tensor of shape (N, 2), \"\n            f\"but got shape {second_example_ids.shape}.\",\n        )\n\n    first_example_ids = first_example_ids.unsqueeze(1)  # shape=(N, 1, 2)\n    second_example_ids = second_example_ids.unsqueeze(0)  # shape=(1, M, 2)\n\n    # compare all elements; results in a shape (N, M) tensor\n    matches = torch.all(first_example_ids == second_example_ids, dim=-1)\n    first_indices, second_indices = torch.where(matches)\n    return first_indices, second_indices\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.modalities","title":"modalities","text":"<p>Module for managing supported modalities in the library.</p>"},{"location":"api/#mmlearn.datasets.core.modalities.Modality","title":"Modality  <code>dataclass</code>","text":"<p>A representation of a modality in the library.</p> <p>This class is used to represent a modality in the library. It contains the name of the modality and the properties that can be associated with it. The properties are dynamically generated based on the name of the modality and can be accessed as attributes of the class.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the modality.</p> required <code>modality_specific_properties</code> <code>Optional[dict[str, str]]</code> <p>Additional properties specific to the modality, by default None</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the property already exists for the modality or if the format string is invalid.</p> Source code in <code>mmlearn/datasets/core/modalities.py</code> <pre><code>@dataclass\nclass Modality:\n    \"\"\"A representation of a modality in the library.\n\n    This class is used to represent a modality in the library. It contains the name of\n    the modality and the properties that can be associated with it. The properties are\n    dynamically generated based on the name of the modality and can be accessed as\n    attributes of the class.\n\n    Parameters\n    ----------\n    name : str\n        The name of the modality.\n    modality_specific_properties : Optional[dict[str, str]], optional, default=None\n        Additional properties specific to the modality, by default None\n\n    Raises\n    ------\n    ValueError\n        If the property already exists for the modality or if the format string is\n        invalid.\n    \"\"\"\n\n    #: The name of the modality.\n    name: str\n\n    #: Target/label associated with the modality. This will return ``name_target``.\n    target: str = field(init=False, repr=False)\n\n    #: Attention mask associated with the modality. This will return\n    # ``name_attention_mask``.\n    attention_mask: str = field(init=False, repr=False)\n\n    #: Input mask associated with the modality. This will return ``name_mask``.\n    mask: str = field(init=False, repr=False)\n\n    #: Embedding associated with the modality. This will return ``name_embedding``.\n    embedding: str = field(init=False, repr=False)\n\n    #: Masked embedding associated with the modality. This will return\n    # ``name_masked_embedding``.\n    masked_embedding: str = field(init=False, repr=False)\n\n    #: Embedding from an Exponential Moving Average (EMA) encoder associated with\n    #: the modality.\n    ema_embedding: str = field(init=False, repr=False)\n\n    #: Other properties specific to the modality.\n    modality_specific_properties: Optional[dict[str, str]] = field(\n        default=None, repr=False\n    )\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Initialize the modality with the name and properties.\"\"\"\n        self.name = self.name.lower()\n        self._properties = {}\n\n        for field_name in self.__dataclass_fields__:\n            if field_name not in (\"name\", \"modality_specific_properties\"):\n                field_value = f\"{self.name}_{field_name}\"\n                self._properties[field_name] = field_value\n                setattr(self, field_name, field_value)\n\n        if self.modality_specific_properties is not None:\n            for (\n                property_name,\n                format_string,\n            ) in self.modality_specific_properties.items():\n                self.add_property(property_name, format_string)\n\n    @property\n    def properties(self) -&gt; dict[str, str]:\n        \"\"\"Return the properties associated with the modality.\"\"\"\n        return self._properties\n\n    def add_property(self, name: str, format_string: str) -&gt; None:\n        \"\"\"Add a new property to the modality.\n\n        Parameters\n        ----------\n        name : str\n            The name of the property.\n        format_string : str\n            The format string for the property. The format string should contain a\n            placeholder that will be replaced with the name of the modality when the\n            property is accessed.\n\n        Warns\n        -----\n        UserWarning\n            If the property already exists for the modality. It will overwrite the\n            existing property.\n\n        Raises\n        ------\n        ValueError\n            If `format_string` is invalid. A valid format string contains at least one\n            placeholder enclosed in curly braces.\n        \"\"\"\n        if name in self._properties:\n            warnings.warn(\n                f\"Property '{name}' already exists for modality '{super().__str__()}'.\"\n                \"Will overwrite the existing property.\",\n                category=UserWarning,\n                stacklevel=2,\n            )\n\n        if not _is_format_string(format_string):\n            raise ValueError(\n                f\"Invalid format string '{format_string}' for property \"\n                f\"'{name}' of modality '{super().__str__()}'.\"\n            )\n\n        self._properties[name] = format_string.format(self.name)\n        setattr(self, name, self._properties[name])\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the object as a string.\"\"\"\n        return self.name.lower()\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.modalities.Modality.properties","title":"properties  <code>property</code>","text":"<pre><code>properties\n</code></pre> <p>Return the properties associated with the modality.</p>"},{"location":"api/#mmlearn.datasets.core.modalities.Modality.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Initialize the modality with the name and properties.</p> Source code in <code>mmlearn/datasets/core/modalities.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Initialize the modality with the name and properties.\"\"\"\n    self.name = self.name.lower()\n    self._properties = {}\n\n    for field_name in self.__dataclass_fields__:\n        if field_name not in (\"name\", \"modality_specific_properties\"):\n            field_value = f\"{self.name}_{field_name}\"\n            self._properties[field_name] = field_value\n            setattr(self, field_name, field_value)\n\n    if self.modality_specific_properties is not None:\n        for (\n            property_name,\n            format_string,\n        ) in self.modality_specific_properties.items():\n            self.add_property(property_name, format_string)\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.modalities.Modality.add_property","title":"add_property","text":"<pre><code>add_property(name, format_string)\n</code></pre> <p>Add a new property to the modality.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the property.</p> required <code>format_string</code> <code>str</code> <p>The format string for the property. The format string should contain a placeholder that will be replaced with the name of the modality when the property is accessed.</p> required <p>Warns:</p> Type Description <code>UserWarning</code> <p>If the property already exists for the modality. It will overwrite the existing property.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>format_string</code> is invalid. A valid format string contains at least one placeholder enclosed in curly braces.</p> Source code in <code>mmlearn/datasets/core/modalities.py</code> <pre><code>def add_property(self, name: str, format_string: str) -&gt; None:\n    \"\"\"Add a new property to the modality.\n\n    Parameters\n    ----------\n    name : str\n        The name of the property.\n    format_string : str\n        The format string for the property. The format string should contain a\n        placeholder that will be replaced with the name of the modality when the\n        property is accessed.\n\n    Warns\n    -----\n    UserWarning\n        If the property already exists for the modality. It will overwrite the\n        existing property.\n\n    Raises\n    ------\n    ValueError\n        If `format_string` is invalid. A valid format string contains at least one\n        placeholder enclosed in curly braces.\n    \"\"\"\n    if name in self._properties:\n        warnings.warn(\n            f\"Property '{name}' already exists for modality '{super().__str__()}'.\"\n            \"Will overwrite the existing property.\",\n            category=UserWarning,\n            stacklevel=2,\n        )\n\n    if not _is_format_string(format_string):\n        raise ValueError(\n            f\"Invalid format string '{format_string}' for property \"\n            f\"'{name}' of modality '{super().__str__()}'.\"\n        )\n\n    self._properties[name] = format_string.format(self.name)\n    setattr(self, name, self._properties[name])\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.modalities.Modality.__str__","title":"__str__","text":"<pre><code>__str__()\n</code></pre> <p>Return the object as a string.</p> Source code in <code>mmlearn/datasets/core/modalities.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the object as a string.\"\"\"\n    return self.name.lower()\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.modalities.ModalityRegistry","title":"ModalityRegistry","text":"<p>Modality registry.</p> <p>A singleton class that manages the supported modalities (and their properties) in the library. The class provides methods to add new modalities and properties, and to access the existing modalities. The class is implemented as a singleton to ensure that there is only one instance of the registry in the library.</p> Source code in <code>mmlearn/datasets/core/modalities.py</code> <pre><code>class ModalityRegistry:\n    \"\"\"Modality registry.\n\n    A singleton class that manages the supported modalities (and their properties) in\n    the library. The class provides methods to add new modalities and properties, and\n    to access the existing modalities. The class is implemented as a singleton to\n    ensure that there is only one instance of the registry in the library.\n    \"\"\"\n\n    _instance: ClassVar[Any] = None\n    _modality_registry: dict[str, Modality] = {}\n\n    def __new__(cls) -&gt; Self:\n        \"\"\"Create a new instance of the class if it does not exist.\"\"\"\n        if cls._instance is None:\n            cls._instance = super().__new__(cls)\n            cls._instance._modality_registry = {}\n        return cls._instance  # type: ignore[no-any-return]\n\n    def register_modality(\n        self, name: str, modality_specific_properties: Optional[dict[str, str]] = None\n    ) -&gt; None:\n        \"\"\"Add a new modality to the registry.\n\n        Parameters\n        ----------\n        name : str\n            The name of the modality.\n        modality_specific_properties : Optional[dict[str, str]], optional, default=None\n            Additional properties specific to the modality.\n\n        Warns\n        -----\n        UserWarning\n            If the modality already exists in the registry. It will overwrite the\n            existing modality.\n\n        \"\"\"\n        if name.lower() in self._modality_registry:\n            warnings.warn(\n                f\"Modality '{name}' already exists in the registry. Overwriting...\",\n                category=UserWarning,\n                stacklevel=2,\n            )\n\n        name = name.lower()\n        modality = Modality(name, modality_specific_properties)\n        self._modality_registry[name] = modality\n        setattr(self, name, modality)\n\n    def add_default_property(self, name: str, format_string: str) -&gt; None:\n        \"\"\"Add a new property that is applicable to all modalities.\n\n        Parameters\n        ----------\n        name : str\n            The name of the property.\n        format_string : str\n            The format string for the property. The format string should contain a\n            placeholder that will be replaced with the name of the modality when the\n            property is accessed.\n\n        Warns\n        -----\n        UserWarning\n            If the property already exists for the default properties. It will\n            overwrite the existing property.\n\n        Raises\n        ------\n        ValueError\n            If the format string is invalid. A valid format string contains at least one\n            placeholder enclosed in curly braces.\n        \"\"\"\n        for modality in self._modality_registry.values():\n            modality.add_property(name, format_string)\n\n    def has_modality(self, name: str) -&gt; bool:\n        \"\"\"Check if the modality exists in the registry.\n\n        Parameters\n        ----------\n        name : str\n            The name of the modality.\n\n        Returns\n        -------\n        bool\n            True if the modality exists in the registry, False otherwise.\n        \"\"\"\n        return name.lower() in self._modality_registry\n\n    def get_modality(self, name: str) -&gt; Modality:\n        \"\"\"Get the modality name from the registry.\n\n        Parameters\n        ----------\n        name : str\n            The name of the modality.\n\n        Returns\n        -------\n        Modality\n            The modality object from the registry.\n        \"\"\"\n        return self._modality_registry[name.lower()]\n\n    def get_modality_properties(self, name: str) -&gt; dict[str, str]:\n        \"\"\"Get the properties of a modality from the registry.\n\n        Parameters\n        ----------\n        name : str\n            The name of the modality.\n\n        Returns\n        -------\n        dict[str, str]\n            The properties associated with the modality.\n        \"\"\"\n        return self.get_modality(name).properties\n\n    def list_modalities(self) -&gt; list[Modality]:\n        \"\"\"Get the list of supported modalities in the registry.\n\n        Returns\n        -------\n        list[Modality]\n            The list of supported modalities in the registry.\n        \"\"\"\n        return list(self._modality_registry.values())\n\n    def __getattr__(self, name: str) -&gt; Modality:\n        \"\"\"Access a modality as an attribute by its name.\"\"\"\n        if name.lower() in self._modality_registry:\n            return self._modality_registry[name.lower()]\n        raise AttributeError(\n            f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n        )\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.modalities.ModalityRegistry.__new__","title":"__new__","text":"<pre><code>__new__()\n</code></pre> <p>Create a new instance of the class if it does not exist.</p> Source code in <code>mmlearn/datasets/core/modalities.py</code> <pre><code>def __new__(cls) -&gt; Self:\n    \"\"\"Create a new instance of the class if it does not exist.\"\"\"\n    if cls._instance is None:\n        cls._instance = super().__new__(cls)\n        cls._instance._modality_registry = {}\n    return cls._instance  # type: ignore[no-any-return]\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.modalities.ModalityRegistry.register_modality","title":"register_modality","text":"<pre><code>register_modality(name, modality_specific_properties=None)\n</code></pre> <p>Add a new modality to the registry.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the modality.</p> required <code>modality_specific_properties</code> <code>Optional[dict[str, str]]</code> <p>Additional properties specific to the modality.</p> <code>None</code> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If the modality already exists in the registry. It will overwrite the existing modality.</p> Source code in <code>mmlearn/datasets/core/modalities.py</code> <pre><code>def register_modality(\n    self, name: str, modality_specific_properties: Optional[dict[str, str]] = None\n) -&gt; None:\n    \"\"\"Add a new modality to the registry.\n\n    Parameters\n    ----------\n    name : str\n        The name of the modality.\n    modality_specific_properties : Optional[dict[str, str]], optional, default=None\n        Additional properties specific to the modality.\n\n    Warns\n    -----\n    UserWarning\n        If the modality already exists in the registry. It will overwrite the\n        existing modality.\n\n    \"\"\"\n    if name.lower() in self._modality_registry:\n        warnings.warn(\n            f\"Modality '{name}' already exists in the registry. Overwriting...\",\n            category=UserWarning,\n            stacklevel=2,\n        )\n\n    name = name.lower()\n    modality = Modality(name, modality_specific_properties)\n    self._modality_registry[name] = modality\n    setattr(self, name, modality)\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.modalities.ModalityRegistry.add_default_property","title":"add_default_property","text":"<pre><code>add_default_property(name, format_string)\n</code></pre> <p>Add a new property that is applicable to all modalities.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the property.</p> required <code>format_string</code> <code>str</code> <p>The format string for the property. The format string should contain a placeholder that will be replaced with the name of the modality when the property is accessed.</p> required <p>Warns:</p> Type Description <code>UserWarning</code> <p>If the property already exists for the default properties. It will overwrite the existing property.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the format string is invalid. A valid format string contains at least one placeholder enclosed in curly braces.</p> Source code in <code>mmlearn/datasets/core/modalities.py</code> <pre><code>def add_default_property(self, name: str, format_string: str) -&gt; None:\n    \"\"\"Add a new property that is applicable to all modalities.\n\n    Parameters\n    ----------\n    name : str\n        The name of the property.\n    format_string : str\n        The format string for the property. The format string should contain a\n        placeholder that will be replaced with the name of the modality when the\n        property is accessed.\n\n    Warns\n    -----\n    UserWarning\n        If the property already exists for the default properties. It will\n        overwrite the existing property.\n\n    Raises\n    ------\n    ValueError\n        If the format string is invalid. A valid format string contains at least one\n        placeholder enclosed in curly braces.\n    \"\"\"\n    for modality in self._modality_registry.values():\n        modality.add_property(name, format_string)\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.modalities.ModalityRegistry.has_modality","title":"has_modality","text":"<pre><code>has_modality(name)\n</code></pre> <p>Check if the modality exists in the registry.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the modality.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the modality exists in the registry, False otherwise.</p> Source code in <code>mmlearn/datasets/core/modalities.py</code> <pre><code>def has_modality(self, name: str) -&gt; bool:\n    \"\"\"Check if the modality exists in the registry.\n\n    Parameters\n    ----------\n    name : str\n        The name of the modality.\n\n    Returns\n    -------\n    bool\n        True if the modality exists in the registry, False otherwise.\n    \"\"\"\n    return name.lower() in self._modality_registry\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.modalities.ModalityRegistry.get_modality","title":"get_modality","text":"<pre><code>get_modality(name)\n</code></pre> <p>Get the modality name from the registry.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the modality.</p> required <p>Returns:</p> Type Description <code>Modality</code> <p>The modality object from the registry.</p> Source code in <code>mmlearn/datasets/core/modalities.py</code> <pre><code>def get_modality(self, name: str) -&gt; Modality:\n    \"\"\"Get the modality name from the registry.\n\n    Parameters\n    ----------\n    name : str\n        The name of the modality.\n\n    Returns\n    -------\n    Modality\n        The modality object from the registry.\n    \"\"\"\n    return self._modality_registry[name.lower()]\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.modalities.ModalityRegistry.get_modality_properties","title":"get_modality_properties","text":"<pre><code>get_modality_properties(name)\n</code></pre> <p>Get the properties of a modality from the registry.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the modality.</p> required <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>The properties associated with the modality.</p> Source code in <code>mmlearn/datasets/core/modalities.py</code> <pre><code>def get_modality_properties(self, name: str) -&gt; dict[str, str]:\n    \"\"\"Get the properties of a modality from the registry.\n\n    Parameters\n    ----------\n    name : str\n        The name of the modality.\n\n    Returns\n    -------\n    dict[str, str]\n        The properties associated with the modality.\n    \"\"\"\n    return self.get_modality(name).properties\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.modalities.ModalityRegistry.list_modalities","title":"list_modalities","text":"<pre><code>list_modalities()\n</code></pre> <p>Get the list of supported modalities in the registry.</p> <p>Returns:</p> Type Description <code>list[Modality]</code> <p>The list of supported modalities in the registry.</p> Source code in <code>mmlearn/datasets/core/modalities.py</code> <pre><code>def list_modalities(self) -&gt; list[Modality]:\n    \"\"\"Get the list of supported modalities in the registry.\n\n    Returns\n    -------\n    list[Modality]\n        The list of supported modalities in the registry.\n    \"\"\"\n    return list(self._modality_registry.values())\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.modalities.ModalityRegistry.__getattr__","title":"__getattr__","text":"<pre><code>__getattr__(name)\n</code></pre> <p>Access a modality as an attribute by its name.</p> Source code in <code>mmlearn/datasets/core/modalities.py</code> <pre><code>def __getattr__(self, name: str) -&gt; Modality:\n    \"\"\"Access a modality as an attribute by its name.\"\"\"\n    if name.lower() in self._modality_registry:\n        return self._modality_registry[name.lower()]\n    raise AttributeError(\n        f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n    )\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.samplers","title":"samplers","text":"<p>Samplers for data loading.</p>"},{"location":"api/#mmlearn.datasets.core.samplers.CombinedDatasetRatioSampler","title":"CombinedDatasetRatioSampler","text":"<p>               Bases: <code>Sampler[int]</code></p> <p>Sampler for weighted sampling from a class:<code>~mmlearn.datasets.core.combined_dataset.CombinedDataset</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>CombinedDataset</code> <p>An instance of class:<code>~mmlearn.datasets.core.combined_dataset.CombinedDataset</code> to sample from.</p> required <code>ratios</code> <code>Optional[Sequence[float]]</code> <p>A sequence of ratios for sampling from each dataset in the combined dataset. The length of the sequence must be equal to the number of datasets in the combined dataset (<code>dataset</code>). If <code>None</code>, the length of each dataset in the combined dataset is used as the ratio. The ratios are normalized to sum to 1.</p> <code>None</code> <code>num_samples</code> <code>Optional[int]</code> <p>The number of samples to draw from the combined dataset. If <code>None</code>, the sampler will draw as many samples as there are in the combined dataset. This number must yield at least one sample per dataset in the combined dataset, when multiplied by the corresponding ratio.</p> <code>None</code> <code>replacement</code> <code>bool</code> <p>Whether to sample with replacement or not.</p> <code>False</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the sampled indices or not. If <code>False</code>, the indices of each dataset will appear in the order they are stored in the combined dataset. This is similar to sequential sampling from each dataset. The datasets that make up the combined dataset are still sampled randomly.</p> <code>True</code> <code>rank</code> <code>Optional[int]</code> <p>Rank of the current process within :attr:<code>num_replicas</code>. By default, :attr:<code>rank</code> is retrieved from the current distributed group.</p> <code>None</code> <code>num_replicas</code> <code>Optional[int]</code> <p>Number of processes participating in distributed training. By default, :attr:<code>num_replicas</code> is retrieved from the current distributed group.</p> <code>None</code> <code>drop_last</code> <code>bool</code> <p>Whether to drop the last incomplete batch or not. If <code>True</code>, the sampler will drop samples to make the number of samples evenly divisible by the number of replicas in distributed mode.</p> <code>False</code> <code>seed</code> <code>int</code> <p>Random seed used to when sampling from the combined dataset and shuffling the sampled indices.</p> <code>0</code> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>CombinedDataset</code> <p>The dataset to sample from.</p> <code>num_samples</code> <code>int</code> <p>The number of samples to draw from the combined dataset.</p> <code>probs</code> <code>Tensor</code> <p>The probabilities for sampling from each dataset in the combined dataset. This is computed from the <code>ratios</code> argument and is normalized to sum to 1.</p> <code>replacement</code> <code>bool</code> <p>Whether to sample with replacement or not.</p> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the sampled indices or not.</p> <code>rank</code> <code>int</code> <p>Rank of the current process within :attr:<code>num_replicas</code>.</p> <code>num_replicas</code> <code>int</code> <p>Number of processes participating in distributed training.</p> <code>drop_last</code> <code>bool</code> <p>Whether to drop samples to make the number of samples evenly divisible by the number of replicas in distributed mode.</p> <code>seed</code> <code>int</code> <p>Random seed used to when sampling from the combined dataset and shuffling the sampled indices.</p> <code>epoch</code> <code>int</code> <p>Current epoch number. This is used to set the random seed. This is useful in distributed mode to ensure that each process receives a different random ordering of the samples.</p> <code>total_size</code> <code>int</code> <p>The total number of samples across all processes.</p> Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>@store(group=\"dataloader/sampler\", provider=\"mmlearn\")\nclass CombinedDatasetRatioSampler(Sampler[int]):\n    \"\"\"Sampler for weighted sampling from a :py:class:`~mmlearn.datasets.core.combined_dataset.CombinedDataset`.\n\n    Parameters\n    ----------\n    dataset : CombinedDataset\n        An instance of :py:class:`~mmlearn.datasets.core.combined_dataset.CombinedDataset`\n        to sample from.\n    ratios : Optional[Sequence[float]], optional, default=None\n        A sequence of ratios for sampling from each dataset in the combined dataset.\n        The length of the sequence must be equal to the number of datasets in the\n        combined dataset (`dataset`). If `None`, the length of each dataset in the\n        combined dataset is used as the ratio. The ratios are normalized to sum to 1.\n    num_samples : Optional[int], optional, default=None\n        The number of samples to draw from the combined dataset. If `None`, the\n        sampler will draw as many samples as there are in the combined dataset.\n        This number must yield at least one sample per dataset in the combined\n        dataset, when multiplied by the corresponding ratio.\n    replacement : bool, default=False\n        Whether to sample with replacement or not.\n    shuffle : bool, default=True\n        Whether to shuffle the sampled indices or not. If `False`, the indices of\n        each dataset will appear in the order they are stored in the combined dataset.\n        This is similar to sequential sampling from each dataset. The datasets\n        that make up the combined dataset are still sampled randomly.\n    rank : Optional[int], optional, default=None\n        Rank of the current process within :attr:`num_replicas`. By default,\n        :attr:`rank` is retrieved from the current distributed group.\n    num_replicas : Optional[int], optional, default=None\n        Number of processes participating in distributed training. By\n        default, :attr:`num_replicas` is retrieved from the current distributed group.\n    drop_last : bool, default=False\n        Whether to drop the last incomplete batch or not. If `True`, the sampler will\n        drop samples to make the number of samples evenly divisible by the number of\n        replicas in distributed mode.\n    seed : int, default=0\n        Random seed used to when sampling from the combined dataset and shuffling\n        the sampled indices.\n\n    Attributes\n    ----------\n    dataset : CombinedDataset\n        The dataset to sample from.\n    num_samples : int\n        The number of samples to draw from the combined dataset.\n    probs : torch.Tensor\n        The probabilities for sampling from each dataset in the combined dataset.\n        This is computed from the `ratios` argument and is normalized to sum to 1.\n    replacement : bool\n        Whether to sample with replacement or not.\n    shuffle : bool\n        Whether to shuffle the sampled indices or not.\n    rank : int\n        Rank of the current process within :attr:`num_replicas`.\n    num_replicas : int\n        Number of processes participating in distributed training.\n    drop_last : bool\n        Whether to drop samples to make the number of samples evenly divisible by the\n        number of replicas in distributed mode.\n    seed : int\n        Random seed used to when sampling from the combined dataset and shuffling\n        the sampled indices.\n    epoch : int\n        Current epoch number. This is used to set the random seed. This is useful\n        in distributed mode to ensure that each process receives a different random\n        ordering of the samples.\n    total_size : int\n        The total number of samples across all processes.\n    \"\"\"  # noqa: W505\n\n    def __init__(  # noqa: PLR0912\n        self,\n        dataset: CombinedDataset,\n        ratios: Optional[Sequence[float]] = None,\n        num_samples: Optional[int] = None,\n        replacement: bool = False,\n        shuffle: bool = True,\n        rank: Optional[int] = None,\n        num_replicas: Optional[int] = None,\n        drop_last: bool = False,\n        seed: int = 0,\n    ):\n        if not isinstance(dataset, CombinedDataset):\n            raise TypeError(\n                \"Expected argument `dataset` to be of type `CombinedDataset`, \"\n                f\"but got {type(dataset)}.\",\n            )\n        if not isinstance(seed, int):\n            raise TypeError(\n                f\"Expected argument `seed` to be an integer, but got {type(seed)}.\",\n            )\n        if num_replicas is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            num_replicas = dist.get_world_size()\n        if rank is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            rank = dist.get_rank()\n        if rank &gt;= num_replicas or rank &lt; 0:\n            raise ValueError(\n                f\"Invalid rank {rank}, rank should be in the interval [0, {num_replicas - 1}]\"\n            )\n\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.drop_last = drop_last\n        self.replacement = replacement\n        self.shuffle = shuffle\n        self.seed = seed\n        self.epoch = 0\n\n        self._num_samples = num_samples\n        if not isinstance(self.num_samples, int) or self.num_samples &lt;= 0:\n            raise ValueError(\n                \"Expected argument `num_samples` to be a positive integer, but got \"\n                f\"{self.num_samples}.\",\n            )\n\n        if ratios is None:\n            ratios = [len(subset) for subset in self.dataset.datasets]\n\n        num_datasets = len(self.dataset.datasets)\n        if len(ratios) != num_datasets:\n            raise ValueError(\n                f\"Expected argument `ratios` to be of length {num_datasets}, \"\n                f\"but got length {len(ratios)}.\",\n            )\n        prob_sum = sum(ratios)\n        if not all(ratio &gt;= 0 for ratio in ratios) and prob_sum &gt; 0:\n            raise ValueError(\n                \"Expected argument `ratios` to be a sequence of non-negative numbers. \"\n                f\"Got {ratios}.\",\n            )\n        self.probs = torch.tensor(\n            [ratio / prob_sum for ratio in ratios],\n            dtype=torch.double,\n        )\n        if any((prob * self.num_samples) &lt;= 0 for prob in self.probs):\n            raise ValueError(\n                \"Expected dataset ratio to result in at least one sample per dataset. \"\n                f\"Got dataset sizes {self.probs * self.num_samples}.\",\n            )\n\n    @property\n    def num_samples(self) -&gt; int:\n        \"\"\"Return the number of samples managed by the sampler.\"\"\"\n        # dataset size might change at runtime\n        if self._num_samples is None:\n            num_samples = len(self.dataset)\n        else:\n            num_samples = self._num_samples\n\n        if self.drop_last and num_samples % self.num_replicas != 0:\n            # split to nearest available length that is evenly divisible.\n            # This is to ensure each rank receives the same amount of data when\n            # using this Sampler.\n            num_samples = math.ceil(\n                (num_samples - self.num_replicas) / self.num_replicas,\n            )\n        else:\n            num_samples = math.ceil(num_samples / self.num_replicas)\n        return num_samples\n\n    @property\n    def total_size(self) -&gt; int:\n        \"\"\"Return the total size of the dataset.\"\"\"\n        return self.num_samples * self.num_replicas\n\n    def __iter__(self) -&gt; Iterator[int]:\n        \"\"\"Return an iterator that yields sample indices for the combined dataset.\"\"\"\n        generator = torch.Generator()\n        seed = self.seed + self.epoch\n        generator.manual_seed(seed)\n\n        cumulative_sizes = [0] + self.dataset._cumulative_sizes\n        num_samples_per_dataset = [int(prob * self.total_size) for prob in self.probs]\n        indices = []\n        for i in range(len(self.dataset.datasets)):\n            per_dataset_indices: torch.Tensor = torch.multinomial(\n                torch.ones(cumulative_sizes[i + 1] - cumulative_sizes[i]),\n                num_samples_per_dataset[i],\n                replacement=self.replacement,\n                generator=generator,\n            )\n            # adjust indices to reflect position in cumulative dataset\n            per_dataset_indices += cumulative_sizes[i]\n            assert per_dataset_indices.max() &lt; cumulative_sizes[i + 1], (\n                f\"Indices from dataset {i} exceed dataset size. \"\n                f\"Got indices {per_dataset_indices} and dataset size {cumulative_sizes[i + 1]}.\",\n            )\n            indices.append(per_dataset_indices)\n\n        indices = torch.cat(indices)\n        if self.shuffle:\n            rand_indices = torch.randperm(len(indices), generator=generator)\n            indices = indices[rand_indices]\n\n        indices = indices.tolist()  # type: ignore[attr-defined]\n        num_indices = len(indices)\n\n        if num_indices &lt; self.total_size:\n            padding_size = self.total_size - num_indices\n            if padding_size &lt;= num_indices:\n                indices += indices[:padding_size]\n            else:\n                indices += (indices * math.ceil(padding_size / num_indices))[\n                    :padding_size\n                ]\n        elif num_indices &gt; self.total_size:\n            indices = indices[: self.total_size]\n        assert len(indices) == self.total_size\n\n        # subsample\n        indices = indices[self.rank : self.total_size : self.num_replicas]\n        assert len(indices) == self.num_samples, (\n            f\"Expected {self.num_samples} samples, but got {len(indices)}.\",\n        )\n\n        yield from iter(indices)\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the total number of samples in the sampler.\"\"\"\n        return self.num_samples\n\n    def set_epoch(self, epoch: int) -&gt; None:\n        \"\"\"Set the epoch for this sampler.\n\n        When :attr:`shuffle=True`, this ensures all replicas use a different random\n        ordering for each epoch. Otherwise, the next iteration of this sampler\n        will yield the same ordering.\n\n        Parameters\n        ----------\n        epoch : int\n            Epoch number.\n\n        \"\"\"\n        self.epoch = epoch\n\n        # some iterable datasets (especially huggingface iterable datasets) might\n        # require setting the epoch to ensure shuffling works properly\n        for dataset in self.dataset.datasets:\n            if hasattr(dataset, \"set_epoch\"):\n                dataset.set_epoch(epoch)\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.samplers.CombinedDatasetRatioSampler.num_samples","title":"num_samples  <code>property</code>","text":"<pre><code>num_samples\n</code></pre> <p>Return the number of samples managed by the sampler.</p>"},{"location":"api/#mmlearn.datasets.core.samplers.CombinedDatasetRatioSampler.total_size","title":"total_size  <code>property</code>","text":"<pre><code>total_size\n</code></pre> <p>Return the total size of the dataset.</p>"},{"location":"api/#mmlearn.datasets.core.samplers.CombinedDatasetRatioSampler.__iter__","title":"__iter__","text":"<pre><code>__iter__()\n</code></pre> <p>Return an iterator that yields sample indices for the combined dataset.</p> Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>def __iter__(self) -&gt; Iterator[int]:\n    \"\"\"Return an iterator that yields sample indices for the combined dataset.\"\"\"\n    generator = torch.Generator()\n    seed = self.seed + self.epoch\n    generator.manual_seed(seed)\n\n    cumulative_sizes = [0] + self.dataset._cumulative_sizes\n    num_samples_per_dataset = [int(prob * self.total_size) for prob in self.probs]\n    indices = []\n    for i in range(len(self.dataset.datasets)):\n        per_dataset_indices: torch.Tensor = torch.multinomial(\n            torch.ones(cumulative_sizes[i + 1] - cumulative_sizes[i]),\n            num_samples_per_dataset[i],\n            replacement=self.replacement,\n            generator=generator,\n        )\n        # adjust indices to reflect position in cumulative dataset\n        per_dataset_indices += cumulative_sizes[i]\n        assert per_dataset_indices.max() &lt; cumulative_sizes[i + 1], (\n            f\"Indices from dataset {i} exceed dataset size. \"\n            f\"Got indices {per_dataset_indices} and dataset size {cumulative_sizes[i + 1]}.\",\n        )\n        indices.append(per_dataset_indices)\n\n    indices = torch.cat(indices)\n    if self.shuffle:\n        rand_indices = torch.randperm(len(indices), generator=generator)\n        indices = indices[rand_indices]\n\n    indices = indices.tolist()  # type: ignore[attr-defined]\n    num_indices = len(indices)\n\n    if num_indices &lt; self.total_size:\n        padding_size = self.total_size - num_indices\n        if padding_size &lt;= num_indices:\n            indices += indices[:padding_size]\n        else:\n            indices += (indices * math.ceil(padding_size / num_indices))[\n                :padding_size\n            ]\n    elif num_indices &gt; self.total_size:\n        indices = indices[: self.total_size]\n    assert len(indices) == self.total_size\n\n    # subsample\n    indices = indices[self.rank : self.total_size : self.num_replicas]\n    assert len(indices) == self.num_samples, (\n        f\"Expected {self.num_samples} samples, but got {len(indices)}.\",\n    )\n\n    yield from iter(indices)\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.samplers.CombinedDatasetRatioSampler.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the total number of samples in the sampler.</p> Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the total number of samples in the sampler.\"\"\"\n    return self.num_samples\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.samplers.CombinedDatasetRatioSampler.set_epoch","title":"set_epoch","text":"<pre><code>set_epoch(epoch)\n</code></pre> <p>Set the epoch for this sampler.</p> <p>When :attr:<code>shuffle=True</code>, this ensures all replicas use a different random ordering for each epoch. Otherwise, the next iteration of this sampler will yield the same ordering.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>Epoch number.</p> required Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>def set_epoch(self, epoch: int) -&gt; None:\n    \"\"\"Set the epoch for this sampler.\n\n    When :attr:`shuffle=True`, this ensures all replicas use a different random\n    ordering for each epoch. Otherwise, the next iteration of this sampler\n    will yield the same ordering.\n\n    Parameters\n    ----------\n    epoch : int\n        Epoch number.\n\n    \"\"\"\n    self.epoch = epoch\n\n    # some iterable datasets (especially huggingface iterable datasets) might\n    # require setting the epoch to ensure shuffling works properly\n    for dataset in self.dataset.datasets:\n        if hasattr(dataset, \"set_epoch\"):\n            dataset.set_epoch(epoch)\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.samplers.DistributedEvalSampler","title":"DistributedEvalSampler","text":"<p>               Bases: <code>Sampler[int]</code></p> <p>Sampler for distributed evaluation.</p> <p>The main differences between this and class:<code>torch.utils.data.DistributedSampler</code> are that this sampler does not add extra samples to make it evenly divisible and shuffling is disabled by default.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>Dataset used for sampling.</p> required <code>num_replicas</code> <code>Optional[int]</code> <p>Number of processes participating in distributed training. By default, :attr:<code>rank</code> is retrieved from the current distributed group.</p> <code>None</code> <code>rank</code> <code>Optional[int]</code> <p>Rank of the current process within :attr:<code>num_replicas</code>. By default, :attr:<code>rank</code> is retrieved from the current distributed group.</p> <code>None</code> <code>shuffle</code> <code>bool</code> <p>If <code>True</code> (default), sampler will shuffle the indices.</p> <code>False</code> <code>seed</code> <code>int</code> <p>Random seed used to shuffle the sampler if :attr:<code>shuffle=True</code>. This number should be identical across all processes in the distributed group.</p> <code>0</code> Warnings <p>DistributedEvalSampler should NOT be used for training. The distributed processes could hang forever. See [1]_ for details</p> Notes <ul> <li>This sampler is for evaluation purpose where synchronization does not happen   every epoch. Synchronization should be done outside the dataloader loop.   It is especially useful in conjunction with   class:<code>torch.nn.parallel.DistributedDataParallel</code> [2]_.</li> <li>The input Dataset is assumed to be of constant size.</li> <li>This implementation is adapted from [3]_.</li> </ul> References <p>.. [1] https://github.com/pytorch/pytorch/issues/22584 .. [2] https://discuss.pytorch.org/t/how-to-validate-in-distributeddataparallel-correctly/94267/11 .. [3] https://github.com/SeungjunNah/DeepDeblur-PyTorch/blob/master/src/data/sampler.py</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; def example():\n...     start_epoch, n_epochs = 0, 2\n...     sampler = DistributedEvalSampler(dataset) if is_distributed else None\n...     loader = DataLoader(dataset, shuffle=(sampler is None), sampler=sampler)\n...     for epoch in range(start_epoch, n_epochs):\n...         if is_distributed:\n...             sampler.set_epoch(epoch)\n...         evaluate(loader)\n</code></pre> Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>@store(group=\"dataloader/sampler\", provider=\"mmlearn\")\nclass DistributedEvalSampler(Sampler[int]):\n    \"\"\"Sampler for distributed evaluation.\n\n    The main differences between this and :py:class:`torch.utils.data.DistributedSampler`\n    are that this sampler does not add extra samples to make it evenly divisible and\n    shuffling is disabled by default.\n\n    Parameters\n    ----------\n    dataset : torch.utils.data.Dataset\n        Dataset used for sampling.\n    num_replicas : Optional[int], optional, default=None\n        Number of processes participating in distributed training. By\n        default, :attr:`rank` is retrieved from the current distributed group.\n    rank : Optional[int], optional, default=None\n        Rank of the current process within :attr:`num_replicas`. By default,\n        :attr:`rank` is retrieved from the current distributed group.\n    shuffle : bool, optional, default=False\n        If `True` (default), sampler will shuffle the indices.\n    seed : int, optional, default=0\n        Random seed used to shuffle the sampler if :attr:`shuffle=True`.\n        This number should be identical across all processes in the\n        distributed group.\n\n    Warnings\n    --------\n    DistributedEvalSampler should NOT be used for training. The distributed processes\n    could hang forever. See [1]_ for details\n\n    Notes\n    -----\n    - This sampler is for evaluation purpose where synchronization does not happen\n      every epoch. Synchronization should be done outside the dataloader loop.\n      It is especially useful in conjunction with\n      :py:class:`torch.nn.parallel.DistributedDataParallel` [2]_.\n    - The input Dataset is assumed to be of constant size.\n    - This implementation is adapted from [3]_.\n\n    References\n    ----------\n    .. [1] https://github.com/pytorch/pytorch/issues/22584\n    .. [2] https://discuss.pytorch.org/t/how-to-validate-in-distributeddataparallel-correctly/94267/11\n    .. [3] https://github.com/SeungjunNah/DeepDeblur-PyTorch/blob/master/src/data/sampler.py\n\n\n    Examples\n    --------\n    &gt;&gt;&gt; def example():\n    ...     start_epoch, n_epochs = 0, 2\n    ...     sampler = DistributedEvalSampler(dataset) if is_distributed else None\n    ...     loader = DataLoader(dataset, shuffle=(sampler is None), sampler=sampler)\n    ...     for epoch in range(start_epoch, n_epochs):\n    ...         if is_distributed:\n    ...             sampler.set_epoch(epoch)\n    ...         evaluate(loader)\n\n    \"\"\"  # noqa: W505\n\n    def __init__(\n        self,\n        dataset: Dataset[Sized],\n        num_replicas: Optional[int] = None,\n        rank: Optional[int] = None,\n        shuffle: bool = False,\n        seed: int = 0,\n    ) -&gt; None:\n        if num_replicas is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            num_replicas = dist.get_world_size()\n        if rank is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            rank = dist.get_rank()\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.epoch = 0\n        self.shuffle = shuffle\n        self.seed = seed\n\n    @property\n    def total_size(self) -&gt; int:\n        \"\"\"Return the total size of the dataset.\"\"\"\n        return len(self.dataset)\n\n    @property\n    def num_samples(self) -&gt; int:\n        \"\"\"Return the number of samples managed by the sampler.\"\"\"\n        indices = list(range(self.total_size))[\n            self.rank : self.total_size : self.num_replicas\n        ]\n        return len(indices)  # true value without extra samples\n\n    def __iter__(self) -&gt; Iterator[int]:\n        \"\"\"Return an iterator that iterates over the indices of the dataset.\"\"\"\n        if self.shuffle:\n            # deterministically shuffle based on epoch and seed\n            g = torch.Generator()\n            g.manual_seed(self.seed + self.epoch)\n            indices = torch.randperm(self.total_size, generator=g).tolist()\n        else:\n            indices = list(range(self.total_size))\n\n        # subsample\n        indices = indices[self.rank : self.total_size : self.num_replicas]\n        assert len(indices) == self.num_samples\n\n        return iter(indices)\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the number of samples.\"\"\"\n        return self.num_samples\n\n    def set_epoch(self, epoch: int) -&gt; None:\n        \"\"\"Set the epoch for this sampler.\n\n        When :attr:`shuffle=True`, this ensures all replicas use a different random\n        ordering for each epoch. Otherwise, the next iteration of this sampler\n        will yield the same ordering.\n\n        Parameters\n        ----------\n        epoch : int\n            Epoch number.\n\n        \"\"\"\n        self.epoch = epoch\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.samplers.DistributedEvalSampler.total_size","title":"total_size  <code>property</code>","text":"<pre><code>total_size\n</code></pre> <p>Return the total size of the dataset.</p>"},{"location":"api/#mmlearn.datasets.core.samplers.DistributedEvalSampler.num_samples","title":"num_samples  <code>property</code>","text":"<pre><code>num_samples\n</code></pre> <p>Return the number of samples managed by the sampler.</p>"},{"location":"api/#mmlearn.datasets.core.samplers.DistributedEvalSampler.__iter__","title":"__iter__","text":"<pre><code>__iter__()\n</code></pre> <p>Return an iterator that iterates over the indices of the dataset.</p> Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>def __iter__(self) -&gt; Iterator[int]:\n    \"\"\"Return an iterator that iterates over the indices of the dataset.\"\"\"\n    if self.shuffle:\n        # deterministically shuffle based on epoch and seed\n        g = torch.Generator()\n        g.manual_seed(self.seed + self.epoch)\n        indices = torch.randperm(self.total_size, generator=g).tolist()\n    else:\n        indices = list(range(self.total_size))\n\n    # subsample\n    indices = indices[self.rank : self.total_size : self.num_replicas]\n    assert len(indices) == self.num_samples\n\n    return iter(indices)\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.samplers.DistributedEvalSampler.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the number of samples.</p> Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the number of samples.\"\"\"\n    return self.num_samples\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.samplers.DistributedEvalSampler.set_epoch","title":"set_epoch","text":"<pre><code>set_epoch(epoch)\n</code></pre> <p>Set the epoch for this sampler.</p> <p>When :attr:<code>shuffle=True</code>, this ensures all replicas use a different random ordering for each epoch. Otherwise, the next iteration of this sampler will yield the same ordering.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>Epoch number.</p> required Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>def set_epoch(self, epoch: int) -&gt; None:\n    \"\"\"Set the epoch for this sampler.\n\n    When :attr:`shuffle=True`, this ensures all replicas use a different random\n    ordering for each epoch. Otherwise, the next iteration of this sampler\n    will yield the same ordering.\n\n    Parameters\n    ----------\n    epoch : int\n        Epoch number.\n\n    \"\"\"\n    self.epoch = epoch\n</code></pre>"},{"location":"api/#mmlearn.datasets.imagenet","title":"imagenet","text":"<p>ImageNet dataset.</p>"},{"location":"api/#mmlearn.datasets.imagenet.ImageNet","title":"ImageNet","text":"<p>               Bases: <code>ImageFolder</code></p> <p>ImageNet dataset.</p> <p>This is a wrapper around the class:<code>~torchvision.datasets.ImageFolder</code> class that returns an class:<code>~mmlearn.datasets.core.example.Example</code> object.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>Path to the root directory of the dataset.</p> required <code>split</code> <code>(train, val)</code> <p>The split of the dataset to use.</p> <code>\"train\"</code> <code>transform</code> <code>Optional[Callable]</code> <p>A callable that takes in a PIL image and returns a transformed version of the image as a PyTorch tensor.</p> <code>None</code> <code>target_transform</code> <code>Optional[Callable]</code> <p>A callable that takes in the target and transforms it.</p> <code>None</code> <code>mask_generator</code> <code>Optional[Callable]</code> <p>A callable that generates a mask for the image.</p> <code>None</code> Source code in <code>mmlearn/datasets/imagenet.py</code> <pre><code>@store(\n    group=\"datasets\",\n    provider=\"mmlearn\",\n    root_dir=os.getenv(\"IMAGENET_ROOT_DIR\", MISSING),\n)\nclass ImageNet(ImageFolder):\n    \"\"\"ImageNet dataset.\n\n    This is a wrapper around the :py:class:`~torchvision.datasets.ImageFolder` class\n    that returns an :py:class:`~mmlearn.datasets.core.example.Example` object.\n\n    Parameters\n    ----------\n    root_dir : str\n        Path to the root directory of the dataset.\n    split : {\"train\", \"val\"}, default=\"train\"\n        The split of the dataset to use.\n    transform : Optional[Callable], optional, default=None\n        A callable that takes in a PIL image and returns a transformed version\n        of the image as a PyTorch tensor.\n    target_transform : Optional[Callable], optional, default=None\n        A callable that takes in the target and transforms it.\n    mask_generator : Optional[Callable], optional, default=None\n        A callable that generates a mask for the image.\n    \"\"\"\n\n    def __init__(\n        self,\n        root_dir: str,\n        split: Literal[\"train\", \"val\"] = \"train\",\n        transform: Optional[Callable[..., Any]] = None,\n        target_transform: Optional[Callable[..., Any]] = None,\n        mask_generator: Optional[Callable[..., Any]] = None,\n    ) -&gt; None:\n        split = \"train\" if split == \"train\" else \"val\"\n        root_dir = os.path.join(root_dir, split)\n        super().__init__(\n            root=root_dir, transform=transform, target_transform=target_transform\n        )\n        self.mask_generator = mask_generator\n\n    def __getitem__(self, index: int) -&gt; Example:\n        \"\"\"Get an example at the given index.\"\"\"\n        image, target = super().__getitem__(index)\n        example = Example(\n            {\n                Modalities.RGB.name: image,\n                Modalities.RGB.target: target,\n                EXAMPLE_INDEX_KEY: index,\n            }\n        )\n        mask = self.mask_generator() if self.mask_generator else None\n        if mask is not None:  # error will be raised during collation if `None`\n            example[Modalities.RGB.mask] = mask\n        return example\n\n    @property\n    def zero_shot_prompt_templates(self) -&gt; list[str]:\n        \"\"\"Return the zero-shot prompt templates.\"\"\"\n        return [\n            \"a bad photo of a {}.\",\n            \"a photo of many {}.\",\n            \"a sculpture of a {}.\",\n            \"a photo of the hard to see {}.\",\n            \"a low resolution photo of the {}.\",\n            \"a rendering of a {}.\",\n            \"graffiti of a {}.\",\n            \"a bad photo of the {}.\",\n            \"a cropped photo of the {}.\",\n            \"a tattoo of a {}.\",\n            \"the embroidered {}.\",\n            \"a photo of a hard to see {}.\",\n            \"a bright photo of a {}.\",\n            \"a photo of a clean {}.\",\n            \"a photo of a dirty {}.\",\n            \"a dark photo of the {}.\",\n            \"a drawing of a {}.\",\n            \"a photo of my {}.\",\n            \"the plastic {}.\",\n            \"a photo of the cool {}.\",\n            \"a close-up photo of a {}.\",\n            \"a black and white photo of the {}.\",\n            \"a painting of the {}.\",\n            \"a painting of a {}.\",\n            \"a pixelated photo of the {}.\",\n            \"a sculpture of the {}.\",\n            \"a bright photo of the {}.\",\n            \"a cropped photo of a {}.\",\n            \"a plastic {}.\",\n            \"a photo of the dirty {}.\",\n            \"a jpeg corrupted photo of a {}.\",\n            \"a blurry photo of the {}.\",\n            \"a photo of the {}.\",\n            \"a good photo of the {}.\",\n            \"a rendering of the {}.\",\n            \"a {} in a video game.\",\n            \"a photo of one {}.\",\n            \"a doodle of a {}.\",\n            \"a close-up photo of the {}.\",\n            \"a photo of a {}.\",\n            \"the origami {}.\",\n            \"the {} in a video game.\",\n            \"a sketch of a {}.\",\n            \"a doodle of the {}.\",\n            \"a origami {}.\",\n            \"a low resolution photo of a {}.\",\n            \"the toy {}.\",\n            \"a rendition of the {}.\",\n            \"a photo of the clean {}.\",\n            \"a photo of a large {}.\",\n            \"a rendition of a {}.\",\n            \"a photo of a nice {}.\",\n            \"a photo of a weird {}.\",\n            \"a blurry photo of a {}.\",\n            \"a cartoon {}.\",\n            \"art of a {}.\",\n            \"a sketch of the {}.\",\n            \"a embroidered {}.\",\n            \"a pixelated photo of a {}.\",\n            \"itap of the {}.\",\n            \"a jpeg corrupted photo of the {}.\",\n            \"a good photo of a {}.\",\n            \"a plushie {}.\",\n            \"a photo of the nice {}.\",\n            \"a photo of the small {}.\",\n            \"a photo of the weird {}.\",\n            \"the cartoon {}.\",\n            \"art of the {}.\",\n            \"a drawing of the {}.\",\n            \"a photo of the large {}.\",\n            \"a black and white photo of a {}.\",\n            \"the plushie {}.\",\n            \"a dark photo of a {}.\",\n            \"itap of a {}.\",\n            \"graffiti of the {}.\",\n            \"a toy {}.\",\n            \"itap of my {}.\",\n            \"a photo of a cool {}.\",\n            \"a photo of a small {}.\",\n            \"a tattoo of the {}.\",\n        ]\n\n    @property\n    def id2label(self) -&gt; dict[int, str]:\n        \"\"\"Return the label mapping.\"\"\"\n        return {\n            0: \"tench\",\n            1: \"goldfish\",\n            2: \"great white shark\",\n            3: \"tiger shark\",\n            4: \"hammerhead shark\",\n            5: \"electric ray\",\n            6: \"stingray\",\n            7: \"rooster\",\n            8: \"hen\",\n            9: \"ostrich\",\n            10: \"brambling\",\n            11: \"goldfinch\",\n            12: \"house finch\",\n            13: \"junco\",\n            14: \"indigo bunting\",\n            15: \"American robin\",\n            16: \"bulbul\",\n            17: \"jay\",\n            18: \"magpie\",\n            19: \"chickadee\",\n            20: \"American dipper\",\n            21: \"kite (bird of prey)\",\n            22: \"bald eagle\",\n            23: \"vulture\",\n            24: \"great grey owl\",\n            25: \"fire salamander\",\n            26: \"smooth newt\",\n            27: \"newt\",\n            28: \"spotted salamander\",\n            29: \"axolotl\",\n            30: \"American bullfrog\",\n            31: \"tree frog\",\n            32: \"tailed frog\",\n            33: \"loggerhead sea turtle\",\n            34: \"leatherback sea turtle\",\n            35: \"mud turtle\",\n            36: \"terrapin\",\n            37: \"box turtle\",\n            38: \"banded gecko\",\n            39: \"green iguana\",\n            40: \"Carolina anole\",\n            41: \"desert grassland whiptail lizard\",\n            42: \"agama\",\n            43: \"frilled-necked lizard\",\n            44: \"alligator lizard\",\n            45: \"Gila monster\",\n            46: \"European green lizard\",\n            47: \"chameleon\",\n            48: \"Komodo dragon\",\n            49: \"Nile crocodile\",\n            50: \"American alligator\",\n            51: \"triceratops\",\n            52: \"worm snake\",\n            53: \"ring-necked snake\",\n            54: \"eastern hog-nosed snake\",\n            55: \"smooth green snake\",\n            56: \"kingsnake\",\n            57: \"garter snake\",\n            58: \"water snake\",\n            59: \"vine snake\",\n            60: \"night snake\",\n            61: \"boa constrictor\",\n            62: \"African rock python\",\n            63: \"Indian cobra\",\n            64: \"green mamba\",\n            65: \"sea snake\",\n            66: \"Saharan horned viper\",\n            67: \"eastern diamondback rattlesnake\",\n            68: \"sidewinder rattlesnake\",\n            69: \"trilobite\",\n            70: \"harvestman\",\n            71: \"scorpion\",\n            72: \"yellow garden spider\",\n            73: \"barn spider\",\n            74: \"European garden spider\",\n            75: \"southern black widow\",\n            76: \"tarantula\",\n            77: \"wolf spider\",\n            78: \"tick\",\n            79: \"centipede\",\n            80: \"black grouse\",\n            81: \"ptarmigan\",\n            82: \"ruffed grouse\",\n            83: \"prairie grouse\",\n            84: \"peafowl\",\n            85: \"quail\",\n            86: \"partridge\",\n            87: \"african grey parrot\",\n            88: \"macaw\",\n            89: \"sulphur-crested cockatoo\",\n            90: \"lorikeet\",\n            91: \"coucal\",\n            92: \"bee eater\",\n            93: \"hornbill\",\n            94: \"hummingbird\",\n            95: \"jacamar\",\n            96: \"toucan\",\n            97: \"duck\",\n            98: \"red-breasted merganser\",\n            99: \"goose\",\n            100: \"black swan\",\n            101: \"tusker\",\n            102: \"echidna\",\n            103: \"platypus\",\n            104: \"wallaby\",\n            105: \"koala\",\n            106: \"wombat\",\n            107: \"jellyfish\",\n            108: \"sea anemone\",\n            109: \"brain coral\",\n            110: \"flatworm\",\n            111: \"nematode\",\n            112: \"conch\",\n            113: \"snail\",\n            114: \"slug\",\n            115: \"sea slug\",\n            116: \"chiton\",\n            117: \"chambered nautilus\",\n            118: \"Dungeness crab\",\n            119: \"rock crab\",\n            120: \"fiddler crab\",\n            121: \"red king crab\",\n            122: \"American lobster\",\n            123: \"spiny lobster\",\n            124: \"crayfish\",\n            125: \"hermit crab\",\n            126: \"isopod\",\n            127: \"white stork\",\n            128: \"black stork\",\n            129: \"spoonbill\",\n            130: \"flamingo\",\n            131: \"little blue heron\",\n            132: \"great egret\",\n            133: \"bittern bird\",\n            134: \"crane bird\",\n            135: \"limpkin\",\n            136: \"common gallinule\",\n            137: \"American coot\",\n            138: \"bustard\",\n            139: \"ruddy turnstone\",\n            140: \"dunlin\",\n            141: \"common redshank\",\n            142: \"dowitcher\",\n            143: \"oystercatcher\",\n            144: \"pelican\",\n            145: \"king penguin\",\n            146: \"albatross\",\n            147: \"grey whale\",\n            148: \"killer whale\",\n            149: \"dugong\",\n            150: \"sea lion\",\n            151: \"Chihuahua\",\n            152: \"Japanese Chin\",\n            153: \"Maltese\",\n            154: \"Pekingese\",\n            155: \"Shih Tzu\",\n            156: \"King Charles Spaniel\",\n            157: \"Papillon\",\n            158: \"toy terrier\",\n            159: \"Rhodesian Ridgeback\",\n            160: \"Afghan Hound\",\n            161: \"Basset Hound\",\n            162: \"Beagle\",\n            163: \"Bloodhound\",\n            164: \"Bluetick Coonhound\",\n            165: \"Black and Tan Coonhound\",\n            166: \"Treeing Walker Coonhound\",\n            167: \"English foxhound\",\n            168: \"Redbone Coonhound\",\n            169: \"borzoi\",\n            170: \"Irish Wolfhound\",\n            171: \"Italian Greyhound\",\n            172: \"Whippet\",\n            173: \"Ibizan Hound\",\n            174: \"Norwegian Elkhound\",\n            175: \"Otterhound\",\n            176: \"Saluki\",\n            177: \"Scottish Deerhound\",\n            178: \"Weimaraner\",\n            179: \"Staffordshire Bull Terrier\",\n            180: \"American Staffordshire Terrier\",\n            181: \"Bedlington Terrier\",\n            182: \"Border Terrier\",\n            183: \"Kerry Blue Terrier\",\n            184: \"Irish Terrier\",\n            185: \"Norfolk Terrier\",\n            186: \"Norwich Terrier\",\n            187: \"Yorkshire Terrier\",\n            188: \"Wire Fox Terrier\",\n            189: \"Lakeland Terrier\",\n            190: \"Sealyham Terrier\",\n            191: \"Airedale Terrier\",\n            192: \"Cairn Terrier\",\n            193: \"Australian Terrier\",\n            194: \"Dandie Dinmont Terrier\",\n            195: \"Boston Terrier\",\n            196: \"Miniature Schnauzer\",\n            197: \"Giant Schnauzer\",\n            198: \"Standard Schnauzer\",\n            199: \"Scottish Terrier\",\n            200: \"Tibetan Terrier\",\n            201: \"Australian Silky Terrier\",\n            202: \"Soft-coated Wheaten Terrier\",\n            203: \"West Highland White Terrier\",\n            204: \"Lhasa Apso\",\n            205: \"Flat-Coated Retriever\",\n            206: \"Curly-coated Retriever\",\n            207: \"Golden Retriever\",\n            208: \"Labrador Retriever\",\n            209: \"Chesapeake Bay Retriever\",\n            210: \"German Shorthaired Pointer\",\n            211: \"Vizsla\",\n            212: \"English Setter\",\n            213: \"Irish Setter\",\n            214: \"Gordon Setter\",\n            215: \"Brittany dog\",\n            216: \"Clumber Spaniel\",\n            217: \"English Springer Spaniel\",\n            218: \"Welsh Springer Spaniel\",\n            219: \"Cocker Spaniel\",\n            220: \"Sussex Spaniel\",\n            221: \"Irish Water Spaniel\",\n            222: \"Kuvasz\",\n            223: \"Schipperke\",\n            224: \"Groenendael dog\",\n            225: \"Malinois\",\n            226: \"Briard\",\n            227: \"Australian Kelpie\",\n            228: \"Komondor\",\n            229: \"Old English Sheepdog\",\n            230: \"Shetland Sheepdog\",\n            231: \"collie\",\n            232: \"Border Collie\",\n            233: \"Bouvier des Flandres dog\",\n            234: \"Rottweiler\",\n            235: \"German Shepherd Dog\",\n            236: \"Dobermann\",\n            237: \"Miniature Pinscher\",\n            238: \"Greater Swiss Mountain Dog\",\n            239: \"Bernese Mountain Dog\",\n            240: \"Appenzeller Sennenhund\",\n            241: \"Entlebucher Sennenhund\",\n            242: \"Boxer\",\n            243: \"Bullmastiff\",\n            244: \"Tibetan Mastiff\",\n            245: \"French Bulldog\",\n            246: \"Great Dane\",\n            247: \"St. Bernard\",\n            248: \"husky\",\n            249: \"Alaskan Malamute\",\n            250: \"Siberian Husky\",\n            251: \"Dalmatian\",\n            252: \"Affenpinscher\",\n            253: \"Basenji\",\n            254: \"pug\",\n            255: \"Leonberger\",\n            256: \"Newfoundland dog\",\n            257: \"Great Pyrenees dog\",\n            258: \"Samoyed\",\n            259: \"Pomeranian\",\n            260: \"Chow Chow\",\n            261: \"Keeshond\",\n            262: \"brussels griffon\",\n            263: \"Pembroke Welsh Corgi\",\n            264: \"Cardigan Welsh Corgi\",\n            265: \"Toy Poodle\",\n            266: \"Miniature Poodle\",\n            267: \"Standard Poodle\",\n            268: \"Mexican hairless dog (xoloitzcuintli)\",\n            269: \"grey wolf\",\n            270: \"Alaskan tundra wolf\",\n            271: \"red wolf or maned wolf\",\n            272: \"coyote\",\n            273: \"dingo\",\n            274: \"dhole\",\n            275: \"African wild dog\",\n            276: \"hyena\",\n            277: \"red fox\",\n            278: \"kit fox\",\n            279: \"Arctic fox\",\n            280: \"grey fox\",\n            281: \"tabby cat\",\n            282: \"tiger cat\",\n            283: \"Persian cat\",\n            284: \"Siamese cat\",\n            285: \"Egyptian Mau\",\n            286: \"cougar\",\n            287: \"lynx\",\n            288: \"leopard\",\n            289: \"snow leopard\",\n            290: \"jaguar\",\n            291: \"lion\",\n            292: \"tiger\",\n            293: \"cheetah\",\n            294: \"brown bear\",\n            295: \"American black bear\",\n            296: \"polar bear\",\n            297: \"sloth bear\",\n            298: \"mongoose\",\n            299: \"meerkat\",\n            300: \"tiger beetle\",\n            301: \"ladybug\",\n            302: \"ground beetle\",\n            303: \"longhorn beetle\",\n            304: \"leaf beetle\",\n            305: \"dung beetle\",\n            306: \"rhinoceros beetle\",\n            307: \"weevil\",\n            308: \"fly\",\n            309: \"bee\",\n            310: \"ant\",\n            311: \"grasshopper\",\n            312: \"cricket insect\",\n            313: \"stick insect\",\n            314: \"cockroach\",\n            315: \"praying mantis\",\n            316: \"cicada\",\n            317: \"leafhopper\",\n            318: \"lacewing\",\n            319: \"dragonfly\",\n            320: \"damselfly\",\n            321: \"red admiral butterfly\",\n            322: \"ringlet butterfly\",\n            323: \"monarch butterfly\",\n            324: \"small white butterfly\",\n            325: \"sulphur butterfly\",\n            326: \"gossamer-winged butterfly\",\n            327: \"starfish\",\n            328: \"sea urchin\",\n            329: \"sea cucumber\",\n            330: \"cottontail rabbit\",\n            331: \"hare\",\n            332: \"Angora rabbit\",\n            333: \"hamster\",\n            334: \"porcupine\",\n            335: \"fox squirrel\",\n            336: \"marmot\",\n            337: \"beaver\",\n            338: \"guinea pig\",\n            339: \"common sorrel horse\",\n            340: \"zebra\",\n            341: \"pig\",\n            342: \"wild boar\",\n            343: \"warthog\",\n            344: \"hippopotamus\",\n            345: \"ox\",\n            346: \"water buffalo\",\n            347: \"bison\",\n            348: \"ram (adult male sheep)\",\n            349: \"bighorn sheep\",\n            350: \"Alpine ibex\",\n            351: \"hartebeest\",\n            352: \"impala (antelope)\",\n            353: \"gazelle\",\n            354: \"arabian camel\",\n            355: \"llama\",\n            356: \"weasel\",\n            357: \"mink\",\n            358: \"European polecat\",\n            359: \"black-footed ferret\",\n            360: \"otter\",\n            361: \"skunk\",\n            362: \"badger\",\n            363: \"armadillo\",\n            364: \"three-toed sloth\",\n            365: \"orangutan\",\n            366: \"gorilla\",\n            367: \"chimpanzee\",\n            368: \"gibbon\",\n            369: \"siamang\",\n            370: \"guenon\",\n            371: \"patas monkey\",\n            372: \"baboon\",\n            373: \"macaque\",\n            374: \"langur\",\n            375: \"black-and-white colobus\",\n            376: \"proboscis monkey\",\n            377: \"marmoset\",\n            378: \"white-headed capuchin\",\n            379: \"howler monkey\",\n            380: \"titi monkey\",\n            381: \"Geoffroy's spider monkey\",\n            382: \"common squirrel monkey\",\n            383: \"ring-tailed lemur\",\n            384: \"indri\",\n            385: \"Asian elephant\",\n            386: \"African bush elephant\",\n            387: \"red panda\",\n            388: \"giant panda\",\n            389: \"snoek fish\",\n            390: \"eel\",\n            391: \"silver salmon\",\n            392: \"rock beauty fish\",\n            393: \"clownfish\",\n            394: \"sturgeon\",\n            395: \"gar fish\",\n            396: \"lionfish\",\n            397: \"pufferfish\",\n            398: \"abacus\",\n            399: \"abaya\",\n            400: \"academic gown\",\n            401: \"accordion\",\n            402: \"acoustic guitar\",\n            403: \"aircraft carrier\",\n            404: \"airliner\",\n            405: \"airship\",\n            406: \"altar\",\n            407: \"ambulance\",\n            408: \"amphibious vehicle\",\n            409: \"analog clock\",\n            410: \"apiary\",\n            411: \"apron\",\n            412: \"trash can\",\n            413: \"assault rifle\",\n            414: \"backpack\",\n            415: \"bakery\",\n            416: \"balance beam\",\n            417: \"balloon\",\n            418: \"ballpoint pen\",\n            419: \"Band-Aid\",\n            420: \"banjo\",\n            421: \"baluster / handrail\",\n            422: \"barbell\",\n            423: \"barber chair\",\n            424: \"barbershop\",\n            425: \"barn\",\n            426: \"barometer\",\n            427: \"barrel\",\n            428: \"wheelbarrow\",\n            429: \"baseball\",\n            430: \"basketball\",\n            431: \"bassinet\",\n            432: \"bassoon\",\n            433: \"swimming cap\",\n            434: \"bath towel\",\n            435: \"bathtub\",\n            436: \"station wagon\",\n            437: \"lighthouse\",\n            438: \"beaker\",\n            439: \"military hat (bearskin or shako)\",\n            440: \"beer bottle\",\n            441: \"beer glass\",\n            442: \"bell tower\",\n            443: \"baby bib\",\n            444: \"tandem bicycle\",\n            445: \"bikini\",\n            446: \"ring binder\",\n            447: \"binoculars\",\n            448: \"birdhouse\",\n            449: \"boathouse\",\n            450: \"bobsleigh\",\n            451: \"bolo tie\",\n            452: \"poke bonnet\",\n            453: \"bookcase\",\n            454: \"bookstore\",\n            455: \"bottle cap\",\n            456: \"hunting bow\",\n            457: \"bow tie\",\n            458: \"brass memorial plaque\",\n            459: \"bra\",\n            460: \"breakwater\",\n            461: \"breastplate\",\n            462: \"broom\",\n            463: \"bucket\",\n            464: \"buckle\",\n            465: \"bulletproof vest\",\n            466: \"high-speed train\",\n            467: \"butcher shop\",\n            468: \"taxicab\",\n            469: \"cauldron\",\n            470: \"candle\",\n            471: \"cannon\",\n            472: \"canoe\",\n            473: \"can opener\",\n            474: \"cardigan\",\n            475: \"car mirror\",\n            476: \"carousel\",\n            477: \"tool kit\",\n            478: \"cardboard box / carton\",\n            479: \"car wheel\",\n            480: \"automated teller machine\",\n            481: \"cassette\",\n            482: \"cassette player\",\n            483: \"castle\",\n            484: \"catamaran\",\n            485: \"CD player\",\n            486: \"cello\",\n            487: \"mobile phone\",\n            488: \"chain\",\n            489: \"chain-link fence\",\n            490: \"chain mail\",\n            491: \"chainsaw\",\n            492: \"storage chest\",\n            493: \"chiffonier\",\n            494: \"bell or wind chime\",\n            495: \"china cabinet\",\n            496: \"Christmas stocking\",\n            497: \"church\",\n            498: \"movie theater\",\n            499: \"cleaver\",\n            500: \"cliff dwelling\",\n            501: \"cloak\",\n            502: \"clogs\",\n            503: \"cocktail shaker\",\n            504: \"coffee mug\",\n            505: \"coffeemaker\",\n            506: \"spiral or coil\",\n            507: \"combination lock\",\n            508: \"computer keyboard\",\n            509: \"candy store\",\n            510: \"container ship\",\n            511: \"convertible\",\n            512: \"corkscrew\",\n            513: \"cornet\",\n            514: \"cowboy boot\",\n            515: \"cowboy hat\",\n            516: \"cradle\",\n            517: \"construction crane\",\n            518: \"crash helmet\",\n            519: \"crate\",\n            520: \"infant bed\",\n            521: \"Crock Pot\",\n            522: \"croquet ball\",\n            523: \"crutch\",\n            524: \"cuirass\",\n            525: \"dam\",\n            526: \"desk\",\n            527: \"desktop computer\",\n            528: \"rotary dial telephone\",\n            529: \"diaper\",\n            530: \"digital clock\",\n            531: \"digital watch\",\n            532: \"dining table\",\n            533: \"dishcloth\",\n            534: \"dishwasher\",\n            535: \"disc brake\",\n            536: \"dock\",\n            537: \"dog sled\",\n            538: \"dome\",\n            539: \"doormat\",\n            540: \"drilling rig\",\n            541: \"drum\",\n            542: \"drumstick\",\n            543: \"dumbbell\",\n            544: \"Dutch oven\",\n            545: \"electric fan\",\n            546: \"electric guitar\",\n            547: \"electric locomotive\",\n            548: \"entertainment center\",\n            549: \"envelope\",\n            550: \"espresso machine\",\n            551: \"face powder\",\n            552: \"feather boa\",\n            553: \"filing cabinet\",\n            554: \"fireboat\",\n            555: \"fire truck\",\n            556: \"fire screen\",\n            557: \"flagpole\",\n            558: \"flute\",\n            559: \"folding chair\",\n            560: \"football helmet\",\n            561: \"forklift\",\n            562: \"fountain\",\n            563: \"fountain pen\",\n            564: \"four-poster bed\",\n            565: \"freight car\",\n            566: \"French horn\",\n            567: \"frying pan\",\n            568: \"fur coat\",\n            569: \"garbage truck\",\n            570: \"gas mask or respirator\",\n            571: \"gas pump\",\n            572: \"goblet\",\n            573: \"go-kart\",\n            574: \"golf ball\",\n            575: \"golf cart\",\n            576: \"gondola\",\n            577: \"gong\",\n            578: \"gown\",\n            579: \"grand piano\",\n            580: \"greenhouse\",\n            581: \"radiator grille\",\n            582: \"grocery store\",\n            583: \"guillotine\",\n            584: \"hair clip\",\n            585: \"hair spray\",\n            586: \"half-track\",\n            587: \"hammer\",\n            588: \"hamper\",\n            589: \"hair dryer\",\n            590: \"hand-held computer\",\n            591: \"handkerchief\",\n            592: \"hard disk drive\",\n            593: \"harmonica\",\n            594: \"harp\",\n            595: \"combine harvester\",\n            596: \"hatchet\",\n            597: \"holster\",\n            598: \"home theater\",\n            599: \"honeycomb\",\n            600: \"hook\",\n            601: \"hoop skirt\",\n            602: \"gymnastic horizontal bar\",\n            603: \"horse-drawn vehicle\",\n            604: \"hourglass\",\n            605: \"iPod\",\n            606: \"clothes iron\",\n            607: \"carved pumpkin\",\n            608: \"jeans\",\n            609: \"jeep\",\n            610: \"T-shirt\",\n            611: \"jigsaw puzzle\",\n            612: \"rickshaw\",\n            613: \"joystick\",\n            614: \"kimono\",\n            615: \"knee pad\",\n            616: \"knot\",\n            617: \"lab coat\",\n            618: \"ladle\",\n            619: \"lampshade\",\n            620: \"laptop computer\",\n            621: \"lawn mower\",\n            622: \"lens cap\",\n            623: \"letter opener\",\n            624: \"library\",\n            625: \"lifeboat\",\n            626: \"lighter\",\n            627: \"limousine\",\n            628: \"ocean liner\",\n            629: \"lipstick\",\n            630: \"slip-on shoe\",\n            631: \"lotion\",\n            632: \"music speaker\",\n            633: \"loupe magnifying glass\",\n            634: \"sawmill\",\n            635: \"magnetic compass\",\n            636: \"messenger bag\",\n            637: \"mailbox\",\n            638: \"tights\",\n            639: \"one-piece bathing suit\",\n            640: \"manhole cover\",\n            641: \"maraca\",\n            642: \"marimba\",\n            643: \"mask\",\n            644: \"matchstick\",\n            645: \"maypole\",\n            646: \"maze\",\n            647: \"measuring cup\",\n            648: \"medicine cabinet\",\n            649: \"megalith\",\n            650: \"microphone\",\n            651: \"microwave oven\",\n            652: \"military uniform\",\n            653: \"milk can\",\n            654: \"minibus\",\n            655: \"miniskirt\",\n            656: \"minivan\",\n            657: \"missile\",\n            658: \"mitten\",\n            659: \"mixing bowl\",\n            660: \"mobile home\",\n            661: \"ford model t\",\n            662: \"modem\",\n            663: \"monastery\",\n            664: \"monitor\",\n            665: \"moped\",\n            666: \"mortar and pestle\",\n            667: \"graduation cap\",\n            668: \"mosque\",\n            669: \"mosquito net\",\n            670: \"vespa\",\n            671: \"mountain bike\",\n            672: \"tent\",\n            673: \"computer mouse\",\n            674: \"mousetrap\",\n            675: \"moving van\",\n            676: \"muzzle\",\n            677: \"metal nail\",\n            678: \"neck brace\",\n            679: \"necklace\",\n            680: \"baby pacifier\",\n            681: \"notebook computer\",\n            682: \"obelisk\",\n            683: \"oboe\",\n            684: \"ocarina\",\n            685: \"odometer\",\n            686: \"oil filter\",\n            687: \"pipe organ\",\n            688: \"oscilloscope\",\n            689: \"overskirt\",\n            690: \"bullock cart\",\n            691: \"oxygen mask\",\n            692: \"product packet / packaging\",\n            693: \"paddle\",\n            694: \"paddle wheel\",\n            695: \"padlock\",\n            696: \"paintbrush\",\n            697: \"pajamas\",\n            698: \"palace\",\n            699: \"pan flute\",\n            700: \"paper towel\",\n            701: \"parachute\",\n            702: \"parallel bars\",\n            703: \"park bench\",\n            704: \"parking meter\",\n            705: \"railroad car\",\n            706: \"patio\",\n            707: \"payphone\",\n            708: \"pedestal\",\n            709: \"pencil case\",\n            710: \"pencil sharpener\",\n            711: \"perfume\",\n            712: \"Petri dish\",\n            713: \"photocopier\",\n            714: \"plectrum\",\n            715: \"Pickelhaube\",\n            716: \"picket fence\",\n            717: \"pickup truck\",\n            718: \"pier\",\n            719: \"piggy bank\",\n            720: \"pill bottle\",\n            721: \"pillow\",\n            722: \"ping-pong ball\",\n            723: \"pinwheel\",\n            724: \"pirate ship\",\n            725: \"drink pitcher\",\n            726: \"block plane\",\n            727: \"planetarium\",\n            728: \"plastic bag\",\n            729: \"plate rack\",\n            730: \"farm plow\",\n            731: \"plunger\",\n            732: \"Polaroid camera\",\n            733: \"pole\",\n            734: \"police van\",\n            735: \"poncho\",\n            736: \"pool table\",\n            737: \"soda bottle\",\n            738: \"plant pot\",\n            739: \"potter's wheel\",\n            740: \"power drill\",\n            741: \"prayer rug\",\n            742: \"printer\",\n            743: \"prison\",\n            744: \"missile\",\n            745: \"projector\",\n            746: \"hockey puck\",\n            747: \"punching bag\",\n            748: \"purse\",\n            749: \"quill\",\n            750: \"quilt\",\n            751: \"race car\",\n            752: \"racket\",\n            753: \"radiator\",\n            754: \"radio\",\n            755: \"radio telescope\",\n            756: \"rain barrel\",\n            757: \"recreational vehicle\",\n            758: \"fishing casting reel\",\n            759: \"reflex camera\",\n            760: \"refrigerator\",\n            761: \"remote control\",\n            762: \"restaurant\",\n            763: \"revolver\",\n            764: \"rifle\",\n            765: \"rocking chair\",\n            766: \"rotisserie\",\n            767: \"eraser\",\n            768: \"rugby ball\",\n            769: \"ruler measuring stick\",\n            770: \"sneaker\",\n            771: \"safe\",\n            772: \"safety pin\",\n            773: \"salt shaker\",\n            774: \"sandal\",\n            775: \"sarong\",\n            776: \"saxophone\",\n            777: \"scabbard\",\n            778: \"weighing scale\",\n            779: \"school bus\",\n            780: \"schooner\",\n            781: \"scoreboard\",\n            782: \"CRT monitor\",\n            783: \"screw\",\n            784: \"screwdriver\",\n            785: \"seat belt\",\n            786: \"sewing machine\",\n            787: \"shield\",\n            788: \"shoe store\",\n            789: \"shoji screen / room divider\",\n            790: \"shopping basket\",\n            791: \"shopping cart\",\n            792: \"shovel\",\n            793: \"shower cap\",\n            794: \"shower curtain\",\n            795: \"ski\",\n            796: \"balaclava ski mask\",\n            797: \"sleeping bag\",\n            798: \"slide rule\",\n            799: \"sliding door\",\n            800: \"slot machine\",\n            801: \"snorkel\",\n            802: \"snowmobile\",\n            803: \"snowplow\",\n            804: \"soap dispenser\",\n            805: \"soccer ball\",\n            806: \"sock\",\n            807: \"solar thermal collector\",\n            808: \"sombrero\",\n            809: \"soup bowl\",\n            810: \"keyboard space bar\",\n            811: \"space heater\",\n            812: \"space shuttle\",\n            813: \"spatula\",\n            814: \"motorboat\",\n            815: \"spider web\",\n            816: \"spindle\",\n            817: \"sports car\",\n            818: \"spotlight\",\n            819: \"stage\",\n            820: \"steam locomotive\",\n            821: \"through arch bridge\",\n            822: \"steel drum\",\n            823: \"stethoscope\",\n            824: \"scarf\",\n            825: \"stone wall\",\n            826: \"stopwatch\",\n            827: \"stove\",\n            828: \"strainer\",\n            829: \"tram\",\n            830: \"stretcher\",\n            831: \"couch\",\n            832: \"stupa\",\n            833: \"submarine\",\n            834: \"suit\",\n            835: \"sundial\",\n            836: \"sunglasses\",\n            837: \"sunglasses\",\n            838: \"sunscreen\",\n            839: \"suspension bridge\",\n            840: \"mop\",\n            841: \"sweatshirt\",\n            842: \"swim trunks / shorts\",\n            843: \"swing\",\n            844: \"electrical switch\",\n            845: \"syringe\",\n            846: \"table lamp\",\n            847: \"tank\",\n            848: \"tape player\",\n            849: \"teapot\",\n            850: \"teddy bear\",\n            851: \"television\",\n            852: \"tennis ball\",\n            853: \"thatched roof\",\n            854: \"front curtain\",\n            855: \"thimble\",\n            856: \"threshing machine\",\n            857: \"throne\",\n            858: \"tile roof\",\n            859: \"toaster\",\n            860: \"tobacco shop\",\n            861: \"toilet seat\",\n            862: \"torch\",\n            863: \"totem pole\",\n            864: \"tow truck\",\n            865: \"toy store\",\n            866: \"tractor\",\n            867: \"semi-trailer truck\",\n            868: \"tray\",\n            869: \"trench coat\",\n            870: \"tricycle\",\n            871: \"trimaran\",\n            872: \"tripod\",\n            873: \"triumphal arch\",\n            874: \"trolleybus\",\n            875: \"trombone\",\n            876: \"hot tub\",\n            877: \"turnstile\",\n            878: \"typewriter keyboard\",\n            879: \"umbrella\",\n            880: \"unicycle\",\n            881: \"upright piano\",\n            882: \"vacuum cleaner\",\n            883: \"vase\",\n            884: \"vaulted or arched ceiling\",\n            885: \"velvet fabric\",\n            886: \"vending machine\",\n            887: \"vestment\",\n            888: \"viaduct\",\n            889: \"violin\",\n            890: \"volleyball\",\n            891: \"waffle iron\",\n            892: \"wall clock\",\n            893: \"wallet\",\n            894: \"wardrobe\",\n            895: \"military aircraft\",\n            896: \"sink\",\n            897: \"washing machine\",\n            898: \"water bottle\",\n            899: \"water jug\",\n            900: \"water tower\",\n            901: \"whiskey jug\",\n            902: \"whistle\",\n            903: \"hair wig\",\n            904: \"window screen\",\n            905: \"window shade\",\n            906: \"Windsor tie\",\n            907: \"wine bottle\",\n            908: \"airplane wing\",\n            909: \"wok\",\n            910: \"wooden spoon\",\n            911: \"wool\",\n            912: \"split-rail fence\",\n            913: \"shipwreck\",\n            914: \"sailboat\",\n            915: \"yurt\",\n            916: \"website\",\n            917: \"comic book\",\n            918: \"crossword\",\n            919: \"traffic or street sign\",\n            920: \"traffic light\",\n            921: \"dust jacket\",\n            922: \"menu\",\n            923: \"plate\",\n            924: \"guacamole\",\n            925: \"consomme\",\n            926: \"hot pot\",\n            927: \"trifle\",\n            928: \"ice cream\",\n            929: \"popsicle\",\n            930: \"baguette\",\n            931: \"bagel\",\n            932: \"pretzel\",\n            933: \"cheeseburger\",\n            934: \"hot dog\",\n            935: \"mashed potatoes\",\n            936: \"cabbage\",\n            937: \"broccoli\",\n            938: \"cauliflower\",\n            939: \"zucchini\",\n            940: \"spaghetti squash\",\n            941: \"acorn squash\",\n            942: \"butternut squash\",\n            943: \"cucumber\",\n            944: \"artichoke\",\n            945: \"bell pepper\",\n            946: \"cardoon\",\n            947: \"mushroom\",\n            948: \"Granny Smith apple\",\n            949: \"strawberry\",\n            950: \"orange\",\n            951: \"lemon\",\n            952: \"fig\",\n            953: \"pineapple\",\n            954: \"banana\",\n            955: \"jackfruit\",\n            956: \"cherimoya (custard apple)\",\n            957: \"pomegranate\",\n            958: \"hay\",\n            959: \"carbonara\",\n            960: \"chocolate syrup\",\n            961: \"dough\",\n            962: \"meatloaf\",\n            963: \"pizza\",\n            964: \"pot pie\",\n            965: \"burrito\",\n            966: \"red wine\",\n            967: \"espresso\",\n            968: \"tea cup\",\n            969: \"eggnog\",\n            970: \"mountain\",\n            971: \"bubble\",\n            972: \"cliff\",\n            973: \"coral reef\",\n            974: \"geyser\",\n            975: \"lakeshore\",\n            976: \"promontory\",\n            977: \"sandbar\",\n            978: \"beach\",\n            979: \"valley\",\n            980: \"volcano\",\n            981: \"baseball player\",\n            982: \"bridegroom\",\n            983: \"scuba diver\",\n            984: \"rapeseed\",\n            985: \"daisy\",\n            986: \"yellow lady's slipper\",\n            987: \"corn\",\n            988: \"acorn\",\n            989: \"rose hip\",\n            990: \"horse chestnut seed\",\n            991: \"coral fungus\",\n            992: \"agaric\",\n            993: \"gyromitra\",\n            994: \"stinkhorn mushroom\",\n            995: \"earth star fungus\",\n            996: \"hen of the woods mushroom\",\n            997: \"bolete\",\n            998: \"corn cob\",\n            999: \"toilet paper\",\n        }\n</code></pre>"},{"location":"api/#mmlearn.datasets.imagenet.ImageNet.zero_shot_prompt_templates","title":"zero_shot_prompt_templates  <code>property</code>","text":"<pre><code>zero_shot_prompt_templates\n</code></pre> <p>Return the zero-shot prompt templates.</p>"},{"location":"api/#mmlearn.datasets.imagenet.ImageNet.id2label","title":"id2label  <code>property</code>","text":"<pre><code>id2label\n</code></pre> <p>Return the label mapping.</p>"},{"location":"api/#mmlearn.datasets.imagenet.ImageNet.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(index)\n</code></pre> <p>Get an example at the given index.</p> Source code in <code>mmlearn/datasets/imagenet.py</code> <pre><code>def __getitem__(self, index: int) -&gt; Example:\n    \"\"\"Get an example at the given index.\"\"\"\n    image, target = super().__getitem__(index)\n    example = Example(\n        {\n            Modalities.RGB.name: image,\n            Modalities.RGB.target: target,\n            EXAMPLE_INDEX_KEY: index,\n        }\n    )\n    mask = self.mask_generator() if self.mask_generator else None\n    if mask is not None:  # error will be raised during collation if `None`\n        example[Modalities.RGB.mask] = mask\n    return example\n</code></pre>"},{"location":"api/#mmlearn.datasets.librispeech","title":"librispeech","text":"<p>LibriSpeech dataset.</p>"},{"location":"api/#mmlearn.datasets.librispeech.LibriSpeech","title":"LibriSpeech","text":"<p>               Bases: <code>Dataset[Example]</code></p> <p>LibriSpeech dataset.</p> <p>This is a wrapper around class:<code>torchaudio.datasets.LIBRISPEECH</code> that assumes that the dataset is already downloaded and the top-level directory of the dataset in the root directory is <code>librispeech</code>.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>Root directory of dataset.</p> required <code>split</code> <code>(train - clean - 100, train - clean - 360, train - other - 500, dev - clean, dev - other, test - clean, test - other)</code> <p>Split of the dataset to use.</p> <code>\"train-clean-100\"</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If <code>torchaudio</code> is not installed.</p> Notes <p>This dataset only returns the audio and transcript from the dataset.</p> Source code in <code>mmlearn/datasets/librispeech.py</code> <pre><code>@store(\n    group=\"datasets\",\n    provider=\"mmlearn\",\n    root_dir=os.getenv(\"LIBRISPEECH_ROOT_DIR\", MISSING),\n)\nclass LibriSpeech(Dataset[Example]):\n    \"\"\"LibriSpeech dataset.\n\n    This is a wrapper around :py:class:`torchaudio.datasets.LIBRISPEECH` that assumes\n    that the dataset is already downloaded and the top-level directory of the dataset\n    in the root directory is `librispeech`.\n\n    Parameters\n    ----------\n    root_dir : str\n        Root directory of dataset.\n    split : {\"train-clean-100\", \"train-clean-360\", \"train-other-500\", \"dev-clean\", \"dev-other\", \"test-clean\", \"test-other\"}, default=\"train-clean-100\"\n        Split of the dataset to use.\n\n    Raises\n    ------\n    ImportError\n        If ``torchaudio`` is not installed.\n\n    Notes\n    -----\n    This dataset only returns the audio and transcript from the dataset.\n\n    \"\"\"  # noqa: W505\n\n    def __init__(self, root_dir: str, split: str = \"train-clean-100\") -&gt; None:\n        super().__init__()\n        if not _TORCHAUDIO_AVAILABLE:\n            raise ImportError(\n                \"LibriSpeech dataset requires `torchaudio`, which is not installed.\"\n            )\n        from torchaudio.datasets import LIBRISPEECH\n\n        self.dataset = LIBRISPEECH(\n            root=root_dir,\n            url=split,\n            download=False,\n            folder_in_archive=\"librispeech\",\n        )\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the length of the dataset.\"\"\"\n        return len(self.dataset)\n\n    def __getitem__(self, idx: int) -&gt; Example:\n        \"\"\"Return an example from the dataset.\"\"\"\n        waveform, sample_rate, transcript, _, _, _ = self.dataset[idx]\n        assert sample_rate == SAMPLE_RATE, (\n            f\"Expected sample rate to be `16000`, got {sample_rate}.\"\n        )\n        waveform = pad_or_trim(waveform.flatten())\n\n        return Example(\n            {\n                Modalities.AUDIO.name: waveform,\n                Modalities.TEXT.name: transcript,\n                EXAMPLE_INDEX_KEY: idx,\n            },\n        )\n</code></pre>"},{"location":"api/#mmlearn.datasets.librispeech.LibriSpeech.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the length of the dataset.</p> Source code in <code>mmlearn/datasets/librispeech.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the length of the dataset.\"\"\"\n    return len(self.dataset)\n</code></pre>"},{"location":"api/#mmlearn.datasets.librispeech.LibriSpeech.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> <p>Return an example from the dataset.</p> Source code in <code>mmlearn/datasets/librispeech.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; Example:\n    \"\"\"Return an example from the dataset.\"\"\"\n    waveform, sample_rate, transcript, _, _, _ = self.dataset[idx]\n    assert sample_rate == SAMPLE_RATE, (\n        f\"Expected sample rate to be `16000`, got {sample_rate}.\"\n    )\n    waveform = pad_or_trim(waveform.flatten())\n\n    return Example(\n        {\n            Modalities.AUDIO.name: waveform,\n            Modalities.TEXT.name: transcript,\n            EXAMPLE_INDEX_KEY: idx,\n        },\n    )\n</code></pre>"},{"location":"api/#mmlearn.datasets.librispeech.pad_or_trim","title":"pad_or_trim","text":"<pre><code>pad_or_trim(array, length=30 * SAMPLE_RATE, *, axis=-1)\n</code></pre> <p>Pad or trim the audio array to <code>length</code> along the given axis.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>Tensor</code> <p>Audio array.</p> required <code>length</code> <code>int</code> <p>Length to pad or trim to. Defaults to 30 seconds at 16 kHz.</p> <code>480000</code> <code>axis</code> <code>int</code> <p>Axis along which to pad or trim.</p> <code>-1</code> <p>Returns:</p> Name Type Description <code>array</code> <code>Tensor</code> <p>Padded or trimmed audio array.</p> References <p>.. [1] https://github.com/openai/whisper/blob/main/whisper/audio.py#L65C1-L88C17</p> Source code in <code>mmlearn/datasets/librispeech.py</code> <pre><code>def pad_or_trim(\n    array: torch.Tensor, length: int = 30 * SAMPLE_RATE, *, axis: int = -1\n) -&gt; torch.Tensor:\n    \"\"\"Pad or trim the audio array to `length` along the given axis.\n\n    Parameters\n    ----------\n    array : torch.Tensor\n        Audio array.\n    length : int, default=480000\n        Length to pad or trim to. Defaults to 30 seconds at 16 kHz.\n    axis : int, default=-1\n        Axis along which to pad or trim.\n\n    Returns\n    -------\n    array : torch.Tensor\n        Padded or trimmed audio array.\n\n    References\n    ----------\n    .. [1] https://github.com/openai/whisper/blob/main/whisper/audio.py#L65C1-L88C17\n\n    \"\"\"\n    if array.shape[axis] &gt; length:\n        array = array.index_select(\n            dim=axis,\n            index=torch.arange(length, device=array.device),\n        )\n\n    if array.shape[axis] &lt; length:\n        pad_widths = [(0, 0)] * array.ndim\n        pad_widths[axis] = (0, length - array.shape[axis])\n        array = F.pad(array, [pad for sizes in pad_widths[::-1] for pad in sizes])\n\n    return array\n</code></pre>"},{"location":"api/#mmlearn.datasets.llvip","title":"llvip","text":"<p>LLVIP dataset.</p>"},{"location":"api/#mmlearn.datasets.llvip.LLVIPDataset","title":"LLVIPDataset","text":"<p>               Bases: <code>Dataset[Example]</code></p> <p>Low-Light Visible-Infrared Pair (LLVIP) dataset.</p> <p>Loads pairs of <code>RGB</code> and <code>THERMAL</code> images from the LLVIP dataset.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>Path to the root directory of the dataset. The directory should contain 'visible' and 'infrared' subdirectories.</p> required <code>train</code> <code>bool</code> <p>Flag to indicate whether to load the training or test set.</p> <code>True</code> <code>transform</code> <code>Optional[Callable[[Image], Tensor]]</code> <p>A callable that takes in a PIL image and returns a transformed version of the image as a PyTorch tensor. This is applied to both RGB and thermal images.</p> <code>None</code> Source code in <code>mmlearn/datasets/llvip.py</code> <pre><code>@store(\n    name=\"LLVIP\",\n    group=\"datasets\",\n    provider=\"mmlearn\",\n    root_dir=os.getenv(\"LLVIP_ROOT_DIR\", MISSING),\n)\nclass LLVIPDataset(Dataset[Example]):\n    \"\"\"Low-Light Visible-Infrared Pair (LLVIP) dataset.\n\n    Loads pairs of `RGB` and `THERMAL` images from the LLVIP dataset.\n\n    Parameters\n    ----------\n    root_dir : str\n        Path to the root directory of the dataset. The directory should contain\n        'visible' and 'infrared' subdirectories.\n    train : bool, default=True\n        Flag to indicate whether to load the training or test set.\n    transform : Optional[Callable[[PIL.Image], torch.Tensor]], optional, default=None\n        A callable that takes in a PIL image and returns a transformed version\n        of the image as a PyTorch tensor. This is applied to both RGB and thermal\n        images.\n    \"\"\"\n\n    def __init__(\n        self,\n        root_dir: str,\n        train: bool = True,\n        transform: Optional[Callable[[PILImage], torch.Tensor]] = None,\n    ):\n        self.path_images_rgb = os.path.join(\n            root_dir,\n            \"visible\",\n            \"train\" if train else \"test\",\n        )\n        self.path_images_ir = os.path.join(\n            root_dir, \"infrared\", \"train\" if train else \"test\"\n        )\n        self.train = train\n        self.transform = transform or transforms.ToTensor()\n\n        self.rgb_images = sorted(glob.glob(os.path.join(self.path_images_rgb, \"*.jpg\")))\n        self.ir_images = sorted(glob.glob(os.path.join(self.path_images_ir, \"*.jpg\")))\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the length of the dataset.\"\"\"\n        return len(self.rgb_images)\n\n    def __getitem__(self, idx: int) -&gt; Example:\n        \"\"\"Return an example from the dataset.\"\"\"\n        rgb_image_path = self.rgb_images[idx]\n        ir_image_path = self.ir_images[idx]\n\n        rgb_image = PILImage.open(rgb_image_path).convert(\"RGB\")\n        ir_image = PILImage.open(ir_image_path).convert(\"L\")\n\n        example = Example(\n            {\n                Modalities.RGB.name: self.transform(rgb_image),\n                Modalities.THERMAL.name: self.transform(ir_image),\n                EXAMPLE_INDEX_KEY: idx,\n            },\n        )\n\n        if self.train:\n            annot_path = (\n                rgb_image_path.replace(\"visible\", \"Annotations\")\n                .replace(\".jpg\", \".xml\")\n                .replace(\"train\", \"\")\n            )\n            annot = self._get_bbox(annot_path)\n            example[\"annotation\"] = {\n                \"bboxes\": torch.from_numpy(annot[\"bboxes\"]),\n                \"labels\": torch.from_numpy(annot[\"labels\"]),\n            }\n        return example\n\n    def _get_bbox(self, filename: str) -&gt; dict[str, np.ndarray]:\n        \"\"\"Parse the XML file to get bounding boxes and labels.\n\n        Parameters\n        ----------\n        filename : str\n            Path to the annotation XML file.\n\n        Returns\n        -------\n        dict\n            A dictionary containing bounding boxes and labels.\n        \"\"\"\n        try:\n            root = ET.parse(filename).getroot()\n\n            bboxes, labels = [], []\n            for obj in root.findall(\"object\"):\n                bbox_obj = obj.find(\"bndbox\")\n                bbox = [\n                    int(bbox_obj.find(dim).text)  # type: ignore[union-attr,arg-type]\n                    for dim in [\"xmin\", \"ymin\", \"xmax\", \"ymax\"]\n                ]\n                bboxes.append(bbox)\n                labels.append(1)  # Assuming 'person' is the only label\n            return {\n                \"bboxes\": np.array(bboxes).astype(\"float\"),\n                \"labels\": np.array(labels).astype(\"int\"),\n            }\n        except ET.ParseError as e:\n            raise ValueError(f\"Error parsing XML: {e}\") from None\n        except Exception as e:\n            raise RuntimeError(\n                f\"Error processing annotation file {filename}: {e}\",\n            ) from None\n</code></pre>"},{"location":"api/#mmlearn.datasets.llvip.LLVIPDataset.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the length of the dataset.</p> Source code in <code>mmlearn/datasets/llvip.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the length of the dataset.\"\"\"\n    return len(self.rgb_images)\n</code></pre>"},{"location":"api/#mmlearn.datasets.llvip.LLVIPDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> <p>Return an example from the dataset.</p> Source code in <code>mmlearn/datasets/llvip.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; Example:\n    \"\"\"Return an example from the dataset.\"\"\"\n    rgb_image_path = self.rgb_images[idx]\n    ir_image_path = self.ir_images[idx]\n\n    rgb_image = PILImage.open(rgb_image_path).convert(\"RGB\")\n    ir_image = PILImage.open(ir_image_path).convert(\"L\")\n\n    example = Example(\n        {\n            Modalities.RGB.name: self.transform(rgb_image),\n            Modalities.THERMAL.name: self.transform(ir_image),\n            EXAMPLE_INDEX_KEY: idx,\n        },\n    )\n\n    if self.train:\n        annot_path = (\n            rgb_image_path.replace(\"visible\", \"Annotations\")\n            .replace(\".jpg\", \".xml\")\n            .replace(\"train\", \"\")\n        )\n        annot = self._get_bbox(annot_path)\n        example[\"annotation\"] = {\n            \"bboxes\": torch.from_numpy(annot[\"bboxes\"]),\n            \"labels\": torch.from_numpy(annot[\"labels\"]),\n        }\n    return example\n</code></pre>"},{"location":"api/#mmlearn.datasets.nihcxr","title":"nihcxr","text":"<p>NIH Chest X-ray Dataset.</p>"},{"location":"api/#mmlearn.datasets.nihcxr.NIHCXR","title":"NIHCXR","text":"<p>               Bases: <code>Dataset[Example]</code></p> <p>NIH Chest X-ray dataset.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>Directory which contains <code>.json</code> files stating all dataset entries.</p> required <code>split</code> <code>(train, test, bbox)</code> <p>Dataset split. \"bbox\" is a subset of \"test\" which contains bounding box info.</p> <code>\"train\"</code> <code>transform</code> <code>Optional[Callable[[Image], Tensor]]</code> <p>A callable that takes in a PIL image and returns a transformed version of the image as a PyTorch tensor.</p> <code>None</code> Source code in <code>mmlearn/datasets/nihcxr.py</code> <pre><code>@store(\n    group=\"datasets\",\n    provider=\"mmlearn\",\n    root_dir=os.getenv(\"NIH_CXR_DIR\", MISSING),\n    split=\"train\",\n)\nclass NIHCXR(Dataset[Example]):\n    \"\"\"NIH Chest X-ray dataset.\n\n    Parameters\n    ----------\n    root_dir : str\n        Directory which contains `.json` files stating all dataset entries.\n    split : {\"train\", \"test\", \"bbox\"}\n        Dataset split. \"bbox\" is a subset of \"test\" which contains bounding box info.\n    transform : Optional[Callable[[PIL.Image], torch.Tensor]], optional, default=None\n        A callable that takes in a PIL image and returns a transformed version\n        of the image as a PyTorch tensor.\n    \"\"\"\n\n    def __init__(\n        self,\n        root_dir: str,\n        split: Literal[\"train\", \"test\", \"bbox\"],\n        transform: Optional[Callable[[Image.Image], torch.Tensor]] = None,\n    ) -&gt; None:\n        assert split in [\"train\", \"test\", \"bbox\"], f\"split {split} is not available.\"\n        assert callable(transform) or transform is None, (\n            \"transform is not callable or None.\"\n        )\n\n        data_path = os.path.join(root_dir, split + \"_data.json\")\n\n        assert os.path.isfile(data_path), f\"entries file does not exist: {data_path}.\"\n\n        with open(data_path, \"rb\") as file:\n            entries = json.load(file)\n        self.entries = entries\n\n        if transform is not None:\n            self.transform = transform\n        else:\n            self.transform = Compose([Resize(224), CenterCrop(224), ToTensor()])\n\n        self.bbox = split == \"bbox\"\n\n    def __getitem__(self, idx: int) -&gt; Example:\n        \"\"\"Return image-label or image-label-tabular(bbox).\"\"\"\n        entry = self.entries[idx]\n        image = Image.open(entry[\"image_path\"]).convert(\"RGB\")\n        image = self.transform(image)\n        label = torch.tensor(entry[\"label\"])\n\n        example = Example(\n            {\n                Modalities.RGB.name: image,\n                Modalities.RGB.target: label,\n                \"qid\": entry[\"qid\"],\n                EXAMPLE_INDEX_KEY: idx,\n            }\n        )\n\n        if self.bbox:\n            example[\"bbox\"] = entry[\"bbox\"]\n\n        return example\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the length of the dataset.\"\"\"\n        return len(self.entries)\n</code></pre>"},{"location":"api/#mmlearn.datasets.nihcxr.NIHCXR.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> <p>Return image-label or image-label-tabular(bbox).</p> Source code in <code>mmlearn/datasets/nihcxr.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; Example:\n    \"\"\"Return image-label or image-label-tabular(bbox).\"\"\"\n    entry = self.entries[idx]\n    image = Image.open(entry[\"image_path\"]).convert(\"RGB\")\n    image = self.transform(image)\n    label = torch.tensor(entry[\"label\"])\n\n    example = Example(\n        {\n            Modalities.RGB.name: image,\n            Modalities.RGB.target: label,\n            \"qid\": entry[\"qid\"],\n            EXAMPLE_INDEX_KEY: idx,\n        }\n    )\n\n    if self.bbox:\n        example[\"bbox\"] = entry[\"bbox\"]\n\n    return example\n</code></pre>"},{"location":"api/#mmlearn.datasets.nihcxr.NIHCXR.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the length of the dataset.</p> Source code in <code>mmlearn/datasets/nihcxr.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the length of the dataset.\"\"\"\n    return len(self.entries)\n</code></pre>"},{"location":"api/#mmlearn.datasets.nyuv2","title":"nyuv2","text":"<p>SUN RGB-D dataset.</p>"},{"location":"api/#mmlearn.datasets.nyuv2.NYUv2Dataset","title":"NYUv2Dataset","text":"<p>               Bases: <code>Dataset[Example]</code></p> <p>NYUv2 dataset.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>Path to the root directory of the dataset.</p> required <code>split</code> <code>(train, test)</code> <p>Split of the dataset to use.</p> <code>\"train\"</code> <code>return_type</code> <code>(disparity, image)</code> <p>Return type of the depth images.</p> <ul> <li><code>\"disparity\"</code>: Return the depth image as disparity map.</li> <li><code>\"image\"</code>: Return the depth image as a 3-channel image.</li> </ul> <code>\"disparity\"</code> <code>rgb_transform</code> <code>Optional[Callable[[Image], Tensor]]</code> <p>A callable that takes in an RGB PIL image and returns a transformed version of the image as a PyTorch tensor.</p> <code>None</code> <code>depth_transform</code> <code>Optional[Callable[[Image], Tensor]]</code> <p>A callable that takes in a depth PIL image and returns a transformed version of the image as a PyTorch tensor.</p> <code>None</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If <code>opencv-python</code> is not installed.</p> Source code in <code>mmlearn/datasets/nyuv2.py</code> <pre><code>@store(\n    name=\"NYUv2\",\n    group=\"datasets\",\n    provider=\"mmlearn\",\n    root_dir=os.getenv(\"NYUV2_ROOT_DIR\", MISSING),\n)\nclass NYUv2Dataset(Dataset[Example]):\n    \"\"\"NYUv2 dataset.\n\n    Parameters\n    ----------\n    root_dir : str\n        Path to the root directory of the dataset.\n    split : {\"train\", \"test\"}, default=\"train\"\n        Split of the dataset to use.\n    return_type : {\"disparity\", \"image\"}, default=\"disparity\"\n        Return type of the depth images.\n\n        - `\"disparity\"`: Return the depth image as disparity map.\n        - `\"image\"`: Return the depth image as a 3-channel image.\n    rgb_transform: Callable[[PIL.Image], torch.Tensor], default=None\n        A callable that takes in an RGB PIL image and returns a transformed version\n        of the image as a PyTorch tensor.\n    depth_transform: Callable[[PIL.Image], torch.Tensor], default=None\n        A callable that takes in a depth PIL image and returns a transformed version\n        of the image as a PyTorch tensor.\n\n    Raises\n    ------\n    ImportError\n        If `opencv-python` is not installed.\n    \"\"\"\n\n    def __init__(\n        self,\n        root_dir: str,\n        split: Literal[\"train\", \"test\"] = \"train\",\n        return_type: Literal[\"disparity\", \"image\"] = \"disparity\",\n        rgb_transform: Optional[Callable[[PILImage], torch.Tensor]] = None,\n        depth_transform: Optional[Callable[[PILImage], torch.Tensor]] = None,\n    ) -&gt; None:\n        super().__init__()\n        if not _OPENCV_AVAILABLE:\n            raise ImportError(\n                \"NYUv2 dataset requires `opencv-python` which is not installed.\",\n            )\n        self._validate_args(root_dir, split, rgb_transform, depth_transform)\n        self.return_type = return_type\n\n        self.root_dir = root_dir\n        with open(os.path.join(root_dir, f\"{split}.txt\"), \"r\") as f:\n            file_ids = f.readlines()\n        file_ids = [f.strip() for f in file_ids]\n\n        root_dir = os.path.join(root_dir, split)\n        depth_files = [os.path.join(root_dir, \"depth\", f\"{f}.png\") for f in file_ids]\n        rgb_files = [os.path.join(root_dir, \"rgb\", f\"{f}.png\") for f in file_ids]\n\n        label_files = [\n            os.path.join(root_dir, \"scene_class\", f\"{f}.txt\") for f in file_ids\n        ]\n        labels = [str(open(f).read().strip()) for f in label_files]  # noqa: SIM115\n        labels = [label.replace(\"_\", \" \") for label in labels]\n        labels = [\n            _LABELS.index(label) if label in _LABELS else len(_LABELS)  # type: ignore\n            for label in labels\n        ]\n\n        # remove the samples with classes not in _LABELS\n        # this is to follow the same classes used in ImageBind\n        if split == \"test\":\n            valid_indices = [\n                i\n                for i, label in enumerate(labels)\n                if label &lt; len(_LABELS)  # type: ignore\n            ]\n            rgb_files = [rgb_files[i] for i in valid_indices]\n            depth_files = [depth_files[i] for i in valid_indices]\n            labels = [labels[i] for i in valid_indices]\n\n        self.samples = list(zip(rgb_files, depth_files, labels, strict=False))\n\n        self.rgb_transform = rgb_transform\n        self.depth_transform = depth_transform\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the length of the dataset.\"\"\"\n        return len(self.samples)\n\n    def _validate_args(\n        self,\n        root_dir: str,\n        split: str,\n        rgb_transform: Optional[Callable[[PILImage], torch.Tensor]],\n        depth_transform: Optional[Callable[[PILImage], torch.Tensor]],\n    ) -&gt; None:\n        \"\"\"Validate arguments.\"\"\"\n        if not os.path.isdir(root_dir):\n            raise NotADirectoryError(\n                f\"The given `root_dir` {root_dir} is not a directory\",\n            )\n        if split not in [\"train\", \"test\"]:\n            raise ValueError(\n                f\"Expected `split` to be one of `'train'` or `'test'`, but got {split}\",\n            )\n        if rgb_transform is not None and not callable(rgb_transform):\n            raise TypeError(\n                f\"Expected argument `rgb_transform` to be callable, but got {type(rgb_transform)}\",\n            )\n        if depth_transform is not None and not callable(depth_transform):\n            raise TypeError(\n                f\"Expected `depth_transform` to be callable, but got {type(depth_transform)}\",\n            )\n\n    def __getitem__(self, idx: int) -&gt; Example:\n        \"\"\"Return RGB and depth images at index `idx`.\"\"\"\n        # Read images\n        rgb_image = cv2.imread(self.samples[idx][0], cv2.IMREAD_UNCHANGED)\n        if self.rgb_transform is not None:\n            rgb_image = self.rgb_transform(to_pil_image(rgb_image))\n\n        if self.return_type == \"disparity\":\n            depth_image = depth_normalize(\n                self.samples[idx][1],\n            )\n        else:\n            # Using cv2 instead of PIL Image since we use PNG grayscale images.\n            depth_image = cv2.imread(\n                self.samples[idx][1],\n                cv2.IMREAD_GRAYSCALE,\n            )\n            # Make a 3-channel depth image to enable passing to a pretrained ViT.\n            depth_image = np.repeat(depth_image[:, :, np.newaxis], 3, axis=-1)\n\n        if self.depth_transform is not None:\n            depth_image = self.depth_transform(to_pil_image(depth_image))\n\n        return Example(\n            {\n                Modalities.RGB.name: rgb_image,\n                Modalities.DEPTH.name: depth_image,\n                EXAMPLE_INDEX_KEY: idx,\n                Modalities.DEPTH.target: self.samples[idx][2],\n            }\n        )\n</code></pre>"},{"location":"api/#mmlearn.datasets.nyuv2.NYUv2Dataset.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the length of the dataset.</p> Source code in <code>mmlearn/datasets/nyuv2.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the length of the dataset.\"\"\"\n    return len(self.samples)\n</code></pre>"},{"location":"api/#mmlearn.datasets.nyuv2.NYUv2Dataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> <p>Return RGB and depth images at index <code>idx</code>.</p> Source code in <code>mmlearn/datasets/nyuv2.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; Example:\n    \"\"\"Return RGB and depth images at index `idx`.\"\"\"\n    # Read images\n    rgb_image = cv2.imread(self.samples[idx][0], cv2.IMREAD_UNCHANGED)\n    if self.rgb_transform is not None:\n        rgb_image = self.rgb_transform(to_pil_image(rgb_image))\n\n    if self.return_type == \"disparity\":\n        depth_image = depth_normalize(\n            self.samples[idx][1],\n        )\n    else:\n        # Using cv2 instead of PIL Image since we use PNG grayscale images.\n        depth_image = cv2.imread(\n            self.samples[idx][1],\n            cv2.IMREAD_GRAYSCALE,\n        )\n        # Make a 3-channel depth image to enable passing to a pretrained ViT.\n        depth_image = np.repeat(depth_image[:, :, np.newaxis], 3, axis=-1)\n\n    if self.depth_transform is not None:\n        depth_image = self.depth_transform(to_pil_image(depth_image))\n\n    return Example(\n        {\n            Modalities.RGB.name: rgb_image,\n            Modalities.DEPTH.name: depth_image,\n            EXAMPLE_INDEX_KEY: idx,\n            Modalities.DEPTH.target: self.samples[idx][2],\n        }\n    )\n</code></pre>"},{"location":"api/#mmlearn.datasets.nyuv2.depth_normalize","title":"depth_normalize","text":"<pre><code>depth_normalize(depth_file, min_depth=0.01, max_depth=50)\n</code></pre> <p>Load depth file and convert to disparity image.</p> <p>Parameters:</p> Name Type Description Default <code>depth_file</code> <code>str</code> <p>Path to the depth file.</p> required <code>min_depth</code> <code>float</code> <p>Minimum depth value to clip the depth image.</p> <code>0.01</code> <code>max_depth</code> <code>int</code> <p>Maximum depth value to clip the depth image.</p> <code>50</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The normalized depth image.</p> Source code in <code>mmlearn/datasets/nyuv2.py</code> <pre><code>def depth_normalize(\n    depth_file: str, min_depth: float = 0.01, max_depth: int = 50\n) -&gt; torch.Tensor:\n    \"\"\"Load depth file and convert to disparity image.\n\n    Parameters\n    ----------\n    depth_file : str\n        Path to the depth file.\n    min_depth : float, default=0.01\n        Minimum depth value to clip the depth image.\n    max_depth : int, default=50\n        Maximum depth value to clip the depth image.\n\n    Returns\n    -------\n    torch.Tensor\n        The normalized depth image.\n    \"\"\"\n    depth_image = np.array(PILImage.open(depth_file))\n    depth = np.array(depth_image).astype(np.float32)\n    depth_in_meters = depth / 1000.0\n\n    if min_depth is not None:\n        depth_in_meters = depth_in_meters.clip(min=min_depth, max=max_depth)\n\n    return torch.from_numpy(depth_in_meters).float()\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors","title":"processors","text":"<p>Data processors.</p>"},{"location":"api/#mmlearn.datasets.processors.BlockwiseImagePatchMaskGenerator","title":"BlockwiseImagePatchMaskGenerator","text":"<p>Blockwise image patch mask generator.</p> <p>This is primarily intended for the data2vec method.</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>Union[int, tuple[int, int]]</code> <p>The size of the input image. If an integer is provided, the image is assumed to be square.</p> required <code>num_masking_patches</code> <code>int</code> <p>The number of patches to mask.</p> required <code>min_num_patches</code> <code>int</code> <p>The minimum number of patches to mask.</p> <code>4</code> <code>max_num_patches</code> <code>int</code> <p>The maximum number of patches to mask.</p> <code>None</code> <code>min_aspect_ratio</code> <code>float</code> <p>The minimum aspect ratio of the patch.</p> <code>0.3</code> <code>max_aspect_ratio</code> <code>float</code> <p>The maximum aspect ratio of the patch.</p> <code>None</code> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>@store(group=\"datasets/masking\", provider=\"mmlearn\")\nclass BlockwiseImagePatchMaskGenerator:\n    \"\"\"Blockwise image patch mask generator.\n\n    This is primarily intended for the data2vec method.\n\n    Parameters\n    ----------\n    input_size : Union[int, tuple[int, int]]\n        The size of the input image. If an integer is provided, the image is assumed\n        to be square.\n    num_masking_patches : int\n        The number of patches to mask.\n    min_num_patches : int, default=4\n        The minimum number of patches to mask.\n    max_num_patches : int, default=None\n        The maximum number of patches to mask.\n    min_aspect_ratio : float, default=0.3\n        The minimum aspect ratio of the patch.\n    max_aspect_ratio : float, default=None\n        The maximum aspect ratio of the patch.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_size: Union[int, tuple[int, int]],\n        num_masking_patches: int,\n        min_num_patches: int = 4,\n        max_num_patches: Any = None,\n        min_aspect_ratio: float = 0.3,\n        max_aspect_ratio: Any = None,\n    ):\n        if not isinstance(input_size, tuple):\n            input_size = (input_size,) * 2\n        self.height, self.width = input_size\n\n        self.num_masking_patches = num_masking_patches\n\n        self.min_num_patches = min_num_patches\n        self.max_num_patches = (\n            num_masking_patches if max_num_patches is None else max_num_patches\n        )\n\n        max_aspect_ratio = max_aspect_ratio or 1 / min_aspect_ratio\n        self.log_aspect_ratio = (math.log(min_aspect_ratio), math.log(max_aspect_ratio))\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Generate a printable representation.\n\n        Returns\n        -------\n        str\n            A printable representation of the object.\n\n        \"\"\"\n        return \"Generator(%d, %d -&gt; [%d ~ %d], max = %d, %.3f ~ %.3f)\" % (\n            self.height,\n            self.width,\n            self.min_num_patches,\n            self.max_num_patches,\n            self.num_masking_patches,\n            self.log_aspect_ratio[0],\n            self.log_aspect_ratio[1],\n        )\n\n    def get_shape(self) -&gt; tuple[int, int]:\n        \"\"\"Get the shape of the input.\n\n        Returns\n        -------\n        tuple[int, int]\n            The shape of the input as a tuple `(height, width)`.\n        \"\"\"\n        return self.height, self.width\n\n    def _mask(self, mask: torch.Tensor, max_mask_patches: int) -&gt; int:\n        \"\"\"Masking function.\n\n        This function mask adjacent patches by first selecting a target area and aspect\n        ratio. Since, there might be overlap between selected areas  or the selected\n        area might already be masked, it runs for a  maximum of 10 attempts or until the\n        specified number of patches (max_mask_patches) is achieved.\n\n\n        Parameters\n        ----------\n        mask: torch.Tensor\n            Current mask. The mask to be updated.\n        max_mask_patches: int\n            The maximum number of patches to be masked.\n\n        Returns\n        -------\n        delta: int\n            The number of patches that were successfully masked.\n\n        Notes\n        -----\n        - `target_area`: Randomly chosen target area for the patch.\n        - `aspect_ratio`: Randomly chosen aspect ratio for the patch.\n        - `h`: Height of the patch based on the target area and aspect ratio.\n        - `w`: Width of the patch based on the target area and aspect ratio.\n        - `top`: Randomly chosen top position for the patch.\n        - `left`: Randomly chosen left position for the patch.\n        - `num_masked`: Number of masked pixels within the proposed patch area.\n        - `delta`: Accumulated count of modified pixels.\n        \"\"\"\n        delta = 0\n        for _ in range(10):\n            target_area = random.uniform(self.min_num_patches, max_mask_patches)\n            aspect_ratio = math.exp(random.uniform(*self.log_aspect_ratio))\n            h = int(round(math.sqrt(target_area * aspect_ratio)))\n            w = int(round(math.sqrt(target_area / aspect_ratio)))\n            if w &lt; self.width and h &lt; self.height:\n                top = random.randint(0, self.height - h)\n                left = random.randint(0, self.width - w)\n\n                num_masked = mask[top : top + h, left : left + w].sum()\n                # Overlap\n                if 0 &lt; h * w - num_masked &lt;= max_mask_patches:\n                    for i in range(top, top + h):\n                        for j in range(left, left + w):\n                            if mask[i, j] == 0:\n                                mask[i, j] = 1\n                                delta += 1\n\n                if delta &gt; 0:\n                    break\n        return delta\n\n    def __call__(self) -&gt; torch.Tensor:\n        \"\"\"Generate a random mask.\n\n        Returns a random mask of shape (nb_patches, nb_patches) based on the\n        configuration where the number of patches to be masked is num_masking_patches.\n\n        Returns\n        -------\n        mask: torch.Tensor\n            A mask of shape (nb_patches, nb_patches)\n\n        \"\"\"\n        mask = torch.zeros(self.get_shape(), dtype=torch.int)\n        mask_count = 0\n        while mask_count &lt; self.num_masking_patches:\n            max_mask_patches = self.num_masking_patches - mask_count\n            max_mask_patches = min(max_mask_patches, self.max_num_patches)\n\n            delta = self._mask(mask, max_mask_patches)\n            if delta == 0:\n                break\n            mask_count += delta\n\n        return mask\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.BlockwiseImagePatchMaskGenerator.__repr__","title":"__repr__","text":"<pre><code>__repr__()\n</code></pre> <p>Generate a printable representation.</p> <p>Returns:</p> Type Description <code>str</code> <p>A printable representation of the object.</p> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Generate a printable representation.\n\n    Returns\n    -------\n    str\n        A printable representation of the object.\n\n    \"\"\"\n    return \"Generator(%d, %d -&gt; [%d ~ %d], max = %d, %.3f ~ %.3f)\" % (\n        self.height,\n        self.width,\n        self.min_num_patches,\n        self.max_num_patches,\n        self.num_masking_patches,\n        self.log_aspect_ratio[0],\n        self.log_aspect_ratio[1],\n    )\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.BlockwiseImagePatchMaskGenerator.get_shape","title":"get_shape","text":"<pre><code>get_shape()\n</code></pre> <p>Get the shape of the input.</p> <p>Returns:</p> Type Description <code>tuple[int, int]</code> <p>The shape of the input as a tuple <code>(height, width)</code>.</p> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>def get_shape(self) -&gt; tuple[int, int]:\n    \"\"\"Get the shape of the input.\n\n    Returns\n    -------\n    tuple[int, int]\n        The shape of the input as a tuple `(height, width)`.\n    \"\"\"\n    return self.height, self.width\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.BlockwiseImagePatchMaskGenerator.__call__","title":"__call__","text":"<pre><code>__call__()\n</code></pre> <p>Generate a random mask.</p> <p>Returns a random mask of shape (nb_patches, nb_patches) based on the configuration where the number of patches to be masked is num_masking_patches.</p> <p>Returns:</p> Name Type Description <code>mask</code> <code>Tensor</code> <p>A mask of shape (nb_patches, nb_patches)</p> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>def __call__(self) -&gt; torch.Tensor:\n    \"\"\"Generate a random mask.\n\n    Returns a random mask of shape (nb_patches, nb_patches) based on the\n    configuration where the number of patches to be masked is num_masking_patches.\n\n    Returns\n    -------\n    mask: torch.Tensor\n        A mask of shape (nb_patches, nb_patches)\n\n    \"\"\"\n    mask = torch.zeros(self.get_shape(), dtype=torch.int)\n    mask_count = 0\n    while mask_count &lt; self.num_masking_patches:\n        max_mask_patches = self.num_masking_patches - mask_count\n        max_mask_patches = min(max_mask_patches, self.max_num_patches)\n\n        delta = self._mask(mask, max_mask_patches)\n        if delta == 0:\n            break\n        mask_count += delta\n\n    return mask\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.RandomMaskGenerator","title":"RandomMaskGenerator","text":"<p>Random mask generator.</p> <p>Returns a random mask of shape <code>(nb_patches, nb_patches)</code> based on the configuration where the number of patches to be masked is num_masking_patches. This is intended to be used for tasks like masked language modeling.</p> <p>Parameters:</p> Name Type Description Default <code>probability</code> <code>float</code> <p>Probability of masking a token.</p> required Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>@store(group=\"datasets/masking\", provider=\"mmlearn\", probability=0.15)\nclass RandomMaskGenerator:\n    \"\"\"Random mask generator.\n\n    Returns a random mask of shape `(nb_patches, nb_patches)` based on the\n    configuration where the number of patches to be masked is num_masking_patches.\n    **This is intended to be used for tasks like masked language modeling.**\n\n    Parameters\n    ----------\n    probability : float\n        Probability of masking a token.\n    \"\"\"\n\n    def __init__(self, probability: float):\n        self.probability = probability\n\n    def __call__(\n        self,\n        inputs: torch.Tensor,\n        tokenizer: PreTrainedTokenizerBase,\n        special_tokens_mask: Optional[torch.Tensor] = None,\n    ) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Generate a random mask.\n\n        Returns a random mask of shape (nb_patches, nb_patches) based on the\n        configuration where the number of patches to be masked is num_masking_patches.\n\n        Returns\n        -------\n        inputs : torch.Tensor\n            The encoded inputs.\n        tokenizer : PreTrainedTokenizer\n            The tokenizer.\n        special_tokens_mask : Optional[torch.Tensor], default=None\n            Mask for special tokens.\n        \"\"\"\n        inputs = tokenizer.pad(inputs, return_tensors=\"pt\")[\"input_ids\"]\n        labels = inputs.clone()\n        # We sample a few tokens in each sequence for MLM training\n        # (with probability `self.probability`)\n        probability_matrix = torch.full(labels.shape, self.probability)\n        if special_tokens_mask is None:\n            special_tokens_mask = tokenizer.get_special_tokens_mask(\n                labels, already_has_special_tokens=True\n            )\n            special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n        else:\n            special_tokens_mask = special_tokens_mask.bool()\n\n        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n        masked_indices = torch.bernoulli(probability_matrix).bool()\n        labels[~masked_indices] = tokenizer.pad_token_id\n        # 80% of the time, replace masked input tokens with tokenizer.mask_token([MASK])\n        indices_replaced = (\n            torch.bernoulli(torch.full(labels.shape, 0.8)).bool() &amp; masked_indices\n        )\n        inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n\n        # 10% of the time, we replace masked input tokens with random word\n        indices_random = (\n            torch.bernoulli(torch.full(labels.shape, 0.5)).bool()\n            &amp; masked_indices\n            &amp; ~indices_replaced\n        )\n        random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n        inputs[indices_random] = random_words[indices_random]\n\n        # Rest of the time (10% of the time) we keep the masked input tokens unchanged\n        return inputs, labels, masked_indices\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.RandomMaskGenerator.__call__","title":"__call__","text":"<pre><code>__call__(inputs, tokenizer, special_tokens_mask=None)\n</code></pre> <p>Generate a random mask.</p> <p>Returns a random mask of shape (nb_patches, nb_patches) based on the configuration where the number of patches to be masked is num_masking_patches.</p> <p>Returns:</p> Name Type Description <code>inputs</code> <code>Tensor</code> <p>The encoded inputs.</p> <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>The tokenizer.</p> <code>special_tokens_mask</code> <code>Optional[torch.Tensor], default=None</code> <p>Mask for special tokens.</p> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>def __call__(\n    self,\n    inputs: torch.Tensor,\n    tokenizer: PreTrainedTokenizerBase,\n    special_tokens_mask: Optional[torch.Tensor] = None,\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Generate a random mask.\n\n    Returns a random mask of shape (nb_patches, nb_patches) based on the\n    configuration where the number of patches to be masked is num_masking_patches.\n\n    Returns\n    -------\n    inputs : torch.Tensor\n        The encoded inputs.\n    tokenizer : PreTrainedTokenizer\n        The tokenizer.\n    special_tokens_mask : Optional[torch.Tensor], default=None\n        Mask for special tokens.\n    \"\"\"\n    inputs = tokenizer.pad(inputs, return_tensors=\"pt\")[\"input_ids\"]\n    labels = inputs.clone()\n    # We sample a few tokens in each sequence for MLM training\n    # (with probability `self.probability`)\n    probability_matrix = torch.full(labels.shape, self.probability)\n    if special_tokens_mask is None:\n        special_tokens_mask = tokenizer.get_special_tokens_mask(\n            labels, already_has_special_tokens=True\n        )\n        special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n    else:\n        special_tokens_mask = special_tokens_mask.bool()\n\n    probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n    masked_indices = torch.bernoulli(probability_matrix).bool()\n    labels[~masked_indices] = tokenizer.pad_token_id\n    # 80% of the time, replace masked input tokens with tokenizer.mask_token([MASK])\n    indices_replaced = (\n        torch.bernoulli(torch.full(labels.shape, 0.8)).bool() &amp; masked_indices\n    )\n    inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n\n    # 10% of the time, we replace masked input tokens with random word\n    indices_random = (\n        torch.bernoulli(torch.full(labels.shape, 0.5)).bool()\n        &amp; masked_indices\n        &amp; ~indices_replaced\n    )\n    random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n    inputs[indices_random] = random_words[indices_random]\n\n    # Rest of the time (10% of the time) we keep the masked input tokens unchanged\n    return inputs, labels, masked_indices\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.HFTokenizer","title":"HFTokenizer","text":"<p>A wrapper for loading HuggingFace tokenizers.</p> <p>This class wraps any huggingface tokenizer that can be initialized with meth:<code>transformers.AutoTokenizer.from_pretrained</code>. It preprocesses the input text and returns a dictionary with the tokenized text and other relevant information like attention masks.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>Pretrained model name or path - same as in meth:<code>transformers.AutoTokenizer.from_pretrained</code>.</p> required <code>max_length</code> <code>Optional[int]</code> <p>Maximum length of the tokenized sequence. This is passed to the tokenizer :meth:<code>__call__</code> method.</p> <code>None</code> <code>padding</code> <code>bool or str</code> <p>Padding strategy. Same as in meth:<code>transformers.AutoTokenizer.from_pretrained</code>; passed to the tokenizer :meth:<code>__call__</code> method.</p> <code>False</code> <code>truncation</code> <code>Optional[Union[bool, str]]</code> <p>Truncation strategy. Same as in meth:<code>transformers.AutoTokenizer.from_pretrained</code>; passed to the tokenizer :meth:<code>__call__</code> method.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to meth:<code>transformers.AutoTokenizer.from_pretrained</code>.</p> <code>{}</code> Source code in <code>mmlearn/datasets/processors/tokenizers.py</code> <pre><code>@store(group=\"datasets/tokenizers\", provider=\"mmlearn\")\nclass HFTokenizer:\n    \"\"\"A wrapper for loading HuggingFace tokenizers.\n\n    This class wraps any huggingface tokenizer that can be initialized with\n    :py:meth:`transformers.AutoTokenizer.from_pretrained`. It preprocesses the\n    input text and returns a dictionary with the tokenized text and other\n    relevant information like attention masks.\n\n    Parameters\n    ----------\n    model_name_or_path : str\n        Pretrained model name or path - same as in :py:meth:`transformers.AutoTokenizer.from_pretrained`.\n    max_length : Optional[int], optional, default=None\n        Maximum length of the tokenized sequence. This is passed to the tokenizer\n        :meth:`__call__` method.\n    padding : bool or str, default=False\n        Padding strategy. Same as in :py:meth:`transformers.AutoTokenizer.from_pretrained`;\n        passed to the tokenizer :meth:`__call__` method.\n    truncation : Optional[Union[bool, str]], optional, default=None\n        Truncation strategy. Same as in :py:meth:`transformers.AutoTokenizer.from_pretrained`;\n        passed to the tokenizer :meth:`__call__` method.\n    **kwargs : Any\n        Additional arguments passed to :py:meth:`transformers.AutoTokenizer.from_pretrained`.\n    \"\"\"  # noqa: W505\n\n    def __init__(\n        self,\n        model_name_or_path: str,\n        max_length: Optional[int] = None,\n        padding: Union[bool, str] = False,\n        truncation: Optional[Union[bool, str]] = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, **kwargs)\n        self.max_length = max_length\n        self.padding = padding\n        self.truncation = truncation\n\n    def __call__(\n        self, sentence: Union[str, list[str]], **kwargs: Any\n    ) -&gt; dict[str, torch.Tensor]:\n        \"\"\"Tokenize a text or a list of texts using the HuggingFace tokenizer.\n\n        Parameters\n        ----------\n        sentence : Union[str, list[str]]\n            Sentence(s) to be tokenized.\n        **kwargs : Any\n            Additional arguments passed to the tokenizer :meth:`__call__` method.\n\n        Returns\n        -------\n        dict[str, torch.Tensor]\n            Tokenized sentence(s).\n\n        Notes\n        -----\n        The ``input_ids`` key is replaced with ``Modalities.TEXT`` for consistency.\n        \"\"\"\n        batch_encoding = self.tokenizer(\n            sentence,\n            max_length=self.max_length,\n            padding=self.padding,\n            truncation=self.truncation,\n            return_tensors=\"pt\",\n            **kwargs,\n        )\n\n        if isinstance(\n            sentence, str\n        ):  # remove batch dimension if input is a single sentence\n            for key, value in batch_encoding.items():\n                if isinstance(value, torch.Tensor):\n                    batch_encoding[key] = torch.squeeze(value, 0)\n\n        # use 'Modalities.TEXT' key for input_ids for consistency\n        batch_encoding[Modalities.TEXT.name] = batch_encoding[\"input_ids\"]\n        return dict(batch_encoding)\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.HFTokenizer.__call__","title":"__call__","text":"<pre><code>__call__(sentence, **kwargs)\n</code></pre> <p>Tokenize a text or a list of texts using the HuggingFace tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>sentence</code> <code>Union[str, list[str]]</code> <p>Sentence(s) to be tokenized.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to the tokenizer :meth:<code>__call__</code> method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>Tokenized sentence(s).</p> Notes <p>The <code>input_ids</code> key is replaced with <code>Modalities.TEXT</code> for consistency.</p> Source code in <code>mmlearn/datasets/processors/tokenizers.py</code> <pre><code>def __call__(\n    self, sentence: Union[str, list[str]], **kwargs: Any\n) -&gt; dict[str, torch.Tensor]:\n    \"\"\"Tokenize a text or a list of texts using the HuggingFace tokenizer.\n\n    Parameters\n    ----------\n    sentence : Union[str, list[str]]\n        Sentence(s) to be tokenized.\n    **kwargs : Any\n        Additional arguments passed to the tokenizer :meth:`__call__` method.\n\n    Returns\n    -------\n    dict[str, torch.Tensor]\n        Tokenized sentence(s).\n\n    Notes\n    -----\n    The ``input_ids`` key is replaced with ``Modalities.TEXT`` for consistency.\n    \"\"\"\n    batch_encoding = self.tokenizer(\n        sentence,\n        max_length=self.max_length,\n        padding=self.padding,\n        truncation=self.truncation,\n        return_tensors=\"pt\",\n        **kwargs,\n    )\n\n    if isinstance(\n        sentence, str\n    ):  # remove batch dimension if input is a single sentence\n        for key, value in batch_encoding.items():\n            if isinstance(value, torch.Tensor):\n                batch_encoding[key] = torch.squeeze(value, 0)\n\n    # use 'Modalities.TEXT' key for input_ids for consistency\n    batch_encoding[Modalities.TEXT.name] = batch_encoding[\"input_ids\"]\n    return dict(batch_encoding)\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.TrimText","title":"TrimText","text":"<p>Trim text strings as a preprocessing step before tokenization.</p> <p>Parameters:</p> Name Type Description Default <code>trim_size</code> <code>int</code> <p>The maximum length of the trimmed text.</p> required Source code in <code>mmlearn/datasets/processors/transforms.py</code> <pre><code>@store(group=\"datasets/transforms\", provider=\"mmlearn\")\nclass TrimText:\n    \"\"\"Trim text strings as a preprocessing step before tokenization.\n\n    Parameters\n    ----------\n    trim_size : int\n        The maximum length of the trimmed text.\n    \"\"\"\n\n    def __init__(self, trim_size: int) -&gt; None:\n        self.trim_size = trim_size\n\n    def __call__(self, sentence: Union[str, list[str]]) -&gt; Union[str, list[str]]:\n        \"\"\"Trim the given sentence(s).\n\n        Parameters\n        ----------\n        sentence : Union[str, list[str]]\n            Sentence(s) to be trimmed.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            Trimmed sentence(s).\n\n        Raises\n        ------\n        TypeError\n            If the input sentence is not a string or list of strings.\n        \"\"\"\n        if not isinstance(sentence, (list, str)):\n            raise TypeError(\n                \"Expected argument `sentence` to be a string or list of strings, \"\n                f\"but got {type(sentence)}\"\n            )\n\n        if isinstance(sentence, str):\n            return sentence[: self.trim_size]\n\n        for i, s in enumerate(sentence):\n            sentence[i] = s[: self.trim_size]\n\n        return sentence\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.TrimText.__call__","title":"__call__","text":"<pre><code>__call__(sentence)\n</code></pre> <p>Trim the given sentence(s).</p> <p>Parameters:</p> Name Type Description Default <code>sentence</code> <code>Union[str, list[str]]</code> <p>Sentence(s) to be trimmed.</p> required <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>Trimmed sentence(s).</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input sentence is not a string or list of strings.</p> Source code in <code>mmlearn/datasets/processors/transforms.py</code> <pre><code>def __call__(self, sentence: Union[str, list[str]]) -&gt; Union[str, list[str]]:\n    \"\"\"Trim the given sentence(s).\n\n    Parameters\n    ----------\n    sentence : Union[str, list[str]]\n        Sentence(s) to be trimmed.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        Trimmed sentence(s).\n\n    Raises\n    ------\n    TypeError\n        If the input sentence is not a string or list of strings.\n    \"\"\"\n    if not isinstance(sentence, (list, str)):\n        raise TypeError(\n            \"Expected argument `sentence` to be a string or list of strings, \"\n            f\"but got {type(sentence)}\"\n        )\n\n    if isinstance(sentence, str):\n        return sentence[: self.trim_size]\n\n    for i, s in enumerate(sentence):\n        sentence[i] = s[: self.trim_size]\n\n    return sentence\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.masking","title":"masking","text":"<p>Token mask generators.</p>"},{"location":"api/#mmlearn.datasets.processors.masking.RandomMaskGenerator","title":"RandomMaskGenerator","text":"<p>Random mask generator.</p> <p>Returns a random mask of shape <code>(nb_patches, nb_patches)</code> based on the configuration where the number of patches to be masked is num_masking_patches. This is intended to be used for tasks like masked language modeling.</p> <p>Parameters:</p> Name Type Description Default <code>probability</code> <code>float</code> <p>Probability of masking a token.</p> required Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>@store(group=\"datasets/masking\", provider=\"mmlearn\", probability=0.15)\nclass RandomMaskGenerator:\n    \"\"\"Random mask generator.\n\n    Returns a random mask of shape `(nb_patches, nb_patches)` based on the\n    configuration where the number of patches to be masked is num_masking_patches.\n    **This is intended to be used for tasks like masked language modeling.**\n\n    Parameters\n    ----------\n    probability : float\n        Probability of masking a token.\n    \"\"\"\n\n    def __init__(self, probability: float):\n        self.probability = probability\n\n    def __call__(\n        self,\n        inputs: torch.Tensor,\n        tokenizer: PreTrainedTokenizerBase,\n        special_tokens_mask: Optional[torch.Tensor] = None,\n    ) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Generate a random mask.\n\n        Returns a random mask of shape (nb_patches, nb_patches) based on the\n        configuration where the number of patches to be masked is num_masking_patches.\n\n        Returns\n        -------\n        inputs : torch.Tensor\n            The encoded inputs.\n        tokenizer : PreTrainedTokenizer\n            The tokenizer.\n        special_tokens_mask : Optional[torch.Tensor], default=None\n            Mask for special tokens.\n        \"\"\"\n        inputs = tokenizer.pad(inputs, return_tensors=\"pt\")[\"input_ids\"]\n        labels = inputs.clone()\n        # We sample a few tokens in each sequence for MLM training\n        # (with probability `self.probability`)\n        probability_matrix = torch.full(labels.shape, self.probability)\n        if special_tokens_mask is None:\n            special_tokens_mask = tokenizer.get_special_tokens_mask(\n                labels, already_has_special_tokens=True\n            )\n            special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n        else:\n            special_tokens_mask = special_tokens_mask.bool()\n\n        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n        masked_indices = torch.bernoulli(probability_matrix).bool()\n        labels[~masked_indices] = tokenizer.pad_token_id\n        # 80% of the time, replace masked input tokens with tokenizer.mask_token([MASK])\n        indices_replaced = (\n            torch.bernoulli(torch.full(labels.shape, 0.8)).bool() &amp; masked_indices\n        )\n        inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n\n        # 10% of the time, we replace masked input tokens with random word\n        indices_random = (\n            torch.bernoulli(torch.full(labels.shape, 0.5)).bool()\n            &amp; masked_indices\n            &amp; ~indices_replaced\n        )\n        random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n        inputs[indices_random] = random_words[indices_random]\n\n        # Rest of the time (10% of the time) we keep the masked input tokens unchanged\n        return inputs, labels, masked_indices\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.masking.RandomMaskGenerator.__call__","title":"__call__","text":"<pre><code>__call__(inputs, tokenizer, special_tokens_mask=None)\n</code></pre> <p>Generate a random mask.</p> <p>Returns a random mask of shape (nb_patches, nb_patches) based on the configuration where the number of patches to be masked is num_masking_patches.</p> <p>Returns:</p> Name Type Description <code>inputs</code> <code>Tensor</code> <p>The encoded inputs.</p> <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>The tokenizer.</p> <code>special_tokens_mask</code> <code>Optional[torch.Tensor], default=None</code> <p>Mask for special tokens.</p> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>def __call__(\n    self,\n    inputs: torch.Tensor,\n    tokenizer: PreTrainedTokenizerBase,\n    special_tokens_mask: Optional[torch.Tensor] = None,\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Generate a random mask.\n\n    Returns a random mask of shape (nb_patches, nb_patches) based on the\n    configuration where the number of patches to be masked is num_masking_patches.\n\n    Returns\n    -------\n    inputs : torch.Tensor\n        The encoded inputs.\n    tokenizer : PreTrainedTokenizer\n        The tokenizer.\n    special_tokens_mask : Optional[torch.Tensor], default=None\n        Mask for special tokens.\n    \"\"\"\n    inputs = tokenizer.pad(inputs, return_tensors=\"pt\")[\"input_ids\"]\n    labels = inputs.clone()\n    # We sample a few tokens in each sequence for MLM training\n    # (with probability `self.probability`)\n    probability_matrix = torch.full(labels.shape, self.probability)\n    if special_tokens_mask is None:\n        special_tokens_mask = tokenizer.get_special_tokens_mask(\n            labels, already_has_special_tokens=True\n        )\n        special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n    else:\n        special_tokens_mask = special_tokens_mask.bool()\n\n    probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n    masked_indices = torch.bernoulli(probability_matrix).bool()\n    labels[~masked_indices] = tokenizer.pad_token_id\n    # 80% of the time, replace masked input tokens with tokenizer.mask_token([MASK])\n    indices_replaced = (\n        torch.bernoulli(torch.full(labels.shape, 0.8)).bool() &amp; masked_indices\n    )\n    inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n\n    # 10% of the time, we replace masked input tokens with random word\n    indices_random = (\n        torch.bernoulli(torch.full(labels.shape, 0.5)).bool()\n        &amp; masked_indices\n        &amp; ~indices_replaced\n    )\n    random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n    inputs[indices_random] = random_words[indices_random]\n\n    # Rest of the time (10% of the time) we keep the masked input tokens unchanged\n    return inputs, labels, masked_indices\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.masking.BlockwiseImagePatchMaskGenerator","title":"BlockwiseImagePatchMaskGenerator","text":"<p>Blockwise image patch mask generator.</p> <p>This is primarily intended for the data2vec method.</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>Union[int, tuple[int, int]]</code> <p>The size of the input image. If an integer is provided, the image is assumed to be square.</p> required <code>num_masking_patches</code> <code>int</code> <p>The number of patches to mask.</p> required <code>min_num_patches</code> <code>int</code> <p>The minimum number of patches to mask.</p> <code>4</code> <code>max_num_patches</code> <code>int</code> <p>The maximum number of patches to mask.</p> <code>None</code> <code>min_aspect_ratio</code> <code>float</code> <p>The minimum aspect ratio of the patch.</p> <code>0.3</code> <code>max_aspect_ratio</code> <code>float</code> <p>The maximum aspect ratio of the patch.</p> <code>None</code> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>@store(group=\"datasets/masking\", provider=\"mmlearn\")\nclass BlockwiseImagePatchMaskGenerator:\n    \"\"\"Blockwise image patch mask generator.\n\n    This is primarily intended for the data2vec method.\n\n    Parameters\n    ----------\n    input_size : Union[int, tuple[int, int]]\n        The size of the input image. If an integer is provided, the image is assumed\n        to be square.\n    num_masking_patches : int\n        The number of patches to mask.\n    min_num_patches : int, default=4\n        The minimum number of patches to mask.\n    max_num_patches : int, default=None\n        The maximum number of patches to mask.\n    min_aspect_ratio : float, default=0.3\n        The minimum aspect ratio of the patch.\n    max_aspect_ratio : float, default=None\n        The maximum aspect ratio of the patch.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_size: Union[int, tuple[int, int]],\n        num_masking_patches: int,\n        min_num_patches: int = 4,\n        max_num_patches: Any = None,\n        min_aspect_ratio: float = 0.3,\n        max_aspect_ratio: Any = None,\n    ):\n        if not isinstance(input_size, tuple):\n            input_size = (input_size,) * 2\n        self.height, self.width = input_size\n\n        self.num_masking_patches = num_masking_patches\n\n        self.min_num_patches = min_num_patches\n        self.max_num_patches = (\n            num_masking_patches if max_num_patches is None else max_num_patches\n        )\n\n        max_aspect_ratio = max_aspect_ratio or 1 / min_aspect_ratio\n        self.log_aspect_ratio = (math.log(min_aspect_ratio), math.log(max_aspect_ratio))\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Generate a printable representation.\n\n        Returns\n        -------\n        str\n            A printable representation of the object.\n\n        \"\"\"\n        return \"Generator(%d, %d -&gt; [%d ~ %d], max = %d, %.3f ~ %.3f)\" % (\n            self.height,\n            self.width,\n            self.min_num_patches,\n            self.max_num_patches,\n            self.num_masking_patches,\n            self.log_aspect_ratio[0],\n            self.log_aspect_ratio[1],\n        )\n\n    def get_shape(self) -&gt; tuple[int, int]:\n        \"\"\"Get the shape of the input.\n\n        Returns\n        -------\n        tuple[int, int]\n            The shape of the input as a tuple `(height, width)`.\n        \"\"\"\n        return self.height, self.width\n\n    def _mask(self, mask: torch.Tensor, max_mask_patches: int) -&gt; int:\n        \"\"\"Masking function.\n\n        This function mask adjacent patches by first selecting a target area and aspect\n        ratio. Since, there might be overlap between selected areas  or the selected\n        area might already be masked, it runs for a  maximum of 10 attempts or until the\n        specified number of patches (max_mask_patches) is achieved.\n\n\n        Parameters\n        ----------\n        mask: torch.Tensor\n            Current mask. The mask to be updated.\n        max_mask_patches: int\n            The maximum number of patches to be masked.\n\n        Returns\n        -------\n        delta: int\n            The number of patches that were successfully masked.\n\n        Notes\n        -----\n        - `target_area`: Randomly chosen target area for the patch.\n        - `aspect_ratio`: Randomly chosen aspect ratio for the patch.\n        - `h`: Height of the patch based on the target area and aspect ratio.\n        - `w`: Width of the patch based on the target area and aspect ratio.\n        - `top`: Randomly chosen top position for the patch.\n        - `left`: Randomly chosen left position for the patch.\n        - `num_masked`: Number of masked pixels within the proposed patch area.\n        - `delta`: Accumulated count of modified pixels.\n        \"\"\"\n        delta = 0\n        for _ in range(10):\n            target_area = random.uniform(self.min_num_patches, max_mask_patches)\n            aspect_ratio = math.exp(random.uniform(*self.log_aspect_ratio))\n            h = int(round(math.sqrt(target_area * aspect_ratio)))\n            w = int(round(math.sqrt(target_area / aspect_ratio)))\n            if w &lt; self.width and h &lt; self.height:\n                top = random.randint(0, self.height - h)\n                left = random.randint(0, self.width - w)\n\n                num_masked = mask[top : top + h, left : left + w].sum()\n                # Overlap\n                if 0 &lt; h * w - num_masked &lt;= max_mask_patches:\n                    for i in range(top, top + h):\n                        for j in range(left, left + w):\n                            if mask[i, j] == 0:\n                                mask[i, j] = 1\n                                delta += 1\n\n                if delta &gt; 0:\n                    break\n        return delta\n\n    def __call__(self) -&gt; torch.Tensor:\n        \"\"\"Generate a random mask.\n\n        Returns a random mask of shape (nb_patches, nb_patches) based on the\n        configuration where the number of patches to be masked is num_masking_patches.\n\n        Returns\n        -------\n        mask: torch.Tensor\n            A mask of shape (nb_patches, nb_patches)\n\n        \"\"\"\n        mask = torch.zeros(self.get_shape(), dtype=torch.int)\n        mask_count = 0\n        while mask_count &lt; self.num_masking_patches:\n            max_mask_patches = self.num_masking_patches - mask_count\n            max_mask_patches = min(max_mask_patches, self.max_num_patches)\n\n            delta = self._mask(mask, max_mask_patches)\n            if delta == 0:\n                break\n            mask_count += delta\n\n        return mask\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.masking.BlockwiseImagePatchMaskGenerator.__repr__","title":"__repr__","text":"<pre><code>__repr__()\n</code></pre> <p>Generate a printable representation.</p> <p>Returns:</p> Type Description <code>str</code> <p>A printable representation of the object.</p> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Generate a printable representation.\n\n    Returns\n    -------\n    str\n        A printable representation of the object.\n\n    \"\"\"\n    return \"Generator(%d, %d -&gt; [%d ~ %d], max = %d, %.3f ~ %.3f)\" % (\n        self.height,\n        self.width,\n        self.min_num_patches,\n        self.max_num_patches,\n        self.num_masking_patches,\n        self.log_aspect_ratio[0],\n        self.log_aspect_ratio[1],\n    )\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.masking.BlockwiseImagePatchMaskGenerator.get_shape","title":"get_shape","text":"<pre><code>get_shape()\n</code></pre> <p>Get the shape of the input.</p> <p>Returns:</p> Type Description <code>tuple[int, int]</code> <p>The shape of the input as a tuple <code>(height, width)</code>.</p> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>def get_shape(self) -&gt; tuple[int, int]:\n    \"\"\"Get the shape of the input.\n\n    Returns\n    -------\n    tuple[int, int]\n        The shape of the input as a tuple `(height, width)`.\n    \"\"\"\n    return self.height, self.width\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.masking.BlockwiseImagePatchMaskGenerator.__call__","title":"__call__","text":"<pre><code>__call__()\n</code></pre> <p>Generate a random mask.</p> <p>Returns a random mask of shape (nb_patches, nb_patches) based on the configuration where the number of patches to be masked is num_masking_patches.</p> <p>Returns:</p> Name Type Description <code>mask</code> <code>Tensor</code> <p>A mask of shape (nb_patches, nb_patches)</p> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>def __call__(self) -&gt; torch.Tensor:\n    \"\"\"Generate a random mask.\n\n    Returns a random mask of shape (nb_patches, nb_patches) based on the\n    configuration where the number of patches to be masked is num_masking_patches.\n\n    Returns\n    -------\n    mask: torch.Tensor\n        A mask of shape (nb_patches, nb_patches)\n\n    \"\"\"\n    mask = torch.zeros(self.get_shape(), dtype=torch.int)\n    mask_count = 0\n    while mask_count &lt; self.num_masking_patches:\n        max_mask_patches = self.num_masking_patches - mask_count\n        max_mask_patches = min(max_mask_patches, self.max_num_patches)\n\n        delta = self._mask(mask, max_mask_patches)\n        if delta == 0:\n            break\n        mask_count += delta\n\n    return mask\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.masking.IJEPAMaskGenerator","title":"IJEPAMaskGenerator  <code>dataclass</code>","text":"<p>Generates encoder and predictor masks for preprocessing.</p> <p>This class generates masks dynamically for batches of examples.</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>tuple[int, int]</code> <p>Input image size.</p> <code>(224, 224)</code> <code>patch_size</code> <code>int</code> <p>Size of each patch.</p> <code>16</code> <code>min_keep</code> <code>int</code> <p>Minimum number of patches to keep.</p> <code>10</code> <code>allow_overlap</code> <code>bool</code> <p>Whether to allow overlap between encoder and predictor masks.</p> <code>False</code> <code>enc_mask_scale</code> <code>tuple[float, float]</code> <p>Scale range for encoder mask.</p> <code>(0.85, 1.0)</code> <code>pred_mask_scale</code> <code>tuple[float, float]</code> <p>Scale range for predictor mask.</p> <code>(0.15, 0.2)</code> <code>aspect_ratio</code> <code>tuple[float, float]</code> <p>Aspect ratio range for mask blocks.</p> <code>(0.75, 1.0)</code> <code>nenc</code> <code>int</code> <p>Number of encoder masks to generate.</p> <code>1</code> <code>npred</code> <code>int</code> <p>Number of predictor masks to generate.</p> <code>4</code> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>@dataclass\nclass IJEPAMaskGenerator:\n    \"\"\"Generates encoder and predictor masks for preprocessing.\n\n    This class generates masks dynamically for batches of examples.\n\n    Parameters\n    ----------\n    input_size : tuple[int, int], default=(224, 224)\n        Input image size.\n    patch_size : int, default=16\n        Size of each patch.\n    min_keep : int, default=10\n        Minimum number of patches to keep.\n    allow_overlap : bool, default=False\n        Whether to allow overlap between encoder and predictor masks.\n    enc_mask_scale : tuple[float, float], default=(0.85, 1.0)\n        Scale range for encoder mask.\n    pred_mask_scale : tuple[float, float], default=(0.15, 0.2)\n        Scale range for predictor mask.\n    aspect_ratio : tuple[float, float], default=(0.75, 1.0)\n        Aspect ratio range for mask blocks.\n    nenc : int, default=1\n        Number of encoder masks to generate.\n    npred : int, default=4\n        Number of predictor masks to generate.\n    \"\"\"\n\n    input_size: tuple[int, int] = (224, 224)\n    patch_size: int = 16\n    min_keep: int = 10\n    allow_overlap: bool = False\n    enc_mask_scale: tuple[float, float] = (0.85, 1.0)\n    pred_mask_scale: tuple[float, float] = (0.15, 0.2)\n    aspect_ratio: tuple[float, float] = (0.75, 1.5)\n    nenc: int = 1\n    npred: int = 4\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Initialize the mask generator.\"\"\"\n        self.height = self.input_size[0] // self.patch_size\n        self.width = self.input_size[1] // self.patch_size\n\n    def _sample_block_size(\n        self,\n        generator: torch.Generator,\n        scale: tuple[float, float],\n        aspect_ratio: tuple[float, float],\n    ) -&gt; tuple[int, int]:\n        \"\"\"Sample the size of the mask block based on scale and aspect ratio.\"\"\"\n        _rand = torch.rand(1, generator=generator).item()\n        min_s, max_s = scale\n        mask_scale = min_s + _rand * (max_s - min_s)\n        max_keep = int(self.height * self.width * mask_scale)\n\n        min_ar, max_ar = aspect_ratio\n        aspect_ratio_val = min_ar + _rand * (max_ar - min_ar)\n\n        h = int(round(math.sqrt(max_keep * aspect_ratio_val)))\n        w = int(round(math.sqrt(max_keep / aspect_ratio_val)))\n\n        h = min(h, self.height - 1)\n        w = min(w, self.width - 1)\n\n        return h, w\n\n    def _sample_block_mask(\n        self, b_size: tuple[int, int]\n    ) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Sample a mask block.\"\"\"\n        h, w = b_size\n        top = torch.randint(0, self.height - h, (1,)).item()\n        left = torch.randint(0, self.width - w, (1,)).item()\n        mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n        mask[top : top + h, left : left + w] = 1\n\n        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n        mask_complement[top : top + h, left : left + w] = 0\n\n        return mask.flatten(), mask_complement.flatten()\n\n    def __call__(self, batch_size: int = 1) -&gt; dict[str, Any]:\n        \"\"\"Generate encoder and predictor masks for a batch of examples.\n\n        Parameters\n        ----------\n        batch_size : int, default=1\n            The batch size for which to generate masks.\n\n        Returns\n        -------\n        dict[str, Any]\n            A dictionary of encoder masks and predictor masks.\n        \"\"\"\n        seed = torch.randint(\n            0, 2**32, (1,)\n        ).item()  # Sample random seed for reproducibility\n        g = torch.Generator().manual_seed(seed)\n\n        # Sample block sizes\n        p_size = self._sample_block_size(\n            generator=g, scale=self.pred_mask_scale, aspect_ratio=self.aspect_ratio\n        )\n        e_size = self._sample_block_size(\n            generator=g, scale=self.enc_mask_scale, aspect_ratio=(1.0, 1.0)\n        )\n\n        # Generate predictor masks\n        masks_pred, masks_enc = [], []\n        for _ in range(self.npred):\n            mask_p, _ = self._sample_block_mask(p_size)\n            # Expand mask to match batch size\n            mask_p = mask_p.unsqueeze(0).expand(batch_size, -1)\n            masks_pred.append(mask_p)\n\n        # Generate encoder masks\n        for _ in range(self.nenc):\n            mask_e, _ = self._sample_block_mask(e_size)\n            # Expand mask to match batch size\n            mask_e = mask_e.unsqueeze(0).expand(batch_size, -1)\n            masks_enc.append(mask_e)\n\n        return {\n            \"encoder_masks\": masks_enc,  # list of tensors of shape (batch_size, N)\n            \"predictor_masks\": masks_pred,  # list of tensors of shape (batch_size, N)\n        }\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.masking.IJEPAMaskGenerator.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Initialize the mask generator.</p> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Initialize the mask generator.\"\"\"\n    self.height = self.input_size[0] // self.patch_size\n    self.width = self.input_size[1] // self.patch_size\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.masking.IJEPAMaskGenerator.__call__","title":"__call__","text":"<pre><code>__call__(batch_size=1)\n</code></pre> <p>Generate encoder and predictor masks for a batch of examples.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>The batch size for which to generate masks.</p> <code>1</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary of encoder masks and predictor masks.</p> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>def __call__(self, batch_size: int = 1) -&gt; dict[str, Any]:\n    \"\"\"Generate encoder and predictor masks for a batch of examples.\n\n    Parameters\n    ----------\n    batch_size : int, default=1\n        The batch size for which to generate masks.\n\n    Returns\n    -------\n    dict[str, Any]\n        A dictionary of encoder masks and predictor masks.\n    \"\"\"\n    seed = torch.randint(\n        0, 2**32, (1,)\n    ).item()  # Sample random seed for reproducibility\n    g = torch.Generator().manual_seed(seed)\n\n    # Sample block sizes\n    p_size = self._sample_block_size(\n        generator=g, scale=self.pred_mask_scale, aspect_ratio=self.aspect_ratio\n    )\n    e_size = self._sample_block_size(\n        generator=g, scale=self.enc_mask_scale, aspect_ratio=(1.0, 1.0)\n    )\n\n    # Generate predictor masks\n    masks_pred, masks_enc = [], []\n    for _ in range(self.npred):\n        mask_p, _ = self._sample_block_mask(p_size)\n        # Expand mask to match batch size\n        mask_p = mask_p.unsqueeze(0).expand(batch_size, -1)\n        masks_pred.append(mask_p)\n\n    # Generate encoder masks\n    for _ in range(self.nenc):\n        mask_e, _ = self._sample_block_mask(e_size)\n        # Expand mask to match batch size\n        mask_e = mask_e.unsqueeze(0).expand(batch_size, -1)\n        masks_enc.append(mask_e)\n\n    return {\n        \"encoder_masks\": masks_enc,  # list of tensors of shape (batch_size, N)\n        \"predictor_masks\": masks_pred,  # list of tensors of shape (batch_size, N)\n    }\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.masking.apply_masks","title":"apply_masks","text":"<pre><code>apply_masks(x, masks)\n</code></pre> <p>Apply masks to the input tensor by selecting the patches to keep based on the masks.</p> <p>This function is primarily intended to be used for the class:<code>i-JEPA &lt;mmlearn.tasks.ijepa.IJEPA&gt;</code>.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape <code>(B, N, D)</code>.</p> required <code>masks</code> <code>Union[Tensor, list[Tensor]]</code> <p>A list of mask tensors of shape <code>(N,)</code>, <code>(1, N)</code>, or <code>(B, N)</code>.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The masked tensor where only the patches indicated by the masks are kept. The output tensor has shape <code>(B * num_masks, N', D)</code>, where <code>N'</code> is the number of patches kept.</p> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>def apply_masks(\n    x: torch.Tensor, masks: Union[torch.Tensor, list[torch.Tensor]]\n) -&gt; torch.Tensor:\n    \"\"\"\n    Apply masks to the input tensor by selecting the patches to keep based on the masks.\n\n    This function is primarily intended to be used for the\n    :py:class:`i-JEPA &lt;mmlearn.tasks.ijepa.IJEPA&gt;`.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape ``(B, N, D)``.\n    masks : Union[torch.Tensor, list[torch.Tensor]]\n        A list of mask tensors of shape ``(N,)``, ``(1, N)``, or ``(B, N)``.\n\n    Returns\n    -------\n    torch.Tensor\n        The masked tensor where only the patches indicated by the masks are kept.\n        The output tensor has shape ``(B * num_masks, N', D)``, where ``N'`` is\n        the number of patches kept.\n    \"\"\"\n    all_x = []\n    batch_size = x.size(0)\n    for m_ in masks:\n        m = m_.to(x.device)\n\n        # Ensure mask is at least 2D\n        if m.dim() == 1:\n            m = m.unsqueeze(0)  # Shape: (1, N)\n\n        # Expand mask to match the batch size if needed\n        if m.size(0) == 1 and batch_size &gt; 1:\n            m = m.expand(batch_size, -1)  # Shape: (B, N)\n\n        # Expand mask to match x's dimensions\n        m_expanded = (\n            m.unsqueeze(-1).expand(-1, -1, x.size(-1)).bool()\n        )  # Shape: (B, N, D)\n\n        # Use boolean indexing\n        selected_patches = x[m_expanded].view(batch_size, -1, x.size(-1))\n        all_x.append(selected_patches)\n\n    # Concatenate along the batch dimension\n    return torch.cat(all_x, dim=0)\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.tokenizers","title":"tokenizers","text":"<p>Tokenizers - modules that convert raw input to sequences of tokens.</p>"},{"location":"api/#mmlearn.datasets.processors.tokenizers.HFTokenizer","title":"HFTokenizer","text":"<p>A wrapper for loading HuggingFace tokenizers.</p> <p>This class wraps any huggingface tokenizer that can be initialized with meth:<code>transformers.AutoTokenizer.from_pretrained</code>. It preprocesses the input text and returns a dictionary with the tokenized text and other relevant information like attention masks.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>Pretrained model name or path - same as in meth:<code>transformers.AutoTokenizer.from_pretrained</code>.</p> required <code>max_length</code> <code>Optional[int]</code> <p>Maximum length of the tokenized sequence. This is passed to the tokenizer :meth:<code>__call__</code> method.</p> <code>None</code> <code>padding</code> <code>bool or str</code> <p>Padding strategy. Same as in meth:<code>transformers.AutoTokenizer.from_pretrained</code>; passed to the tokenizer :meth:<code>__call__</code> method.</p> <code>False</code> <code>truncation</code> <code>Optional[Union[bool, str]]</code> <p>Truncation strategy. Same as in meth:<code>transformers.AutoTokenizer.from_pretrained</code>; passed to the tokenizer :meth:<code>__call__</code> method.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to meth:<code>transformers.AutoTokenizer.from_pretrained</code>.</p> <code>{}</code> Source code in <code>mmlearn/datasets/processors/tokenizers.py</code> <pre><code>@store(group=\"datasets/tokenizers\", provider=\"mmlearn\")\nclass HFTokenizer:\n    \"\"\"A wrapper for loading HuggingFace tokenizers.\n\n    This class wraps any huggingface tokenizer that can be initialized with\n    :py:meth:`transformers.AutoTokenizer.from_pretrained`. It preprocesses the\n    input text and returns a dictionary with the tokenized text and other\n    relevant information like attention masks.\n\n    Parameters\n    ----------\n    model_name_or_path : str\n        Pretrained model name or path - same as in :py:meth:`transformers.AutoTokenizer.from_pretrained`.\n    max_length : Optional[int], optional, default=None\n        Maximum length of the tokenized sequence. This is passed to the tokenizer\n        :meth:`__call__` method.\n    padding : bool or str, default=False\n        Padding strategy. Same as in :py:meth:`transformers.AutoTokenizer.from_pretrained`;\n        passed to the tokenizer :meth:`__call__` method.\n    truncation : Optional[Union[bool, str]], optional, default=None\n        Truncation strategy. Same as in :py:meth:`transformers.AutoTokenizer.from_pretrained`;\n        passed to the tokenizer :meth:`__call__` method.\n    **kwargs : Any\n        Additional arguments passed to :py:meth:`transformers.AutoTokenizer.from_pretrained`.\n    \"\"\"  # noqa: W505\n\n    def __init__(\n        self,\n        model_name_or_path: str,\n        max_length: Optional[int] = None,\n        padding: Union[bool, str] = False,\n        truncation: Optional[Union[bool, str]] = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, **kwargs)\n        self.max_length = max_length\n        self.padding = padding\n        self.truncation = truncation\n\n    def __call__(\n        self, sentence: Union[str, list[str]], **kwargs: Any\n    ) -&gt; dict[str, torch.Tensor]:\n        \"\"\"Tokenize a text or a list of texts using the HuggingFace tokenizer.\n\n        Parameters\n        ----------\n        sentence : Union[str, list[str]]\n            Sentence(s) to be tokenized.\n        **kwargs : Any\n            Additional arguments passed to the tokenizer :meth:`__call__` method.\n\n        Returns\n        -------\n        dict[str, torch.Tensor]\n            Tokenized sentence(s).\n\n        Notes\n        -----\n        The ``input_ids`` key is replaced with ``Modalities.TEXT`` for consistency.\n        \"\"\"\n        batch_encoding = self.tokenizer(\n            sentence,\n            max_length=self.max_length,\n            padding=self.padding,\n            truncation=self.truncation,\n            return_tensors=\"pt\",\n            **kwargs,\n        )\n\n        if isinstance(\n            sentence, str\n        ):  # remove batch dimension if input is a single sentence\n            for key, value in batch_encoding.items():\n                if isinstance(value, torch.Tensor):\n                    batch_encoding[key] = torch.squeeze(value, 0)\n\n        # use 'Modalities.TEXT' key for input_ids for consistency\n        batch_encoding[Modalities.TEXT.name] = batch_encoding[\"input_ids\"]\n        return dict(batch_encoding)\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.tokenizers.HFTokenizer.__call__","title":"__call__","text":"<pre><code>__call__(sentence, **kwargs)\n</code></pre> <p>Tokenize a text or a list of texts using the HuggingFace tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>sentence</code> <code>Union[str, list[str]]</code> <p>Sentence(s) to be tokenized.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to the tokenizer :meth:<code>__call__</code> method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>Tokenized sentence(s).</p> Notes <p>The <code>input_ids</code> key is replaced with <code>Modalities.TEXT</code> for consistency.</p> Source code in <code>mmlearn/datasets/processors/tokenizers.py</code> <pre><code>def __call__(\n    self, sentence: Union[str, list[str]], **kwargs: Any\n) -&gt; dict[str, torch.Tensor]:\n    \"\"\"Tokenize a text or a list of texts using the HuggingFace tokenizer.\n\n    Parameters\n    ----------\n    sentence : Union[str, list[str]]\n        Sentence(s) to be tokenized.\n    **kwargs : Any\n        Additional arguments passed to the tokenizer :meth:`__call__` method.\n\n    Returns\n    -------\n    dict[str, torch.Tensor]\n        Tokenized sentence(s).\n\n    Notes\n    -----\n    The ``input_ids`` key is replaced with ``Modalities.TEXT`` for consistency.\n    \"\"\"\n    batch_encoding = self.tokenizer(\n        sentence,\n        max_length=self.max_length,\n        padding=self.padding,\n        truncation=self.truncation,\n        return_tensors=\"pt\",\n        **kwargs,\n    )\n\n    if isinstance(\n        sentence, str\n    ):  # remove batch dimension if input is a single sentence\n        for key, value in batch_encoding.items():\n            if isinstance(value, torch.Tensor):\n                batch_encoding[key] = torch.squeeze(value, 0)\n\n    # use 'Modalities.TEXT' key for input_ids for consistency\n    batch_encoding[Modalities.TEXT.name] = batch_encoding[\"input_ids\"]\n    return dict(batch_encoding)\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.tokenizers.Img2Seq","title":"Img2Seq","text":"<p>               Bases: <code>Module</code></p> <p>Convert a batch of images to a batch of sequences.</p> <p>Parameters:</p> Name Type Description Default <code>img_size</code> <code>tuple of int</code> <p>The size of the input image.</p> required <code>patch_size</code> <code>tuple of int</code> <p>The size of the patch.</p> required <code>n_channels</code> <code>int</code> <p>The number of channels in the input image.</p> required <code>d_model</code> <code>int</code> <p>The dimension of the output sequence.</p> required Source code in <code>mmlearn/datasets/processors/tokenizers.py</code> <pre><code>class Img2Seq(nn.Module):\n    \"\"\"Convert a batch of images to a batch of sequences.\n\n    Parameters\n    ----------\n    img_size : tuple of int\n        The size of the input image.\n    patch_size : tuple of int\n        The size of the patch.\n    n_channels : int\n        The number of channels in the input image.\n    d_model : int\n        The dimension of the output sequence.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        img_size: tuple[int, int],\n        patch_size: tuple[int, int],\n        n_channels: int,\n        d_model: int,\n    ) -&gt; None:\n        super().__init__()\n        self.patch_size = patch_size\n        self.img_size = img_size\n\n        nh, nw = img_size[0] // patch_size[0], img_size[1] // patch_size[1]\n        n_tokens = nh * nw\n\n        token_dim = patch_size[0] * patch_size[1] * n_channels\n        self.linear = nn.Linear(token_dim, d_model)\n        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n        self.pos_emb = nn.Parameter(torch.randn(n_tokens, d_model))\n\n    def __call__(self, batch: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Convert a batch of images to a batch of sequences.\n\n        Parameters\n        ----------\n        batch : torch.Tensor\n            Batch of images of shape ``(b, h, w, c)`` where ``b`` is the batch size,\n            ``h`` is the height, ``w`` is the width, and ``c`` is the number of\n            channels.\n\n        Returns\n        -------\n        torch.Tensor\n            Batch of sequences of shape ``(b, s, d)`` where ``b`` is the batch size,\n            ``s`` is the sequence length, and ``d`` is the dimension of the output\n            sequence.\n        \"\"\"\n        batch = _patchify(batch, self.patch_size)\n\n        b, c, nh, nw, ph, pw = batch.shape\n\n        # Flattening the patches\n        batch = torch.permute(batch, [0, 2, 3, 4, 5, 1])\n        batch = torch.reshape(batch, [b, nh * nw, ph * pw * c])\n\n        batch = self.linear(batch)\n        cls: torch.Tensor = self.cls_token.expand([b, -1, -1])\n        emb: torch.Tensor = batch + self.pos_emb\n\n        return torch.cat([cls, emb], axis=1)\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.tokenizers.Img2Seq.__call__","title":"__call__","text":"<pre><code>__call__(batch)\n</code></pre> <p>Convert a batch of images to a batch of sequences.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Tensor</code> <p>Batch of images of shape <code>(b, h, w, c)</code> where <code>b</code> is the batch size, <code>h</code> is the height, <code>w</code> is the width, and <code>c</code> is the number of channels.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Batch of sequences of shape <code>(b, s, d)</code> where <code>b</code> is the batch size, <code>s</code> is the sequence length, and <code>d</code> is the dimension of the output sequence.</p> Source code in <code>mmlearn/datasets/processors/tokenizers.py</code> <pre><code>def __call__(self, batch: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Convert a batch of images to a batch of sequences.\n\n    Parameters\n    ----------\n    batch : torch.Tensor\n        Batch of images of shape ``(b, h, w, c)`` where ``b`` is the batch size,\n        ``h`` is the height, ``w`` is the width, and ``c`` is the number of\n        channels.\n\n    Returns\n    -------\n    torch.Tensor\n        Batch of sequences of shape ``(b, s, d)`` where ``b`` is the batch size,\n        ``s`` is the sequence length, and ``d`` is the dimension of the output\n        sequence.\n    \"\"\"\n    batch = _patchify(batch, self.patch_size)\n\n    b, c, nh, nw, ph, pw = batch.shape\n\n    # Flattening the patches\n    batch = torch.permute(batch, [0, 2, 3, 4, 5, 1])\n    batch = torch.reshape(batch, [b, nh * nw, ph * pw * c])\n\n    batch = self.linear(batch)\n    cls: torch.Tensor = self.cls_token.expand([b, -1, -1])\n    emb: torch.Tensor = batch + self.pos_emb\n\n    return torch.cat([cls, emb], axis=1)\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.transforms","title":"transforms","text":"<p>Custom transforms for datasets/inputs.</p>"},{"location":"api/#mmlearn.datasets.processors.transforms.TrimText","title":"TrimText","text":"<p>Trim text strings as a preprocessing step before tokenization.</p> <p>Parameters:</p> Name Type Description Default <code>trim_size</code> <code>int</code> <p>The maximum length of the trimmed text.</p> required Source code in <code>mmlearn/datasets/processors/transforms.py</code> <pre><code>@store(group=\"datasets/transforms\", provider=\"mmlearn\")\nclass TrimText:\n    \"\"\"Trim text strings as a preprocessing step before tokenization.\n\n    Parameters\n    ----------\n    trim_size : int\n        The maximum length of the trimmed text.\n    \"\"\"\n\n    def __init__(self, trim_size: int) -&gt; None:\n        self.trim_size = trim_size\n\n    def __call__(self, sentence: Union[str, list[str]]) -&gt; Union[str, list[str]]:\n        \"\"\"Trim the given sentence(s).\n\n        Parameters\n        ----------\n        sentence : Union[str, list[str]]\n            Sentence(s) to be trimmed.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            Trimmed sentence(s).\n\n        Raises\n        ------\n        TypeError\n            If the input sentence is not a string or list of strings.\n        \"\"\"\n        if not isinstance(sentence, (list, str)):\n            raise TypeError(\n                \"Expected argument `sentence` to be a string or list of strings, \"\n                f\"but got {type(sentence)}\"\n            )\n\n        if isinstance(sentence, str):\n            return sentence[: self.trim_size]\n\n        for i, s in enumerate(sentence):\n            sentence[i] = s[: self.trim_size]\n\n        return sentence\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.transforms.TrimText.__call__","title":"__call__","text":"<pre><code>__call__(sentence)\n</code></pre> <p>Trim the given sentence(s).</p> <p>Parameters:</p> Name Type Description Default <code>sentence</code> <code>Union[str, list[str]]</code> <p>Sentence(s) to be trimmed.</p> required <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>Trimmed sentence(s).</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input sentence is not a string or list of strings.</p> Source code in <code>mmlearn/datasets/processors/transforms.py</code> <pre><code>def __call__(self, sentence: Union[str, list[str]]) -&gt; Union[str, list[str]]:\n    \"\"\"Trim the given sentence(s).\n\n    Parameters\n    ----------\n    sentence : Union[str, list[str]]\n        Sentence(s) to be trimmed.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        Trimmed sentence(s).\n\n    Raises\n    ------\n    TypeError\n        If the input sentence is not a string or list of strings.\n    \"\"\"\n    if not isinstance(sentence, (list, str)):\n        raise TypeError(\n            \"Expected argument `sentence` to be a string or list of strings, \"\n            f\"but got {type(sentence)}\"\n        )\n\n    if isinstance(sentence, str):\n        return sentence[: self.trim_size]\n\n    for i, s in enumerate(sentence):\n        sentence[i] = s[: self.trim_size]\n\n    return sentence\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.transforms.repeat_interleave_batch","title":"repeat_interleave_batch","text":"<pre><code>repeat_interleave_batch(x, b, repeat)\n</code></pre> <p>Repeat and interleave a tensor across the batch dimension.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor to be repeated.</p> required <code>b</code> <code>int</code> <p>Size of the batch to be repeated.</p> required <code>repeat</code> <code>int</code> <p>Number of times to repeat each batch.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The repeated tensor with shape adjusted for the batch.</p> Source code in <code>mmlearn/datasets/processors/transforms.py</code> <pre><code>def repeat_interleave_batch(x: torch.Tensor, b: int, repeat: int) -&gt; torch.Tensor:\n    \"\"\"Repeat and interleave a tensor across the batch dimension.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor to be repeated.\n    b : int\n        Size of the batch to be repeated.\n    repeat : int\n        Number of times to repeat each batch.\n\n    Returns\n    -------\n    torch.Tensor\n        The repeated tensor with shape adjusted for the batch.\n    \"\"\"\n    n = len(x) // b\n    return torch.cat(\n        [\n            torch.cat([x[i * b : (i + 1) * b] for _ in range(repeat)], dim=0)\n            for i in range(n)\n        ],\n        dim=0,\n    )\n</code></pre>"},{"location":"api/#mmlearn.datasets.sunrgbd","title":"sunrgbd","text":"<p>SUN RGB-D dataset.</p>"},{"location":"api/#mmlearn.datasets.sunrgbd.SUNRGBDDataset","title":"SUNRGBDDataset","text":"<p>               Bases: <code>Dataset[Example]</code></p> <p>SUN RGB-D dataset.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>Path to the root directory of the dataset.</p> required <code>split</code> <code>(train, test)</code> <p>Split of the dataset to use.</p> <code>\"train\"</code> <code>return_type</code> <code>(disparity, image)</code> <p>Return type of the depth images. If \"disparity\", the depth images are converted to disparity similar to the ImageBind implementation. Otherwise, return the depth image as a 3-channel image.</p> <code>\"disparity\"</code> <code>rgb_transform</code> <code>Optional[Callable[[Image], Tensor]]</code> <p>A callable that takes in an RGB PIL image and returns a transformed version of the image as a PyTorch tensor.</p> <code>None</code> <code>depth_transform</code> <code>Optional[Callable[[Image], Tensor]]</code> <p>A callable that takes in a depth PIL image and returns a transformed version of the image as a PyTorch tensor.</p> <code>None</code> References <p>.. [1] Repo followed to extract the dataset: https://github.com/TUI-NICR/nicr-scene-analysis-datasets</p> Source code in <code>mmlearn/datasets/sunrgbd.py</code> <pre><code>@store(\n    name=\"SUNRGBD\",\n    group=\"datasets\",\n    provider=\"mmlearn\",\n    root_dir=os.getenv(\"SUNRGBD_ROOT_DIR\", MISSING),\n)\nclass SUNRGBDDataset(Dataset[Example]):\n    \"\"\"SUN RGB-D dataset.\n\n    Parameters\n    ----------\n    root_dir : str\n        Path to the root directory of the dataset.\n    split : {\"train\", \"test\"}, default=\"train\"\n        Split of the dataset to use.\n    return_type : {\"disparity\", \"image\"}, default=\"disparity\"\n        Return type of the depth images. If \"disparity\", the depth images are\n        converted to disparity similar to the ImageBind implementation.\n        Otherwise, return the depth image as a 3-channel image.\n    rgb_transform: Callable[[PIL.Image], torch.Tensor], default=None\n        A callable that takes in an RGB PIL image and returns a transformed version\n        of the image as a PyTorch tensor.\n    depth_transform: Callable[[PIL.Image], torch.Tensor], default=None\n        A callable that takes in a depth PIL image and returns a transformed version\n        of the image as a PyTorch tensor.\n\n    References\n    ----------\n    .. [1] Repo followed to extract the dataset: https://github.com/TUI-NICR/nicr-scene-analysis-datasets\n    \"\"\"\n\n    def __init__(\n        self,\n        root_dir: str,\n        split: Literal[\"train\", \"test\"] = \"train\",\n        return_type: Literal[\"disparity\", \"image\"] = \"disparity\",\n        rgb_transform: Optional[Callable[[PILImage], torch.Tensor]] = None,\n        depth_transform: Optional[Callable[[PILImage], torch.Tensor]] = None,\n    ) -&gt; None:\n        super().__init__()\n        if not _OPENCV_AVAILABLE:\n            raise ImportError(\n                \"SUN RGB-D dataset requires `opencv-python` which is not installed.\",\n            )\n\n        self._validate_args(root_dir, split, rgb_transform, depth_transform)\n        self.return_type = return_type\n\n        self.root_dir = root_dir\n        with open(os.path.join(root_dir, f\"{split}.txt\"), \"r\") as f:\n            file_ids = f.readlines()\n        file_ids = [f.strip() for f in file_ids]\n\n        root_dir = os.path.join(root_dir, split)\n        depth_files = [os.path.join(root_dir, \"depth\", f\"{f}.png\") for f in file_ids]\n        rgb_files = [os.path.join(root_dir, \"rgb\", f\"{f}.jpg\") for f in file_ids]\n        intrinsic_files = [\n            os.path.join(root_dir, \"intrinsics\", f\"{f}.txt\") for f in file_ids\n        ]\n\n        sensor_types = [\n            file.removeprefix(os.path.join(root_dir, \"depth\")).split(os.sep)[1]\n            for file in depth_files\n        ]\n\n        label_files = [\n            os.path.join(root_dir, \"scene_class\", f\"{f}.txt\") for f in file_ids\n        ]\n        labels = []\n        for label_file in label_files:\n            with open(label_file, \"r\") as file:  # noqa: SIM115\n                labels.append(file.read().strip())\n        labels = [label.replace(\"_\", \" \") for label in labels]\n        labels = [\n            _LABELS.index(label) if label in _LABELS else len(_LABELS)  # type: ignore\n            for label in labels\n        ]\n\n        # remove the samples with classes not in _LABELS\n        # this is to follow the same classes used in ImageBind\n        if split == \"test\":\n            valid_indices = [\n                i\n                for i, label in enumerate(labels)\n                if label &lt; len(_LABELS)  # type: ignore\n            ]\n            rgb_files = [rgb_files[i] for i in valid_indices]\n            depth_files = [depth_files[i] for i in valid_indices]\n            labels = [labels[i] for i in valid_indices]\n            intrinsic_files = [intrinsic_files[i] for i in valid_indices]\n            sensor_types = [sensor_types[i] for i in valid_indices]\n\n        self.samples = list(\n            zip(\n                rgb_files,\n                depth_files,\n                labels,\n                intrinsic_files,\n                sensor_types,\n                strict=False,\n            )\n        )\n\n        self.rgb_transform = rgb_transform\n        self.depth_transform = depth_transform\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the length of the dataset.\"\"\"\n        return len(self.samples)\n\n    def _validate_args(\n        self,\n        root_dir: str,\n        split: str,\n        rgb_transform: Optional[Callable[[PILImage], torch.Tensor]],\n        depth_transform: Optional[Callable[[PILImage], torch.Tensor]],\n    ) -&gt; None:\n        \"\"\"Validate arguments.\"\"\"\n        if not os.path.isdir(root_dir):\n            raise NotADirectoryError(\n                f\"The given `root_dir` {root_dir} is not a directory\",\n            )\n        if split not in [\"train\", \"test\"]:\n            raise ValueError(\n                f\"Expected `split` to be one of `'train'` or `'test'`, but got {split}\",\n            )\n        if rgb_transform is not None and not callable(rgb_transform):\n            raise TypeError(\n                f\"Expected argument `rgb_transform` to be callable, but got {type(rgb_transform)}\",\n            )\n        if depth_transform is not None and not callable(depth_transform):\n            raise TypeError(\n                f\"Expected `depth_transform` to be callable, but got {type(depth_transform)}\",\n            )\n\n    def __getitem__(self, idx: int) -&gt; Example:\n        \"\"\"Return RGB and depth images at index `idx`.\"\"\"\n        # Read images\n        rgb_image = cv2.imread(self.samples[idx][0], cv2.IMREAD_UNCHANGED)\n        if self.rgb_transform is not None:\n            rgb_image = self.rgb_transform(to_pil_image(rgb_image))\n\n        if self.return_type == \"disparity\":\n            depth_image = convert_depth_to_disparity(\n                self.samples[idx][1],\n                self.samples[idx][3],\n                self.samples[idx][4],\n            )\n        else:\n            # Using cv2 instead of PIL Image since we use PNG grayscale images.\n            depth_image = cv2.imread(\n                self.samples[idx][1],\n                cv2.IMREAD_GRAYSCALE,\n            )\n            # Make a 3-channel depth image to enable passing to a pretrained ViT.\n            depth_image = np.repeat(depth_image[:, :, np.newaxis], 3, axis=-1)\n\n        if self.depth_transform is not None:\n            depth_image = self.depth_transform(to_pil_image(depth_image))\n\n        return Example(\n            {\n                Modalities.RGB.name: rgb_image,\n                Modalities.DEPTH.name: depth_image,\n                EXAMPLE_INDEX_KEY: idx,\n                Modalities.DEPTH.target: self.samples[idx][2],\n            }\n        )\n</code></pre>"},{"location":"api/#mmlearn.datasets.sunrgbd.SUNRGBDDataset.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the length of the dataset.</p> Source code in <code>mmlearn/datasets/sunrgbd.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the length of the dataset.\"\"\"\n    return len(self.samples)\n</code></pre>"},{"location":"api/#mmlearn.datasets.sunrgbd.SUNRGBDDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> <p>Return RGB and depth images at index <code>idx</code>.</p> Source code in <code>mmlearn/datasets/sunrgbd.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; Example:\n    \"\"\"Return RGB and depth images at index `idx`.\"\"\"\n    # Read images\n    rgb_image = cv2.imread(self.samples[idx][0], cv2.IMREAD_UNCHANGED)\n    if self.rgb_transform is not None:\n        rgb_image = self.rgb_transform(to_pil_image(rgb_image))\n\n    if self.return_type == \"disparity\":\n        depth_image = convert_depth_to_disparity(\n            self.samples[idx][1],\n            self.samples[idx][3],\n            self.samples[idx][4],\n        )\n    else:\n        # Using cv2 instead of PIL Image since we use PNG grayscale images.\n        depth_image = cv2.imread(\n            self.samples[idx][1],\n            cv2.IMREAD_GRAYSCALE,\n        )\n        # Make a 3-channel depth image to enable passing to a pretrained ViT.\n        depth_image = np.repeat(depth_image[:, :, np.newaxis], 3, axis=-1)\n\n    if self.depth_transform is not None:\n        depth_image = self.depth_transform(to_pil_image(depth_image))\n\n    return Example(\n        {\n            Modalities.RGB.name: rgb_image,\n            Modalities.DEPTH.name: depth_image,\n            EXAMPLE_INDEX_KEY: idx,\n            Modalities.DEPTH.target: self.samples[idx][2],\n        }\n    )\n</code></pre>"},{"location":"api/#mmlearn.datasets.sunrgbd.convert_depth_to_disparity","title":"convert_depth_to_disparity","text":"<pre><code>convert_depth_to_disparity(\n    depth_file,\n    intrinsics_file,\n    sensor_type,\n    min_depth=0.01,\n    max_depth=50,\n)\n</code></pre> <p>Load depth file and convert to disparity.</p> <p>Parameters:</p> Name Type Description Default <code>depth_file</code> <code>str</code> <p>Path to the depth file.</p> required <code>intrinsics_file</code> <code>str</code> <p>Intrinsics_file is a txt file supplied in SUNRGBD with sensor information Can be found at the path: os.path.join(root_dir, room_name, \"intrinsics.txt\")</p> required <code>sensor_type</code> <code>str</code> <p>Sensor type of the depth file.</p> required <code>min_depth</code> <code>float</code> <p>Minimum depth value to clip the depth image.</p> <code>0.01</code> <code>max_depth</code> <code>int</code> <p>Maximum depth value to clip the depth image.</p> <code>50</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Disparity image from the depth image following the ImageBind implementation.</p> Source code in <code>mmlearn/datasets/sunrgbd.py</code> <pre><code>def convert_depth_to_disparity(\n    depth_file: str,\n    intrinsics_file: str,\n    sensor_type: str,\n    min_depth: float = 0.01,\n    max_depth: int = 50,\n) -&gt; torch.Tensor:\n    \"\"\"Load depth file and convert to disparity.\n\n    Parameters\n    ----------\n    depth_file : str\n        Path to the depth file.\n    intrinsics_file : str\n        Intrinsics_file is a txt file supplied in SUNRGBD with sensor information\n        Can be found at the path: os.path.join(root_dir, room_name, \"intrinsics.txt\")\n    sensor_type : str\n        Sensor type of the depth file.\n    min_depth : float, default=0.01\n        Minimum depth value to clip the depth image.\n    max_depth : int, default=50\n        Maximum depth value to clip the depth image.\n\n    Returns\n    -------\n    torch.Tensor\n        Disparity image from the depth image following the ImageBind implementation.\n    \"\"\"\n    with open(intrinsics_file, \"r\") as fh:\n        lines = fh.readlines()\n        focal_length = float(lines[0].strip().split()[0])\n    baseline = sensor_to_params[sensor_type][\"baseline\"]\n    depth_image = np.array(PILImage.open(depth_file))\n    depth = np.array(depth_image).astype(np.float32)\n    depth_in_meters = depth / 1000.0\n    if min_depth is not None:\n        depth_in_meters = depth_in_meters.clip(min=min_depth, max=max_depth)\n    disparity = baseline * focal_length / depth_in_meters\n    return torch.from_numpy(disparity).float()\n</code></pre>"},{"location":"api/#core-datasets-components","title":"Core Datasets Components","text":""},{"location":"api/#mmlearn.datasets.core","title":"mmlearn.datasets.core","text":"<p>Modules for core dataloading functionality.</p>"},{"location":"api/#mmlearn.datasets.core.CombinedDataset","title":"CombinedDataset","text":"<p>               Bases: <code>Dataset[Example]</code></p> <p>Combine multiple datasets into one.</p> <p>This class is similar to class:<code>~torch.utils.data.ConcatDataset</code> but allows for combining iterable-style datasets with map-style datasets. The iterable-style datasets must implement the :meth:<code>__len__</code> method, which is used to determine the total length of the combined dataset. When an index is passed to the combined dataset, the dataset that contains the example at that index is determined and the example is retrieved from that dataset. Since iterable-style datasets do not support random access, the examples are retrieved sequentially from the iterable-style datasets. When the end of an iterable-style dataset is reached, the iterator is reset and the next example is retrieved from the beginning of the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>datasets</code> <code>Iterable[Union[Dataset, IterableDataset]]</code> <p>Iterable of datasets to combine.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the datasets in the input iterable are not instances of class:<code>~torch.utils.data.Dataset</code> or class:<code>~torch.utils.data.IterableDataset</code>.</p> <code>ValueError</code> <p>If the input iterable of datasets is empty.</p> Source code in <code>mmlearn/datasets/core/combined_dataset.py</code> <pre><code>class CombinedDataset(Dataset[Example]):\n    \"\"\"Combine multiple datasets into one.\n\n    This class is similar to :py:class:`~torch.utils.data.ConcatDataset` but allows\n    for combining iterable-style datasets with map-style datasets. The iterable-style\n    datasets must implement the :meth:`__len__` method, which is used to determine the\n    total length of the combined dataset. When an index is passed to the combined\n    dataset, the dataset that contains the example at that index is determined and\n    the example is retrieved from that dataset. Since iterable-style datasets do\n    not support random access, the examples are retrieved sequentially from the\n    iterable-style datasets. When the end of an iterable-style dataset is reached,\n    the iterator is reset and the next example is retrieved from the beginning of\n    the dataset.\n\n\n    Parameters\n    ----------\n    datasets : Iterable[Union[torch.utils.data.Dataset, torch.utils.data.IterableDataset]]\n        Iterable of datasets to combine.\n\n    Raises\n    ------\n    TypeError\n        If any of the datasets in the input iterable are not instances of\n        :py:class:`~torch.utils.data.Dataset` or :py:class:`~torch.utils.data.IterableDataset`.\n    ValueError\n        If the input iterable of datasets is empty.\n\n    \"\"\"  # noqa: W505\n\n    def __init__(\n        self, datasets: Iterable[Union[Dataset[Example], IterableDataset[Example]]]\n    ) -&gt; None:\n        self.datasets, _ = tree_flatten(datasets)\n        if not all(\n            isinstance(dataset, (Dataset, IterableDataset)) for dataset in self.datasets\n        ):\n            raise TypeError(\n                \"Expected argument `datasets` to be an iterable of `Dataset` or \"\n                f\"`IterableDataset` instances, but found: {self.datasets}\",\n            )\n        if len(self.datasets) == 0:\n            raise ValueError(\n                \"Expected a non-empty iterable of datasets but found an empty iterable\",\n            )\n\n        self._cumulative_sizes: list[int] = np.cumsum(\n            [len(dataset) for dataset in self.datasets]\n        ).tolist()\n        self._iterators: list[Iterator[Example]] = []\n        self._iter_dataset_mapping: dict[int, int] = {}\n\n        # create iterators for iterable datasets and map dataset index to iterator index\n        for idx, dataset in enumerate(self.datasets):\n            if isinstance(dataset, IterableDataset):\n                self._iterators.append(iter(dataset))\n                self._iter_dataset_mapping[idx] = len(self._iterators) - 1\n\n    def __getitem__(self, idx: int) -&gt; Example:\n        \"\"\"Return an example from the combined dataset.\"\"\"\n        if idx &lt; 0:  # handle negative indices\n            if -idx &gt; len(self):\n                raise IndexError(\n                    f\"Index {idx} is out of bounds for the combined dataset with \"\n                    f\"length {len(self)}\",\n                )\n            idx = len(self) + idx\n\n        dataset_idx = bisect.bisect_right(self._cumulative_sizes, idx)\n\n        curr_dataset = self.datasets[dataset_idx]\n        if isinstance(curr_dataset, IterableDataset):\n            iter_idx = self._iter_dataset_mapping[dataset_idx]\n            try:\n                example = next(self._iterators[iter_idx])\n            except StopIteration:\n                self._iterators[iter_idx] = iter(curr_dataset)\n                example = next(self._iterators[iter_idx])\n        else:\n            if dataset_idx == 0:\n                example_idx = idx\n            else:\n                example_idx = idx - self._cumulative_sizes[dataset_idx - 1]\n            example = curr_dataset[example_idx]\n\n        if not isinstance(example, Example):\n            raise TypeError(\n                \"Expected dataset examples to be instances of `Example` \"\n                f\"but found {type(example)}\",\n            )\n\n        if not hasattr(example, \"dataset_index\"):\n            example.dataset_index = dataset_idx\n        if not hasattr(example, \"example_ids\"):\n            example.create_ids()\n\n        return example\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the total number of examples in the combined dataset.\"\"\"\n        return self._cumulative_sizes[-1]\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.CombinedDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> <p>Return an example from the combined dataset.</p> Source code in <code>mmlearn/datasets/core/combined_dataset.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; Example:\n    \"\"\"Return an example from the combined dataset.\"\"\"\n    if idx &lt; 0:  # handle negative indices\n        if -idx &gt; len(self):\n            raise IndexError(\n                f\"Index {idx} is out of bounds for the combined dataset with \"\n                f\"length {len(self)}\",\n            )\n        idx = len(self) + idx\n\n    dataset_idx = bisect.bisect_right(self._cumulative_sizes, idx)\n\n    curr_dataset = self.datasets[dataset_idx]\n    if isinstance(curr_dataset, IterableDataset):\n        iter_idx = self._iter_dataset_mapping[dataset_idx]\n        try:\n            example = next(self._iterators[iter_idx])\n        except StopIteration:\n            self._iterators[iter_idx] = iter(curr_dataset)\n            example = next(self._iterators[iter_idx])\n    else:\n        if dataset_idx == 0:\n            example_idx = idx\n        else:\n            example_idx = idx - self._cumulative_sizes[dataset_idx - 1]\n        example = curr_dataset[example_idx]\n\n    if not isinstance(example, Example):\n        raise TypeError(\n            \"Expected dataset examples to be instances of `Example` \"\n            f\"but found {type(example)}\",\n        )\n\n    if not hasattr(example, \"dataset_index\"):\n        example.dataset_index = dataset_idx\n    if not hasattr(example, \"example_ids\"):\n        example.create_ids()\n\n    return example\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.CombinedDataset.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the total number of examples in the combined dataset.</p> Source code in <code>mmlearn/datasets/core/combined_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the total number of examples in the combined dataset.\"\"\"\n    return self._cumulative_sizes[-1]\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.DefaultDataCollator","title":"DefaultDataCollator  <code>dataclass</code>","text":"<p>Default data collator for batching examples.</p> <p>This data collator will collate a list of class:<code>~mmlearn.datasets.core.example.Example</code> objects into a batch. It can also apply processing functions to specified keys in the batch before returning it.</p> <p>Parameters:</p> Name Type Description Default <code>batch_processors</code> <code>Optional[dict[str, Callable[[Any], Any]]]</code> <p>Dictionary of callables to apply to the batch before returning it.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the batch processor for a key does not return a dictionary with the key in it.</p> Source code in <code>mmlearn/datasets/core/data_collator.py</code> <pre><code>@dataclass\nclass DefaultDataCollator:\n    \"\"\"Default data collator for batching examples.\n\n    This data collator will collate a list of :py:class:`~mmlearn.datasets.core.example.Example`\n    objects into a batch. It can also apply processing functions to specified keys\n    in the batch before returning it.\n\n    Parameters\n    ----------\n    batch_processors : Optional[dict[str, Callable[[Any], Any]]], optional, default=None\n        Dictionary of callables to apply to the batch before returning it.\n\n    Raises\n    ------\n    ValueError\n        If the batch processor for a key does not return a dictionary with the\n        key in it.\n    \"\"\"  # noqa: W505\n\n    #: Dictionary of callables to apply to the batch before returning it.\n    #: The key is the name of the key in the batch, and the value is the processing\n    #: function to apply to the key. The processing function must take a single\n    #: argument and return a single value. If the processing function returns\n    #: a dictionary, it must contain the key that was processed in it (all the\n    #: other keys will also be included in the batch).\n    batch_processors: Optional[dict[str, Callable[[Any], Any]]] = None\n\n    def __call__(self, examples: list[Example]) -&gt; dict[str, Any]:\n        \"\"\"Collate a list of `Example` objects and apply processing functions.\"\"\"\n        batch = collate_example_list(examples)\n\n        if self.batch_processors is not None:\n            for key, processor in self.batch_processors.items():\n                batch_key: str = key\n                if Modalities.has_modality(key):\n                    batch_key = Modalities.get_modality(key).name\n\n                if batch_key in batch:\n                    batch_processed = processor(batch[batch_key])\n                    if isinstance(batch_processed, Mapping):\n                        if batch_key not in batch_processed:\n                            raise ValueError(\n                                f\"Batch processor for '{key}' key must return a dictionary \"\n                                f\"with '{batch_key}' in it.\"\n                            )\n                        batch.update(batch_processed)\n                    else:\n                        batch[batch_key] = batch_processed\n\n        return batch\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.DefaultDataCollator.__call__","title":"__call__","text":"<pre><code>__call__(examples)\n</code></pre> <p>Collate a list of <code>Example</code> objects and apply processing functions.</p> Source code in <code>mmlearn/datasets/core/data_collator.py</code> <pre><code>def __call__(self, examples: list[Example]) -&gt; dict[str, Any]:\n    \"\"\"Collate a list of `Example` objects and apply processing functions.\"\"\"\n    batch = collate_example_list(examples)\n\n    if self.batch_processors is not None:\n        for key, processor in self.batch_processors.items():\n            batch_key: str = key\n            if Modalities.has_modality(key):\n                batch_key = Modalities.get_modality(key).name\n\n            if batch_key in batch:\n                batch_processed = processor(batch[batch_key])\n                if isinstance(batch_processed, Mapping):\n                    if batch_key not in batch_processed:\n                        raise ValueError(\n                            f\"Batch processor for '{key}' key must return a dictionary \"\n                            f\"with '{batch_key}' in it.\"\n                        )\n                    batch.update(batch_processed)\n                else:\n                    batch[batch_key] = batch_processed\n\n    return batch\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.Example","title":"Example","text":"<p>               Bases: <code>OrderedDict[Any, Any]</code></p> <p>A representation of a single example from a dataset.</p> <p>This class is a subclass of class:<code>~collections.OrderedDict</code> and provides attribute-style access. This means that <code>example[\"text\"]</code> and <code>example.text</code> are equivalent. All datasets in this library return examples as class:<code>~mmlearn.datasets.core.example.Example</code> objects.</p> <p>Parameters:</p> Name Type Description Default <code>init_dict</code> <code>Optional[MutableMapping[Hashable, Any]]</code> <p>Dictionary to init <code>Example</code> class with.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; example = Example({\"text\": torch.tensor(2)})\n&gt;&gt;&gt; example.text.zero_()\ntensor(0)\n&gt;&gt;&gt; example.context = torch.tensor(4)  # set custom attributes after initialization\n</code></pre> Source code in <code>mmlearn/datasets/core/example.py</code> <pre><code>class Example(OrderedDict[Any, Any]):\n    \"\"\"A representation of a single example from a dataset.\n\n    This class is a subclass of :py:class:`~collections.OrderedDict` and provides\n    attribute-style access. This means that `example[\"text\"]` and `example.text`\n    are equivalent. All datasets in this library return examples as\n    :py:class:`~mmlearn.datasets.core.example.Example` objects.\n\n\n    Parameters\n    ----------\n    init_dict : Optional[MutableMapping[Hashable, Any]], optional, default=None\n        Dictionary to init `Example` class with.\n\n    Examples\n    --------\n    &gt;&gt;&gt; example = Example({\"text\": torch.tensor(2)})\n    &gt;&gt;&gt; example.text.zero_()\n    tensor(0)\n    &gt;&gt;&gt; example.context = torch.tensor(4)  # set custom attributes after initialization\n    \"\"\"\n\n    def __init__(\n        self,\n        init_dict: Optional[MutableMapping[Hashable, Any]] = None,\n    ) -&gt; None:\n        if init_dict is None:\n            init_dict = {}\n        super().__init__(init_dict)\n\n    def create_ids(self) -&gt; None:\n        \"\"\"Create a unique id for the example from the dataset and example index.\n\n        This method combines the dataset index and example index to create an\n        attribute called `example_ids`, which is a dictionary of tensors. The\n        dictionary keys are all the keys in the example except for `example_ids`,\n        `example_index`, and `dataset_index`. The values are tensors of shape `(2,)`\n        containing the tuple `(dataset_index, example_index)` for each key.\n        The `example_ids` is used to (re-)identify pairs of examples from different\n        modalities after they have been combined into a batch.\n\n        Warns\n        -----\n        UserWarning\n            If the `example_index` and `dataset_index` attributes are not set.\n\n        Notes\n        -----\n        - The Example must have the following attributes set before calling this\n          this method: `example_index` (usually set/returned by the dataset) and\n          `dataset_index` (usually set by the :py:class:`~mmlearn.datasets.core.combined_dataset.CombinedDataset` object)\n        - The :py:func:`~mmlearn.datasets.core.example.find_matching_indices`\n          function can be used to find matching examples given two tensors of example ids.\n\n        \"\"\"  # noqa: W505\n        if hasattr(self, \"example_index\") and hasattr(self, \"dataset_index\"):\n            self.example_ids = {\n                key: torch.tensor([self.dataset_index, self.example_index])\n                for key in self.keys()\n                if key not in (\"example_ids\", \"example_index\", \"dataset_index\")\n            }\n        else:\n            rank_zero_warn(\n                \"Cannot create `example_ids` without `example_index` and `dataset_index` \"\n                \"attributes. Set these attributes before calling `create_ids`. \"\n                \"No `example_ids` was created.\",\n                stacklevel=2,\n                category=UserWarning,\n            )\n\n    def __getattr__(self, key: str) -&gt; Any:\n        \"\"\"Get attribute by key.\"\"\"\n        try:\n            return self[key]\n        except KeyError:\n            raise AttributeError(key) from None\n\n    def __setattr__(self, key: str, value: Any) -&gt; None:\n        \"\"\"Set attribute by key.\"\"\"\n        if isinstance(value, MutableMapping):\n            value = Example(value)\n        self[key] = value\n\n    def __setitem__(self, key: Hashable, value: Any) -&gt; None:\n        \"\"\"Set item by key.\"\"\"\n        if isinstance(value, MutableMapping):\n            value = Example(value)\n        super().__setitem__(key, value)\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.Example.create_ids","title":"create_ids","text":"<pre><code>create_ids()\n</code></pre> <p>Create a unique id for the example from the dataset and example index.</p> <p>This method combines the dataset index and example index to create an attribute called <code>example_ids</code>, which is a dictionary of tensors. The dictionary keys are all the keys in the example except for <code>example_ids</code>, <code>example_index</code>, and <code>dataset_index</code>. The values are tensors of shape <code>(2,)</code> containing the tuple <code>(dataset_index, example_index)</code> for each key. The <code>example_ids</code> is used to (re-)identify pairs of examples from different modalities after they have been combined into a batch.</p> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If the <code>example_index</code> and <code>dataset_index</code> attributes are not set.</p> Notes <ul> <li>The Example must have the following attributes set before calling this   this method: <code>example_index</code> (usually set/returned by the dataset) and   <code>dataset_index</code> (usually set by the class:<code>~mmlearn.datasets.core.combined_dataset.CombinedDataset</code> object)</li> <li>The func:<code>~mmlearn.datasets.core.example.find_matching_indices</code>   function can be used to find matching examples given two tensors of example ids.</li> </ul> Source code in <code>mmlearn/datasets/core/example.py</code> <pre><code>def create_ids(self) -&gt; None:\n    \"\"\"Create a unique id for the example from the dataset and example index.\n\n    This method combines the dataset index and example index to create an\n    attribute called `example_ids`, which is a dictionary of tensors. The\n    dictionary keys are all the keys in the example except for `example_ids`,\n    `example_index`, and `dataset_index`. The values are tensors of shape `(2,)`\n    containing the tuple `(dataset_index, example_index)` for each key.\n    The `example_ids` is used to (re-)identify pairs of examples from different\n    modalities after they have been combined into a batch.\n\n    Warns\n    -----\n    UserWarning\n        If the `example_index` and `dataset_index` attributes are not set.\n\n    Notes\n    -----\n    - The Example must have the following attributes set before calling this\n      this method: `example_index` (usually set/returned by the dataset) and\n      `dataset_index` (usually set by the :py:class:`~mmlearn.datasets.core.combined_dataset.CombinedDataset` object)\n    - The :py:func:`~mmlearn.datasets.core.example.find_matching_indices`\n      function can be used to find matching examples given two tensors of example ids.\n\n    \"\"\"  # noqa: W505\n    if hasattr(self, \"example_index\") and hasattr(self, \"dataset_index\"):\n        self.example_ids = {\n            key: torch.tensor([self.dataset_index, self.example_index])\n            for key in self.keys()\n            if key not in (\"example_ids\", \"example_index\", \"dataset_index\")\n        }\n    else:\n        rank_zero_warn(\n            \"Cannot create `example_ids` without `example_index` and `dataset_index` \"\n            \"attributes. Set these attributes before calling `create_ids`. \"\n            \"No `example_ids` was created.\",\n            stacklevel=2,\n            category=UserWarning,\n        )\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.Example.__getattr__","title":"__getattr__","text":"<pre><code>__getattr__(key)\n</code></pre> <p>Get attribute by key.</p> Source code in <code>mmlearn/datasets/core/example.py</code> <pre><code>def __getattr__(self, key: str) -&gt; Any:\n    \"\"\"Get attribute by key.\"\"\"\n    try:\n        return self[key]\n    except KeyError:\n        raise AttributeError(key) from None\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.Example.__setattr__","title":"__setattr__","text":"<pre><code>__setattr__(key, value)\n</code></pre> <p>Set attribute by key.</p> Source code in <code>mmlearn/datasets/core/example.py</code> <pre><code>def __setattr__(self, key: str, value: Any) -&gt; None:\n    \"\"\"Set attribute by key.\"\"\"\n    if isinstance(value, MutableMapping):\n        value = Example(value)\n    self[key] = value\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.Example.__setitem__","title":"__setitem__","text":"<pre><code>__setitem__(key, value)\n</code></pre> <p>Set item by key.</p> Source code in <code>mmlearn/datasets/core/example.py</code> <pre><code>def __setitem__(self, key: Hashable, value: Any) -&gt; None:\n    \"\"\"Set item by key.\"\"\"\n    if isinstance(value, MutableMapping):\n        value = Example(value)\n    super().__setitem__(key, value)\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.CombinedDatasetRatioSampler","title":"CombinedDatasetRatioSampler","text":"<p>               Bases: <code>Sampler[int]</code></p> <p>Sampler for weighted sampling from a class:<code>~mmlearn.datasets.core.combined_dataset.CombinedDataset</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>CombinedDataset</code> <p>An instance of class:<code>~mmlearn.datasets.core.combined_dataset.CombinedDataset</code> to sample from.</p> required <code>ratios</code> <code>Optional[Sequence[float]]</code> <p>A sequence of ratios for sampling from each dataset in the combined dataset. The length of the sequence must be equal to the number of datasets in the combined dataset (<code>dataset</code>). If <code>None</code>, the length of each dataset in the combined dataset is used as the ratio. The ratios are normalized to sum to 1.</p> <code>None</code> <code>num_samples</code> <code>Optional[int]</code> <p>The number of samples to draw from the combined dataset. If <code>None</code>, the sampler will draw as many samples as there are in the combined dataset. This number must yield at least one sample per dataset in the combined dataset, when multiplied by the corresponding ratio.</p> <code>None</code> <code>replacement</code> <code>bool</code> <p>Whether to sample with replacement or not.</p> <code>False</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the sampled indices or not. If <code>False</code>, the indices of each dataset will appear in the order they are stored in the combined dataset. This is similar to sequential sampling from each dataset. The datasets that make up the combined dataset are still sampled randomly.</p> <code>True</code> <code>rank</code> <code>Optional[int]</code> <p>Rank of the current process within :attr:<code>num_replicas</code>. By default, :attr:<code>rank</code> is retrieved from the current distributed group.</p> <code>None</code> <code>num_replicas</code> <code>Optional[int]</code> <p>Number of processes participating in distributed training. By default, :attr:<code>num_replicas</code> is retrieved from the current distributed group.</p> <code>None</code> <code>drop_last</code> <code>bool</code> <p>Whether to drop the last incomplete batch or not. If <code>True</code>, the sampler will drop samples to make the number of samples evenly divisible by the number of replicas in distributed mode.</p> <code>False</code> <code>seed</code> <code>int</code> <p>Random seed used to when sampling from the combined dataset and shuffling the sampled indices.</p> <code>0</code> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>CombinedDataset</code> <p>The dataset to sample from.</p> <code>num_samples</code> <code>int</code> <p>The number of samples to draw from the combined dataset.</p> <code>probs</code> <code>Tensor</code> <p>The probabilities for sampling from each dataset in the combined dataset. This is computed from the <code>ratios</code> argument and is normalized to sum to 1.</p> <code>replacement</code> <code>bool</code> <p>Whether to sample with replacement or not.</p> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the sampled indices or not.</p> <code>rank</code> <code>int</code> <p>Rank of the current process within :attr:<code>num_replicas</code>.</p> <code>num_replicas</code> <code>int</code> <p>Number of processes participating in distributed training.</p> <code>drop_last</code> <code>bool</code> <p>Whether to drop samples to make the number of samples evenly divisible by the number of replicas in distributed mode.</p> <code>seed</code> <code>int</code> <p>Random seed used to when sampling from the combined dataset and shuffling the sampled indices.</p> <code>epoch</code> <code>int</code> <p>Current epoch number. This is used to set the random seed. This is useful in distributed mode to ensure that each process receives a different random ordering of the samples.</p> <code>total_size</code> <code>int</code> <p>The total number of samples across all processes.</p> Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>@store(group=\"dataloader/sampler\", provider=\"mmlearn\")\nclass CombinedDatasetRatioSampler(Sampler[int]):\n    \"\"\"Sampler for weighted sampling from a :py:class:`~mmlearn.datasets.core.combined_dataset.CombinedDataset`.\n\n    Parameters\n    ----------\n    dataset : CombinedDataset\n        An instance of :py:class:`~mmlearn.datasets.core.combined_dataset.CombinedDataset`\n        to sample from.\n    ratios : Optional[Sequence[float]], optional, default=None\n        A sequence of ratios for sampling from each dataset in the combined dataset.\n        The length of the sequence must be equal to the number of datasets in the\n        combined dataset (`dataset`). If `None`, the length of each dataset in the\n        combined dataset is used as the ratio. The ratios are normalized to sum to 1.\n    num_samples : Optional[int], optional, default=None\n        The number of samples to draw from the combined dataset. If `None`, the\n        sampler will draw as many samples as there are in the combined dataset.\n        This number must yield at least one sample per dataset in the combined\n        dataset, when multiplied by the corresponding ratio.\n    replacement : bool, default=False\n        Whether to sample with replacement or not.\n    shuffle : bool, default=True\n        Whether to shuffle the sampled indices or not. If `False`, the indices of\n        each dataset will appear in the order they are stored in the combined dataset.\n        This is similar to sequential sampling from each dataset. The datasets\n        that make up the combined dataset are still sampled randomly.\n    rank : Optional[int], optional, default=None\n        Rank of the current process within :attr:`num_replicas`. By default,\n        :attr:`rank` is retrieved from the current distributed group.\n    num_replicas : Optional[int], optional, default=None\n        Number of processes participating in distributed training. By\n        default, :attr:`num_replicas` is retrieved from the current distributed group.\n    drop_last : bool, default=False\n        Whether to drop the last incomplete batch or not. If `True`, the sampler will\n        drop samples to make the number of samples evenly divisible by the number of\n        replicas in distributed mode.\n    seed : int, default=0\n        Random seed used to when sampling from the combined dataset and shuffling\n        the sampled indices.\n\n    Attributes\n    ----------\n    dataset : CombinedDataset\n        The dataset to sample from.\n    num_samples : int\n        The number of samples to draw from the combined dataset.\n    probs : torch.Tensor\n        The probabilities for sampling from each dataset in the combined dataset.\n        This is computed from the `ratios` argument and is normalized to sum to 1.\n    replacement : bool\n        Whether to sample with replacement or not.\n    shuffle : bool\n        Whether to shuffle the sampled indices or not.\n    rank : int\n        Rank of the current process within :attr:`num_replicas`.\n    num_replicas : int\n        Number of processes participating in distributed training.\n    drop_last : bool\n        Whether to drop samples to make the number of samples evenly divisible by the\n        number of replicas in distributed mode.\n    seed : int\n        Random seed used to when sampling from the combined dataset and shuffling\n        the sampled indices.\n    epoch : int\n        Current epoch number. This is used to set the random seed. This is useful\n        in distributed mode to ensure that each process receives a different random\n        ordering of the samples.\n    total_size : int\n        The total number of samples across all processes.\n    \"\"\"  # noqa: W505\n\n    def __init__(  # noqa: PLR0912\n        self,\n        dataset: CombinedDataset,\n        ratios: Optional[Sequence[float]] = None,\n        num_samples: Optional[int] = None,\n        replacement: bool = False,\n        shuffle: bool = True,\n        rank: Optional[int] = None,\n        num_replicas: Optional[int] = None,\n        drop_last: bool = False,\n        seed: int = 0,\n    ):\n        if not isinstance(dataset, CombinedDataset):\n            raise TypeError(\n                \"Expected argument `dataset` to be of type `CombinedDataset`, \"\n                f\"but got {type(dataset)}.\",\n            )\n        if not isinstance(seed, int):\n            raise TypeError(\n                f\"Expected argument `seed` to be an integer, but got {type(seed)}.\",\n            )\n        if num_replicas is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            num_replicas = dist.get_world_size()\n        if rank is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            rank = dist.get_rank()\n        if rank &gt;= num_replicas or rank &lt; 0:\n            raise ValueError(\n                f\"Invalid rank {rank}, rank should be in the interval [0, {num_replicas - 1}]\"\n            )\n\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.drop_last = drop_last\n        self.replacement = replacement\n        self.shuffle = shuffle\n        self.seed = seed\n        self.epoch = 0\n\n        self._num_samples = num_samples\n        if not isinstance(self.num_samples, int) or self.num_samples &lt;= 0:\n            raise ValueError(\n                \"Expected argument `num_samples` to be a positive integer, but got \"\n                f\"{self.num_samples}.\",\n            )\n\n        if ratios is None:\n            ratios = [len(subset) for subset in self.dataset.datasets]\n\n        num_datasets = len(self.dataset.datasets)\n        if len(ratios) != num_datasets:\n            raise ValueError(\n                f\"Expected argument `ratios` to be of length {num_datasets}, \"\n                f\"but got length {len(ratios)}.\",\n            )\n        prob_sum = sum(ratios)\n        if not all(ratio &gt;= 0 for ratio in ratios) and prob_sum &gt; 0:\n            raise ValueError(\n                \"Expected argument `ratios` to be a sequence of non-negative numbers. \"\n                f\"Got {ratios}.\",\n            )\n        self.probs = torch.tensor(\n            [ratio / prob_sum for ratio in ratios],\n            dtype=torch.double,\n        )\n        if any((prob * self.num_samples) &lt;= 0 for prob in self.probs):\n            raise ValueError(\n                \"Expected dataset ratio to result in at least one sample per dataset. \"\n                f\"Got dataset sizes {self.probs * self.num_samples}.\",\n            )\n\n    @property\n    def num_samples(self) -&gt; int:\n        \"\"\"Return the number of samples managed by the sampler.\"\"\"\n        # dataset size might change at runtime\n        if self._num_samples is None:\n            num_samples = len(self.dataset)\n        else:\n            num_samples = self._num_samples\n\n        if self.drop_last and num_samples % self.num_replicas != 0:\n            # split to nearest available length that is evenly divisible.\n            # This is to ensure each rank receives the same amount of data when\n            # using this Sampler.\n            num_samples = math.ceil(\n                (num_samples - self.num_replicas) / self.num_replicas,\n            )\n        else:\n            num_samples = math.ceil(num_samples / self.num_replicas)\n        return num_samples\n\n    @property\n    def total_size(self) -&gt; int:\n        \"\"\"Return the total size of the dataset.\"\"\"\n        return self.num_samples * self.num_replicas\n\n    def __iter__(self) -&gt; Iterator[int]:\n        \"\"\"Return an iterator that yields sample indices for the combined dataset.\"\"\"\n        generator = torch.Generator()\n        seed = self.seed + self.epoch\n        generator.manual_seed(seed)\n\n        cumulative_sizes = [0] + self.dataset._cumulative_sizes\n        num_samples_per_dataset = [int(prob * self.total_size) for prob in self.probs]\n        indices = []\n        for i in range(len(self.dataset.datasets)):\n            per_dataset_indices: torch.Tensor = torch.multinomial(\n                torch.ones(cumulative_sizes[i + 1] - cumulative_sizes[i]),\n                num_samples_per_dataset[i],\n                replacement=self.replacement,\n                generator=generator,\n            )\n            # adjust indices to reflect position in cumulative dataset\n            per_dataset_indices += cumulative_sizes[i]\n            assert per_dataset_indices.max() &lt; cumulative_sizes[i + 1], (\n                f\"Indices from dataset {i} exceed dataset size. \"\n                f\"Got indices {per_dataset_indices} and dataset size {cumulative_sizes[i + 1]}.\",\n            )\n            indices.append(per_dataset_indices)\n\n        indices = torch.cat(indices)\n        if self.shuffle:\n            rand_indices = torch.randperm(len(indices), generator=generator)\n            indices = indices[rand_indices]\n\n        indices = indices.tolist()  # type: ignore[attr-defined]\n        num_indices = len(indices)\n\n        if num_indices &lt; self.total_size:\n            padding_size = self.total_size - num_indices\n            if padding_size &lt;= num_indices:\n                indices += indices[:padding_size]\n            else:\n                indices += (indices * math.ceil(padding_size / num_indices))[\n                    :padding_size\n                ]\n        elif num_indices &gt; self.total_size:\n            indices = indices[: self.total_size]\n        assert len(indices) == self.total_size\n\n        # subsample\n        indices = indices[self.rank : self.total_size : self.num_replicas]\n        assert len(indices) == self.num_samples, (\n            f\"Expected {self.num_samples} samples, but got {len(indices)}.\",\n        )\n\n        yield from iter(indices)\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the total number of samples in the sampler.\"\"\"\n        return self.num_samples\n\n    def set_epoch(self, epoch: int) -&gt; None:\n        \"\"\"Set the epoch for this sampler.\n\n        When :attr:`shuffle=True`, this ensures all replicas use a different random\n        ordering for each epoch. Otherwise, the next iteration of this sampler\n        will yield the same ordering.\n\n        Parameters\n        ----------\n        epoch : int\n            Epoch number.\n\n        \"\"\"\n        self.epoch = epoch\n\n        # some iterable datasets (especially huggingface iterable datasets) might\n        # require setting the epoch to ensure shuffling works properly\n        for dataset in self.dataset.datasets:\n            if hasattr(dataset, \"set_epoch\"):\n                dataset.set_epoch(epoch)\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.CombinedDatasetRatioSampler.num_samples","title":"num_samples  <code>property</code>","text":"<pre><code>num_samples\n</code></pre> <p>Return the number of samples managed by the sampler.</p>"},{"location":"api/#mmlearn.datasets.core.CombinedDatasetRatioSampler.total_size","title":"total_size  <code>property</code>","text":"<pre><code>total_size\n</code></pre> <p>Return the total size of the dataset.</p>"},{"location":"api/#mmlearn.datasets.core.CombinedDatasetRatioSampler.__iter__","title":"__iter__","text":"<pre><code>__iter__()\n</code></pre> <p>Return an iterator that yields sample indices for the combined dataset.</p> Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>def __iter__(self) -&gt; Iterator[int]:\n    \"\"\"Return an iterator that yields sample indices for the combined dataset.\"\"\"\n    generator = torch.Generator()\n    seed = self.seed + self.epoch\n    generator.manual_seed(seed)\n\n    cumulative_sizes = [0] + self.dataset._cumulative_sizes\n    num_samples_per_dataset = [int(prob * self.total_size) for prob in self.probs]\n    indices = []\n    for i in range(len(self.dataset.datasets)):\n        per_dataset_indices: torch.Tensor = torch.multinomial(\n            torch.ones(cumulative_sizes[i + 1] - cumulative_sizes[i]),\n            num_samples_per_dataset[i],\n            replacement=self.replacement,\n            generator=generator,\n        )\n        # adjust indices to reflect position in cumulative dataset\n        per_dataset_indices += cumulative_sizes[i]\n        assert per_dataset_indices.max() &lt; cumulative_sizes[i + 1], (\n            f\"Indices from dataset {i} exceed dataset size. \"\n            f\"Got indices {per_dataset_indices} and dataset size {cumulative_sizes[i + 1]}.\",\n        )\n        indices.append(per_dataset_indices)\n\n    indices = torch.cat(indices)\n    if self.shuffle:\n        rand_indices = torch.randperm(len(indices), generator=generator)\n        indices = indices[rand_indices]\n\n    indices = indices.tolist()  # type: ignore[attr-defined]\n    num_indices = len(indices)\n\n    if num_indices &lt; self.total_size:\n        padding_size = self.total_size - num_indices\n        if padding_size &lt;= num_indices:\n            indices += indices[:padding_size]\n        else:\n            indices += (indices * math.ceil(padding_size / num_indices))[\n                :padding_size\n            ]\n    elif num_indices &gt; self.total_size:\n        indices = indices[: self.total_size]\n    assert len(indices) == self.total_size\n\n    # subsample\n    indices = indices[self.rank : self.total_size : self.num_replicas]\n    assert len(indices) == self.num_samples, (\n        f\"Expected {self.num_samples} samples, but got {len(indices)}.\",\n    )\n\n    yield from iter(indices)\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.CombinedDatasetRatioSampler.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the total number of samples in the sampler.</p> Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the total number of samples in the sampler.\"\"\"\n    return self.num_samples\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.CombinedDatasetRatioSampler.set_epoch","title":"set_epoch","text":"<pre><code>set_epoch(epoch)\n</code></pre> <p>Set the epoch for this sampler.</p> <p>When :attr:<code>shuffle=True</code>, this ensures all replicas use a different random ordering for each epoch. Otherwise, the next iteration of this sampler will yield the same ordering.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>Epoch number.</p> required Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>def set_epoch(self, epoch: int) -&gt; None:\n    \"\"\"Set the epoch for this sampler.\n\n    When :attr:`shuffle=True`, this ensures all replicas use a different random\n    ordering for each epoch. Otherwise, the next iteration of this sampler\n    will yield the same ordering.\n\n    Parameters\n    ----------\n    epoch : int\n        Epoch number.\n\n    \"\"\"\n    self.epoch = epoch\n\n    # some iterable datasets (especially huggingface iterable datasets) might\n    # require setting the epoch to ensure shuffling works properly\n    for dataset in self.dataset.datasets:\n        if hasattr(dataset, \"set_epoch\"):\n            dataset.set_epoch(epoch)\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.DistributedEvalSampler","title":"DistributedEvalSampler","text":"<p>               Bases: <code>Sampler[int]</code></p> <p>Sampler for distributed evaluation.</p> <p>The main differences between this and class:<code>torch.utils.data.DistributedSampler</code> are that this sampler does not add extra samples to make it evenly divisible and shuffling is disabled by default.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>Dataset used for sampling.</p> required <code>num_replicas</code> <code>Optional[int]</code> <p>Number of processes participating in distributed training. By default, :attr:<code>rank</code> is retrieved from the current distributed group.</p> <code>None</code> <code>rank</code> <code>Optional[int]</code> <p>Rank of the current process within :attr:<code>num_replicas</code>. By default, :attr:<code>rank</code> is retrieved from the current distributed group.</p> <code>None</code> <code>shuffle</code> <code>bool</code> <p>If <code>True</code> (default), sampler will shuffle the indices.</p> <code>False</code> <code>seed</code> <code>int</code> <p>Random seed used to shuffle the sampler if :attr:<code>shuffle=True</code>. This number should be identical across all processes in the distributed group.</p> <code>0</code> Warnings <p>DistributedEvalSampler should NOT be used for training. The distributed processes could hang forever. See [1]_ for details</p> Notes <ul> <li>This sampler is for evaluation purpose where synchronization does not happen   every epoch. Synchronization should be done outside the dataloader loop.   It is especially useful in conjunction with   class:<code>torch.nn.parallel.DistributedDataParallel</code> [2]_.</li> <li>The input Dataset is assumed to be of constant size.</li> <li>This implementation is adapted from [3]_.</li> </ul> References <p>.. [1] https://github.com/pytorch/pytorch/issues/22584 .. [2] https://discuss.pytorch.org/t/how-to-validate-in-distributeddataparallel-correctly/94267/11 .. [3] https://github.com/SeungjunNah/DeepDeblur-PyTorch/blob/master/src/data/sampler.py</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; def example():\n...     start_epoch, n_epochs = 0, 2\n...     sampler = DistributedEvalSampler(dataset) if is_distributed else None\n...     loader = DataLoader(dataset, shuffle=(sampler is None), sampler=sampler)\n...     for epoch in range(start_epoch, n_epochs):\n...         if is_distributed:\n...             sampler.set_epoch(epoch)\n...         evaluate(loader)\n</code></pre> Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>@store(group=\"dataloader/sampler\", provider=\"mmlearn\")\nclass DistributedEvalSampler(Sampler[int]):\n    \"\"\"Sampler for distributed evaluation.\n\n    The main differences between this and :py:class:`torch.utils.data.DistributedSampler`\n    are that this sampler does not add extra samples to make it evenly divisible and\n    shuffling is disabled by default.\n\n    Parameters\n    ----------\n    dataset : torch.utils.data.Dataset\n        Dataset used for sampling.\n    num_replicas : Optional[int], optional, default=None\n        Number of processes participating in distributed training. By\n        default, :attr:`rank` is retrieved from the current distributed group.\n    rank : Optional[int], optional, default=None\n        Rank of the current process within :attr:`num_replicas`. By default,\n        :attr:`rank` is retrieved from the current distributed group.\n    shuffle : bool, optional, default=False\n        If `True` (default), sampler will shuffle the indices.\n    seed : int, optional, default=0\n        Random seed used to shuffle the sampler if :attr:`shuffle=True`.\n        This number should be identical across all processes in the\n        distributed group.\n\n    Warnings\n    --------\n    DistributedEvalSampler should NOT be used for training. The distributed processes\n    could hang forever. See [1]_ for details\n\n    Notes\n    -----\n    - This sampler is for evaluation purpose where synchronization does not happen\n      every epoch. Synchronization should be done outside the dataloader loop.\n      It is especially useful in conjunction with\n      :py:class:`torch.nn.parallel.DistributedDataParallel` [2]_.\n    - The input Dataset is assumed to be of constant size.\n    - This implementation is adapted from [3]_.\n\n    References\n    ----------\n    .. [1] https://github.com/pytorch/pytorch/issues/22584\n    .. [2] https://discuss.pytorch.org/t/how-to-validate-in-distributeddataparallel-correctly/94267/11\n    .. [3] https://github.com/SeungjunNah/DeepDeblur-PyTorch/blob/master/src/data/sampler.py\n\n\n    Examples\n    --------\n    &gt;&gt;&gt; def example():\n    ...     start_epoch, n_epochs = 0, 2\n    ...     sampler = DistributedEvalSampler(dataset) if is_distributed else None\n    ...     loader = DataLoader(dataset, shuffle=(sampler is None), sampler=sampler)\n    ...     for epoch in range(start_epoch, n_epochs):\n    ...         if is_distributed:\n    ...             sampler.set_epoch(epoch)\n    ...         evaluate(loader)\n\n    \"\"\"  # noqa: W505\n\n    def __init__(\n        self,\n        dataset: Dataset[Sized],\n        num_replicas: Optional[int] = None,\n        rank: Optional[int] = None,\n        shuffle: bool = False,\n        seed: int = 0,\n    ) -&gt; None:\n        if num_replicas is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            num_replicas = dist.get_world_size()\n        if rank is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            rank = dist.get_rank()\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.epoch = 0\n        self.shuffle = shuffle\n        self.seed = seed\n\n    @property\n    def total_size(self) -&gt; int:\n        \"\"\"Return the total size of the dataset.\"\"\"\n        return len(self.dataset)\n\n    @property\n    def num_samples(self) -&gt; int:\n        \"\"\"Return the number of samples managed by the sampler.\"\"\"\n        indices = list(range(self.total_size))[\n            self.rank : self.total_size : self.num_replicas\n        ]\n        return len(indices)  # true value without extra samples\n\n    def __iter__(self) -&gt; Iterator[int]:\n        \"\"\"Return an iterator that iterates over the indices of the dataset.\"\"\"\n        if self.shuffle:\n            # deterministically shuffle based on epoch and seed\n            g = torch.Generator()\n            g.manual_seed(self.seed + self.epoch)\n            indices = torch.randperm(self.total_size, generator=g).tolist()\n        else:\n            indices = list(range(self.total_size))\n\n        # subsample\n        indices = indices[self.rank : self.total_size : self.num_replicas]\n        assert len(indices) == self.num_samples\n\n        return iter(indices)\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the number of samples.\"\"\"\n        return self.num_samples\n\n    def set_epoch(self, epoch: int) -&gt; None:\n        \"\"\"Set the epoch for this sampler.\n\n        When :attr:`shuffle=True`, this ensures all replicas use a different random\n        ordering for each epoch. Otherwise, the next iteration of this sampler\n        will yield the same ordering.\n\n        Parameters\n        ----------\n        epoch : int\n            Epoch number.\n\n        \"\"\"\n        self.epoch = epoch\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.DistributedEvalSampler.total_size","title":"total_size  <code>property</code>","text":"<pre><code>total_size\n</code></pre> <p>Return the total size of the dataset.</p>"},{"location":"api/#mmlearn.datasets.core.DistributedEvalSampler.num_samples","title":"num_samples  <code>property</code>","text":"<pre><code>num_samples\n</code></pre> <p>Return the number of samples managed by the sampler.</p>"},{"location":"api/#mmlearn.datasets.core.DistributedEvalSampler.__iter__","title":"__iter__","text":"<pre><code>__iter__()\n</code></pre> <p>Return an iterator that iterates over the indices of the dataset.</p> Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>def __iter__(self) -&gt; Iterator[int]:\n    \"\"\"Return an iterator that iterates over the indices of the dataset.\"\"\"\n    if self.shuffle:\n        # deterministically shuffle based on epoch and seed\n        g = torch.Generator()\n        g.manual_seed(self.seed + self.epoch)\n        indices = torch.randperm(self.total_size, generator=g).tolist()\n    else:\n        indices = list(range(self.total_size))\n\n    # subsample\n    indices = indices[self.rank : self.total_size : self.num_replicas]\n    assert len(indices) == self.num_samples\n\n    return iter(indices)\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.DistributedEvalSampler.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the number of samples.</p> Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the number of samples.\"\"\"\n    return self.num_samples\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.DistributedEvalSampler.set_epoch","title":"set_epoch","text":"<pre><code>set_epoch(epoch)\n</code></pre> <p>Set the epoch for this sampler.</p> <p>When :attr:<code>shuffle=True</code>, this ensures all replicas use a different random ordering for each epoch. Otherwise, the next iteration of this sampler will yield the same ordering.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>Epoch number.</p> required Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>def set_epoch(self, epoch: int) -&gt; None:\n    \"\"\"Set the epoch for this sampler.\n\n    When :attr:`shuffle=True`, this ensures all replicas use a different random\n    ordering for each epoch. Otherwise, the next iteration of this sampler\n    will yield the same ordering.\n\n    Parameters\n    ----------\n    epoch : int\n        Epoch number.\n\n    \"\"\"\n    self.epoch = epoch\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.find_matching_indices","title":"find_matching_indices","text":"<pre><code>find_matching_indices(\n    first_example_ids, second_example_ids\n)\n</code></pre> <p>Find the indices of matching examples given two tensors of example ids.</p> <p>Matching examples are defined as examples with the same value in both tensors. This method is useful for finding pairs of examples from different modalities that are related to each other in a batch.</p> <p>Parameters:</p> Name Type Description Default <code>first_example_ids</code> <code>Tensor</code> <p>A tensor of example ids of shape <code>(N, 2)</code>, where <code>N</code> is the number of examples.</p> required <code>second_example_ids</code> <code>Tensor</code> <p>A tensor of example ids of shape <code>(M, 2)</code>, where <code>M</code> is the number of examples.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>A tuple of tensors containing the indices of matching examples in the first and second tensor, respectively.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If either <code>first_example_ids</code> or <code>second_example_ids</code> is not a tensor.</p> <code>ValueError</code> <p>If either <code>first_example_ids</code> or <code>second_example_ids</code> is not a 2D tensor with the second dimension having a size of <code>2</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; img_example_ids = torch.tensor([(0, 0), (0, 1), (1, 0), (1, 1)])\n&gt;&gt;&gt; text_example_ids = torch.tensor([(1, 0), (1, 1), (2, 0), (2, 1), (2, 2)])\n&gt;&gt;&gt; find_matching_indices(img_example_ids, text_example_ids)\n(tensor([2, 3]), tensor([0, 1]))\n</code></pre> Source code in <code>mmlearn/datasets/core/example.py</code> <pre><code>def find_matching_indices(\n    first_example_ids: torch.Tensor, second_example_ids: torch.Tensor\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Find the indices of matching examples given two tensors of example ids.\n\n    Matching examples are defined as examples with the same value in both tensors.\n    This method is useful for finding pairs of examples from different modalities\n    that are related to each other in a batch.\n\n    Parameters\n    ----------\n    first_example_ids : torch.Tensor\n        A tensor of example ids of shape `(N, 2)`, where `N` is the number of examples.\n    second_example_ids : torch.Tensor\n        A tensor of example ids of shape `(M, 2)`, where `M` is the number of examples.\n\n    Returns\n    -------\n    tuple[torch.Tensor, torch.Tensor]\n        A tuple of tensors containing the indices of matching examples in the first and\n        second tensor, respectively.\n\n    Raises\n    ------\n    TypeError\n        If either `first_example_ids` or `second_example_ids` is not a tensor.\n    ValueError\n        If either `first_example_ids` or `second_example_ids` is not a 2D tensor\n        with the second dimension having a size of `2`.\n\n    Examples\n    --------\n    &gt;&gt;&gt; img_example_ids = torch.tensor([(0, 0), (0, 1), (1, 0), (1, 1)])\n    &gt;&gt;&gt; text_example_ids = torch.tensor([(1, 0), (1, 1), (2, 0), (2, 1), (2, 2)])\n    &gt;&gt;&gt; find_matching_indices(img_example_ids, text_example_ids)\n    (tensor([2, 3]), tensor([0, 1]))\n\n\n    \"\"\"\n    if not isinstance(first_example_ids, torch.Tensor) or not isinstance(\n        second_example_ids,\n        torch.Tensor,\n    ):\n        raise TypeError(\n            f\"Expected inputs to be tensors, but got {type(first_example_ids)} \"\n            f\"and {type(second_example_ids)}.\",\n        )\n    val = 2\n    if not (first_example_ids.ndim == val and first_example_ids.shape[1] == val):\n        raise ValueError(\n            \"Expected argument `first_example_ids` to be a tensor of shape (N, 2), \"\n            f\"but got shape {first_example_ids.shape}.\",\n        )\n    if not (second_example_ids.ndim == val and second_example_ids.shape[1] == val):\n        raise ValueError(\n            \"Expected argument `second_example_ids` to be a tensor of shape (N, 2), \"\n            f\"but got shape {second_example_ids.shape}.\",\n        )\n\n    first_example_ids = first_example_ids.unsqueeze(1)  # shape=(N, 1, 2)\n    second_example_ids = second_example_ids.unsqueeze(0)  # shape=(1, M, 2)\n\n    # compare all elements; results in a shape (N, M) tensor\n    matches = torch.all(first_example_ids == second_example_ids, dim=-1)\n    first_indices, second_indices = torch.where(matches)\n    return first_indices, second_indices\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.combined_dataset","title":"combined_dataset","text":"<p>Wrapper for combining multiple datasets into one.</p>"},{"location":"api/#mmlearn.datasets.core.combined_dataset.CombinedDataset","title":"CombinedDataset","text":"<p>               Bases: <code>Dataset[Example]</code></p> <p>Combine multiple datasets into one.</p> <p>This class is similar to class:<code>~torch.utils.data.ConcatDataset</code> but allows for combining iterable-style datasets with map-style datasets. The iterable-style datasets must implement the :meth:<code>__len__</code> method, which is used to determine the total length of the combined dataset. When an index is passed to the combined dataset, the dataset that contains the example at that index is determined and the example is retrieved from that dataset. Since iterable-style datasets do not support random access, the examples are retrieved sequentially from the iterable-style datasets. When the end of an iterable-style dataset is reached, the iterator is reset and the next example is retrieved from the beginning of the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>datasets</code> <code>Iterable[Union[Dataset, IterableDataset]]</code> <p>Iterable of datasets to combine.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the datasets in the input iterable are not instances of class:<code>~torch.utils.data.Dataset</code> or class:<code>~torch.utils.data.IterableDataset</code>.</p> <code>ValueError</code> <p>If the input iterable of datasets is empty.</p> Source code in <code>mmlearn/datasets/core/combined_dataset.py</code> <pre><code>class CombinedDataset(Dataset[Example]):\n    \"\"\"Combine multiple datasets into one.\n\n    This class is similar to :py:class:`~torch.utils.data.ConcatDataset` but allows\n    for combining iterable-style datasets with map-style datasets. The iterable-style\n    datasets must implement the :meth:`__len__` method, which is used to determine the\n    total length of the combined dataset. When an index is passed to the combined\n    dataset, the dataset that contains the example at that index is determined and\n    the example is retrieved from that dataset. Since iterable-style datasets do\n    not support random access, the examples are retrieved sequentially from the\n    iterable-style datasets. When the end of an iterable-style dataset is reached,\n    the iterator is reset and the next example is retrieved from the beginning of\n    the dataset.\n\n\n    Parameters\n    ----------\n    datasets : Iterable[Union[torch.utils.data.Dataset, torch.utils.data.IterableDataset]]\n        Iterable of datasets to combine.\n\n    Raises\n    ------\n    TypeError\n        If any of the datasets in the input iterable are not instances of\n        :py:class:`~torch.utils.data.Dataset` or :py:class:`~torch.utils.data.IterableDataset`.\n    ValueError\n        If the input iterable of datasets is empty.\n\n    \"\"\"  # noqa: W505\n\n    def __init__(\n        self, datasets: Iterable[Union[Dataset[Example], IterableDataset[Example]]]\n    ) -&gt; None:\n        self.datasets, _ = tree_flatten(datasets)\n        if not all(\n            isinstance(dataset, (Dataset, IterableDataset)) for dataset in self.datasets\n        ):\n            raise TypeError(\n                \"Expected argument `datasets` to be an iterable of `Dataset` or \"\n                f\"`IterableDataset` instances, but found: {self.datasets}\",\n            )\n        if len(self.datasets) == 0:\n            raise ValueError(\n                \"Expected a non-empty iterable of datasets but found an empty iterable\",\n            )\n\n        self._cumulative_sizes: list[int] = np.cumsum(\n            [len(dataset) for dataset in self.datasets]\n        ).tolist()\n        self._iterators: list[Iterator[Example]] = []\n        self._iter_dataset_mapping: dict[int, int] = {}\n\n        # create iterators for iterable datasets and map dataset index to iterator index\n        for idx, dataset in enumerate(self.datasets):\n            if isinstance(dataset, IterableDataset):\n                self._iterators.append(iter(dataset))\n                self._iter_dataset_mapping[idx] = len(self._iterators) - 1\n\n    def __getitem__(self, idx: int) -&gt; Example:\n        \"\"\"Return an example from the combined dataset.\"\"\"\n        if idx &lt; 0:  # handle negative indices\n            if -idx &gt; len(self):\n                raise IndexError(\n                    f\"Index {idx} is out of bounds for the combined dataset with \"\n                    f\"length {len(self)}\",\n                )\n            idx = len(self) + idx\n\n        dataset_idx = bisect.bisect_right(self._cumulative_sizes, idx)\n\n        curr_dataset = self.datasets[dataset_idx]\n        if isinstance(curr_dataset, IterableDataset):\n            iter_idx = self._iter_dataset_mapping[dataset_idx]\n            try:\n                example = next(self._iterators[iter_idx])\n            except StopIteration:\n                self._iterators[iter_idx] = iter(curr_dataset)\n                example = next(self._iterators[iter_idx])\n        else:\n            if dataset_idx == 0:\n                example_idx = idx\n            else:\n                example_idx = idx - self._cumulative_sizes[dataset_idx - 1]\n            example = curr_dataset[example_idx]\n\n        if not isinstance(example, Example):\n            raise TypeError(\n                \"Expected dataset examples to be instances of `Example` \"\n                f\"but found {type(example)}\",\n            )\n\n        if not hasattr(example, \"dataset_index\"):\n            example.dataset_index = dataset_idx\n        if not hasattr(example, \"example_ids\"):\n            example.create_ids()\n\n        return example\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the total number of examples in the combined dataset.\"\"\"\n        return self._cumulative_sizes[-1]\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.combined_dataset.CombinedDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> <p>Return an example from the combined dataset.</p> Source code in <code>mmlearn/datasets/core/combined_dataset.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; Example:\n    \"\"\"Return an example from the combined dataset.\"\"\"\n    if idx &lt; 0:  # handle negative indices\n        if -idx &gt; len(self):\n            raise IndexError(\n                f\"Index {idx} is out of bounds for the combined dataset with \"\n                f\"length {len(self)}\",\n            )\n        idx = len(self) + idx\n\n    dataset_idx = bisect.bisect_right(self._cumulative_sizes, idx)\n\n    curr_dataset = self.datasets[dataset_idx]\n    if isinstance(curr_dataset, IterableDataset):\n        iter_idx = self._iter_dataset_mapping[dataset_idx]\n        try:\n            example = next(self._iterators[iter_idx])\n        except StopIteration:\n            self._iterators[iter_idx] = iter(curr_dataset)\n            example = next(self._iterators[iter_idx])\n    else:\n        if dataset_idx == 0:\n            example_idx = idx\n        else:\n            example_idx = idx - self._cumulative_sizes[dataset_idx - 1]\n        example = curr_dataset[example_idx]\n\n    if not isinstance(example, Example):\n        raise TypeError(\n            \"Expected dataset examples to be instances of `Example` \"\n            f\"but found {type(example)}\",\n        )\n\n    if not hasattr(example, \"dataset_index\"):\n        example.dataset_index = dataset_idx\n    if not hasattr(example, \"example_ids\"):\n        example.create_ids()\n\n    return example\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.combined_dataset.CombinedDataset.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the total number of examples in the combined dataset.</p> Source code in <code>mmlearn/datasets/core/combined_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the total number of examples in the combined dataset.\"\"\"\n    return self._cumulative_sizes[-1]\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.data_collator","title":"data_collator","text":"<p>Data collators for batching examples.</p>"},{"location":"api/#mmlearn.datasets.core.data_collator.DefaultDataCollator","title":"DefaultDataCollator  <code>dataclass</code>","text":"<p>Default data collator for batching examples.</p> <p>This data collator will collate a list of class:<code>~mmlearn.datasets.core.example.Example</code> objects into a batch. It can also apply processing functions to specified keys in the batch before returning it.</p> <p>Parameters:</p> Name Type Description Default <code>batch_processors</code> <code>Optional[dict[str, Callable[[Any], Any]]]</code> <p>Dictionary of callables to apply to the batch before returning it.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the batch processor for a key does not return a dictionary with the key in it.</p> Source code in <code>mmlearn/datasets/core/data_collator.py</code> <pre><code>@dataclass\nclass DefaultDataCollator:\n    \"\"\"Default data collator for batching examples.\n\n    This data collator will collate a list of :py:class:`~mmlearn.datasets.core.example.Example`\n    objects into a batch. It can also apply processing functions to specified keys\n    in the batch before returning it.\n\n    Parameters\n    ----------\n    batch_processors : Optional[dict[str, Callable[[Any], Any]]], optional, default=None\n        Dictionary of callables to apply to the batch before returning it.\n\n    Raises\n    ------\n    ValueError\n        If the batch processor for a key does not return a dictionary with the\n        key in it.\n    \"\"\"  # noqa: W505\n\n    #: Dictionary of callables to apply to the batch before returning it.\n    #: The key is the name of the key in the batch, and the value is the processing\n    #: function to apply to the key. The processing function must take a single\n    #: argument and return a single value. If the processing function returns\n    #: a dictionary, it must contain the key that was processed in it (all the\n    #: other keys will also be included in the batch).\n    batch_processors: Optional[dict[str, Callable[[Any], Any]]] = None\n\n    def __call__(self, examples: list[Example]) -&gt; dict[str, Any]:\n        \"\"\"Collate a list of `Example` objects and apply processing functions.\"\"\"\n        batch = collate_example_list(examples)\n\n        if self.batch_processors is not None:\n            for key, processor in self.batch_processors.items():\n                batch_key: str = key\n                if Modalities.has_modality(key):\n                    batch_key = Modalities.get_modality(key).name\n\n                if batch_key in batch:\n                    batch_processed = processor(batch[batch_key])\n                    if isinstance(batch_processed, Mapping):\n                        if batch_key not in batch_processed:\n                            raise ValueError(\n                                f\"Batch processor for '{key}' key must return a dictionary \"\n                                f\"with '{batch_key}' in it.\"\n                            )\n                        batch.update(batch_processed)\n                    else:\n                        batch[batch_key] = batch_processed\n\n        return batch\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.data_collator.DefaultDataCollator.__call__","title":"__call__","text":"<pre><code>__call__(examples)\n</code></pre> <p>Collate a list of <code>Example</code> objects and apply processing functions.</p> Source code in <code>mmlearn/datasets/core/data_collator.py</code> <pre><code>def __call__(self, examples: list[Example]) -&gt; dict[str, Any]:\n    \"\"\"Collate a list of `Example` objects and apply processing functions.\"\"\"\n    batch = collate_example_list(examples)\n\n    if self.batch_processors is not None:\n        for key, processor in self.batch_processors.items():\n            batch_key: str = key\n            if Modalities.has_modality(key):\n                batch_key = Modalities.get_modality(key).name\n\n            if batch_key in batch:\n                batch_processed = processor(batch[batch_key])\n                if isinstance(batch_processed, Mapping):\n                    if batch_key not in batch_processed:\n                        raise ValueError(\n                            f\"Batch processor for '{key}' key must return a dictionary \"\n                            f\"with '{batch_key}' in it.\"\n                        )\n                    batch.update(batch_processed)\n                else:\n                    batch[batch_key] = batch_processed\n\n    return batch\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.data_collator.collate_example_list","title":"collate_example_list","text":"<pre><code>collate_example_list(examples)\n</code></pre> <p>Collate a list of class:<code>~mmlearn.datasets.core.example.Example</code> objects into a batch.</p> <p>Parameters:</p> Name Type Description Default <code>examples</code> <code>list[Example]</code> <p>list of examples to collate.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary of batched examples.</p> Source code in <code>mmlearn/datasets/core/data_collator.py</code> <pre><code>def collate_example_list(examples: list[Example]) -&gt; dict[str, Any]:\n    \"\"\"Collate a list of :py:class:`~mmlearn.datasets.core.example.Example` objects into a batch.\n\n    Parameters\n    ----------\n    examples : list[Example]\n        list of examples to collate.\n\n    Returns\n    -------\n    dict[str, Any]\n        Dictionary of batched examples.\n\n    \"\"\"  # noqa: W505\n    return _collate_example_dict(_merge_examples(examples))\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.example","title":"example","text":"<p>Module for example-related classes and functions.</p>"},{"location":"api/#mmlearn.datasets.core.example.Example","title":"Example","text":"<p>               Bases: <code>OrderedDict[Any, Any]</code></p> <p>A representation of a single example from a dataset.</p> <p>This class is a subclass of class:<code>~collections.OrderedDict</code> and provides attribute-style access. This means that <code>example[\"text\"]</code> and <code>example.text</code> are equivalent. All datasets in this library return examples as class:<code>~mmlearn.datasets.core.example.Example</code> objects.</p> <p>Parameters:</p> Name Type Description Default <code>init_dict</code> <code>Optional[MutableMapping[Hashable, Any]]</code> <p>Dictionary to init <code>Example</code> class with.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; example = Example({\"text\": torch.tensor(2)})\n&gt;&gt;&gt; example.text.zero_()\ntensor(0)\n&gt;&gt;&gt; example.context = torch.tensor(4)  # set custom attributes after initialization\n</code></pre> Source code in <code>mmlearn/datasets/core/example.py</code> <pre><code>class Example(OrderedDict[Any, Any]):\n    \"\"\"A representation of a single example from a dataset.\n\n    This class is a subclass of :py:class:`~collections.OrderedDict` and provides\n    attribute-style access. This means that `example[\"text\"]` and `example.text`\n    are equivalent. All datasets in this library return examples as\n    :py:class:`~mmlearn.datasets.core.example.Example` objects.\n\n\n    Parameters\n    ----------\n    init_dict : Optional[MutableMapping[Hashable, Any]], optional, default=None\n        Dictionary to init `Example` class with.\n\n    Examples\n    --------\n    &gt;&gt;&gt; example = Example({\"text\": torch.tensor(2)})\n    &gt;&gt;&gt; example.text.zero_()\n    tensor(0)\n    &gt;&gt;&gt; example.context = torch.tensor(4)  # set custom attributes after initialization\n    \"\"\"\n\n    def __init__(\n        self,\n        init_dict: Optional[MutableMapping[Hashable, Any]] = None,\n    ) -&gt; None:\n        if init_dict is None:\n            init_dict = {}\n        super().__init__(init_dict)\n\n    def create_ids(self) -&gt; None:\n        \"\"\"Create a unique id for the example from the dataset and example index.\n\n        This method combines the dataset index and example index to create an\n        attribute called `example_ids`, which is a dictionary of tensors. The\n        dictionary keys are all the keys in the example except for `example_ids`,\n        `example_index`, and `dataset_index`. The values are tensors of shape `(2,)`\n        containing the tuple `(dataset_index, example_index)` for each key.\n        The `example_ids` is used to (re-)identify pairs of examples from different\n        modalities after they have been combined into a batch.\n\n        Warns\n        -----\n        UserWarning\n            If the `example_index` and `dataset_index` attributes are not set.\n\n        Notes\n        -----\n        - The Example must have the following attributes set before calling this\n          this method: `example_index` (usually set/returned by the dataset) and\n          `dataset_index` (usually set by the :py:class:`~mmlearn.datasets.core.combined_dataset.CombinedDataset` object)\n        - The :py:func:`~mmlearn.datasets.core.example.find_matching_indices`\n          function can be used to find matching examples given two tensors of example ids.\n\n        \"\"\"  # noqa: W505\n        if hasattr(self, \"example_index\") and hasattr(self, \"dataset_index\"):\n            self.example_ids = {\n                key: torch.tensor([self.dataset_index, self.example_index])\n                for key in self.keys()\n                if key not in (\"example_ids\", \"example_index\", \"dataset_index\")\n            }\n        else:\n            rank_zero_warn(\n                \"Cannot create `example_ids` without `example_index` and `dataset_index` \"\n                \"attributes. Set these attributes before calling `create_ids`. \"\n                \"No `example_ids` was created.\",\n                stacklevel=2,\n                category=UserWarning,\n            )\n\n    def __getattr__(self, key: str) -&gt; Any:\n        \"\"\"Get attribute by key.\"\"\"\n        try:\n            return self[key]\n        except KeyError:\n            raise AttributeError(key) from None\n\n    def __setattr__(self, key: str, value: Any) -&gt; None:\n        \"\"\"Set attribute by key.\"\"\"\n        if isinstance(value, MutableMapping):\n            value = Example(value)\n        self[key] = value\n\n    def __setitem__(self, key: Hashable, value: Any) -&gt; None:\n        \"\"\"Set item by key.\"\"\"\n        if isinstance(value, MutableMapping):\n            value = Example(value)\n        super().__setitem__(key, value)\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.example.Example.create_ids","title":"create_ids","text":"<pre><code>create_ids()\n</code></pre> <p>Create a unique id for the example from the dataset and example index.</p> <p>This method combines the dataset index and example index to create an attribute called <code>example_ids</code>, which is a dictionary of tensors. The dictionary keys are all the keys in the example except for <code>example_ids</code>, <code>example_index</code>, and <code>dataset_index</code>. The values are tensors of shape <code>(2,)</code> containing the tuple <code>(dataset_index, example_index)</code> for each key. The <code>example_ids</code> is used to (re-)identify pairs of examples from different modalities after they have been combined into a batch.</p> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If the <code>example_index</code> and <code>dataset_index</code> attributes are not set.</p> Notes <ul> <li>The Example must have the following attributes set before calling this   this method: <code>example_index</code> (usually set/returned by the dataset) and   <code>dataset_index</code> (usually set by the class:<code>~mmlearn.datasets.core.combined_dataset.CombinedDataset</code> object)</li> <li>The func:<code>~mmlearn.datasets.core.example.find_matching_indices</code>   function can be used to find matching examples given two tensors of example ids.</li> </ul> Source code in <code>mmlearn/datasets/core/example.py</code> <pre><code>def create_ids(self) -&gt; None:\n    \"\"\"Create a unique id for the example from the dataset and example index.\n\n    This method combines the dataset index and example index to create an\n    attribute called `example_ids`, which is a dictionary of tensors. The\n    dictionary keys are all the keys in the example except for `example_ids`,\n    `example_index`, and `dataset_index`. The values are tensors of shape `(2,)`\n    containing the tuple `(dataset_index, example_index)` for each key.\n    The `example_ids` is used to (re-)identify pairs of examples from different\n    modalities after they have been combined into a batch.\n\n    Warns\n    -----\n    UserWarning\n        If the `example_index` and `dataset_index` attributes are not set.\n\n    Notes\n    -----\n    - The Example must have the following attributes set before calling this\n      this method: `example_index` (usually set/returned by the dataset) and\n      `dataset_index` (usually set by the :py:class:`~mmlearn.datasets.core.combined_dataset.CombinedDataset` object)\n    - The :py:func:`~mmlearn.datasets.core.example.find_matching_indices`\n      function can be used to find matching examples given two tensors of example ids.\n\n    \"\"\"  # noqa: W505\n    if hasattr(self, \"example_index\") and hasattr(self, \"dataset_index\"):\n        self.example_ids = {\n            key: torch.tensor([self.dataset_index, self.example_index])\n            for key in self.keys()\n            if key not in (\"example_ids\", \"example_index\", \"dataset_index\")\n        }\n    else:\n        rank_zero_warn(\n            \"Cannot create `example_ids` without `example_index` and `dataset_index` \"\n            \"attributes. Set these attributes before calling `create_ids`. \"\n            \"No `example_ids` was created.\",\n            stacklevel=2,\n            category=UserWarning,\n        )\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.example.Example.__getattr__","title":"__getattr__","text":"<pre><code>__getattr__(key)\n</code></pre> <p>Get attribute by key.</p> Source code in <code>mmlearn/datasets/core/example.py</code> <pre><code>def __getattr__(self, key: str) -&gt; Any:\n    \"\"\"Get attribute by key.\"\"\"\n    try:\n        return self[key]\n    except KeyError:\n        raise AttributeError(key) from None\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.example.Example.__setattr__","title":"__setattr__","text":"<pre><code>__setattr__(key, value)\n</code></pre> <p>Set attribute by key.</p> Source code in <code>mmlearn/datasets/core/example.py</code> <pre><code>def __setattr__(self, key: str, value: Any) -&gt; None:\n    \"\"\"Set attribute by key.\"\"\"\n    if isinstance(value, MutableMapping):\n        value = Example(value)\n    self[key] = value\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.example.Example.__setitem__","title":"__setitem__","text":"<pre><code>__setitem__(key, value)\n</code></pre> <p>Set item by key.</p> Source code in <code>mmlearn/datasets/core/example.py</code> <pre><code>def __setitem__(self, key: Hashable, value: Any) -&gt; None:\n    \"\"\"Set item by key.\"\"\"\n    if isinstance(value, MutableMapping):\n        value = Example(value)\n    super().__setitem__(key, value)\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.example.find_matching_indices","title":"find_matching_indices","text":"<pre><code>find_matching_indices(\n    first_example_ids, second_example_ids\n)\n</code></pre> <p>Find the indices of matching examples given two tensors of example ids.</p> <p>Matching examples are defined as examples with the same value in both tensors. This method is useful for finding pairs of examples from different modalities that are related to each other in a batch.</p> <p>Parameters:</p> Name Type Description Default <code>first_example_ids</code> <code>Tensor</code> <p>A tensor of example ids of shape <code>(N, 2)</code>, where <code>N</code> is the number of examples.</p> required <code>second_example_ids</code> <code>Tensor</code> <p>A tensor of example ids of shape <code>(M, 2)</code>, where <code>M</code> is the number of examples.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>A tuple of tensors containing the indices of matching examples in the first and second tensor, respectively.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If either <code>first_example_ids</code> or <code>second_example_ids</code> is not a tensor.</p> <code>ValueError</code> <p>If either <code>first_example_ids</code> or <code>second_example_ids</code> is not a 2D tensor with the second dimension having a size of <code>2</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; img_example_ids = torch.tensor([(0, 0), (0, 1), (1, 0), (1, 1)])\n&gt;&gt;&gt; text_example_ids = torch.tensor([(1, 0), (1, 1), (2, 0), (2, 1), (2, 2)])\n&gt;&gt;&gt; find_matching_indices(img_example_ids, text_example_ids)\n(tensor([2, 3]), tensor([0, 1]))\n</code></pre> Source code in <code>mmlearn/datasets/core/example.py</code> <pre><code>def find_matching_indices(\n    first_example_ids: torch.Tensor, second_example_ids: torch.Tensor\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Find the indices of matching examples given two tensors of example ids.\n\n    Matching examples are defined as examples with the same value in both tensors.\n    This method is useful for finding pairs of examples from different modalities\n    that are related to each other in a batch.\n\n    Parameters\n    ----------\n    first_example_ids : torch.Tensor\n        A tensor of example ids of shape `(N, 2)`, where `N` is the number of examples.\n    second_example_ids : torch.Tensor\n        A tensor of example ids of shape `(M, 2)`, where `M` is the number of examples.\n\n    Returns\n    -------\n    tuple[torch.Tensor, torch.Tensor]\n        A tuple of tensors containing the indices of matching examples in the first and\n        second tensor, respectively.\n\n    Raises\n    ------\n    TypeError\n        If either `first_example_ids` or `second_example_ids` is not a tensor.\n    ValueError\n        If either `first_example_ids` or `second_example_ids` is not a 2D tensor\n        with the second dimension having a size of `2`.\n\n    Examples\n    --------\n    &gt;&gt;&gt; img_example_ids = torch.tensor([(0, 0), (0, 1), (1, 0), (1, 1)])\n    &gt;&gt;&gt; text_example_ids = torch.tensor([(1, 0), (1, 1), (2, 0), (2, 1), (2, 2)])\n    &gt;&gt;&gt; find_matching_indices(img_example_ids, text_example_ids)\n    (tensor([2, 3]), tensor([0, 1]))\n\n\n    \"\"\"\n    if not isinstance(first_example_ids, torch.Tensor) or not isinstance(\n        second_example_ids,\n        torch.Tensor,\n    ):\n        raise TypeError(\n            f\"Expected inputs to be tensors, but got {type(first_example_ids)} \"\n            f\"and {type(second_example_ids)}.\",\n        )\n    val = 2\n    if not (first_example_ids.ndim == val and first_example_ids.shape[1] == val):\n        raise ValueError(\n            \"Expected argument `first_example_ids` to be a tensor of shape (N, 2), \"\n            f\"but got shape {first_example_ids.shape}.\",\n        )\n    if not (second_example_ids.ndim == val and second_example_ids.shape[1] == val):\n        raise ValueError(\n            \"Expected argument `second_example_ids` to be a tensor of shape (N, 2), \"\n            f\"but got shape {second_example_ids.shape}.\",\n        )\n\n    first_example_ids = first_example_ids.unsqueeze(1)  # shape=(N, 1, 2)\n    second_example_ids = second_example_ids.unsqueeze(0)  # shape=(1, M, 2)\n\n    # compare all elements; results in a shape (N, M) tensor\n    matches = torch.all(first_example_ids == second_example_ids, dim=-1)\n    first_indices, second_indices = torch.where(matches)\n    return first_indices, second_indices\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.modalities","title":"modalities","text":"<p>Module for managing supported modalities in the library.</p>"},{"location":"api/#mmlearn.datasets.core.modalities.Modality","title":"Modality  <code>dataclass</code>","text":"<p>A representation of a modality in the library.</p> <p>This class is used to represent a modality in the library. It contains the name of the modality and the properties that can be associated with it. The properties are dynamically generated based on the name of the modality and can be accessed as attributes of the class.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the modality.</p> required <code>modality_specific_properties</code> <code>Optional[dict[str, str]]</code> <p>Additional properties specific to the modality, by default None</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the property already exists for the modality or if the format string is invalid.</p> Source code in <code>mmlearn/datasets/core/modalities.py</code> <pre><code>@dataclass\nclass Modality:\n    \"\"\"A representation of a modality in the library.\n\n    This class is used to represent a modality in the library. It contains the name of\n    the modality and the properties that can be associated with it. The properties are\n    dynamically generated based on the name of the modality and can be accessed as\n    attributes of the class.\n\n    Parameters\n    ----------\n    name : str\n        The name of the modality.\n    modality_specific_properties : Optional[dict[str, str]], optional, default=None\n        Additional properties specific to the modality, by default None\n\n    Raises\n    ------\n    ValueError\n        If the property already exists for the modality or if the format string is\n        invalid.\n    \"\"\"\n\n    #: The name of the modality.\n    name: str\n\n    #: Target/label associated with the modality. This will return ``name_target``.\n    target: str = field(init=False, repr=False)\n\n    #: Attention mask associated with the modality. This will return\n    # ``name_attention_mask``.\n    attention_mask: str = field(init=False, repr=False)\n\n    #: Input mask associated with the modality. This will return ``name_mask``.\n    mask: str = field(init=False, repr=False)\n\n    #: Embedding associated with the modality. This will return ``name_embedding``.\n    embedding: str = field(init=False, repr=False)\n\n    #: Masked embedding associated with the modality. This will return\n    # ``name_masked_embedding``.\n    masked_embedding: str = field(init=False, repr=False)\n\n    #: Embedding from an Exponential Moving Average (EMA) encoder associated with\n    #: the modality.\n    ema_embedding: str = field(init=False, repr=False)\n\n    #: Other properties specific to the modality.\n    modality_specific_properties: Optional[dict[str, str]] = field(\n        default=None, repr=False\n    )\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Initialize the modality with the name and properties.\"\"\"\n        self.name = self.name.lower()\n        self._properties = {}\n\n        for field_name in self.__dataclass_fields__:\n            if field_name not in (\"name\", \"modality_specific_properties\"):\n                field_value = f\"{self.name}_{field_name}\"\n                self._properties[field_name] = field_value\n                setattr(self, field_name, field_value)\n\n        if self.modality_specific_properties is not None:\n            for (\n                property_name,\n                format_string,\n            ) in self.modality_specific_properties.items():\n                self.add_property(property_name, format_string)\n\n    @property\n    def properties(self) -&gt; dict[str, str]:\n        \"\"\"Return the properties associated with the modality.\"\"\"\n        return self._properties\n\n    def add_property(self, name: str, format_string: str) -&gt; None:\n        \"\"\"Add a new property to the modality.\n\n        Parameters\n        ----------\n        name : str\n            The name of the property.\n        format_string : str\n            The format string for the property. The format string should contain a\n            placeholder that will be replaced with the name of the modality when the\n            property is accessed.\n\n        Warns\n        -----\n        UserWarning\n            If the property already exists for the modality. It will overwrite the\n            existing property.\n\n        Raises\n        ------\n        ValueError\n            If `format_string` is invalid. A valid format string contains at least one\n            placeholder enclosed in curly braces.\n        \"\"\"\n        if name in self._properties:\n            warnings.warn(\n                f\"Property '{name}' already exists for modality '{super().__str__()}'.\"\n                \"Will overwrite the existing property.\",\n                category=UserWarning,\n                stacklevel=2,\n            )\n\n        if not _is_format_string(format_string):\n            raise ValueError(\n                f\"Invalid format string '{format_string}' for property \"\n                f\"'{name}' of modality '{super().__str__()}'.\"\n            )\n\n        self._properties[name] = format_string.format(self.name)\n        setattr(self, name, self._properties[name])\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the object as a string.\"\"\"\n        return self.name.lower()\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.modalities.Modality.properties","title":"properties  <code>property</code>","text":"<pre><code>properties\n</code></pre> <p>Return the properties associated with the modality.</p>"},{"location":"api/#mmlearn.datasets.core.modalities.Modality.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Initialize the modality with the name and properties.</p> Source code in <code>mmlearn/datasets/core/modalities.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Initialize the modality with the name and properties.\"\"\"\n    self.name = self.name.lower()\n    self._properties = {}\n\n    for field_name in self.__dataclass_fields__:\n        if field_name not in (\"name\", \"modality_specific_properties\"):\n            field_value = f\"{self.name}_{field_name}\"\n            self._properties[field_name] = field_value\n            setattr(self, field_name, field_value)\n\n    if self.modality_specific_properties is not None:\n        for (\n            property_name,\n            format_string,\n        ) in self.modality_specific_properties.items():\n            self.add_property(property_name, format_string)\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.modalities.Modality.add_property","title":"add_property","text":"<pre><code>add_property(name, format_string)\n</code></pre> <p>Add a new property to the modality.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the property.</p> required <code>format_string</code> <code>str</code> <p>The format string for the property. The format string should contain a placeholder that will be replaced with the name of the modality when the property is accessed.</p> required <p>Warns:</p> Type Description <code>UserWarning</code> <p>If the property already exists for the modality. It will overwrite the existing property.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>format_string</code> is invalid. A valid format string contains at least one placeholder enclosed in curly braces.</p> Source code in <code>mmlearn/datasets/core/modalities.py</code> <pre><code>def add_property(self, name: str, format_string: str) -&gt; None:\n    \"\"\"Add a new property to the modality.\n\n    Parameters\n    ----------\n    name : str\n        The name of the property.\n    format_string : str\n        The format string for the property. The format string should contain a\n        placeholder that will be replaced with the name of the modality when the\n        property is accessed.\n\n    Warns\n    -----\n    UserWarning\n        If the property already exists for the modality. It will overwrite the\n        existing property.\n\n    Raises\n    ------\n    ValueError\n        If `format_string` is invalid. A valid format string contains at least one\n        placeholder enclosed in curly braces.\n    \"\"\"\n    if name in self._properties:\n        warnings.warn(\n            f\"Property '{name}' already exists for modality '{super().__str__()}'.\"\n            \"Will overwrite the existing property.\",\n            category=UserWarning,\n            stacklevel=2,\n        )\n\n    if not _is_format_string(format_string):\n        raise ValueError(\n            f\"Invalid format string '{format_string}' for property \"\n            f\"'{name}' of modality '{super().__str__()}'.\"\n        )\n\n    self._properties[name] = format_string.format(self.name)\n    setattr(self, name, self._properties[name])\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.modalities.Modality.__str__","title":"__str__","text":"<pre><code>__str__()\n</code></pre> <p>Return the object as a string.</p> Source code in <code>mmlearn/datasets/core/modalities.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the object as a string.\"\"\"\n    return self.name.lower()\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.modalities.ModalityRegistry","title":"ModalityRegistry","text":"<p>Modality registry.</p> <p>A singleton class that manages the supported modalities (and their properties) in the library. The class provides methods to add new modalities and properties, and to access the existing modalities. The class is implemented as a singleton to ensure that there is only one instance of the registry in the library.</p> Source code in <code>mmlearn/datasets/core/modalities.py</code> <pre><code>class ModalityRegistry:\n    \"\"\"Modality registry.\n\n    A singleton class that manages the supported modalities (and their properties) in\n    the library. The class provides methods to add new modalities and properties, and\n    to access the existing modalities. The class is implemented as a singleton to\n    ensure that there is only one instance of the registry in the library.\n    \"\"\"\n\n    _instance: ClassVar[Any] = None\n    _modality_registry: dict[str, Modality] = {}\n\n    def __new__(cls) -&gt; Self:\n        \"\"\"Create a new instance of the class if it does not exist.\"\"\"\n        if cls._instance is None:\n            cls._instance = super().__new__(cls)\n            cls._instance._modality_registry = {}\n        return cls._instance  # type: ignore[no-any-return]\n\n    def register_modality(\n        self, name: str, modality_specific_properties: Optional[dict[str, str]] = None\n    ) -&gt; None:\n        \"\"\"Add a new modality to the registry.\n\n        Parameters\n        ----------\n        name : str\n            The name of the modality.\n        modality_specific_properties : Optional[dict[str, str]], optional, default=None\n            Additional properties specific to the modality.\n\n        Warns\n        -----\n        UserWarning\n            If the modality already exists in the registry. It will overwrite the\n            existing modality.\n\n        \"\"\"\n        if name.lower() in self._modality_registry:\n            warnings.warn(\n                f\"Modality '{name}' already exists in the registry. Overwriting...\",\n                category=UserWarning,\n                stacklevel=2,\n            )\n\n        name = name.lower()\n        modality = Modality(name, modality_specific_properties)\n        self._modality_registry[name] = modality\n        setattr(self, name, modality)\n\n    def add_default_property(self, name: str, format_string: str) -&gt; None:\n        \"\"\"Add a new property that is applicable to all modalities.\n\n        Parameters\n        ----------\n        name : str\n            The name of the property.\n        format_string : str\n            The format string for the property. The format string should contain a\n            placeholder that will be replaced with the name of the modality when the\n            property is accessed.\n\n        Warns\n        -----\n        UserWarning\n            If the property already exists for the default properties. It will\n            overwrite the existing property.\n\n        Raises\n        ------\n        ValueError\n            If the format string is invalid. A valid format string contains at least one\n            placeholder enclosed in curly braces.\n        \"\"\"\n        for modality in self._modality_registry.values():\n            modality.add_property(name, format_string)\n\n    def has_modality(self, name: str) -&gt; bool:\n        \"\"\"Check if the modality exists in the registry.\n\n        Parameters\n        ----------\n        name : str\n            The name of the modality.\n\n        Returns\n        -------\n        bool\n            True if the modality exists in the registry, False otherwise.\n        \"\"\"\n        return name.lower() in self._modality_registry\n\n    def get_modality(self, name: str) -&gt; Modality:\n        \"\"\"Get the modality name from the registry.\n\n        Parameters\n        ----------\n        name : str\n            The name of the modality.\n\n        Returns\n        -------\n        Modality\n            The modality object from the registry.\n        \"\"\"\n        return self._modality_registry[name.lower()]\n\n    def get_modality_properties(self, name: str) -&gt; dict[str, str]:\n        \"\"\"Get the properties of a modality from the registry.\n\n        Parameters\n        ----------\n        name : str\n            The name of the modality.\n\n        Returns\n        -------\n        dict[str, str]\n            The properties associated with the modality.\n        \"\"\"\n        return self.get_modality(name).properties\n\n    def list_modalities(self) -&gt; list[Modality]:\n        \"\"\"Get the list of supported modalities in the registry.\n\n        Returns\n        -------\n        list[Modality]\n            The list of supported modalities in the registry.\n        \"\"\"\n        return list(self._modality_registry.values())\n\n    def __getattr__(self, name: str) -&gt; Modality:\n        \"\"\"Access a modality as an attribute by its name.\"\"\"\n        if name.lower() in self._modality_registry:\n            return self._modality_registry[name.lower()]\n        raise AttributeError(\n            f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n        )\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.modalities.ModalityRegistry.__new__","title":"__new__","text":"<pre><code>__new__()\n</code></pre> <p>Create a new instance of the class if it does not exist.</p> Source code in <code>mmlearn/datasets/core/modalities.py</code> <pre><code>def __new__(cls) -&gt; Self:\n    \"\"\"Create a new instance of the class if it does not exist.\"\"\"\n    if cls._instance is None:\n        cls._instance = super().__new__(cls)\n        cls._instance._modality_registry = {}\n    return cls._instance  # type: ignore[no-any-return]\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.modalities.ModalityRegistry.register_modality","title":"register_modality","text":"<pre><code>register_modality(name, modality_specific_properties=None)\n</code></pre> <p>Add a new modality to the registry.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the modality.</p> required <code>modality_specific_properties</code> <code>Optional[dict[str, str]]</code> <p>Additional properties specific to the modality.</p> <code>None</code> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If the modality already exists in the registry. It will overwrite the existing modality.</p> Source code in <code>mmlearn/datasets/core/modalities.py</code> <pre><code>def register_modality(\n    self, name: str, modality_specific_properties: Optional[dict[str, str]] = None\n) -&gt; None:\n    \"\"\"Add a new modality to the registry.\n\n    Parameters\n    ----------\n    name : str\n        The name of the modality.\n    modality_specific_properties : Optional[dict[str, str]], optional, default=None\n        Additional properties specific to the modality.\n\n    Warns\n    -----\n    UserWarning\n        If the modality already exists in the registry. It will overwrite the\n        existing modality.\n\n    \"\"\"\n    if name.lower() in self._modality_registry:\n        warnings.warn(\n            f\"Modality '{name}' already exists in the registry. Overwriting...\",\n            category=UserWarning,\n            stacklevel=2,\n        )\n\n    name = name.lower()\n    modality = Modality(name, modality_specific_properties)\n    self._modality_registry[name] = modality\n    setattr(self, name, modality)\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.modalities.ModalityRegistry.add_default_property","title":"add_default_property","text":"<pre><code>add_default_property(name, format_string)\n</code></pre> <p>Add a new property that is applicable to all modalities.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the property.</p> required <code>format_string</code> <code>str</code> <p>The format string for the property. The format string should contain a placeholder that will be replaced with the name of the modality when the property is accessed.</p> required <p>Warns:</p> Type Description <code>UserWarning</code> <p>If the property already exists for the default properties. It will overwrite the existing property.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the format string is invalid. A valid format string contains at least one placeholder enclosed in curly braces.</p> Source code in <code>mmlearn/datasets/core/modalities.py</code> <pre><code>def add_default_property(self, name: str, format_string: str) -&gt; None:\n    \"\"\"Add a new property that is applicable to all modalities.\n\n    Parameters\n    ----------\n    name : str\n        The name of the property.\n    format_string : str\n        The format string for the property. The format string should contain a\n        placeholder that will be replaced with the name of the modality when the\n        property is accessed.\n\n    Warns\n    -----\n    UserWarning\n        If the property already exists for the default properties. It will\n        overwrite the existing property.\n\n    Raises\n    ------\n    ValueError\n        If the format string is invalid. A valid format string contains at least one\n        placeholder enclosed in curly braces.\n    \"\"\"\n    for modality in self._modality_registry.values():\n        modality.add_property(name, format_string)\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.modalities.ModalityRegistry.has_modality","title":"has_modality","text":"<pre><code>has_modality(name)\n</code></pre> <p>Check if the modality exists in the registry.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the modality.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the modality exists in the registry, False otherwise.</p> Source code in <code>mmlearn/datasets/core/modalities.py</code> <pre><code>def has_modality(self, name: str) -&gt; bool:\n    \"\"\"Check if the modality exists in the registry.\n\n    Parameters\n    ----------\n    name : str\n        The name of the modality.\n\n    Returns\n    -------\n    bool\n        True if the modality exists in the registry, False otherwise.\n    \"\"\"\n    return name.lower() in self._modality_registry\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.modalities.ModalityRegistry.get_modality","title":"get_modality","text":"<pre><code>get_modality(name)\n</code></pre> <p>Get the modality name from the registry.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the modality.</p> required <p>Returns:</p> Type Description <code>Modality</code> <p>The modality object from the registry.</p> Source code in <code>mmlearn/datasets/core/modalities.py</code> <pre><code>def get_modality(self, name: str) -&gt; Modality:\n    \"\"\"Get the modality name from the registry.\n\n    Parameters\n    ----------\n    name : str\n        The name of the modality.\n\n    Returns\n    -------\n    Modality\n        The modality object from the registry.\n    \"\"\"\n    return self._modality_registry[name.lower()]\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.modalities.ModalityRegistry.get_modality_properties","title":"get_modality_properties","text":"<pre><code>get_modality_properties(name)\n</code></pre> <p>Get the properties of a modality from the registry.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the modality.</p> required <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>The properties associated with the modality.</p> Source code in <code>mmlearn/datasets/core/modalities.py</code> <pre><code>def get_modality_properties(self, name: str) -&gt; dict[str, str]:\n    \"\"\"Get the properties of a modality from the registry.\n\n    Parameters\n    ----------\n    name : str\n        The name of the modality.\n\n    Returns\n    -------\n    dict[str, str]\n        The properties associated with the modality.\n    \"\"\"\n    return self.get_modality(name).properties\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.modalities.ModalityRegistry.list_modalities","title":"list_modalities","text":"<pre><code>list_modalities()\n</code></pre> <p>Get the list of supported modalities in the registry.</p> <p>Returns:</p> Type Description <code>list[Modality]</code> <p>The list of supported modalities in the registry.</p> Source code in <code>mmlearn/datasets/core/modalities.py</code> <pre><code>def list_modalities(self) -&gt; list[Modality]:\n    \"\"\"Get the list of supported modalities in the registry.\n\n    Returns\n    -------\n    list[Modality]\n        The list of supported modalities in the registry.\n    \"\"\"\n    return list(self._modality_registry.values())\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.modalities.ModalityRegistry.__getattr__","title":"__getattr__","text":"<pre><code>__getattr__(name)\n</code></pre> <p>Access a modality as an attribute by its name.</p> Source code in <code>mmlearn/datasets/core/modalities.py</code> <pre><code>def __getattr__(self, name: str) -&gt; Modality:\n    \"\"\"Access a modality as an attribute by its name.\"\"\"\n    if name.lower() in self._modality_registry:\n        return self._modality_registry[name.lower()]\n    raise AttributeError(\n        f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n    )\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.samplers","title":"samplers","text":"<p>Samplers for data loading.</p>"},{"location":"api/#mmlearn.datasets.core.samplers.CombinedDatasetRatioSampler","title":"CombinedDatasetRatioSampler","text":"<p>               Bases: <code>Sampler[int]</code></p> <p>Sampler for weighted sampling from a class:<code>~mmlearn.datasets.core.combined_dataset.CombinedDataset</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>CombinedDataset</code> <p>An instance of class:<code>~mmlearn.datasets.core.combined_dataset.CombinedDataset</code> to sample from.</p> required <code>ratios</code> <code>Optional[Sequence[float]]</code> <p>A sequence of ratios for sampling from each dataset in the combined dataset. The length of the sequence must be equal to the number of datasets in the combined dataset (<code>dataset</code>). If <code>None</code>, the length of each dataset in the combined dataset is used as the ratio. The ratios are normalized to sum to 1.</p> <code>None</code> <code>num_samples</code> <code>Optional[int]</code> <p>The number of samples to draw from the combined dataset. If <code>None</code>, the sampler will draw as many samples as there are in the combined dataset. This number must yield at least one sample per dataset in the combined dataset, when multiplied by the corresponding ratio.</p> <code>None</code> <code>replacement</code> <code>bool</code> <p>Whether to sample with replacement or not.</p> <code>False</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the sampled indices or not. If <code>False</code>, the indices of each dataset will appear in the order they are stored in the combined dataset. This is similar to sequential sampling from each dataset. The datasets that make up the combined dataset are still sampled randomly.</p> <code>True</code> <code>rank</code> <code>Optional[int]</code> <p>Rank of the current process within :attr:<code>num_replicas</code>. By default, :attr:<code>rank</code> is retrieved from the current distributed group.</p> <code>None</code> <code>num_replicas</code> <code>Optional[int]</code> <p>Number of processes participating in distributed training. By default, :attr:<code>num_replicas</code> is retrieved from the current distributed group.</p> <code>None</code> <code>drop_last</code> <code>bool</code> <p>Whether to drop the last incomplete batch or not. If <code>True</code>, the sampler will drop samples to make the number of samples evenly divisible by the number of replicas in distributed mode.</p> <code>False</code> <code>seed</code> <code>int</code> <p>Random seed used to when sampling from the combined dataset and shuffling the sampled indices.</p> <code>0</code> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>CombinedDataset</code> <p>The dataset to sample from.</p> <code>num_samples</code> <code>int</code> <p>The number of samples to draw from the combined dataset.</p> <code>probs</code> <code>Tensor</code> <p>The probabilities for sampling from each dataset in the combined dataset. This is computed from the <code>ratios</code> argument and is normalized to sum to 1.</p> <code>replacement</code> <code>bool</code> <p>Whether to sample with replacement or not.</p> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the sampled indices or not.</p> <code>rank</code> <code>int</code> <p>Rank of the current process within :attr:<code>num_replicas</code>.</p> <code>num_replicas</code> <code>int</code> <p>Number of processes participating in distributed training.</p> <code>drop_last</code> <code>bool</code> <p>Whether to drop samples to make the number of samples evenly divisible by the number of replicas in distributed mode.</p> <code>seed</code> <code>int</code> <p>Random seed used to when sampling from the combined dataset and shuffling the sampled indices.</p> <code>epoch</code> <code>int</code> <p>Current epoch number. This is used to set the random seed. This is useful in distributed mode to ensure that each process receives a different random ordering of the samples.</p> <code>total_size</code> <code>int</code> <p>The total number of samples across all processes.</p> Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>@store(group=\"dataloader/sampler\", provider=\"mmlearn\")\nclass CombinedDatasetRatioSampler(Sampler[int]):\n    \"\"\"Sampler for weighted sampling from a :py:class:`~mmlearn.datasets.core.combined_dataset.CombinedDataset`.\n\n    Parameters\n    ----------\n    dataset : CombinedDataset\n        An instance of :py:class:`~mmlearn.datasets.core.combined_dataset.CombinedDataset`\n        to sample from.\n    ratios : Optional[Sequence[float]], optional, default=None\n        A sequence of ratios for sampling from each dataset in the combined dataset.\n        The length of the sequence must be equal to the number of datasets in the\n        combined dataset (`dataset`). If `None`, the length of each dataset in the\n        combined dataset is used as the ratio. The ratios are normalized to sum to 1.\n    num_samples : Optional[int], optional, default=None\n        The number of samples to draw from the combined dataset. If `None`, the\n        sampler will draw as many samples as there are in the combined dataset.\n        This number must yield at least one sample per dataset in the combined\n        dataset, when multiplied by the corresponding ratio.\n    replacement : bool, default=False\n        Whether to sample with replacement or not.\n    shuffle : bool, default=True\n        Whether to shuffle the sampled indices or not. If `False`, the indices of\n        each dataset will appear in the order they are stored in the combined dataset.\n        This is similar to sequential sampling from each dataset. The datasets\n        that make up the combined dataset are still sampled randomly.\n    rank : Optional[int], optional, default=None\n        Rank of the current process within :attr:`num_replicas`. By default,\n        :attr:`rank` is retrieved from the current distributed group.\n    num_replicas : Optional[int], optional, default=None\n        Number of processes participating in distributed training. By\n        default, :attr:`num_replicas` is retrieved from the current distributed group.\n    drop_last : bool, default=False\n        Whether to drop the last incomplete batch or not. If `True`, the sampler will\n        drop samples to make the number of samples evenly divisible by the number of\n        replicas in distributed mode.\n    seed : int, default=0\n        Random seed used to when sampling from the combined dataset and shuffling\n        the sampled indices.\n\n    Attributes\n    ----------\n    dataset : CombinedDataset\n        The dataset to sample from.\n    num_samples : int\n        The number of samples to draw from the combined dataset.\n    probs : torch.Tensor\n        The probabilities for sampling from each dataset in the combined dataset.\n        This is computed from the `ratios` argument and is normalized to sum to 1.\n    replacement : bool\n        Whether to sample with replacement or not.\n    shuffle : bool\n        Whether to shuffle the sampled indices or not.\n    rank : int\n        Rank of the current process within :attr:`num_replicas`.\n    num_replicas : int\n        Number of processes participating in distributed training.\n    drop_last : bool\n        Whether to drop samples to make the number of samples evenly divisible by the\n        number of replicas in distributed mode.\n    seed : int\n        Random seed used to when sampling from the combined dataset and shuffling\n        the sampled indices.\n    epoch : int\n        Current epoch number. This is used to set the random seed. This is useful\n        in distributed mode to ensure that each process receives a different random\n        ordering of the samples.\n    total_size : int\n        The total number of samples across all processes.\n    \"\"\"  # noqa: W505\n\n    def __init__(  # noqa: PLR0912\n        self,\n        dataset: CombinedDataset,\n        ratios: Optional[Sequence[float]] = None,\n        num_samples: Optional[int] = None,\n        replacement: bool = False,\n        shuffle: bool = True,\n        rank: Optional[int] = None,\n        num_replicas: Optional[int] = None,\n        drop_last: bool = False,\n        seed: int = 0,\n    ):\n        if not isinstance(dataset, CombinedDataset):\n            raise TypeError(\n                \"Expected argument `dataset` to be of type `CombinedDataset`, \"\n                f\"but got {type(dataset)}.\",\n            )\n        if not isinstance(seed, int):\n            raise TypeError(\n                f\"Expected argument `seed` to be an integer, but got {type(seed)}.\",\n            )\n        if num_replicas is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            num_replicas = dist.get_world_size()\n        if rank is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            rank = dist.get_rank()\n        if rank &gt;= num_replicas or rank &lt; 0:\n            raise ValueError(\n                f\"Invalid rank {rank}, rank should be in the interval [0, {num_replicas - 1}]\"\n            )\n\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.drop_last = drop_last\n        self.replacement = replacement\n        self.shuffle = shuffle\n        self.seed = seed\n        self.epoch = 0\n\n        self._num_samples = num_samples\n        if not isinstance(self.num_samples, int) or self.num_samples &lt;= 0:\n            raise ValueError(\n                \"Expected argument `num_samples` to be a positive integer, but got \"\n                f\"{self.num_samples}.\",\n            )\n\n        if ratios is None:\n            ratios = [len(subset) for subset in self.dataset.datasets]\n\n        num_datasets = len(self.dataset.datasets)\n        if len(ratios) != num_datasets:\n            raise ValueError(\n                f\"Expected argument `ratios` to be of length {num_datasets}, \"\n                f\"but got length {len(ratios)}.\",\n            )\n        prob_sum = sum(ratios)\n        if not all(ratio &gt;= 0 for ratio in ratios) and prob_sum &gt; 0:\n            raise ValueError(\n                \"Expected argument `ratios` to be a sequence of non-negative numbers. \"\n                f\"Got {ratios}.\",\n            )\n        self.probs = torch.tensor(\n            [ratio / prob_sum for ratio in ratios],\n            dtype=torch.double,\n        )\n        if any((prob * self.num_samples) &lt;= 0 for prob in self.probs):\n            raise ValueError(\n                \"Expected dataset ratio to result in at least one sample per dataset. \"\n                f\"Got dataset sizes {self.probs * self.num_samples}.\",\n            )\n\n    @property\n    def num_samples(self) -&gt; int:\n        \"\"\"Return the number of samples managed by the sampler.\"\"\"\n        # dataset size might change at runtime\n        if self._num_samples is None:\n            num_samples = len(self.dataset)\n        else:\n            num_samples = self._num_samples\n\n        if self.drop_last and num_samples % self.num_replicas != 0:\n            # split to nearest available length that is evenly divisible.\n            # This is to ensure each rank receives the same amount of data when\n            # using this Sampler.\n            num_samples = math.ceil(\n                (num_samples - self.num_replicas) / self.num_replicas,\n            )\n        else:\n            num_samples = math.ceil(num_samples / self.num_replicas)\n        return num_samples\n\n    @property\n    def total_size(self) -&gt; int:\n        \"\"\"Return the total size of the dataset.\"\"\"\n        return self.num_samples * self.num_replicas\n\n    def __iter__(self) -&gt; Iterator[int]:\n        \"\"\"Return an iterator that yields sample indices for the combined dataset.\"\"\"\n        generator = torch.Generator()\n        seed = self.seed + self.epoch\n        generator.manual_seed(seed)\n\n        cumulative_sizes = [0] + self.dataset._cumulative_sizes\n        num_samples_per_dataset = [int(prob * self.total_size) for prob in self.probs]\n        indices = []\n        for i in range(len(self.dataset.datasets)):\n            per_dataset_indices: torch.Tensor = torch.multinomial(\n                torch.ones(cumulative_sizes[i + 1] - cumulative_sizes[i]),\n                num_samples_per_dataset[i],\n                replacement=self.replacement,\n                generator=generator,\n            )\n            # adjust indices to reflect position in cumulative dataset\n            per_dataset_indices += cumulative_sizes[i]\n            assert per_dataset_indices.max() &lt; cumulative_sizes[i + 1], (\n                f\"Indices from dataset {i} exceed dataset size. \"\n                f\"Got indices {per_dataset_indices} and dataset size {cumulative_sizes[i + 1]}.\",\n            )\n            indices.append(per_dataset_indices)\n\n        indices = torch.cat(indices)\n        if self.shuffle:\n            rand_indices = torch.randperm(len(indices), generator=generator)\n            indices = indices[rand_indices]\n\n        indices = indices.tolist()  # type: ignore[attr-defined]\n        num_indices = len(indices)\n\n        if num_indices &lt; self.total_size:\n            padding_size = self.total_size - num_indices\n            if padding_size &lt;= num_indices:\n                indices += indices[:padding_size]\n            else:\n                indices += (indices * math.ceil(padding_size / num_indices))[\n                    :padding_size\n                ]\n        elif num_indices &gt; self.total_size:\n            indices = indices[: self.total_size]\n        assert len(indices) == self.total_size\n\n        # subsample\n        indices = indices[self.rank : self.total_size : self.num_replicas]\n        assert len(indices) == self.num_samples, (\n            f\"Expected {self.num_samples} samples, but got {len(indices)}.\",\n        )\n\n        yield from iter(indices)\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the total number of samples in the sampler.\"\"\"\n        return self.num_samples\n\n    def set_epoch(self, epoch: int) -&gt; None:\n        \"\"\"Set the epoch for this sampler.\n\n        When :attr:`shuffle=True`, this ensures all replicas use a different random\n        ordering for each epoch. Otherwise, the next iteration of this sampler\n        will yield the same ordering.\n\n        Parameters\n        ----------\n        epoch : int\n            Epoch number.\n\n        \"\"\"\n        self.epoch = epoch\n\n        # some iterable datasets (especially huggingface iterable datasets) might\n        # require setting the epoch to ensure shuffling works properly\n        for dataset in self.dataset.datasets:\n            if hasattr(dataset, \"set_epoch\"):\n                dataset.set_epoch(epoch)\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.samplers.CombinedDatasetRatioSampler.num_samples","title":"num_samples  <code>property</code>","text":"<pre><code>num_samples\n</code></pre> <p>Return the number of samples managed by the sampler.</p>"},{"location":"api/#mmlearn.datasets.core.samplers.CombinedDatasetRatioSampler.total_size","title":"total_size  <code>property</code>","text":"<pre><code>total_size\n</code></pre> <p>Return the total size of the dataset.</p>"},{"location":"api/#mmlearn.datasets.core.samplers.CombinedDatasetRatioSampler.__iter__","title":"__iter__","text":"<pre><code>__iter__()\n</code></pre> <p>Return an iterator that yields sample indices for the combined dataset.</p> Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>def __iter__(self) -&gt; Iterator[int]:\n    \"\"\"Return an iterator that yields sample indices for the combined dataset.\"\"\"\n    generator = torch.Generator()\n    seed = self.seed + self.epoch\n    generator.manual_seed(seed)\n\n    cumulative_sizes = [0] + self.dataset._cumulative_sizes\n    num_samples_per_dataset = [int(prob * self.total_size) for prob in self.probs]\n    indices = []\n    for i in range(len(self.dataset.datasets)):\n        per_dataset_indices: torch.Tensor = torch.multinomial(\n            torch.ones(cumulative_sizes[i + 1] - cumulative_sizes[i]),\n            num_samples_per_dataset[i],\n            replacement=self.replacement,\n            generator=generator,\n        )\n        # adjust indices to reflect position in cumulative dataset\n        per_dataset_indices += cumulative_sizes[i]\n        assert per_dataset_indices.max() &lt; cumulative_sizes[i + 1], (\n            f\"Indices from dataset {i} exceed dataset size. \"\n            f\"Got indices {per_dataset_indices} and dataset size {cumulative_sizes[i + 1]}.\",\n        )\n        indices.append(per_dataset_indices)\n\n    indices = torch.cat(indices)\n    if self.shuffle:\n        rand_indices = torch.randperm(len(indices), generator=generator)\n        indices = indices[rand_indices]\n\n    indices = indices.tolist()  # type: ignore[attr-defined]\n    num_indices = len(indices)\n\n    if num_indices &lt; self.total_size:\n        padding_size = self.total_size - num_indices\n        if padding_size &lt;= num_indices:\n            indices += indices[:padding_size]\n        else:\n            indices += (indices * math.ceil(padding_size / num_indices))[\n                :padding_size\n            ]\n    elif num_indices &gt; self.total_size:\n        indices = indices[: self.total_size]\n    assert len(indices) == self.total_size\n\n    # subsample\n    indices = indices[self.rank : self.total_size : self.num_replicas]\n    assert len(indices) == self.num_samples, (\n        f\"Expected {self.num_samples} samples, but got {len(indices)}.\",\n    )\n\n    yield from iter(indices)\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.samplers.CombinedDatasetRatioSampler.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the total number of samples in the sampler.</p> Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the total number of samples in the sampler.\"\"\"\n    return self.num_samples\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.samplers.CombinedDatasetRatioSampler.set_epoch","title":"set_epoch","text":"<pre><code>set_epoch(epoch)\n</code></pre> <p>Set the epoch for this sampler.</p> <p>When :attr:<code>shuffle=True</code>, this ensures all replicas use a different random ordering for each epoch. Otherwise, the next iteration of this sampler will yield the same ordering.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>Epoch number.</p> required Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>def set_epoch(self, epoch: int) -&gt; None:\n    \"\"\"Set the epoch for this sampler.\n\n    When :attr:`shuffle=True`, this ensures all replicas use a different random\n    ordering for each epoch. Otherwise, the next iteration of this sampler\n    will yield the same ordering.\n\n    Parameters\n    ----------\n    epoch : int\n        Epoch number.\n\n    \"\"\"\n    self.epoch = epoch\n\n    # some iterable datasets (especially huggingface iterable datasets) might\n    # require setting the epoch to ensure shuffling works properly\n    for dataset in self.dataset.datasets:\n        if hasattr(dataset, \"set_epoch\"):\n            dataset.set_epoch(epoch)\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.samplers.DistributedEvalSampler","title":"DistributedEvalSampler","text":"<p>               Bases: <code>Sampler[int]</code></p> <p>Sampler for distributed evaluation.</p> <p>The main differences between this and class:<code>torch.utils.data.DistributedSampler</code> are that this sampler does not add extra samples to make it evenly divisible and shuffling is disabled by default.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>Dataset used for sampling.</p> required <code>num_replicas</code> <code>Optional[int]</code> <p>Number of processes participating in distributed training. By default, :attr:<code>rank</code> is retrieved from the current distributed group.</p> <code>None</code> <code>rank</code> <code>Optional[int]</code> <p>Rank of the current process within :attr:<code>num_replicas</code>. By default, :attr:<code>rank</code> is retrieved from the current distributed group.</p> <code>None</code> <code>shuffle</code> <code>bool</code> <p>If <code>True</code> (default), sampler will shuffle the indices.</p> <code>False</code> <code>seed</code> <code>int</code> <p>Random seed used to shuffle the sampler if :attr:<code>shuffle=True</code>. This number should be identical across all processes in the distributed group.</p> <code>0</code> Warnings <p>DistributedEvalSampler should NOT be used for training. The distributed processes could hang forever. See [1]_ for details</p> Notes <ul> <li>This sampler is for evaluation purpose where synchronization does not happen   every epoch. Synchronization should be done outside the dataloader loop.   It is especially useful in conjunction with   class:<code>torch.nn.parallel.DistributedDataParallel</code> [2]_.</li> <li>The input Dataset is assumed to be of constant size.</li> <li>This implementation is adapted from [3]_.</li> </ul> References <p>.. [1] https://github.com/pytorch/pytorch/issues/22584 .. [2] https://discuss.pytorch.org/t/how-to-validate-in-distributeddataparallel-correctly/94267/11 .. [3] https://github.com/SeungjunNah/DeepDeblur-PyTorch/blob/master/src/data/sampler.py</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; def example():\n...     start_epoch, n_epochs = 0, 2\n...     sampler = DistributedEvalSampler(dataset) if is_distributed else None\n...     loader = DataLoader(dataset, shuffle=(sampler is None), sampler=sampler)\n...     for epoch in range(start_epoch, n_epochs):\n...         if is_distributed:\n...             sampler.set_epoch(epoch)\n...         evaluate(loader)\n</code></pre> Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>@store(group=\"dataloader/sampler\", provider=\"mmlearn\")\nclass DistributedEvalSampler(Sampler[int]):\n    \"\"\"Sampler for distributed evaluation.\n\n    The main differences between this and :py:class:`torch.utils.data.DistributedSampler`\n    are that this sampler does not add extra samples to make it evenly divisible and\n    shuffling is disabled by default.\n\n    Parameters\n    ----------\n    dataset : torch.utils.data.Dataset\n        Dataset used for sampling.\n    num_replicas : Optional[int], optional, default=None\n        Number of processes participating in distributed training. By\n        default, :attr:`rank` is retrieved from the current distributed group.\n    rank : Optional[int], optional, default=None\n        Rank of the current process within :attr:`num_replicas`. By default,\n        :attr:`rank` is retrieved from the current distributed group.\n    shuffle : bool, optional, default=False\n        If `True` (default), sampler will shuffle the indices.\n    seed : int, optional, default=0\n        Random seed used to shuffle the sampler if :attr:`shuffle=True`.\n        This number should be identical across all processes in the\n        distributed group.\n\n    Warnings\n    --------\n    DistributedEvalSampler should NOT be used for training. The distributed processes\n    could hang forever. See [1]_ for details\n\n    Notes\n    -----\n    - This sampler is for evaluation purpose where synchronization does not happen\n      every epoch. Synchronization should be done outside the dataloader loop.\n      It is especially useful in conjunction with\n      :py:class:`torch.nn.parallel.DistributedDataParallel` [2]_.\n    - The input Dataset is assumed to be of constant size.\n    - This implementation is adapted from [3]_.\n\n    References\n    ----------\n    .. [1] https://github.com/pytorch/pytorch/issues/22584\n    .. [2] https://discuss.pytorch.org/t/how-to-validate-in-distributeddataparallel-correctly/94267/11\n    .. [3] https://github.com/SeungjunNah/DeepDeblur-PyTorch/blob/master/src/data/sampler.py\n\n\n    Examples\n    --------\n    &gt;&gt;&gt; def example():\n    ...     start_epoch, n_epochs = 0, 2\n    ...     sampler = DistributedEvalSampler(dataset) if is_distributed else None\n    ...     loader = DataLoader(dataset, shuffle=(sampler is None), sampler=sampler)\n    ...     for epoch in range(start_epoch, n_epochs):\n    ...         if is_distributed:\n    ...             sampler.set_epoch(epoch)\n    ...         evaluate(loader)\n\n    \"\"\"  # noqa: W505\n\n    def __init__(\n        self,\n        dataset: Dataset[Sized],\n        num_replicas: Optional[int] = None,\n        rank: Optional[int] = None,\n        shuffle: bool = False,\n        seed: int = 0,\n    ) -&gt; None:\n        if num_replicas is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            num_replicas = dist.get_world_size()\n        if rank is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            rank = dist.get_rank()\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.epoch = 0\n        self.shuffle = shuffle\n        self.seed = seed\n\n    @property\n    def total_size(self) -&gt; int:\n        \"\"\"Return the total size of the dataset.\"\"\"\n        return len(self.dataset)\n\n    @property\n    def num_samples(self) -&gt; int:\n        \"\"\"Return the number of samples managed by the sampler.\"\"\"\n        indices = list(range(self.total_size))[\n            self.rank : self.total_size : self.num_replicas\n        ]\n        return len(indices)  # true value without extra samples\n\n    def __iter__(self) -&gt; Iterator[int]:\n        \"\"\"Return an iterator that iterates over the indices of the dataset.\"\"\"\n        if self.shuffle:\n            # deterministically shuffle based on epoch and seed\n            g = torch.Generator()\n            g.manual_seed(self.seed + self.epoch)\n            indices = torch.randperm(self.total_size, generator=g).tolist()\n        else:\n            indices = list(range(self.total_size))\n\n        # subsample\n        indices = indices[self.rank : self.total_size : self.num_replicas]\n        assert len(indices) == self.num_samples\n\n        return iter(indices)\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the number of samples.\"\"\"\n        return self.num_samples\n\n    def set_epoch(self, epoch: int) -&gt; None:\n        \"\"\"Set the epoch for this sampler.\n\n        When :attr:`shuffle=True`, this ensures all replicas use a different random\n        ordering for each epoch. Otherwise, the next iteration of this sampler\n        will yield the same ordering.\n\n        Parameters\n        ----------\n        epoch : int\n            Epoch number.\n\n        \"\"\"\n        self.epoch = epoch\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.samplers.DistributedEvalSampler.total_size","title":"total_size  <code>property</code>","text":"<pre><code>total_size\n</code></pre> <p>Return the total size of the dataset.</p>"},{"location":"api/#mmlearn.datasets.core.samplers.DistributedEvalSampler.num_samples","title":"num_samples  <code>property</code>","text":"<pre><code>num_samples\n</code></pre> <p>Return the number of samples managed by the sampler.</p>"},{"location":"api/#mmlearn.datasets.core.samplers.DistributedEvalSampler.__iter__","title":"__iter__","text":"<pre><code>__iter__()\n</code></pre> <p>Return an iterator that iterates over the indices of the dataset.</p> Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>def __iter__(self) -&gt; Iterator[int]:\n    \"\"\"Return an iterator that iterates over the indices of the dataset.\"\"\"\n    if self.shuffle:\n        # deterministically shuffle based on epoch and seed\n        g = torch.Generator()\n        g.manual_seed(self.seed + self.epoch)\n        indices = torch.randperm(self.total_size, generator=g).tolist()\n    else:\n        indices = list(range(self.total_size))\n\n    # subsample\n    indices = indices[self.rank : self.total_size : self.num_replicas]\n    assert len(indices) == self.num_samples\n\n    return iter(indices)\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.samplers.DistributedEvalSampler.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the number of samples.</p> Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the number of samples.\"\"\"\n    return self.num_samples\n</code></pre>"},{"location":"api/#mmlearn.datasets.core.samplers.DistributedEvalSampler.set_epoch","title":"set_epoch","text":"<pre><code>set_epoch(epoch)\n</code></pre> <p>Set the epoch for this sampler.</p> <p>When :attr:<code>shuffle=True</code>, this ensures all replicas use a different random ordering for each epoch. Otherwise, the next iteration of this sampler will yield the same ordering.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>Epoch number.</p> required Source code in <code>mmlearn/datasets/core/samplers.py</code> <pre><code>def set_epoch(self, epoch: int) -&gt; None:\n    \"\"\"Set the epoch for this sampler.\n\n    When :attr:`shuffle=True`, this ensures all replicas use a different random\n    ordering for each epoch. Otherwise, the next iteration of this sampler\n    will yield the same ordering.\n\n    Parameters\n    ----------\n    epoch : int\n        Epoch number.\n\n    \"\"\"\n    self.epoch = epoch\n</code></pre>"},{"location":"api/#dataset-processors","title":"Dataset Processors","text":""},{"location":"api/#mmlearn.datasets.processors","title":"mmlearn.datasets.processors","text":"<p>Data processors.</p>"},{"location":"api/#mmlearn.datasets.processors.BlockwiseImagePatchMaskGenerator","title":"BlockwiseImagePatchMaskGenerator","text":"<p>Blockwise image patch mask generator.</p> <p>This is primarily intended for the data2vec method.</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>Union[int, tuple[int, int]]</code> <p>The size of the input image. If an integer is provided, the image is assumed to be square.</p> required <code>num_masking_patches</code> <code>int</code> <p>The number of patches to mask.</p> required <code>min_num_patches</code> <code>int</code> <p>The minimum number of patches to mask.</p> <code>4</code> <code>max_num_patches</code> <code>int</code> <p>The maximum number of patches to mask.</p> <code>None</code> <code>min_aspect_ratio</code> <code>float</code> <p>The minimum aspect ratio of the patch.</p> <code>0.3</code> <code>max_aspect_ratio</code> <code>float</code> <p>The maximum aspect ratio of the patch.</p> <code>None</code> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>@store(group=\"datasets/masking\", provider=\"mmlearn\")\nclass BlockwiseImagePatchMaskGenerator:\n    \"\"\"Blockwise image patch mask generator.\n\n    This is primarily intended for the data2vec method.\n\n    Parameters\n    ----------\n    input_size : Union[int, tuple[int, int]]\n        The size of the input image. If an integer is provided, the image is assumed\n        to be square.\n    num_masking_patches : int\n        The number of patches to mask.\n    min_num_patches : int, default=4\n        The minimum number of patches to mask.\n    max_num_patches : int, default=None\n        The maximum number of patches to mask.\n    min_aspect_ratio : float, default=0.3\n        The minimum aspect ratio of the patch.\n    max_aspect_ratio : float, default=None\n        The maximum aspect ratio of the patch.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_size: Union[int, tuple[int, int]],\n        num_masking_patches: int,\n        min_num_patches: int = 4,\n        max_num_patches: Any = None,\n        min_aspect_ratio: float = 0.3,\n        max_aspect_ratio: Any = None,\n    ):\n        if not isinstance(input_size, tuple):\n            input_size = (input_size,) * 2\n        self.height, self.width = input_size\n\n        self.num_masking_patches = num_masking_patches\n\n        self.min_num_patches = min_num_patches\n        self.max_num_patches = (\n            num_masking_patches if max_num_patches is None else max_num_patches\n        )\n\n        max_aspect_ratio = max_aspect_ratio or 1 / min_aspect_ratio\n        self.log_aspect_ratio = (math.log(min_aspect_ratio), math.log(max_aspect_ratio))\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Generate a printable representation.\n\n        Returns\n        -------\n        str\n            A printable representation of the object.\n\n        \"\"\"\n        return \"Generator(%d, %d -&gt; [%d ~ %d], max = %d, %.3f ~ %.3f)\" % (\n            self.height,\n            self.width,\n            self.min_num_patches,\n            self.max_num_patches,\n            self.num_masking_patches,\n            self.log_aspect_ratio[0],\n            self.log_aspect_ratio[1],\n        )\n\n    def get_shape(self) -&gt; tuple[int, int]:\n        \"\"\"Get the shape of the input.\n\n        Returns\n        -------\n        tuple[int, int]\n            The shape of the input as a tuple `(height, width)`.\n        \"\"\"\n        return self.height, self.width\n\n    def _mask(self, mask: torch.Tensor, max_mask_patches: int) -&gt; int:\n        \"\"\"Masking function.\n\n        This function mask adjacent patches by first selecting a target area and aspect\n        ratio. Since, there might be overlap between selected areas  or the selected\n        area might already be masked, it runs for a  maximum of 10 attempts or until the\n        specified number of patches (max_mask_patches) is achieved.\n\n\n        Parameters\n        ----------\n        mask: torch.Tensor\n            Current mask. The mask to be updated.\n        max_mask_patches: int\n            The maximum number of patches to be masked.\n\n        Returns\n        -------\n        delta: int\n            The number of patches that were successfully masked.\n\n        Notes\n        -----\n        - `target_area`: Randomly chosen target area for the patch.\n        - `aspect_ratio`: Randomly chosen aspect ratio for the patch.\n        - `h`: Height of the patch based on the target area and aspect ratio.\n        - `w`: Width of the patch based on the target area and aspect ratio.\n        - `top`: Randomly chosen top position for the patch.\n        - `left`: Randomly chosen left position for the patch.\n        - `num_masked`: Number of masked pixels within the proposed patch area.\n        - `delta`: Accumulated count of modified pixels.\n        \"\"\"\n        delta = 0\n        for _ in range(10):\n            target_area = random.uniform(self.min_num_patches, max_mask_patches)\n            aspect_ratio = math.exp(random.uniform(*self.log_aspect_ratio))\n            h = int(round(math.sqrt(target_area * aspect_ratio)))\n            w = int(round(math.sqrt(target_area / aspect_ratio)))\n            if w &lt; self.width and h &lt; self.height:\n                top = random.randint(0, self.height - h)\n                left = random.randint(0, self.width - w)\n\n                num_masked = mask[top : top + h, left : left + w].sum()\n                # Overlap\n                if 0 &lt; h * w - num_masked &lt;= max_mask_patches:\n                    for i in range(top, top + h):\n                        for j in range(left, left + w):\n                            if mask[i, j] == 0:\n                                mask[i, j] = 1\n                                delta += 1\n\n                if delta &gt; 0:\n                    break\n        return delta\n\n    def __call__(self) -&gt; torch.Tensor:\n        \"\"\"Generate a random mask.\n\n        Returns a random mask of shape (nb_patches, nb_patches) based on the\n        configuration where the number of patches to be masked is num_masking_patches.\n\n        Returns\n        -------\n        mask: torch.Tensor\n            A mask of shape (nb_patches, nb_patches)\n\n        \"\"\"\n        mask = torch.zeros(self.get_shape(), dtype=torch.int)\n        mask_count = 0\n        while mask_count &lt; self.num_masking_patches:\n            max_mask_patches = self.num_masking_patches - mask_count\n            max_mask_patches = min(max_mask_patches, self.max_num_patches)\n\n            delta = self._mask(mask, max_mask_patches)\n            if delta == 0:\n                break\n            mask_count += delta\n\n        return mask\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.BlockwiseImagePatchMaskGenerator.__repr__","title":"__repr__","text":"<pre><code>__repr__()\n</code></pre> <p>Generate a printable representation.</p> <p>Returns:</p> Type Description <code>str</code> <p>A printable representation of the object.</p> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Generate a printable representation.\n\n    Returns\n    -------\n    str\n        A printable representation of the object.\n\n    \"\"\"\n    return \"Generator(%d, %d -&gt; [%d ~ %d], max = %d, %.3f ~ %.3f)\" % (\n        self.height,\n        self.width,\n        self.min_num_patches,\n        self.max_num_patches,\n        self.num_masking_patches,\n        self.log_aspect_ratio[0],\n        self.log_aspect_ratio[1],\n    )\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.BlockwiseImagePatchMaskGenerator.get_shape","title":"get_shape","text":"<pre><code>get_shape()\n</code></pre> <p>Get the shape of the input.</p> <p>Returns:</p> Type Description <code>tuple[int, int]</code> <p>The shape of the input as a tuple <code>(height, width)</code>.</p> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>def get_shape(self) -&gt; tuple[int, int]:\n    \"\"\"Get the shape of the input.\n\n    Returns\n    -------\n    tuple[int, int]\n        The shape of the input as a tuple `(height, width)`.\n    \"\"\"\n    return self.height, self.width\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.BlockwiseImagePatchMaskGenerator.__call__","title":"__call__","text":"<pre><code>__call__()\n</code></pre> <p>Generate a random mask.</p> <p>Returns a random mask of shape (nb_patches, nb_patches) based on the configuration where the number of patches to be masked is num_masking_patches.</p> <p>Returns:</p> Name Type Description <code>mask</code> <code>Tensor</code> <p>A mask of shape (nb_patches, nb_patches)</p> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>def __call__(self) -&gt; torch.Tensor:\n    \"\"\"Generate a random mask.\n\n    Returns a random mask of shape (nb_patches, nb_patches) based on the\n    configuration where the number of patches to be masked is num_masking_patches.\n\n    Returns\n    -------\n    mask: torch.Tensor\n        A mask of shape (nb_patches, nb_patches)\n\n    \"\"\"\n    mask = torch.zeros(self.get_shape(), dtype=torch.int)\n    mask_count = 0\n    while mask_count &lt; self.num_masking_patches:\n        max_mask_patches = self.num_masking_patches - mask_count\n        max_mask_patches = min(max_mask_patches, self.max_num_patches)\n\n        delta = self._mask(mask, max_mask_patches)\n        if delta == 0:\n            break\n        mask_count += delta\n\n    return mask\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.RandomMaskGenerator","title":"RandomMaskGenerator","text":"<p>Random mask generator.</p> <p>Returns a random mask of shape <code>(nb_patches, nb_patches)</code> based on the configuration where the number of patches to be masked is num_masking_patches. This is intended to be used for tasks like masked language modeling.</p> <p>Parameters:</p> Name Type Description Default <code>probability</code> <code>float</code> <p>Probability of masking a token.</p> required Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>@store(group=\"datasets/masking\", provider=\"mmlearn\", probability=0.15)\nclass RandomMaskGenerator:\n    \"\"\"Random mask generator.\n\n    Returns a random mask of shape `(nb_patches, nb_patches)` based on the\n    configuration where the number of patches to be masked is num_masking_patches.\n    **This is intended to be used for tasks like masked language modeling.**\n\n    Parameters\n    ----------\n    probability : float\n        Probability of masking a token.\n    \"\"\"\n\n    def __init__(self, probability: float):\n        self.probability = probability\n\n    def __call__(\n        self,\n        inputs: torch.Tensor,\n        tokenizer: PreTrainedTokenizerBase,\n        special_tokens_mask: Optional[torch.Tensor] = None,\n    ) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Generate a random mask.\n\n        Returns a random mask of shape (nb_patches, nb_patches) based on the\n        configuration where the number of patches to be masked is num_masking_patches.\n\n        Returns\n        -------\n        inputs : torch.Tensor\n            The encoded inputs.\n        tokenizer : PreTrainedTokenizer\n            The tokenizer.\n        special_tokens_mask : Optional[torch.Tensor], default=None\n            Mask for special tokens.\n        \"\"\"\n        inputs = tokenizer.pad(inputs, return_tensors=\"pt\")[\"input_ids\"]\n        labels = inputs.clone()\n        # We sample a few tokens in each sequence for MLM training\n        # (with probability `self.probability`)\n        probability_matrix = torch.full(labels.shape, self.probability)\n        if special_tokens_mask is None:\n            special_tokens_mask = tokenizer.get_special_tokens_mask(\n                labels, already_has_special_tokens=True\n            )\n            special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n        else:\n            special_tokens_mask = special_tokens_mask.bool()\n\n        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n        masked_indices = torch.bernoulli(probability_matrix).bool()\n        labels[~masked_indices] = tokenizer.pad_token_id\n        # 80% of the time, replace masked input tokens with tokenizer.mask_token([MASK])\n        indices_replaced = (\n            torch.bernoulli(torch.full(labels.shape, 0.8)).bool() &amp; masked_indices\n        )\n        inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n\n        # 10% of the time, we replace masked input tokens with random word\n        indices_random = (\n            torch.bernoulli(torch.full(labels.shape, 0.5)).bool()\n            &amp; masked_indices\n            &amp; ~indices_replaced\n        )\n        random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n        inputs[indices_random] = random_words[indices_random]\n\n        # Rest of the time (10% of the time) we keep the masked input tokens unchanged\n        return inputs, labels, masked_indices\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.RandomMaskGenerator.__call__","title":"__call__","text":"<pre><code>__call__(inputs, tokenizer, special_tokens_mask=None)\n</code></pre> <p>Generate a random mask.</p> <p>Returns a random mask of shape (nb_patches, nb_patches) based on the configuration where the number of patches to be masked is num_masking_patches.</p> <p>Returns:</p> Name Type Description <code>inputs</code> <code>Tensor</code> <p>The encoded inputs.</p> <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>The tokenizer.</p> <code>special_tokens_mask</code> <code>Optional[torch.Tensor], default=None</code> <p>Mask for special tokens.</p> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>def __call__(\n    self,\n    inputs: torch.Tensor,\n    tokenizer: PreTrainedTokenizerBase,\n    special_tokens_mask: Optional[torch.Tensor] = None,\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Generate a random mask.\n\n    Returns a random mask of shape (nb_patches, nb_patches) based on the\n    configuration where the number of patches to be masked is num_masking_patches.\n\n    Returns\n    -------\n    inputs : torch.Tensor\n        The encoded inputs.\n    tokenizer : PreTrainedTokenizer\n        The tokenizer.\n    special_tokens_mask : Optional[torch.Tensor], default=None\n        Mask for special tokens.\n    \"\"\"\n    inputs = tokenizer.pad(inputs, return_tensors=\"pt\")[\"input_ids\"]\n    labels = inputs.clone()\n    # We sample a few tokens in each sequence for MLM training\n    # (with probability `self.probability`)\n    probability_matrix = torch.full(labels.shape, self.probability)\n    if special_tokens_mask is None:\n        special_tokens_mask = tokenizer.get_special_tokens_mask(\n            labels, already_has_special_tokens=True\n        )\n        special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n    else:\n        special_tokens_mask = special_tokens_mask.bool()\n\n    probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n    masked_indices = torch.bernoulli(probability_matrix).bool()\n    labels[~masked_indices] = tokenizer.pad_token_id\n    # 80% of the time, replace masked input tokens with tokenizer.mask_token([MASK])\n    indices_replaced = (\n        torch.bernoulli(torch.full(labels.shape, 0.8)).bool() &amp; masked_indices\n    )\n    inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n\n    # 10% of the time, we replace masked input tokens with random word\n    indices_random = (\n        torch.bernoulli(torch.full(labels.shape, 0.5)).bool()\n        &amp; masked_indices\n        &amp; ~indices_replaced\n    )\n    random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n    inputs[indices_random] = random_words[indices_random]\n\n    # Rest of the time (10% of the time) we keep the masked input tokens unchanged\n    return inputs, labels, masked_indices\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.HFTokenizer","title":"HFTokenizer","text":"<p>A wrapper for loading HuggingFace tokenizers.</p> <p>This class wraps any huggingface tokenizer that can be initialized with meth:<code>transformers.AutoTokenizer.from_pretrained</code>. It preprocesses the input text and returns a dictionary with the tokenized text and other relevant information like attention masks.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>Pretrained model name or path - same as in meth:<code>transformers.AutoTokenizer.from_pretrained</code>.</p> required <code>max_length</code> <code>Optional[int]</code> <p>Maximum length of the tokenized sequence. This is passed to the tokenizer :meth:<code>__call__</code> method.</p> <code>None</code> <code>padding</code> <code>bool or str</code> <p>Padding strategy. Same as in meth:<code>transformers.AutoTokenizer.from_pretrained</code>; passed to the tokenizer :meth:<code>__call__</code> method.</p> <code>False</code> <code>truncation</code> <code>Optional[Union[bool, str]]</code> <p>Truncation strategy. Same as in meth:<code>transformers.AutoTokenizer.from_pretrained</code>; passed to the tokenizer :meth:<code>__call__</code> method.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to meth:<code>transformers.AutoTokenizer.from_pretrained</code>.</p> <code>{}</code> Source code in <code>mmlearn/datasets/processors/tokenizers.py</code> <pre><code>@store(group=\"datasets/tokenizers\", provider=\"mmlearn\")\nclass HFTokenizer:\n    \"\"\"A wrapper for loading HuggingFace tokenizers.\n\n    This class wraps any huggingface tokenizer that can be initialized with\n    :py:meth:`transformers.AutoTokenizer.from_pretrained`. It preprocesses the\n    input text and returns a dictionary with the tokenized text and other\n    relevant information like attention masks.\n\n    Parameters\n    ----------\n    model_name_or_path : str\n        Pretrained model name or path - same as in :py:meth:`transformers.AutoTokenizer.from_pretrained`.\n    max_length : Optional[int], optional, default=None\n        Maximum length of the tokenized sequence. This is passed to the tokenizer\n        :meth:`__call__` method.\n    padding : bool or str, default=False\n        Padding strategy. Same as in :py:meth:`transformers.AutoTokenizer.from_pretrained`;\n        passed to the tokenizer :meth:`__call__` method.\n    truncation : Optional[Union[bool, str]], optional, default=None\n        Truncation strategy. Same as in :py:meth:`transformers.AutoTokenizer.from_pretrained`;\n        passed to the tokenizer :meth:`__call__` method.\n    **kwargs : Any\n        Additional arguments passed to :py:meth:`transformers.AutoTokenizer.from_pretrained`.\n    \"\"\"  # noqa: W505\n\n    def __init__(\n        self,\n        model_name_or_path: str,\n        max_length: Optional[int] = None,\n        padding: Union[bool, str] = False,\n        truncation: Optional[Union[bool, str]] = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, **kwargs)\n        self.max_length = max_length\n        self.padding = padding\n        self.truncation = truncation\n\n    def __call__(\n        self, sentence: Union[str, list[str]], **kwargs: Any\n    ) -&gt; dict[str, torch.Tensor]:\n        \"\"\"Tokenize a text or a list of texts using the HuggingFace tokenizer.\n\n        Parameters\n        ----------\n        sentence : Union[str, list[str]]\n            Sentence(s) to be tokenized.\n        **kwargs : Any\n            Additional arguments passed to the tokenizer :meth:`__call__` method.\n\n        Returns\n        -------\n        dict[str, torch.Tensor]\n            Tokenized sentence(s).\n\n        Notes\n        -----\n        The ``input_ids`` key is replaced with ``Modalities.TEXT`` for consistency.\n        \"\"\"\n        batch_encoding = self.tokenizer(\n            sentence,\n            max_length=self.max_length,\n            padding=self.padding,\n            truncation=self.truncation,\n            return_tensors=\"pt\",\n            **kwargs,\n        )\n\n        if isinstance(\n            sentence, str\n        ):  # remove batch dimension if input is a single sentence\n            for key, value in batch_encoding.items():\n                if isinstance(value, torch.Tensor):\n                    batch_encoding[key] = torch.squeeze(value, 0)\n\n        # use 'Modalities.TEXT' key for input_ids for consistency\n        batch_encoding[Modalities.TEXT.name] = batch_encoding[\"input_ids\"]\n        return dict(batch_encoding)\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.HFTokenizer.__call__","title":"__call__","text":"<pre><code>__call__(sentence, **kwargs)\n</code></pre> <p>Tokenize a text or a list of texts using the HuggingFace tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>sentence</code> <code>Union[str, list[str]]</code> <p>Sentence(s) to be tokenized.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to the tokenizer :meth:<code>__call__</code> method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>Tokenized sentence(s).</p> Notes <p>The <code>input_ids</code> key is replaced with <code>Modalities.TEXT</code> for consistency.</p> Source code in <code>mmlearn/datasets/processors/tokenizers.py</code> <pre><code>def __call__(\n    self, sentence: Union[str, list[str]], **kwargs: Any\n) -&gt; dict[str, torch.Tensor]:\n    \"\"\"Tokenize a text or a list of texts using the HuggingFace tokenizer.\n\n    Parameters\n    ----------\n    sentence : Union[str, list[str]]\n        Sentence(s) to be tokenized.\n    **kwargs : Any\n        Additional arguments passed to the tokenizer :meth:`__call__` method.\n\n    Returns\n    -------\n    dict[str, torch.Tensor]\n        Tokenized sentence(s).\n\n    Notes\n    -----\n    The ``input_ids`` key is replaced with ``Modalities.TEXT`` for consistency.\n    \"\"\"\n    batch_encoding = self.tokenizer(\n        sentence,\n        max_length=self.max_length,\n        padding=self.padding,\n        truncation=self.truncation,\n        return_tensors=\"pt\",\n        **kwargs,\n    )\n\n    if isinstance(\n        sentence, str\n    ):  # remove batch dimension if input is a single sentence\n        for key, value in batch_encoding.items():\n            if isinstance(value, torch.Tensor):\n                batch_encoding[key] = torch.squeeze(value, 0)\n\n    # use 'Modalities.TEXT' key for input_ids for consistency\n    batch_encoding[Modalities.TEXT.name] = batch_encoding[\"input_ids\"]\n    return dict(batch_encoding)\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.TrimText","title":"TrimText","text":"<p>Trim text strings as a preprocessing step before tokenization.</p> <p>Parameters:</p> Name Type Description Default <code>trim_size</code> <code>int</code> <p>The maximum length of the trimmed text.</p> required Source code in <code>mmlearn/datasets/processors/transforms.py</code> <pre><code>@store(group=\"datasets/transforms\", provider=\"mmlearn\")\nclass TrimText:\n    \"\"\"Trim text strings as a preprocessing step before tokenization.\n\n    Parameters\n    ----------\n    trim_size : int\n        The maximum length of the trimmed text.\n    \"\"\"\n\n    def __init__(self, trim_size: int) -&gt; None:\n        self.trim_size = trim_size\n\n    def __call__(self, sentence: Union[str, list[str]]) -&gt; Union[str, list[str]]:\n        \"\"\"Trim the given sentence(s).\n\n        Parameters\n        ----------\n        sentence : Union[str, list[str]]\n            Sentence(s) to be trimmed.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            Trimmed sentence(s).\n\n        Raises\n        ------\n        TypeError\n            If the input sentence is not a string or list of strings.\n        \"\"\"\n        if not isinstance(sentence, (list, str)):\n            raise TypeError(\n                \"Expected argument `sentence` to be a string or list of strings, \"\n                f\"but got {type(sentence)}\"\n            )\n\n        if isinstance(sentence, str):\n            return sentence[: self.trim_size]\n\n        for i, s in enumerate(sentence):\n            sentence[i] = s[: self.trim_size]\n\n        return sentence\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.TrimText.__call__","title":"__call__","text":"<pre><code>__call__(sentence)\n</code></pre> <p>Trim the given sentence(s).</p> <p>Parameters:</p> Name Type Description Default <code>sentence</code> <code>Union[str, list[str]]</code> <p>Sentence(s) to be trimmed.</p> required <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>Trimmed sentence(s).</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input sentence is not a string or list of strings.</p> Source code in <code>mmlearn/datasets/processors/transforms.py</code> <pre><code>def __call__(self, sentence: Union[str, list[str]]) -&gt; Union[str, list[str]]:\n    \"\"\"Trim the given sentence(s).\n\n    Parameters\n    ----------\n    sentence : Union[str, list[str]]\n        Sentence(s) to be trimmed.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        Trimmed sentence(s).\n\n    Raises\n    ------\n    TypeError\n        If the input sentence is not a string or list of strings.\n    \"\"\"\n    if not isinstance(sentence, (list, str)):\n        raise TypeError(\n            \"Expected argument `sentence` to be a string or list of strings, \"\n            f\"but got {type(sentence)}\"\n        )\n\n    if isinstance(sentence, str):\n        return sentence[: self.trim_size]\n\n    for i, s in enumerate(sentence):\n        sentence[i] = s[: self.trim_size]\n\n    return sentence\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.masking","title":"masking","text":"<p>Token mask generators.</p>"},{"location":"api/#mmlearn.datasets.processors.masking.RandomMaskGenerator","title":"RandomMaskGenerator","text":"<p>Random mask generator.</p> <p>Returns a random mask of shape <code>(nb_patches, nb_patches)</code> based on the configuration where the number of patches to be masked is num_masking_patches. This is intended to be used for tasks like masked language modeling.</p> <p>Parameters:</p> Name Type Description Default <code>probability</code> <code>float</code> <p>Probability of masking a token.</p> required Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>@store(group=\"datasets/masking\", provider=\"mmlearn\", probability=0.15)\nclass RandomMaskGenerator:\n    \"\"\"Random mask generator.\n\n    Returns a random mask of shape `(nb_patches, nb_patches)` based on the\n    configuration where the number of patches to be masked is num_masking_patches.\n    **This is intended to be used for tasks like masked language modeling.**\n\n    Parameters\n    ----------\n    probability : float\n        Probability of masking a token.\n    \"\"\"\n\n    def __init__(self, probability: float):\n        self.probability = probability\n\n    def __call__(\n        self,\n        inputs: torch.Tensor,\n        tokenizer: PreTrainedTokenizerBase,\n        special_tokens_mask: Optional[torch.Tensor] = None,\n    ) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Generate a random mask.\n\n        Returns a random mask of shape (nb_patches, nb_patches) based on the\n        configuration where the number of patches to be masked is num_masking_patches.\n\n        Returns\n        -------\n        inputs : torch.Tensor\n            The encoded inputs.\n        tokenizer : PreTrainedTokenizer\n            The tokenizer.\n        special_tokens_mask : Optional[torch.Tensor], default=None\n            Mask for special tokens.\n        \"\"\"\n        inputs = tokenizer.pad(inputs, return_tensors=\"pt\")[\"input_ids\"]\n        labels = inputs.clone()\n        # We sample a few tokens in each sequence for MLM training\n        # (with probability `self.probability`)\n        probability_matrix = torch.full(labels.shape, self.probability)\n        if special_tokens_mask is None:\n            special_tokens_mask = tokenizer.get_special_tokens_mask(\n                labels, already_has_special_tokens=True\n            )\n            special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n        else:\n            special_tokens_mask = special_tokens_mask.bool()\n\n        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n        masked_indices = torch.bernoulli(probability_matrix).bool()\n        labels[~masked_indices] = tokenizer.pad_token_id\n        # 80% of the time, replace masked input tokens with tokenizer.mask_token([MASK])\n        indices_replaced = (\n            torch.bernoulli(torch.full(labels.shape, 0.8)).bool() &amp; masked_indices\n        )\n        inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n\n        # 10% of the time, we replace masked input tokens with random word\n        indices_random = (\n            torch.bernoulli(torch.full(labels.shape, 0.5)).bool()\n            &amp; masked_indices\n            &amp; ~indices_replaced\n        )\n        random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n        inputs[indices_random] = random_words[indices_random]\n\n        # Rest of the time (10% of the time) we keep the masked input tokens unchanged\n        return inputs, labels, masked_indices\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.masking.RandomMaskGenerator.__call__","title":"__call__","text":"<pre><code>__call__(inputs, tokenizer, special_tokens_mask=None)\n</code></pre> <p>Generate a random mask.</p> <p>Returns a random mask of shape (nb_patches, nb_patches) based on the configuration where the number of patches to be masked is num_masking_patches.</p> <p>Returns:</p> Name Type Description <code>inputs</code> <code>Tensor</code> <p>The encoded inputs.</p> <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>The tokenizer.</p> <code>special_tokens_mask</code> <code>Optional[torch.Tensor], default=None</code> <p>Mask for special tokens.</p> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>def __call__(\n    self,\n    inputs: torch.Tensor,\n    tokenizer: PreTrainedTokenizerBase,\n    special_tokens_mask: Optional[torch.Tensor] = None,\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Generate a random mask.\n\n    Returns a random mask of shape (nb_patches, nb_patches) based on the\n    configuration where the number of patches to be masked is num_masking_patches.\n\n    Returns\n    -------\n    inputs : torch.Tensor\n        The encoded inputs.\n    tokenizer : PreTrainedTokenizer\n        The tokenizer.\n    special_tokens_mask : Optional[torch.Tensor], default=None\n        Mask for special tokens.\n    \"\"\"\n    inputs = tokenizer.pad(inputs, return_tensors=\"pt\")[\"input_ids\"]\n    labels = inputs.clone()\n    # We sample a few tokens in each sequence for MLM training\n    # (with probability `self.probability`)\n    probability_matrix = torch.full(labels.shape, self.probability)\n    if special_tokens_mask is None:\n        special_tokens_mask = tokenizer.get_special_tokens_mask(\n            labels, already_has_special_tokens=True\n        )\n        special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n    else:\n        special_tokens_mask = special_tokens_mask.bool()\n\n    probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n    masked_indices = torch.bernoulli(probability_matrix).bool()\n    labels[~masked_indices] = tokenizer.pad_token_id\n    # 80% of the time, replace masked input tokens with tokenizer.mask_token([MASK])\n    indices_replaced = (\n        torch.bernoulli(torch.full(labels.shape, 0.8)).bool() &amp; masked_indices\n    )\n    inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n\n    # 10% of the time, we replace masked input tokens with random word\n    indices_random = (\n        torch.bernoulli(torch.full(labels.shape, 0.5)).bool()\n        &amp; masked_indices\n        &amp; ~indices_replaced\n    )\n    random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n    inputs[indices_random] = random_words[indices_random]\n\n    # Rest of the time (10% of the time) we keep the masked input tokens unchanged\n    return inputs, labels, masked_indices\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.masking.BlockwiseImagePatchMaskGenerator","title":"BlockwiseImagePatchMaskGenerator","text":"<p>Blockwise image patch mask generator.</p> <p>This is primarily intended for the data2vec method.</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>Union[int, tuple[int, int]]</code> <p>The size of the input image. If an integer is provided, the image is assumed to be square.</p> required <code>num_masking_patches</code> <code>int</code> <p>The number of patches to mask.</p> required <code>min_num_patches</code> <code>int</code> <p>The minimum number of patches to mask.</p> <code>4</code> <code>max_num_patches</code> <code>int</code> <p>The maximum number of patches to mask.</p> <code>None</code> <code>min_aspect_ratio</code> <code>float</code> <p>The minimum aspect ratio of the patch.</p> <code>0.3</code> <code>max_aspect_ratio</code> <code>float</code> <p>The maximum aspect ratio of the patch.</p> <code>None</code> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>@store(group=\"datasets/masking\", provider=\"mmlearn\")\nclass BlockwiseImagePatchMaskGenerator:\n    \"\"\"Blockwise image patch mask generator.\n\n    This is primarily intended for the data2vec method.\n\n    Parameters\n    ----------\n    input_size : Union[int, tuple[int, int]]\n        The size of the input image. If an integer is provided, the image is assumed\n        to be square.\n    num_masking_patches : int\n        The number of patches to mask.\n    min_num_patches : int, default=4\n        The minimum number of patches to mask.\n    max_num_patches : int, default=None\n        The maximum number of patches to mask.\n    min_aspect_ratio : float, default=0.3\n        The minimum aspect ratio of the patch.\n    max_aspect_ratio : float, default=None\n        The maximum aspect ratio of the patch.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_size: Union[int, tuple[int, int]],\n        num_masking_patches: int,\n        min_num_patches: int = 4,\n        max_num_patches: Any = None,\n        min_aspect_ratio: float = 0.3,\n        max_aspect_ratio: Any = None,\n    ):\n        if not isinstance(input_size, tuple):\n            input_size = (input_size,) * 2\n        self.height, self.width = input_size\n\n        self.num_masking_patches = num_masking_patches\n\n        self.min_num_patches = min_num_patches\n        self.max_num_patches = (\n            num_masking_patches if max_num_patches is None else max_num_patches\n        )\n\n        max_aspect_ratio = max_aspect_ratio or 1 / min_aspect_ratio\n        self.log_aspect_ratio = (math.log(min_aspect_ratio), math.log(max_aspect_ratio))\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Generate a printable representation.\n\n        Returns\n        -------\n        str\n            A printable representation of the object.\n\n        \"\"\"\n        return \"Generator(%d, %d -&gt; [%d ~ %d], max = %d, %.3f ~ %.3f)\" % (\n            self.height,\n            self.width,\n            self.min_num_patches,\n            self.max_num_patches,\n            self.num_masking_patches,\n            self.log_aspect_ratio[0],\n            self.log_aspect_ratio[1],\n        )\n\n    def get_shape(self) -&gt; tuple[int, int]:\n        \"\"\"Get the shape of the input.\n\n        Returns\n        -------\n        tuple[int, int]\n            The shape of the input as a tuple `(height, width)`.\n        \"\"\"\n        return self.height, self.width\n\n    def _mask(self, mask: torch.Tensor, max_mask_patches: int) -&gt; int:\n        \"\"\"Masking function.\n\n        This function mask adjacent patches by first selecting a target area and aspect\n        ratio. Since, there might be overlap between selected areas  or the selected\n        area might already be masked, it runs for a  maximum of 10 attempts or until the\n        specified number of patches (max_mask_patches) is achieved.\n\n\n        Parameters\n        ----------\n        mask: torch.Tensor\n            Current mask. The mask to be updated.\n        max_mask_patches: int\n            The maximum number of patches to be masked.\n\n        Returns\n        -------\n        delta: int\n            The number of patches that were successfully masked.\n\n        Notes\n        -----\n        - `target_area`: Randomly chosen target area for the patch.\n        - `aspect_ratio`: Randomly chosen aspect ratio for the patch.\n        - `h`: Height of the patch based on the target area and aspect ratio.\n        - `w`: Width of the patch based on the target area and aspect ratio.\n        - `top`: Randomly chosen top position for the patch.\n        - `left`: Randomly chosen left position for the patch.\n        - `num_masked`: Number of masked pixels within the proposed patch area.\n        - `delta`: Accumulated count of modified pixels.\n        \"\"\"\n        delta = 0\n        for _ in range(10):\n            target_area = random.uniform(self.min_num_patches, max_mask_patches)\n            aspect_ratio = math.exp(random.uniform(*self.log_aspect_ratio))\n            h = int(round(math.sqrt(target_area * aspect_ratio)))\n            w = int(round(math.sqrt(target_area / aspect_ratio)))\n            if w &lt; self.width and h &lt; self.height:\n                top = random.randint(0, self.height - h)\n                left = random.randint(0, self.width - w)\n\n                num_masked = mask[top : top + h, left : left + w].sum()\n                # Overlap\n                if 0 &lt; h * w - num_masked &lt;= max_mask_patches:\n                    for i in range(top, top + h):\n                        for j in range(left, left + w):\n                            if mask[i, j] == 0:\n                                mask[i, j] = 1\n                                delta += 1\n\n                if delta &gt; 0:\n                    break\n        return delta\n\n    def __call__(self) -&gt; torch.Tensor:\n        \"\"\"Generate a random mask.\n\n        Returns a random mask of shape (nb_patches, nb_patches) based on the\n        configuration where the number of patches to be masked is num_masking_patches.\n\n        Returns\n        -------\n        mask: torch.Tensor\n            A mask of shape (nb_patches, nb_patches)\n\n        \"\"\"\n        mask = torch.zeros(self.get_shape(), dtype=torch.int)\n        mask_count = 0\n        while mask_count &lt; self.num_masking_patches:\n            max_mask_patches = self.num_masking_patches - mask_count\n            max_mask_patches = min(max_mask_patches, self.max_num_patches)\n\n            delta = self._mask(mask, max_mask_patches)\n            if delta == 0:\n                break\n            mask_count += delta\n\n        return mask\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.masking.BlockwiseImagePatchMaskGenerator.__repr__","title":"__repr__","text":"<pre><code>__repr__()\n</code></pre> <p>Generate a printable representation.</p> <p>Returns:</p> Type Description <code>str</code> <p>A printable representation of the object.</p> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Generate a printable representation.\n\n    Returns\n    -------\n    str\n        A printable representation of the object.\n\n    \"\"\"\n    return \"Generator(%d, %d -&gt; [%d ~ %d], max = %d, %.3f ~ %.3f)\" % (\n        self.height,\n        self.width,\n        self.min_num_patches,\n        self.max_num_patches,\n        self.num_masking_patches,\n        self.log_aspect_ratio[0],\n        self.log_aspect_ratio[1],\n    )\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.masking.BlockwiseImagePatchMaskGenerator.get_shape","title":"get_shape","text":"<pre><code>get_shape()\n</code></pre> <p>Get the shape of the input.</p> <p>Returns:</p> Type Description <code>tuple[int, int]</code> <p>The shape of the input as a tuple <code>(height, width)</code>.</p> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>def get_shape(self) -&gt; tuple[int, int]:\n    \"\"\"Get the shape of the input.\n\n    Returns\n    -------\n    tuple[int, int]\n        The shape of the input as a tuple `(height, width)`.\n    \"\"\"\n    return self.height, self.width\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.masking.BlockwiseImagePatchMaskGenerator.__call__","title":"__call__","text":"<pre><code>__call__()\n</code></pre> <p>Generate a random mask.</p> <p>Returns a random mask of shape (nb_patches, nb_patches) based on the configuration where the number of patches to be masked is num_masking_patches.</p> <p>Returns:</p> Name Type Description <code>mask</code> <code>Tensor</code> <p>A mask of shape (nb_patches, nb_patches)</p> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>def __call__(self) -&gt; torch.Tensor:\n    \"\"\"Generate a random mask.\n\n    Returns a random mask of shape (nb_patches, nb_patches) based on the\n    configuration where the number of patches to be masked is num_masking_patches.\n\n    Returns\n    -------\n    mask: torch.Tensor\n        A mask of shape (nb_patches, nb_patches)\n\n    \"\"\"\n    mask = torch.zeros(self.get_shape(), dtype=torch.int)\n    mask_count = 0\n    while mask_count &lt; self.num_masking_patches:\n        max_mask_patches = self.num_masking_patches - mask_count\n        max_mask_patches = min(max_mask_patches, self.max_num_patches)\n\n        delta = self._mask(mask, max_mask_patches)\n        if delta == 0:\n            break\n        mask_count += delta\n\n    return mask\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.masking.IJEPAMaskGenerator","title":"IJEPAMaskGenerator  <code>dataclass</code>","text":"<p>Generates encoder and predictor masks for preprocessing.</p> <p>This class generates masks dynamically for batches of examples.</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>tuple[int, int]</code> <p>Input image size.</p> <code>(224, 224)</code> <code>patch_size</code> <code>int</code> <p>Size of each patch.</p> <code>16</code> <code>min_keep</code> <code>int</code> <p>Minimum number of patches to keep.</p> <code>10</code> <code>allow_overlap</code> <code>bool</code> <p>Whether to allow overlap between encoder and predictor masks.</p> <code>False</code> <code>enc_mask_scale</code> <code>tuple[float, float]</code> <p>Scale range for encoder mask.</p> <code>(0.85, 1.0)</code> <code>pred_mask_scale</code> <code>tuple[float, float]</code> <p>Scale range for predictor mask.</p> <code>(0.15, 0.2)</code> <code>aspect_ratio</code> <code>tuple[float, float]</code> <p>Aspect ratio range for mask blocks.</p> <code>(0.75, 1.0)</code> <code>nenc</code> <code>int</code> <p>Number of encoder masks to generate.</p> <code>1</code> <code>npred</code> <code>int</code> <p>Number of predictor masks to generate.</p> <code>4</code> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>@dataclass\nclass IJEPAMaskGenerator:\n    \"\"\"Generates encoder and predictor masks for preprocessing.\n\n    This class generates masks dynamically for batches of examples.\n\n    Parameters\n    ----------\n    input_size : tuple[int, int], default=(224, 224)\n        Input image size.\n    patch_size : int, default=16\n        Size of each patch.\n    min_keep : int, default=10\n        Minimum number of patches to keep.\n    allow_overlap : bool, default=False\n        Whether to allow overlap between encoder and predictor masks.\n    enc_mask_scale : tuple[float, float], default=(0.85, 1.0)\n        Scale range for encoder mask.\n    pred_mask_scale : tuple[float, float], default=(0.15, 0.2)\n        Scale range for predictor mask.\n    aspect_ratio : tuple[float, float], default=(0.75, 1.0)\n        Aspect ratio range for mask blocks.\n    nenc : int, default=1\n        Number of encoder masks to generate.\n    npred : int, default=4\n        Number of predictor masks to generate.\n    \"\"\"\n\n    input_size: tuple[int, int] = (224, 224)\n    patch_size: int = 16\n    min_keep: int = 10\n    allow_overlap: bool = False\n    enc_mask_scale: tuple[float, float] = (0.85, 1.0)\n    pred_mask_scale: tuple[float, float] = (0.15, 0.2)\n    aspect_ratio: tuple[float, float] = (0.75, 1.5)\n    nenc: int = 1\n    npred: int = 4\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Initialize the mask generator.\"\"\"\n        self.height = self.input_size[0] // self.patch_size\n        self.width = self.input_size[1] // self.patch_size\n\n    def _sample_block_size(\n        self,\n        generator: torch.Generator,\n        scale: tuple[float, float],\n        aspect_ratio: tuple[float, float],\n    ) -&gt; tuple[int, int]:\n        \"\"\"Sample the size of the mask block based on scale and aspect ratio.\"\"\"\n        _rand = torch.rand(1, generator=generator).item()\n        min_s, max_s = scale\n        mask_scale = min_s + _rand * (max_s - min_s)\n        max_keep = int(self.height * self.width * mask_scale)\n\n        min_ar, max_ar = aspect_ratio\n        aspect_ratio_val = min_ar + _rand * (max_ar - min_ar)\n\n        h = int(round(math.sqrt(max_keep * aspect_ratio_val)))\n        w = int(round(math.sqrt(max_keep / aspect_ratio_val)))\n\n        h = min(h, self.height - 1)\n        w = min(w, self.width - 1)\n\n        return h, w\n\n    def _sample_block_mask(\n        self, b_size: tuple[int, int]\n    ) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Sample a mask block.\"\"\"\n        h, w = b_size\n        top = torch.randint(0, self.height - h, (1,)).item()\n        left = torch.randint(0, self.width - w, (1,)).item()\n        mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n        mask[top : top + h, left : left + w] = 1\n\n        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n        mask_complement[top : top + h, left : left + w] = 0\n\n        return mask.flatten(), mask_complement.flatten()\n\n    def __call__(self, batch_size: int = 1) -&gt; dict[str, Any]:\n        \"\"\"Generate encoder and predictor masks for a batch of examples.\n\n        Parameters\n        ----------\n        batch_size : int, default=1\n            The batch size for which to generate masks.\n\n        Returns\n        -------\n        dict[str, Any]\n            A dictionary of encoder masks and predictor masks.\n        \"\"\"\n        seed = torch.randint(\n            0, 2**32, (1,)\n        ).item()  # Sample random seed for reproducibility\n        g = torch.Generator().manual_seed(seed)\n\n        # Sample block sizes\n        p_size = self._sample_block_size(\n            generator=g, scale=self.pred_mask_scale, aspect_ratio=self.aspect_ratio\n        )\n        e_size = self._sample_block_size(\n            generator=g, scale=self.enc_mask_scale, aspect_ratio=(1.0, 1.0)\n        )\n\n        # Generate predictor masks\n        masks_pred, masks_enc = [], []\n        for _ in range(self.npred):\n            mask_p, _ = self._sample_block_mask(p_size)\n            # Expand mask to match batch size\n            mask_p = mask_p.unsqueeze(0).expand(batch_size, -1)\n            masks_pred.append(mask_p)\n\n        # Generate encoder masks\n        for _ in range(self.nenc):\n            mask_e, _ = self._sample_block_mask(e_size)\n            # Expand mask to match batch size\n            mask_e = mask_e.unsqueeze(0).expand(batch_size, -1)\n            masks_enc.append(mask_e)\n\n        return {\n            \"encoder_masks\": masks_enc,  # list of tensors of shape (batch_size, N)\n            \"predictor_masks\": masks_pred,  # list of tensors of shape (batch_size, N)\n        }\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.masking.IJEPAMaskGenerator.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Initialize the mask generator.</p> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Initialize the mask generator.\"\"\"\n    self.height = self.input_size[0] // self.patch_size\n    self.width = self.input_size[1] // self.patch_size\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.masking.IJEPAMaskGenerator.__call__","title":"__call__","text":"<pre><code>__call__(batch_size=1)\n</code></pre> <p>Generate encoder and predictor masks for a batch of examples.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>The batch size for which to generate masks.</p> <code>1</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary of encoder masks and predictor masks.</p> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>def __call__(self, batch_size: int = 1) -&gt; dict[str, Any]:\n    \"\"\"Generate encoder and predictor masks for a batch of examples.\n\n    Parameters\n    ----------\n    batch_size : int, default=1\n        The batch size for which to generate masks.\n\n    Returns\n    -------\n    dict[str, Any]\n        A dictionary of encoder masks and predictor masks.\n    \"\"\"\n    seed = torch.randint(\n        0, 2**32, (1,)\n    ).item()  # Sample random seed for reproducibility\n    g = torch.Generator().manual_seed(seed)\n\n    # Sample block sizes\n    p_size = self._sample_block_size(\n        generator=g, scale=self.pred_mask_scale, aspect_ratio=self.aspect_ratio\n    )\n    e_size = self._sample_block_size(\n        generator=g, scale=self.enc_mask_scale, aspect_ratio=(1.0, 1.0)\n    )\n\n    # Generate predictor masks\n    masks_pred, masks_enc = [], []\n    for _ in range(self.npred):\n        mask_p, _ = self._sample_block_mask(p_size)\n        # Expand mask to match batch size\n        mask_p = mask_p.unsqueeze(0).expand(batch_size, -1)\n        masks_pred.append(mask_p)\n\n    # Generate encoder masks\n    for _ in range(self.nenc):\n        mask_e, _ = self._sample_block_mask(e_size)\n        # Expand mask to match batch size\n        mask_e = mask_e.unsqueeze(0).expand(batch_size, -1)\n        masks_enc.append(mask_e)\n\n    return {\n        \"encoder_masks\": masks_enc,  # list of tensors of shape (batch_size, N)\n        \"predictor_masks\": masks_pred,  # list of tensors of shape (batch_size, N)\n    }\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.masking.apply_masks","title":"apply_masks","text":"<pre><code>apply_masks(x, masks)\n</code></pre> <p>Apply masks to the input tensor by selecting the patches to keep based on the masks.</p> <p>This function is primarily intended to be used for the class:<code>i-JEPA &lt;mmlearn.tasks.ijepa.IJEPA&gt;</code>.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape <code>(B, N, D)</code>.</p> required <code>masks</code> <code>Union[Tensor, list[Tensor]]</code> <p>A list of mask tensors of shape <code>(N,)</code>, <code>(1, N)</code>, or <code>(B, N)</code>.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The masked tensor where only the patches indicated by the masks are kept. The output tensor has shape <code>(B * num_masks, N', D)</code>, where <code>N'</code> is the number of patches kept.</p> Source code in <code>mmlearn/datasets/processors/masking.py</code> <pre><code>def apply_masks(\n    x: torch.Tensor, masks: Union[torch.Tensor, list[torch.Tensor]]\n) -&gt; torch.Tensor:\n    \"\"\"\n    Apply masks to the input tensor by selecting the patches to keep based on the masks.\n\n    This function is primarily intended to be used for the\n    :py:class:`i-JEPA &lt;mmlearn.tasks.ijepa.IJEPA&gt;`.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape ``(B, N, D)``.\n    masks : Union[torch.Tensor, list[torch.Tensor]]\n        A list of mask tensors of shape ``(N,)``, ``(1, N)``, or ``(B, N)``.\n\n    Returns\n    -------\n    torch.Tensor\n        The masked tensor where only the patches indicated by the masks are kept.\n        The output tensor has shape ``(B * num_masks, N', D)``, where ``N'`` is\n        the number of patches kept.\n    \"\"\"\n    all_x = []\n    batch_size = x.size(0)\n    for m_ in masks:\n        m = m_.to(x.device)\n\n        # Ensure mask is at least 2D\n        if m.dim() == 1:\n            m = m.unsqueeze(0)  # Shape: (1, N)\n\n        # Expand mask to match the batch size if needed\n        if m.size(0) == 1 and batch_size &gt; 1:\n            m = m.expand(batch_size, -1)  # Shape: (B, N)\n\n        # Expand mask to match x's dimensions\n        m_expanded = (\n            m.unsqueeze(-1).expand(-1, -1, x.size(-1)).bool()\n        )  # Shape: (B, N, D)\n\n        # Use boolean indexing\n        selected_patches = x[m_expanded].view(batch_size, -1, x.size(-1))\n        all_x.append(selected_patches)\n\n    # Concatenate along the batch dimension\n    return torch.cat(all_x, dim=0)\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.tokenizers","title":"tokenizers","text":"<p>Tokenizers - modules that convert raw input to sequences of tokens.</p>"},{"location":"api/#mmlearn.datasets.processors.tokenizers.HFTokenizer","title":"HFTokenizer","text":"<p>A wrapper for loading HuggingFace tokenizers.</p> <p>This class wraps any huggingface tokenizer that can be initialized with meth:<code>transformers.AutoTokenizer.from_pretrained</code>. It preprocesses the input text and returns a dictionary with the tokenized text and other relevant information like attention masks.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>Pretrained model name or path - same as in meth:<code>transformers.AutoTokenizer.from_pretrained</code>.</p> required <code>max_length</code> <code>Optional[int]</code> <p>Maximum length of the tokenized sequence. This is passed to the tokenizer :meth:<code>__call__</code> method.</p> <code>None</code> <code>padding</code> <code>bool or str</code> <p>Padding strategy. Same as in meth:<code>transformers.AutoTokenizer.from_pretrained</code>; passed to the tokenizer :meth:<code>__call__</code> method.</p> <code>False</code> <code>truncation</code> <code>Optional[Union[bool, str]]</code> <p>Truncation strategy. Same as in meth:<code>transformers.AutoTokenizer.from_pretrained</code>; passed to the tokenizer :meth:<code>__call__</code> method.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to meth:<code>transformers.AutoTokenizer.from_pretrained</code>.</p> <code>{}</code> Source code in <code>mmlearn/datasets/processors/tokenizers.py</code> <pre><code>@store(group=\"datasets/tokenizers\", provider=\"mmlearn\")\nclass HFTokenizer:\n    \"\"\"A wrapper for loading HuggingFace tokenizers.\n\n    This class wraps any huggingface tokenizer that can be initialized with\n    :py:meth:`transformers.AutoTokenizer.from_pretrained`. It preprocesses the\n    input text and returns a dictionary with the tokenized text and other\n    relevant information like attention masks.\n\n    Parameters\n    ----------\n    model_name_or_path : str\n        Pretrained model name or path - same as in :py:meth:`transformers.AutoTokenizer.from_pretrained`.\n    max_length : Optional[int], optional, default=None\n        Maximum length of the tokenized sequence. This is passed to the tokenizer\n        :meth:`__call__` method.\n    padding : bool or str, default=False\n        Padding strategy. Same as in :py:meth:`transformers.AutoTokenizer.from_pretrained`;\n        passed to the tokenizer :meth:`__call__` method.\n    truncation : Optional[Union[bool, str]], optional, default=None\n        Truncation strategy. Same as in :py:meth:`transformers.AutoTokenizer.from_pretrained`;\n        passed to the tokenizer :meth:`__call__` method.\n    **kwargs : Any\n        Additional arguments passed to :py:meth:`transformers.AutoTokenizer.from_pretrained`.\n    \"\"\"  # noqa: W505\n\n    def __init__(\n        self,\n        model_name_or_path: str,\n        max_length: Optional[int] = None,\n        padding: Union[bool, str] = False,\n        truncation: Optional[Union[bool, str]] = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, **kwargs)\n        self.max_length = max_length\n        self.padding = padding\n        self.truncation = truncation\n\n    def __call__(\n        self, sentence: Union[str, list[str]], **kwargs: Any\n    ) -&gt; dict[str, torch.Tensor]:\n        \"\"\"Tokenize a text or a list of texts using the HuggingFace tokenizer.\n\n        Parameters\n        ----------\n        sentence : Union[str, list[str]]\n            Sentence(s) to be tokenized.\n        **kwargs : Any\n            Additional arguments passed to the tokenizer :meth:`__call__` method.\n\n        Returns\n        -------\n        dict[str, torch.Tensor]\n            Tokenized sentence(s).\n\n        Notes\n        -----\n        The ``input_ids`` key is replaced with ``Modalities.TEXT`` for consistency.\n        \"\"\"\n        batch_encoding = self.tokenizer(\n            sentence,\n            max_length=self.max_length,\n            padding=self.padding,\n            truncation=self.truncation,\n            return_tensors=\"pt\",\n            **kwargs,\n        )\n\n        if isinstance(\n            sentence, str\n        ):  # remove batch dimension if input is a single sentence\n            for key, value in batch_encoding.items():\n                if isinstance(value, torch.Tensor):\n                    batch_encoding[key] = torch.squeeze(value, 0)\n\n        # use 'Modalities.TEXT' key for input_ids for consistency\n        batch_encoding[Modalities.TEXT.name] = batch_encoding[\"input_ids\"]\n        return dict(batch_encoding)\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.tokenizers.HFTokenizer.__call__","title":"__call__","text":"<pre><code>__call__(sentence, **kwargs)\n</code></pre> <p>Tokenize a text or a list of texts using the HuggingFace tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>sentence</code> <code>Union[str, list[str]]</code> <p>Sentence(s) to be tokenized.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to the tokenizer :meth:<code>__call__</code> method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>Tokenized sentence(s).</p> Notes <p>The <code>input_ids</code> key is replaced with <code>Modalities.TEXT</code> for consistency.</p> Source code in <code>mmlearn/datasets/processors/tokenizers.py</code> <pre><code>def __call__(\n    self, sentence: Union[str, list[str]], **kwargs: Any\n) -&gt; dict[str, torch.Tensor]:\n    \"\"\"Tokenize a text or a list of texts using the HuggingFace tokenizer.\n\n    Parameters\n    ----------\n    sentence : Union[str, list[str]]\n        Sentence(s) to be tokenized.\n    **kwargs : Any\n        Additional arguments passed to the tokenizer :meth:`__call__` method.\n\n    Returns\n    -------\n    dict[str, torch.Tensor]\n        Tokenized sentence(s).\n\n    Notes\n    -----\n    The ``input_ids`` key is replaced with ``Modalities.TEXT`` for consistency.\n    \"\"\"\n    batch_encoding = self.tokenizer(\n        sentence,\n        max_length=self.max_length,\n        padding=self.padding,\n        truncation=self.truncation,\n        return_tensors=\"pt\",\n        **kwargs,\n    )\n\n    if isinstance(\n        sentence, str\n    ):  # remove batch dimension if input is a single sentence\n        for key, value in batch_encoding.items():\n            if isinstance(value, torch.Tensor):\n                batch_encoding[key] = torch.squeeze(value, 0)\n\n    # use 'Modalities.TEXT' key for input_ids for consistency\n    batch_encoding[Modalities.TEXT.name] = batch_encoding[\"input_ids\"]\n    return dict(batch_encoding)\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.tokenizers.Img2Seq","title":"Img2Seq","text":"<p>               Bases: <code>Module</code></p> <p>Convert a batch of images to a batch of sequences.</p> <p>Parameters:</p> Name Type Description Default <code>img_size</code> <code>tuple of int</code> <p>The size of the input image.</p> required <code>patch_size</code> <code>tuple of int</code> <p>The size of the patch.</p> required <code>n_channels</code> <code>int</code> <p>The number of channels in the input image.</p> required <code>d_model</code> <code>int</code> <p>The dimension of the output sequence.</p> required Source code in <code>mmlearn/datasets/processors/tokenizers.py</code> <pre><code>class Img2Seq(nn.Module):\n    \"\"\"Convert a batch of images to a batch of sequences.\n\n    Parameters\n    ----------\n    img_size : tuple of int\n        The size of the input image.\n    patch_size : tuple of int\n        The size of the patch.\n    n_channels : int\n        The number of channels in the input image.\n    d_model : int\n        The dimension of the output sequence.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        img_size: tuple[int, int],\n        patch_size: tuple[int, int],\n        n_channels: int,\n        d_model: int,\n    ) -&gt; None:\n        super().__init__()\n        self.patch_size = patch_size\n        self.img_size = img_size\n\n        nh, nw = img_size[0] // patch_size[0], img_size[1] // patch_size[1]\n        n_tokens = nh * nw\n\n        token_dim = patch_size[0] * patch_size[1] * n_channels\n        self.linear = nn.Linear(token_dim, d_model)\n        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n        self.pos_emb = nn.Parameter(torch.randn(n_tokens, d_model))\n\n    def __call__(self, batch: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Convert a batch of images to a batch of sequences.\n\n        Parameters\n        ----------\n        batch : torch.Tensor\n            Batch of images of shape ``(b, h, w, c)`` where ``b`` is the batch size,\n            ``h`` is the height, ``w`` is the width, and ``c`` is the number of\n            channels.\n\n        Returns\n        -------\n        torch.Tensor\n            Batch of sequences of shape ``(b, s, d)`` where ``b`` is the batch size,\n            ``s`` is the sequence length, and ``d`` is the dimension of the output\n            sequence.\n        \"\"\"\n        batch = _patchify(batch, self.patch_size)\n\n        b, c, nh, nw, ph, pw = batch.shape\n\n        # Flattening the patches\n        batch = torch.permute(batch, [0, 2, 3, 4, 5, 1])\n        batch = torch.reshape(batch, [b, nh * nw, ph * pw * c])\n\n        batch = self.linear(batch)\n        cls: torch.Tensor = self.cls_token.expand([b, -1, -1])\n        emb: torch.Tensor = batch + self.pos_emb\n\n        return torch.cat([cls, emb], axis=1)\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.tokenizers.Img2Seq.__call__","title":"__call__","text":"<pre><code>__call__(batch)\n</code></pre> <p>Convert a batch of images to a batch of sequences.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Tensor</code> <p>Batch of images of shape <code>(b, h, w, c)</code> where <code>b</code> is the batch size, <code>h</code> is the height, <code>w</code> is the width, and <code>c</code> is the number of channels.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Batch of sequences of shape <code>(b, s, d)</code> where <code>b</code> is the batch size, <code>s</code> is the sequence length, and <code>d</code> is the dimension of the output sequence.</p> Source code in <code>mmlearn/datasets/processors/tokenizers.py</code> <pre><code>def __call__(self, batch: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Convert a batch of images to a batch of sequences.\n\n    Parameters\n    ----------\n    batch : torch.Tensor\n        Batch of images of shape ``(b, h, w, c)`` where ``b`` is the batch size,\n        ``h`` is the height, ``w`` is the width, and ``c`` is the number of\n        channels.\n\n    Returns\n    -------\n    torch.Tensor\n        Batch of sequences of shape ``(b, s, d)`` where ``b`` is the batch size,\n        ``s`` is the sequence length, and ``d`` is the dimension of the output\n        sequence.\n    \"\"\"\n    batch = _patchify(batch, self.patch_size)\n\n    b, c, nh, nw, ph, pw = batch.shape\n\n    # Flattening the patches\n    batch = torch.permute(batch, [0, 2, 3, 4, 5, 1])\n    batch = torch.reshape(batch, [b, nh * nw, ph * pw * c])\n\n    batch = self.linear(batch)\n    cls: torch.Tensor = self.cls_token.expand([b, -1, -1])\n    emb: torch.Tensor = batch + self.pos_emb\n\n    return torch.cat([cls, emb], axis=1)\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.transforms","title":"transforms","text":"<p>Custom transforms for datasets/inputs.</p>"},{"location":"api/#mmlearn.datasets.processors.transforms.TrimText","title":"TrimText","text":"<p>Trim text strings as a preprocessing step before tokenization.</p> <p>Parameters:</p> Name Type Description Default <code>trim_size</code> <code>int</code> <p>The maximum length of the trimmed text.</p> required Source code in <code>mmlearn/datasets/processors/transforms.py</code> <pre><code>@store(group=\"datasets/transforms\", provider=\"mmlearn\")\nclass TrimText:\n    \"\"\"Trim text strings as a preprocessing step before tokenization.\n\n    Parameters\n    ----------\n    trim_size : int\n        The maximum length of the trimmed text.\n    \"\"\"\n\n    def __init__(self, trim_size: int) -&gt; None:\n        self.trim_size = trim_size\n\n    def __call__(self, sentence: Union[str, list[str]]) -&gt; Union[str, list[str]]:\n        \"\"\"Trim the given sentence(s).\n\n        Parameters\n        ----------\n        sentence : Union[str, list[str]]\n            Sentence(s) to be trimmed.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            Trimmed sentence(s).\n\n        Raises\n        ------\n        TypeError\n            If the input sentence is not a string or list of strings.\n        \"\"\"\n        if not isinstance(sentence, (list, str)):\n            raise TypeError(\n                \"Expected argument `sentence` to be a string or list of strings, \"\n                f\"but got {type(sentence)}\"\n            )\n\n        if isinstance(sentence, str):\n            return sentence[: self.trim_size]\n\n        for i, s in enumerate(sentence):\n            sentence[i] = s[: self.trim_size]\n\n        return sentence\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.transforms.TrimText.__call__","title":"__call__","text":"<pre><code>__call__(sentence)\n</code></pre> <p>Trim the given sentence(s).</p> <p>Parameters:</p> Name Type Description Default <code>sentence</code> <code>Union[str, list[str]]</code> <p>Sentence(s) to be trimmed.</p> required <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>Trimmed sentence(s).</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input sentence is not a string or list of strings.</p> Source code in <code>mmlearn/datasets/processors/transforms.py</code> <pre><code>def __call__(self, sentence: Union[str, list[str]]) -&gt; Union[str, list[str]]:\n    \"\"\"Trim the given sentence(s).\n\n    Parameters\n    ----------\n    sentence : Union[str, list[str]]\n        Sentence(s) to be trimmed.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        Trimmed sentence(s).\n\n    Raises\n    ------\n    TypeError\n        If the input sentence is not a string or list of strings.\n    \"\"\"\n    if not isinstance(sentence, (list, str)):\n        raise TypeError(\n            \"Expected argument `sentence` to be a string or list of strings, \"\n            f\"but got {type(sentence)}\"\n        )\n\n    if isinstance(sentence, str):\n        return sentence[: self.trim_size]\n\n    for i, s in enumerate(sentence):\n        sentence[i] = s[: self.trim_size]\n\n    return sentence\n</code></pre>"},{"location":"api/#mmlearn.datasets.processors.transforms.repeat_interleave_batch","title":"repeat_interleave_batch","text":"<pre><code>repeat_interleave_batch(x, b, repeat)\n</code></pre> <p>Repeat and interleave a tensor across the batch dimension.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor to be repeated.</p> required <code>b</code> <code>int</code> <p>Size of the batch to be repeated.</p> required <code>repeat</code> <code>int</code> <p>Number of times to repeat each batch.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The repeated tensor with shape adjusted for the batch.</p> Source code in <code>mmlearn/datasets/processors/transforms.py</code> <pre><code>def repeat_interleave_batch(x: torch.Tensor, b: int, repeat: int) -&gt; torch.Tensor:\n    \"\"\"Repeat and interleave a tensor across the batch dimension.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor to be repeated.\n    b : int\n        Size of the batch to be repeated.\n    repeat : int\n        Number of times to repeat each batch.\n\n    Returns\n    -------\n    torch.Tensor\n        The repeated tensor with shape adjusted for the batch.\n    \"\"\"\n    n = len(x) // b\n    return torch.cat(\n        [\n            torch.cat([x[i * b : (i + 1) * b] for _ in range(repeat)], dim=0)\n            for i in range(n)\n        ],\n        dim=0,\n    )\n</code></pre>"},{"location":"api/#modules","title":"Modules","text":""},{"location":"api/#mmlearn.modules","title":"mmlearn.modules","text":"<p>Reusable components for building tasks.</p>"},{"location":"api/#mmlearn.modules.ema","title":"ema","text":"<p>Exponential Moving Average (EMA) module.</p>"},{"location":"api/#mmlearn.modules.ema.ExponentialMovingAverage","title":"ExponentialMovingAverage","text":"<p>Exponential Moving Average (EMA) for the input model.</p> <p>At each step the parameter of the EMA model is updates as the weighted average of the model's parameters.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to apply EMA to.</p> required <code>ema_decay</code> <code>float</code> <p>The initial decay value for EMA.</p> required <code>ema_end_decay</code> <code>float</code> <p>The final decay value for EMA.</p> required <code>ema_anneal_end_step</code> <code>int</code> <p>The number of steps to anneal the decay from <code>ema_decay</code> to <code>ema_end_decay</code>.</p> required <code>skip_keys</code> <code>Optional[Union[list[str], Set[str]]]</code> <p>The keys to skip in the EMA update. These parameters will be copied directly from the model to the EMA model.</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If a deep copy of the model cannot be created.</p> Source code in <code>mmlearn/modules/ema.py</code> <pre><code>class ExponentialMovingAverage:\n    \"\"\"Exponential Moving Average (EMA) for the input model.\n\n    At each step the parameter of the EMA model is updates as the weighted average\n    of the model's parameters.\n\n    Parameters\n    ----------\n    model : torch.nn.Module\n        The model to apply EMA to.\n    ema_decay : float\n        The initial decay value for EMA.\n    ema_end_decay : float\n        The final decay value for EMA.\n    ema_anneal_end_step : int\n        The number of steps to anneal the decay from ``ema_decay`` to ``ema_end_decay``.\n    skip_keys : Optional[Union[list[str], Set[str]]], optional, default=None\n        The keys to skip in the EMA update. These parameters will be copied directly\n        from the model to the EMA model.\n\n    Raises\n    ------\n    RuntimeError\n        If a deep copy of the model cannot be created.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: torch.nn.Module,\n        ema_decay: float,\n        ema_end_decay: float,\n        ema_anneal_end_step: int,\n        skip_keys: Optional[Union[list[str], Set[str]]] = None,\n    ) -&gt; None:\n        self.model = self.deepcopy_model(model)\n\n        self.skip_keys: Union[list[str], set[str]] = skip_keys or set()\n        self.num_updates = 0\n        self.decay = ema_decay  # stores the current decay value\n        self.ema_decay = ema_decay\n        self.ema_end_decay = ema_end_decay\n        self.ema_anneal_end_step = ema_anneal_end_step\n\n        self._model_configured = False\n\n    @staticmethod\n    def deepcopy_model(model: torch.nn.Module) -&gt; torch.nn.Module:\n        \"\"\"Deep copy the model.\n\n        Parameters\n        ----------\n        model : torch.nn.Module\n            The model to copy.\n\n        Returns\n        -------\n        torch.nn.Module\n            The copied model.\n\n        Raises\n        ------\n        RuntimeError\n            If the model cannot be copied.\n        \"\"\"\n        try:\n            return copy.deepcopy(model)\n        except RuntimeError as e:\n            raise RuntimeError(\"Unable to copy the model \", e) from e\n\n    @staticmethod\n    def get_annealed_rate(\n        start: float,\n        end: float,\n        curr_step: int,\n        total_steps: int,\n    ) -&gt; float:\n        \"\"\"Calculate EMA annealing rate.\"\"\"\n        r = end - start\n        pct_remaining = 1 - curr_step / total_steps\n        return end - r * pct_remaining\n\n    def configure_model(self, device_id: Union[int, torch.device]) -&gt; None:\n        \"\"\"Configure the model for EMA.\"\"\"\n        if self._model_configured:\n            return\n\n        self.model.requires_grad_(False)\n        self.model.to(device_id)\n\n        self._model_configured = True\n\n    def step(self, new_model: torch.nn.Module) -&gt; None:\n        \"\"\"Perform single EMA update step.\"\"\"\n        if not self._model_configured:\n            raise RuntimeError(\n                \"Model is not configured for EMA. Call `configure_model` first.\"\n            )\n\n        self._update_weights(new_model)\n        self._update_ema_decay()\n\n    def restore(self, model: torch.nn.Module) -&gt; torch.nn.Module:\n        \"\"\"Reassign weights from another model.\n\n        Parameters\n        ----------\n        model : torch.nn.Module\n            Model to load weights from.\n\n        Returns\n        -------\n        torch.nn.Module\n            model with new weights\n        \"\"\"\n        d = self.model.state_dict()\n        model.load_state_dict(d, strict=False)\n        return model\n\n    def state_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Return the state dict of the model.\"\"\"\n        return self.model.state_dict()  # type: ignore[no-any-return]\n\n    @torch.no_grad()  # type: ignore[misc]\n    def _update_weights(self, new_model: torch.nn.Module) -&gt; None:\n        if self.decay &lt; 1:\n            ema_state_dict = {}\n            ema_params = self.model.state_dict()\n\n            for key, param in new_model.state_dict().items():\n                ema_param = ema_params[key].float()\n\n                if param.shape != ema_param.shape:\n                    raise ValueError(\n                        \"Incompatible tensor shapes between student param and teacher param\"\n                        + \"{} vs. {}\".format(param.shape, ema_param.shape)\n                    )\n\n                if key in self.skip_keys or not param.requires_grad:\n                    ema_param = param.to(dtype=ema_param.dtype).clone()\n                else:\n                    ema_param.mul_(self.decay)\n                    ema_param.add_(\n                        param.to(dtype=ema_param.dtype),\n                        alpha=1 - self.decay,\n                    )\n                ema_state_dict[key] = ema_param\n\n            self.model.load_state_dict(ema_state_dict, strict=False)\n            self.num_updates += 1\n        else:\n            rank_zero_warn(\n                \"Exponential Moving Average decay is 1.0, no update is applied to the model.\",\n                stacklevel=1,\n                category=UserWarning,\n            )\n\n    def _update_ema_decay(self) -&gt; None:\n        if self.ema_decay != self.ema_end_decay:\n            if self.num_updates &gt;= self.ema_anneal_end_step:\n                decay = self.ema_end_decay\n            else:\n                decay = self.get_annealed_rate(\n                    self.ema_decay,\n                    self.ema_end_decay,\n                    self.num_updates,\n                    self.ema_anneal_end_step,\n                )\n            self.decay = decay\n</code></pre>"},{"location":"api/#mmlearn.modules.ema.ExponentialMovingAverage.deepcopy_model","title":"deepcopy_model  <code>staticmethod</code>","text":"<pre><code>deepcopy_model(model)\n</code></pre> <p>Deep copy the model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to copy.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>The copied model.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the model cannot be copied.</p> Source code in <code>mmlearn/modules/ema.py</code> <pre><code>@staticmethod\ndef deepcopy_model(model: torch.nn.Module) -&gt; torch.nn.Module:\n    \"\"\"Deep copy the model.\n\n    Parameters\n    ----------\n    model : torch.nn.Module\n        The model to copy.\n\n    Returns\n    -------\n    torch.nn.Module\n        The copied model.\n\n    Raises\n    ------\n    RuntimeError\n        If the model cannot be copied.\n    \"\"\"\n    try:\n        return copy.deepcopy(model)\n    except RuntimeError as e:\n        raise RuntimeError(\"Unable to copy the model \", e) from e\n</code></pre>"},{"location":"api/#mmlearn.modules.ema.ExponentialMovingAverage.get_annealed_rate","title":"get_annealed_rate  <code>staticmethod</code>","text":"<pre><code>get_annealed_rate(start, end, curr_step, total_steps)\n</code></pre> <p>Calculate EMA annealing rate.</p> Source code in <code>mmlearn/modules/ema.py</code> <pre><code>@staticmethod\ndef get_annealed_rate(\n    start: float,\n    end: float,\n    curr_step: int,\n    total_steps: int,\n) -&gt; float:\n    \"\"\"Calculate EMA annealing rate.\"\"\"\n    r = end - start\n    pct_remaining = 1 - curr_step / total_steps\n    return end - r * pct_remaining\n</code></pre>"},{"location":"api/#mmlearn.modules.ema.ExponentialMovingAverage.configure_model","title":"configure_model","text":"<pre><code>configure_model(device_id)\n</code></pre> <p>Configure the model for EMA.</p> Source code in <code>mmlearn/modules/ema.py</code> <pre><code>def configure_model(self, device_id: Union[int, torch.device]) -&gt; None:\n    \"\"\"Configure the model for EMA.\"\"\"\n    if self._model_configured:\n        return\n\n    self.model.requires_grad_(False)\n    self.model.to(device_id)\n\n    self._model_configured = True\n</code></pre>"},{"location":"api/#mmlearn.modules.ema.ExponentialMovingAverage.step","title":"step","text":"<pre><code>step(new_model)\n</code></pre> <p>Perform single EMA update step.</p> Source code in <code>mmlearn/modules/ema.py</code> <pre><code>def step(self, new_model: torch.nn.Module) -&gt; None:\n    \"\"\"Perform single EMA update step.\"\"\"\n    if not self._model_configured:\n        raise RuntimeError(\n            \"Model is not configured for EMA. Call `configure_model` first.\"\n        )\n\n    self._update_weights(new_model)\n    self._update_ema_decay()\n</code></pre>"},{"location":"api/#mmlearn.modules.ema.ExponentialMovingAverage.restore","title":"restore","text":"<pre><code>restore(model)\n</code></pre> <p>Reassign weights from another model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Model to load weights from.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>model with new weights</p> Source code in <code>mmlearn/modules/ema.py</code> <pre><code>def restore(self, model: torch.nn.Module) -&gt; torch.nn.Module:\n    \"\"\"Reassign weights from another model.\n\n    Parameters\n    ----------\n    model : torch.nn.Module\n        Model to load weights from.\n\n    Returns\n    -------\n    torch.nn.Module\n        model with new weights\n    \"\"\"\n    d = self.model.state_dict()\n    model.load_state_dict(d, strict=False)\n    return model\n</code></pre>"},{"location":"api/#mmlearn.modules.ema.ExponentialMovingAverage.state_dict","title":"state_dict","text":"<pre><code>state_dict()\n</code></pre> <p>Return the state dict of the model.</p> Source code in <code>mmlearn/modules/ema.py</code> <pre><code>def state_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Return the state dict of the model.\"\"\"\n    return self.model.state_dict()  # type: ignore[no-any-return]\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders","title":"encoders","text":"<p>Encoders.</p>"},{"location":"api/#mmlearn.modules.encoders.HFCLIPTextEncoder","title":"HFCLIPTextEncoder","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper around the <code>CLIPTextModel</code> from HuggingFace.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>The huggingface model name or a local path from which to load the model.</p> required <code>pretrained</code> <code>bool</code> <p>Whether to load the pretrained weights or not.</p> <code>True</code> <code>pooling_layer</code> <code>Optional[Module]</code> <p>Pooling layer to apply to the last hidden state of the model.</p> <code>None</code> <code>freeze_layers</code> <code>Union[int, float, list[int], bool]</code> <p>Whether to freeze layers of the model and which layers to freeze. If <code>True</code>, all model layers are frozen. If it is an integer, the first <code>N</code> layers of the model are frozen. If it is a float, the first <code>N</code> percent of the layers are frozen. If it is a list of integers, the layers at the indices in the list are frozen.</p> <code>False</code> <code>freeze_layer_norm</code> <code>bool</code> <p>Whether to freeze the layer normalization layers of the model.</p> <code>True</code> <code>peft_config</code> <code>Optional[PeftConfig]</code> <p>The configuration from the <code>peft &lt;https://huggingface.co/docs/peft/index&gt;</code>_ library to use to wrap the model for parameter-efficient finetuning.</p> <code>None</code> <code>model_config_kwargs</code> <code>Optional[dict[str, Any]]</code> <p>Additional keyword arguments to pass to the model configuration.</p> <code>None</code> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If both <code>peft_config</code> and <code>freeze_layers</code> are set. The <code>peft_config</code> will override the <code>freeze_layers</code> setting.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>@store(\n    group=\"modules/encoders\",\n    provider=\"mmlearn\",\n    model_name_or_path=\"openai/clip-vit-base-patch16\",\n    hydra_convert=\"object\",  # required for `peft_config` to be converted to a `PeftConfig` object\n)\nclass HFCLIPTextEncoder(nn.Module):\n    \"\"\"Wrapper around the ``CLIPTextModel`` from HuggingFace.\n\n    Parameters\n    ----------\n    model_name_or_path : str\n        The huggingface model name or a local path from which to load the model.\n    pretrained : bool, default=True\n        Whether to load the pretrained weights or not.\n    pooling_layer : Optional[torch.nn.Module], optional, default=None\n        Pooling layer to apply to the last hidden state of the model.\n    freeze_layers : Union[int, float, list[int], bool], default=False\n        Whether to freeze layers of the model and which layers to freeze. If ``True``,\n        all model layers are frozen. If it is an integer, the first ``N`` layers of\n        the model are frozen. If it is a float, the first ``N`` percent of the layers\n        are frozen. If it is a list of integers, the layers at the indices in the\n        list are frozen.\n    freeze_layer_norm : bool, default=True\n        Whether to freeze the layer normalization layers of the model.\n    peft_config : Optional[PeftConfig], optional, default=None\n        The configuration from the `peft &lt;https://huggingface.co/docs/peft/index&gt;`_\n        library to use to wrap the model for parameter-efficient finetuning.\n    model_config_kwargs : Optional[dict[str, Any]], optional, default=None\n        Additional keyword arguments to pass to the model configuration.\n\n    Warns\n    -----\n    UserWarning\n        If both ``peft_config`` and ``freeze_layers`` are set. The ``peft_config``\n        will override the ``freeze_layers`` setting.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name_or_path: str,\n        pretrained: bool = True,\n        pooling_layer: Optional[nn.Module] = None,\n        freeze_layers: Union[int, float, list[int], bool] = False,\n        freeze_layer_norm: bool = True,\n        peft_config: Optional[\"PeftConfig\"] = None,\n        model_config_kwargs: Optional[dict[str, Any]] = None,\n    ) -&gt; None:\n        super().__init__()\n        _warn_freeze_with_peft(peft_config, freeze_layers)\n\n        model = hf_utils.load_huggingface_model(\n            transformers.CLIPTextModel,\n            model_name_or_path=model_name_or_path,\n            load_pretrained_weights=pretrained,\n            model_config_kwargs=model_config_kwargs,\n        )\n        model = _freeze_text_model(model, freeze_layers, freeze_layer_norm)\n        if peft_config is not None:\n            model = hf_utils._wrap_peft_model(model, peft_config)\n\n        self.model = model\n        self.pooling_layer = pooling_layer\n\n    def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The ``input_ids`` will be expected under the\n            ``Modalities.TEXT`` key.\n\n        Returns\n        -------\n        BaseModelOutput\n            The output of the model, including the last hidden state, all hidden\n            states, and the attention weights, if ``output_attentions`` is set\n            to ``True``.\n        \"\"\"\n        outputs = self.model(\n            input_ids=inputs[Modalities.TEXT.name],\n            attention_mask=inputs.get(\"attention_mask\")\n            or inputs.get(Modalities.TEXT.attention_mask),\n            position_ids=inputs.get(\"position_ids\"),\n            output_attentions=inputs.get(\"output_attentions\"),\n            return_dict=True,\n        )\n        last_hidden_state = outputs.hidden_states[-1]  # NOTE: no layer norm applied\n        if self.pooling_layer:\n            last_hidden_state = self.pooling_layer(last_hidden_state)\n\n        return BaseModelOutput(\n            last_hidden_state=last_hidden_state,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.HFCLIPTextEncoder.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The <code>input_ids</code> will be expected under the <code>Modalities.TEXT</code> key.</p> required <p>Returns:</p> Type Description <code>BaseModelOutput</code> <p>The output of the model, including the last hidden state, all hidden states, and the attention weights, if <code>output_attentions</code> is set to <code>True</code>.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The ``input_ids`` will be expected under the\n        ``Modalities.TEXT`` key.\n\n    Returns\n    -------\n    BaseModelOutput\n        The output of the model, including the last hidden state, all hidden\n        states, and the attention weights, if ``output_attentions`` is set\n        to ``True``.\n    \"\"\"\n    outputs = self.model(\n        input_ids=inputs[Modalities.TEXT.name],\n        attention_mask=inputs.get(\"attention_mask\")\n        or inputs.get(Modalities.TEXT.attention_mask),\n        position_ids=inputs.get(\"position_ids\"),\n        output_attentions=inputs.get(\"output_attentions\"),\n        return_dict=True,\n    )\n    last_hidden_state = outputs.hidden_states[-1]  # NOTE: no layer norm applied\n    if self.pooling_layer:\n        last_hidden_state = self.pooling_layer(last_hidden_state)\n\n    return BaseModelOutput(\n        last_hidden_state=last_hidden_state,\n        hidden_states=outputs.hidden_states,\n        attentions=outputs.attentions,\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.HFCLIPTextEncoderWithProjection","title":"HFCLIPTextEncoderWithProjection","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper around the <code>CLIPTextModelWithProjection</code> from HuggingFace.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>The huggingface model name or a local path from which to load the model.</p> required <code>pretrained</code> <code>bool</code> <p>Whether to load the pretrained weights or not.</p> <code>True</code> <code>use_all_token_embeddings</code> <code>bool</code> <p>Whether to use all token embeddings for the text. If <code>False</code> the first token embedding will be used.</p> <code>False</code> <code>freeze_layers</code> <code>Union[int, float, list[int], bool]</code> <p>Whether to freeze layers of the model and which layers to freeze. If <code>True</code>, all model layers are frozen. If it is an integer, the first <code>N</code> layers of the model are frozen. If it is a float, the first <code>N</code> percent of the layers are frozen. If it is a list of integers, the layers at the indices in the list are frozen.</p> <code>False</code> <code>freeze_layer_norm</code> <code>bool</code> <p>Whether to freeze the layer normalization layers of the model.</p> <code>True</code> <code>peft_config</code> <code>Optional[PeftConfig]</code> <p>The configuration from the <code>peft &lt;https://huggingface.co/docs/peft/index&gt;</code>_ library to use to wrap the model for parameter-efficient finetuning.</p> <code>None</code> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If both <code>peft_config</code> and <code>freeze_layers</code> are set. The <code>peft_config</code> will override the <code>freeze_layers</code> setting.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>@store(\n    group=\"modules/encoders\",\n    provider=\"mmlearn\",\n    model_name_or_path=\"openai/clip-vit-base-patch16\",\n    hydra_convert=\"object\",\n)\nclass HFCLIPTextEncoderWithProjection(nn.Module):\n    \"\"\"Wrapper around the ``CLIPTextModelWithProjection`` from HuggingFace.\n\n    Parameters\n    ----------\n    model_name_or_path : str\n        The huggingface model name or a local path from which to load the model.\n    pretrained : bool, default=True\n        Whether to load the pretrained weights or not.\n    use_all_token_embeddings : bool, default=False\n        Whether to use all token embeddings for the text. If ``False`` the first token\n        embedding will be used.\n    freeze_layers : Union[int, float, list[int], bool], default=False\n        Whether to freeze layers of the model and which layers to freeze. If ``True``,\n        all model layers are frozen. If it is an integer, the first ``N`` layers of\n        the model are frozen. If it is a float, the first ``N`` percent of the layers\n        are frozen. If it is a list of integers, the layers at the indices in the\n        list are frozen.\n    freeze_layer_norm : bool, default=True\n        Whether to freeze the layer normalization layers of the model.\n    peft_config : Optional[PeftConfig], optional, default=None\n        The configuration from the `peft &lt;https://huggingface.co/docs/peft/index&gt;`_\n        library to use to wrap the model for parameter-efficient finetuning.\n\n    Warns\n    -----\n    UserWarning\n        If both ``peft_config`` and ``freeze_layers`` are set. The ``peft_config``\n        will override the ``freeze_layers`` setting.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name_or_path: str,\n        pretrained: bool = True,\n        use_all_token_embeddings: bool = False,\n        freeze_layers: Union[int, float, list[int], bool] = False,\n        freeze_layer_norm: bool = True,\n        peft_config: Optional[\"PeftConfig\"] = None,\n        model_config_kwargs: Optional[dict[str, Any]] = None,\n    ) -&gt; None:\n        super().__init__()\n        _warn_freeze_with_peft(peft_config, freeze_layers)\n\n        self.use_all_token_embeddings = use_all_token_embeddings\n\n        model = hf_utils.load_huggingface_model(\n            transformers.CLIPTextModelWithProjection,\n            model_name_or_path,\n            load_pretrained_weights=pretrained,\n            model_config_kwargs=model_config_kwargs,\n            config_type=CLIPTextConfig,\n        )\n\n        model = _freeze_text_model(model, freeze_layers, freeze_layer_norm)\n        if peft_config is not None:\n            model = hf_utils._wrap_peft_model(model, peft_config)\n\n        self.model = model\n\n    def forward(self, inputs: dict[str, Any]) -&gt; tuple[torch.Tensor]:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The ``input_ids`` will be expected under the\n            ``Modalities.TEXT`` key.\n\n        Returns\n        -------\n        tuple[torch.Tensor]\n            The text embeddings. Will be a tuple with a single element.\n        \"\"\"\n        input_ids = inputs[Modalities.TEXT.name]\n        attention_mask: Optional[torch.Tensor] = inputs.get(\n            \"attention_mask\", inputs.get(Modalities.TEXT.attention_mask, None)\n        )\n        position_ids = inputs.get(\"position_ids\")\n\n        if self.use_all_token_embeddings:\n            text_outputs = self.model.text_model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                return_dict=True,\n            )\n            # TODO: add more options for pooling before projection\n            text_embeds = self.model.text_projection(text_outputs.last_hidden_state)\n        else:\n            text_embeds = self.model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                return_dict=True,\n            ).text_embeds\n\n        return (text_embeds,)\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.HFCLIPTextEncoderWithProjection.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The <code>input_ids</code> will be expected under the <code>Modalities.TEXT</code> key.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor]</code> <p>The text embeddings. Will be a tuple with a single element.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; tuple[torch.Tensor]:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The ``input_ids`` will be expected under the\n        ``Modalities.TEXT`` key.\n\n    Returns\n    -------\n    tuple[torch.Tensor]\n        The text embeddings. Will be a tuple with a single element.\n    \"\"\"\n    input_ids = inputs[Modalities.TEXT.name]\n    attention_mask: Optional[torch.Tensor] = inputs.get(\n        \"attention_mask\", inputs.get(Modalities.TEXT.attention_mask, None)\n    )\n    position_ids = inputs.get(\"position_ids\")\n\n    if self.use_all_token_embeddings:\n        text_outputs = self.model.text_model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            return_dict=True,\n        )\n        # TODO: add more options for pooling before projection\n        text_embeds = self.model.text_projection(text_outputs.last_hidden_state)\n    else:\n        text_embeds = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            return_dict=True,\n        ).text_embeds\n\n    return (text_embeds,)\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.HFCLIPVisionEncoder","title":"HFCLIPVisionEncoder","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper around the <code>CLIPVisionModel</code> from HuggingFace.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>The huggingface model name or a local path from which to load the model.</p> required <code>pretrained</code> <code>bool</code> <p>Whether to load the pretrained weights or not.</p> <code>True</code> <code>pooling_layer</code> <code>Optional[Module]</code> <p>Pooling layer to apply to the last hidden state of the model.</p> <code>None</code> <code>freeze_layers</code> <code>Union[int, float, list[int], bool]</code> <p>Whether to freeze layers of the model and which layers to freeze. If <code>True</code>, all model layers are frozen. If it is an integer, the first <code>N</code> layers of the model are frozen. If it is a float, the first <code>N</code> percent of the layers are frozen. If it is a list of integers, the layers at the indices in the list are frozen.</p> <code>False</code> <code>freeze_layer_norm</code> <code>bool</code> <p>Whether to freeze the layer normalization layers of the model.</p> <code>True</code> <code>patch_dropout_rate</code> <code>float</code> <p>The proportion of patch embeddings to drop out.</p> <code>0.0</code> <code>patch_dropout_shuffle</code> <code>bool</code> <p>Whether to shuffle the patches while applying patch dropout.</p> <code>False</code> <code>patch_dropout_bias</code> <code>Optional[float]</code> <p>The bias to apply to the patch dropout mask.</p> <code>None</code> <code>peft_config</code> <code>Optional[PeftConfig]</code> <p>The configuration from the <code>peft &lt;https://huggingface.co/docs/peft/index&gt;</code>_ library to use to wrap the model for parameter-efficient finetuning.</p> <code>None</code> <code>model_config_kwargs</code> <code>Optional[dict[str, Any]]</code> <p>Additional keyword arguments to pass to the model configuration.</p> <code>None</code> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If both <code>peft_config</code> and <code>freeze_layers</code> are set. The <code>peft_config</code> will override the <code>freeze_layers</code> setting.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>@store(\n    group=\"modules/encoders\",\n    provider=\"mmlearn\",\n    model_name_or_path=\"openai/clip-vit-base-patch16\",\n    hydra_convert=\"object\",\n)\nclass HFCLIPVisionEncoder(nn.Module):\n    \"\"\"Wrapper around the ``CLIPVisionModel`` from HuggingFace.\n\n    Parameters\n    ----------\n    model_name_or_path : str\n        The huggingface model name or a local path from which to load the model.\n    pretrained : bool, default=True\n        Whether to load the pretrained weights or not.\n    pooling_layer : Optional[torch.nn.Module], optional, default=None\n        Pooling layer to apply to the last hidden state of the model.\n    freeze_layers : Union[int, float, list[int], bool], default=False\n        Whether to freeze layers of the model and which layers to freeze. If ``True``,\n        all model layers are frozen. If it is an integer, the first ``N`` layers of\n        the model are frozen. If it is a float, the first ``N`` percent of the layers\n        are frozen. If it is a list of integers, the layers at the indices in the\n        list are frozen.\n    freeze_layer_norm : bool, default=True\n        Whether to freeze the layer normalization layers of the model.\n    patch_dropout_rate : float, default=0.0\n        The proportion of patch embeddings to drop out.\n    patch_dropout_shuffle : bool, default=False\n        Whether to shuffle the patches while applying patch dropout.\n    patch_dropout_bias : Optional[float], optional, default=None\n        The bias to apply to the patch dropout mask.\n    peft_config : Optional[PeftConfig], optional, default=None\n        The configuration from the `peft &lt;https://huggingface.co/docs/peft/index&gt;`_\n        library to use to wrap the model for parameter-efficient finetuning.\n    model_config_kwargs : Optional[dict[str, Any]], optional, default=None\n        Additional keyword arguments to pass to the model configuration.\n\n    Warns\n    -----\n    UserWarning\n        If both ``peft_config`` and ``freeze_layers`` are set. The ``peft_config``\n        will override the ``freeze_layers`` setting.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name_or_path: str,\n        pretrained: bool = True,\n        pooling_layer: Optional[nn.Module] = None,\n        freeze_layers: Union[int, float, list[int], bool] = False,\n        freeze_layer_norm: bool = True,\n        patch_dropout_rate: float = 0.0,\n        patch_dropout_shuffle: bool = False,\n        patch_dropout_bias: Optional[float] = None,\n        peft_config: Optional[\"PeftConfig\"] = None,\n        model_config_kwargs: Optional[dict[str, Any]] = None,\n    ) -&gt; None:\n        super().__init__()\n        _warn_freeze_with_peft(peft_config, freeze_layers)\n\n        model = hf_utils.load_huggingface_model(\n            transformers.CLIPVisionModel,\n            model_name_or_path=model_name_or_path,\n            load_pretrained_weights=pretrained,\n            model_config_kwargs=model_config_kwargs,\n        )\n        model = _freeze_vision_model(model, freeze_layers, freeze_layer_norm)\n        if peft_config is not None:\n            model = hf_utils._wrap_peft_model(model, peft_config)\n\n        self.model = model.vision_model\n        self.pooling_layer = pooling_layer\n        self.patch_dropout = None\n        if patch_dropout_rate &gt; 0:\n            self.patch_dropout = PatchDropout(\n                keep_rate=1 - patch_dropout_rate,\n                token_shuffling=patch_dropout_shuffle,\n                bias=patch_dropout_bias,\n            )\n\n    def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The image tensor will be expected under the\n            ``Modalities.RGB`` key.\n\n        Returns\n        -------\n        BaseModelOutput\n            The output of the model, including the last hidden state, all hidden states,\n            and the attention weights, if ``output_attentions`` is set to ``True``.\n\n        \"\"\"\n        # FIXME: handle other vision modalities\n        pixel_values = inputs[Modalities.RGB.name]\n        hidden_states = self.model.embeddings(pixel_values)\n        if self.patch_dropout is not None:\n            hidden_states = self.patch_dropout(hidden_states)\n        hidden_states = self.model.pre_layrnorm(hidden_states)\n\n        encoder_outputs = self.model.encoder(\n            inputs_embeds=hidden_states,\n            output_attentions=inputs.get(\n                \"output_attentions\", self.model.config.output_attentions\n            ),\n            output_hidden_states=True,\n            return_dict=True,\n        )\n\n        last_hidden_state = encoder_outputs[0]\n        if self.pooling_layer is not None:\n            last_hidden_state = self.pooling_layer(last_hidden_state)\n\n        return BaseModelOutput(\n            last_hidden_state=last_hidden_state,\n            hidden_states=encoder_outputs.hidden_states,\n            attentions=encoder_outputs.attentions,\n        )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.HFCLIPVisionEncoder.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The image tensor will be expected under the <code>Modalities.RGB</code> key.</p> required <p>Returns:</p> Type Description <code>BaseModelOutput</code> <p>The output of the model, including the last hidden state, all hidden states, and the attention weights, if <code>output_attentions</code> is set to <code>True</code>.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The image tensor will be expected under the\n        ``Modalities.RGB`` key.\n\n    Returns\n    -------\n    BaseModelOutput\n        The output of the model, including the last hidden state, all hidden states,\n        and the attention weights, if ``output_attentions`` is set to ``True``.\n\n    \"\"\"\n    # FIXME: handle other vision modalities\n    pixel_values = inputs[Modalities.RGB.name]\n    hidden_states = self.model.embeddings(pixel_values)\n    if self.patch_dropout is not None:\n        hidden_states = self.patch_dropout(hidden_states)\n    hidden_states = self.model.pre_layrnorm(hidden_states)\n\n    encoder_outputs = self.model.encoder(\n        inputs_embeds=hidden_states,\n        output_attentions=inputs.get(\n            \"output_attentions\", self.model.config.output_attentions\n        ),\n        output_hidden_states=True,\n        return_dict=True,\n    )\n\n    last_hidden_state = encoder_outputs[0]\n    if self.pooling_layer is not None:\n        last_hidden_state = self.pooling_layer(last_hidden_state)\n\n    return BaseModelOutput(\n        last_hidden_state=last_hidden_state,\n        hidden_states=encoder_outputs.hidden_states,\n        attentions=encoder_outputs.attentions,\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.HFCLIPVisionEncoderWithProjection","title":"HFCLIPVisionEncoderWithProjection","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper around the <code>CLIPVisionModelWithProjection</code> class from HuggingFace.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>The huggingface model name or a local path from which to load the model.</p> required <code>pretrained</code> <code>bool</code> <p>Whether to load the pretrained weights or not.</p> <code>True</code> <code>use_all_token_embeddings</code> <code>bool</code> <p>Whether to use all token embeddings for the text. If <code>False</code> the first token embedding will be used.</p> <code>False</code> <code>freeze_layers</code> <code>Union[int, float, list[int], bool]</code> <p>Whether to freeze layers of the model and which layers to freeze. If <code>True</code>, all model layers are frozen. If it is an integer, the first <code>N` layers of the model are frozen. If it is a float, the first</code>N`` percent of the layers are frozen. If it is a list of integers, the layers at the indices in the list are frozen.</p> <code>False</code> <code>freeze_layer_norm</code> <code>bool</code> <p>Whether to freeze the layer normalization layers of the model.</p> <code>True</code> <code>patch_dropout_rate</code> <code>float</code> <p>The proportion of patch embeddings to drop out.</p> <code>0.0</code> <code>patch_dropout_shuffle</code> <code>bool</code> <p>Whether to shuffle the patches while applying patch dropout.</p> <code>False</code> <code>patch_dropout_bias</code> <code>float</code> <p>The bias to apply to the patch dropout mask.</p> <code>None</code> <code>peft_config</code> <code>Optional[PeftConfig]</code> <p>The configuration from the <code>peft &lt;https://huggingface.co/docs/peft/index&gt;</code>_ library to use to wrap the model for parameter-efficient finetuning.</p> <code>None</code> <code>model_config_kwargs</code> <code>dict[str, Any]</code> <p>Additional keyword arguments to pass to the model configuration.</p> <code>None</code> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If both <code>peft_config</code> and <code>freeze_layers</code> are set. The <code>peft_config</code> will override the <code>freeze_layers</code> setting.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>@store(\n    group=\"modules/encoders\",\n    provider=\"mmlearn\",\n    model_name_or_path=\"openai/clip-vit-base-patch16\",\n    hydra_convert=\"object\",\n)\nclass HFCLIPVisionEncoderWithProjection(nn.Module):\n    \"\"\"Wrapper around the ``CLIPVisionModelWithProjection`` class from HuggingFace.\n\n    Parameters\n    ----------\n    model_name_or_path : str\n        The huggingface model name or a local path from which to load the model.\n    pretrained : bool, default=True\n        Whether to load the pretrained weights or not.\n    use_all_token_embeddings : bool, default=False\n        Whether to use all token embeddings for the text. If ``False`` the first token\n        embedding will be used.\n    freeze_layers : Union[int, float, list[int], bool], default=False\n        Whether to freeze layers of the model and which layers to freeze. If ``True``,\n        all model layers are frozen. If it is an integer, the first ``N` layers of\n        the model are frozen. If it is a float, the first ``N`` percent of the layers\n        are frozen. If it is a list of integers, the layers at the indices in the\n        list are frozen.\n    freeze_layer_norm : bool, default=True\n        Whether to freeze the layer normalization layers of the model.\n    patch_dropout_rate : float, default=0.0\n        The proportion of patch embeddings to drop out.\n    patch_dropout_shuffle : bool, default=False\n        Whether to shuffle the patches while applying patch dropout.\n    patch_dropout_bias : float, optional, default=None\n        The bias to apply to the patch dropout mask.\n    peft_config : Optional[PeftConfig], optional, default=None\n        The configuration from the `peft &lt;https://huggingface.co/docs/peft/index&gt;`_\n        library to use to wrap the model for parameter-efficient finetuning.\n    model_config_kwargs : dict[str, Any], optional, default=None\n        Additional keyword arguments to pass to the model configuration.\n\n    Warns\n    -----\n    UserWarning\n        If both ``peft_config`` and ``freeze_layers`` are set. The ``peft_config``\n        will override the ``freeze_layers`` setting.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name_or_path: str,\n        pretrained: bool = True,\n        use_all_token_embeddings: bool = False,\n        patch_dropout_rate: float = 0.0,\n        patch_dropout_shuffle: bool = False,\n        patch_dropout_bias: Optional[float] = None,\n        freeze_layers: Union[int, float, list[int], bool] = False,\n        freeze_layer_norm: bool = True,\n        peft_config: Optional[\"PeftConfig\"] = None,\n        model_config_kwargs: Optional[dict[str, Any]] = None,\n    ) -&gt; None:\n        super().__init__()\n        _warn_freeze_with_peft(peft_config, freeze_layers)\n\n        self.use_all_token_embeddings = use_all_token_embeddings\n\n        model = hf_utils.load_huggingface_model(\n            transformers.CLIPVisionModelWithProjection,\n            model_name_or_path,\n            load_pretrained_weights=pretrained,\n            model_config_kwargs=model_config_kwargs,\n            config_type=CLIPVisionConfig,\n        )\n\n        model = _freeze_vision_model(model, freeze_layers, freeze_layer_norm)\n        if peft_config is not None:\n            model = hf_utils._wrap_peft_model(model, peft_config)\n\n        self.model = model\n        self.patch_dropout = None\n        if patch_dropout_rate &gt; 0:\n            self.patch_dropout = PatchDropout(\n                keep_rate=1 - patch_dropout_rate,\n                token_shuffling=patch_dropout_shuffle,\n                bias=patch_dropout_bias,\n            )\n\n    def forward(self, inputs: dict[str, Any]) -&gt; tuple[torch.Tensor]:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The image tensor will be expected under the\n            ``Modalities.RGB`` key.\n\n        Returns\n        -------\n        tuple[torch.Tensor]\n            The image embeddings. Will be a tuple with a single element.\n        \"\"\"\n        pixel_values = inputs[Modalities.RGB.name]\n        hidden_states = self.model.vision_model.embeddings(pixel_values)\n        if self.patch_dropout is not None:\n            hidden_states = self.patch_dropout(hidden_states)\n        hidden_states = self.model.vision_model.pre_layrnorm(hidden_states)\n\n        encoder_outputs = self.model.vision_model.encoder(\n            inputs_embeds=hidden_states, return_dict=True\n        )\n\n        last_hidden_state = encoder_outputs.last_hidden_state\n        if self.use_all_token_embeddings:\n            pooled_output = last_hidden_state\n        else:\n            pooled_output = last_hidden_state[:, 0, :]\n        pooled_output = self.model.vision_model.post_layernorm(pooled_output)\n\n        return (self.model.visual_projection(pooled_output),)\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.HFCLIPVisionEncoderWithProjection.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The image tensor will be expected under the <code>Modalities.RGB</code> key.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor]</code> <p>The image embeddings. Will be a tuple with a single element.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; tuple[torch.Tensor]:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The image tensor will be expected under the\n        ``Modalities.RGB`` key.\n\n    Returns\n    -------\n    tuple[torch.Tensor]\n        The image embeddings. Will be a tuple with a single element.\n    \"\"\"\n    pixel_values = inputs[Modalities.RGB.name]\n    hidden_states = self.model.vision_model.embeddings(pixel_values)\n    if self.patch_dropout is not None:\n        hidden_states = self.patch_dropout(hidden_states)\n    hidden_states = self.model.vision_model.pre_layrnorm(hidden_states)\n\n    encoder_outputs = self.model.vision_model.encoder(\n        inputs_embeds=hidden_states, return_dict=True\n    )\n\n    last_hidden_state = encoder_outputs.last_hidden_state\n    if self.use_all_token_embeddings:\n        pooled_output = last_hidden_state\n    else:\n        pooled_output = last_hidden_state[:, 0, :]\n    pooled_output = self.model.vision_model.post_layernorm(pooled_output)\n\n    return (self.model.visual_projection(pooled_output),)\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.HFTextEncoder","title":"HFTextEncoder","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper around huggingface models in the <code>AutoModelForTextEncoding</code> class.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>The huggingface model name or a local path from which to load the model.</p> required <code>pretrained</code> <code>bool</code> <p>Whether to load the pretrained weights or not.</p> <code>True</code> <code>pooling_layer</code> <code>Optional[Module]</code> <p>Pooling layer to apply to the last hidden state of the model.</p> <code>None</code> <code>freeze_layers</code> <code>Union[int, float, list[int], bool]</code> <p>Whether to freeze layers of the model and which layers to freeze. If <code>True</code>, all model layers are frozen. If it is an integer, the first <code>N</code> layers of the model are frozen. If it is a float, the first <code>N</code> percent of the layers are frozen. If it is a list of integers, the layers at the indices in the list are frozen.</p> <code>False</code> <code>freeze_layer_norm</code> <code>bool</code> <p>Whether to freeze the layer normalization layers of the model.</p> <code>True</code> <code>peft_config</code> <code>Optional[PeftConfig]</code> <p>The configuration from the <code>peft &lt;https://huggingface.co/docs/peft/index&gt;</code>_ library to use to wrap the model for parameter-efficient finetuning.</p> <code>None</code> <code>model_config_kwargs</code> <code>Optional[dict[str, Any]]</code> <p>Additional keyword arguments to pass to the model configuration.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model is a decoder model or if freezing individual layers is not supported for the model type.</p> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If both <code>peft_config</code> and <code>freeze_layers</code> are set. The <code>peft_config</code> will override the <code>freeze_layers</code> setting.</p> Source code in <code>mmlearn/modules/encoders/text.py</code> <pre><code>@store(group=\"modules/encoders\", provider=\"mmlearn\", hydra_convert=\"object\")\nclass HFTextEncoder(nn.Module):\n    \"\"\"Wrapper around huggingface models in the ``AutoModelForTextEncoding`` class.\n\n    Parameters\n    ----------\n    model_name_or_path : str\n        The huggingface model name or a local path from which to load the model.\n    pretrained : bool, default=True\n        Whether to load the pretrained weights or not.\n    pooling_layer : Optional[torch.nn.Module], optional, default=None\n        Pooling layer to apply to the last hidden state of the model.\n    freeze_layers : Union[int, float, list[int], bool], default=False\n        Whether to freeze layers of the model and which layers to freeze. If ``True``,\n        all model layers are frozen. If it is an integer, the first ``N`` layers of\n        the model are frozen. If it is a float, the first ``N`` percent of the layers\n        are frozen. If it is a list of integers, the layers at the indices in the\n        list are frozen.\n    freeze_layer_norm : bool, default=True\n        Whether to freeze the layer normalization layers of the model.\n    peft_config : Optional[PeftConfig], optional, default=None\n        The configuration from the `peft &lt;https://huggingface.co/docs/peft/index&gt;`_\n        library to use to wrap the model for parameter-efficient finetuning.\n    model_config_kwargs : Optional[dict[str, Any]], optional, default=None\n        Additional keyword arguments to pass to the model configuration.\n\n    Raises\n    ------\n    ValueError\n        If the model is a decoder model or if freezing individual layers is not\n        supported for the model type.\n\n    Warns\n    -----\n    UserWarning\n        If both ``peft_config`` and ``freeze_layers`` are set. The ``peft_config``\n        will override the ``freeze_layers`` setting.\n\n\n    \"\"\"\n\n    def __init__(  # noqa: PLR0912\n        self,\n        model_name_or_path: str,\n        pretrained: bool = True,\n        pooling_layer: Optional[nn.Module] = None,\n        freeze_layers: Union[int, float, list[int], bool] = False,\n        freeze_layer_norm: bool = True,\n        peft_config: Optional[\"PeftConfig\"] = None,\n        model_config_kwargs: Optional[dict[str, Any]] = None,\n    ):\n        super().__init__()\n        if model_config_kwargs is None:\n            model_config_kwargs = {}\n        model_config_kwargs[\"output_hidden_states\"] = True\n        model_config_kwargs[\"add_pooling_layer\"] = False\n        model = hf_utils.load_huggingface_model(\n            AutoModelForTextEncoding,\n            model_name_or_path,\n            load_pretrained_weights=pretrained,\n            model_config_kwargs=model_config_kwargs,\n        )\n        if hasattr(model.config, \"is_decoder\") and model.config.is_decoder:\n            raise ValueError(\"Model is a decoder. Only encoder models are supported.\")\n\n        if not pretrained and freeze_layers:\n            rank_zero_warn(\n                \"Freezing layers when loading a model with random weights may lead to \"\n                \"unexpected behavior. Consider setting `freeze_layers=False` if \"\n                \"`pretrained=False`.\",\n            )\n\n        if isinstance(freeze_layers, bool) and freeze_layers:\n            for name, param in model.named_parameters():\n                param.requires_grad = (\n                    (not freeze_layer_norm) if \"LayerNorm\" in name else False\n                )\n\n        if isinstance(\n            freeze_layers, (float, int, list)\n        ) and model.config.model_type in [\"flaubert\", \"xlm\"]:\n            # flaubert and xlm models have a different architecture that does not\n            # support freezing individual layers in the same way as other models\n            raise ValueError(\n                f\"Freezing individual layers is not supported for {model.config.model_type} \"\n                \"models. Please use `freeze_layers=False` or `freeze_layers=True`.\"\n            )\n\n        # get list of layers\n        embeddings = model.embeddings\n        encoder = getattr(model, \"encoder\", None) or getattr(\n            model, \"transformer\", model\n        )\n        encoder_layers = (\n            getattr(encoder, \"layer\", None)\n            or getattr(encoder, \"layers\", None)\n            or getattr(encoder, \"block\", None)\n        )\n        if encoder_layers is None and hasattr(encoder, \"albert_layer_groups\"):\n            encoder_layers = [\n                layer\n                for group in encoder.albert_layer_groups\n                for layer in group.albert_layers\n            ]\n        modules = [embeddings]\n        if encoder_layers is not None and isinstance(encoder_layers, list):\n            modules.extend(encoder_layers)\n\n        if isinstance(freeze_layers, float):\n            freeze_layers = int(freeze_layers * len(modules))\n        if isinstance(freeze_layers, int):\n            freeze_layers = list(range(freeze_layers))\n\n        if isinstance(freeze_layers, list):\n            for idx, module in enumerate(modules):\n                if idx in freeze_layers:\n                    for name, param in module.named_parameters():\n                        param.requires_grad = (\n                            (not freeze_layer_norm) if \"LayerNorm\" in name else False\n                        )\n\n        if peft_config is not None:\n            model = hf_utils._wrap_peft_model(model, peft_config)\n\n        self.model = model\n        self.pooling_layer = pooling_layer\n\n    def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The ``input_ids`` will be expected under the\n            ``Modalities.TEXT`` key.\n\n        Returns\n        -------\n        BaseModelOutput\n            The output of the model, including the last hidden state, all hidden states,\n            and the attention weights, if ``output_attentions`` is set to ``True``.\n        \"\"\"\n        outputs = self.model(\n            input_ids=inputs[Modalities.TEXT.name],\n            attention_mask=inputs.get(\n                \"attention_mask\", inputs.get(Modalities.TEXT.attention_mask, None)\n            ),\n            position_ids=inputs.get(\"position_ids\"),\n            output_attentions=inputs.get(\"output_attentions\"),\n            return_dict=True,\n        )\n        last_hidden_state = outputs.hidden_states[-1]  # NOTE: no layer norm applied\n        if self.pooling_layer:\n            last_hidden_state = self.pooling_layer(last_hidden_state)\n\n        return BaseModelOutput(\n            last_hidden_state=last_hidden_state,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.HFTextEncoder.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The <code>input_ids</code> will be expected under the <code>Modalities.TEXT</code> key.</p> required <p>Returns:</p> Type Description <code>BaseModelOutput</code> <p>The output of the model, including the last hidden state, all hidden states, and the attention weights, if <code>output_attentions</code> is set to <code>True</code>.</p> Source code in <code>mmlearn/modules/encoders/text.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The ``input_ids`` will be expected under the\n        ``Modalities.TEXT`` key.\n\n    Returns\n    -------\n    BaseModelOutput\n        The output of the model, including the last hidden state, all hidden states,\n        and the attention weights, if ``output_attentions`` is set to ``True``.\n    \"\"\"\n    outputs = self.model(\n        input_ids=inputs[Modalities.TEXT.name],\n        attention_mask=inputs.get(\n            \"attention_mask\", inputs.get(Modalities.TEXT.attention_mask, None)\n        ),\n        position_ids=inputs.get(\"position_ids\"),\n        output_attentions=inputs.get(\"output_attentions\"),\n        return_dict=True,\n    )\n    last_hidden_state = outputs.hidden_states[-1]  # NOTE: no layer norm applied\n    if self.pooling_layer:\n        last_hidden_state = self.pooling_layer(last_hidden_state)\n\n    return BaseModelOutput(\n        last_hidden_state=last_hidden_state,\n        hidden_states=outputs.hidden_states,\n        attentions=outputs.attentions,\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.TimmViT","title":"TimmViT","text":"<p>               Bases: <code>Module</code></p> <p>Vision Transformer model from timm.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the model to use.</p> required <code>modality</code> <code>str</code> <p>The modality of the input data. This allows this model to be used with different image modalities e.g. RGB, Depth, etc.</p> <code>\"RGB\"</code> <code>projection_dim</code> <code>int</code> <p>The dimension of the projection head.</p> <code>768</code> <code>pretrained</code> <code>bool</code> <p>Whether to use the pretrained weights.</p> <code>True</code> <code>freeze_layers</code> <code>Union[int, float, list[int], bool]</code> <p>Whether to freeze the layers.</p> <code>False</code> <code>freeze_layer_norm</code> <code>bool</code> <p>Whether to freeze the layer norm.</p> <code>True</code> <code>peft_config</code> <code>Optional[PeftConfig]</code> <p>The configuration from the <code>peft &lt;https://huggingface.co/docs/peft/index&gt;</code>_ library to use to wrap the model for parameter-efficient finetuning.</p> <code>None</code> <code>model_kwargs</code> <code>Optional[dict[str, Any]]</code> <p>Additional keyword arguments for the model.</p> <code>None</code> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>@store(\n    group=\"modules/encoders\",\n    provider=\"mmlearn\",\n    model_name=\"vit_base_patch16_224\",\n    hydra_convert=\"object\",\n)\nclass TimmViT(nn.Module):\n    \"\"\"Vision Transformer model from timm.\n\n    Parameters\n    ----------\n    model_name : str\n        The name of the model to use.\n    modality : str, default=\"RGB\"\n        The modality of the input data. This allows this model to be used with different\n        image modalities e.g. RGB, Depth, etc.\n    projection_dim : int, default=768\n        The dimension of the projection head.\n    pretrained : bool, default=True\n        Whether to use the pretrained weights.\n    freeze_layers : Union[int, float, list[int], bool], default=False\n        Whether to freeze the layers.\n    freeze_layer_norm : bool, default=True\n        Whether to freeze the layer norm.\n    peft_config : Optional[PeftConfig], optional, default=None\n        The configuration from the `peft &lt;https://huggingface.co/docs/peft/index&gt;`_\n        library to use to wrap the model for parameter-efficient finetuning.\n    model_kwargs : Optional[dict[str, Any]], default=None\n        Additional keyword arguments for the model.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str,\n        modality: str = \"RGB\",\n        projection_dim: int = 768,\n        pretrained: bool = True,\n        freeze_layers: Union[int, float, list[int], bool] = False,\n        freeze_layer_norm: bool = True,\n        peft_config: Optional[\"PeftConfig\"] = None,\n        model_kwargs: Optional[dict[str, Any]] = None,\n    ) -&gt; None:\n        super().__init__()\n        self.modality = Modalities.get_modality(modality)\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        self.model: TimmVisionTransformer = timm.create_model(\n            model_name,\n            pretrained=pretrained,\n            num_classes=projection_dim,\n            **model_kwargs,\n        )\n        assert isinstance(self.model, TimmVisionTransformer), (\n            f\"Model {model_name} is not a Vision Transformer. \"\n            \"Please provide a model name that corresponds to a Vision Transformer.\"\n        )\n\n        self._freeze_layers(freeze_layers, freeze_layer_norm)\n\n        if peft_config is not None:\n            self.model = hf_utils._wrap_peft_model(self.model, peft_config)\n\n    def _freeze_layers(\n        self, freeze_layers: Union[int, float, list[int], bool], freeze_layer_norm: bool\n    ) -&gt; None:\n        \"\"\"Freeze the layers of the model.\n\n        Parameters\n        ----------\n        freeze_layers : Union[int, float, list[int], bool]\n            Whether to freeze the layers.\n        freeze_layer_norm : bool\n            Whether to freeze the layer norm.\n        \"\"\"\n        if isinstance(freeze_layers, bool) and freeze_layers:\n            for name, param in self.model.named_parameters():\n                param.requires_grad = (\n                    (not freeze_layer_norm) if \"norm\" in name else False\n                )\n\n        modules = [self.model.patch_embed, *self.model.blocks, self.model.norm]\n        if isinstance(freeze_layers, float):\n            freeze_layers = int(freeze_layers * len(modules))\n        if isinstance(freeze_layers, int):\n            freeze_layers = list(range(freeze_layers))\n\n        if isinstance(freeze_layers, list):\n            for idx, module in enumerate(modules):\n                if idx in freeze_layers:\n                    for name, param in module.named_parameters():\n                        param.requires_grad = (\n                            (not freeze_layer_norm) if \"norm\" in name else False\n                        )\n\n    def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The ``image`` will be expected under the\n            ``Modalities.RGB`` key.\n\n        Returns\n        -------\n        BaseModelOutput\n            The output of the model.\n        \"\"\"\n        x = inputs[self.modality.name]\n        last_hidden_state, hidden_states = self.model.forward_intermediates(\n            x, output_fmt=\"NLC\"\n        )\n        last_hidden_state = self.model.forward_head(last_hidden_state)\n\n        return BaseModelOutput(\n            last_hidden_state=last_hidden_state, hidden_states=hidden_states\n        )\n\n    def get_intermediate_layers(\n        self, inputs: dict[str, Any], n: int = 1\n    ) -&gt; list[torch.Tensor]:\n        \"\"\"Get the output of the intermediate layers.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The ``image`` will be expected under the ``Modalities.RGB``\n            key.\n        n : int, default=1\n            The number of intermediate layers to return.\n\n        Returns\n        -------\n        list[torch.Tensor]\n            The outputs of the last n intermediate layers.\n        \"\"\"\n        return self.model.get_intermediate_layers(inputs[Modalities.RGB], n)  # type: ignore\n\n    def get_patch_info(self) -&gt; tuple[int, int]:\n        \"\"\"Get patch size and number of patches.\n\n        Returns\n        -------\n        tuple[int, int]\n            Patch size and number of patches.\n        \"\"\"\n        patch_size = self.model.patch_embed.patch_size[0]\n        num_patches = self.model.patch_embed.num_patches\n        return patch_size, num_patches\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.TimmViT.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The <code>image</code> will be expected under the <code>Modalities.RGB</code> key.</p> required <p>Returns:</p> Type Description <code>BaseModelOutput</code> <p>The output of the model.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The ``image`` will be expected under the\n        ``Modalities.RGB`` key.\n\n    Returns\n    -------\n    BaseModelOutput\n        The output of the model.\n    \"\"\"\n    x = inputs[self.modality.name]\n    last_hidden_state, hidden_states = self.model.forward_intermediates(\n        x, output_fmt=\"NLC\"\n    )\n    last_hidden_state = self.model.forward_head(last_hidden_state)\n\n    return BaseModelOutput(\n        last_hidden_state=last_hidden_state, hidden_states=hidden_states\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.TimmViT.get_intermediate_layers","title":"get_intermediate_layers","text":"<pre><code>get_intermediate_layers(inputs, n=1)\n</code></pre> <p>Get the output of the intermediate layers.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The <code>image</code> will be expected under the <code>Modalities.RGB</code> key.</p> required <code>n</code> <code>int</code> <p>The number of intermediate layers to return.</p> <code>1</code> <p>Returns:</p> Type Description <code>list[Tensor]</code> <p>The outputs of the last n intermediate layers.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>def get_intermediate_layers(\n    self, inputs: dict[str, Any], n: int = 1\n) -&gt; list[torch.Tensor]:\n    \"\"\"Get the output of the intermediate layers.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The ``image`` will be expected under the ``Modalities.RGB``\n        key.\n    n : int, default=1\n        The number of intermediate layers to return.\n\n    Returns\n    -------\n    list[torch.Tensor]\n        The outputs of the last n intermediate layers.\n    \"\"\"\n    return self.model.get_intermediate_layers(inputs[Modalities.RGB], n)  # type: ignore\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.TimmViT.get_patch_info","title":"get_patch_info","text":"<pre><code>get_patch_info()\n</code></pre> <p>Get patch size and number of patches.</p> <p>Returns:</p> Type Description <code>tuple[int, int]</code> <p>Patch size and number of patches.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>def get_patch_info(self) -&gt; tuple[int, int]:\n    \"\"\"Get patch size and number of patches.\n\n    Returns\n    -------\n    tuple[int, int]\n        Patch size and number of patches.\n    \"\"\"\n    patch_size = self.model.patch_embed.patch_size[0]\n    num_patches = self.model.patch_embed.num_patches\n    return patch_size, num_patches\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.clip","title":"clip","text":"<p>Wrappers and interfaces for CLIP models.</p>"},{"location":"api/#mmlearn.modules.encoders.clip.HFCLIPTextEncoder","title":"HFCLIPTextEncoder","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper around the <code>CLIPTextModel</code> from HuggingFace.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>The huggingface model name or a local path from which to load the model.</p> required <code>pretrained</code> <code>bool</code> <p>Whether to load the pretrained weights or not.</p> <code>True</code> <code>pooling_layer</code> <code>Optional[Module]</code> <p>Pooling layer to apply to the last hidden state of the model.</p> <code>None</code> <code>freeze_layers</code> <code>Union[int, float, list[int], bool]</code> <p>Whether to freeze layers of the model and which layers to freeze. If <code>True</code>, all model layers are frozen. If it is an integer, the first <code>N</code> layers of the model are frozen. If it is a float, the first <code>N</code> percent of the layers are frozen. If it is a list of integers, the layers at the indices in the list are frozen.</p> <code>False</code> <code>freeze_layer_norm</code> <code>bool</code> <p>Whether to freeze the layer normalization layers of the model.</p> <code>True</code> <code>peft_config</code> <code>Optional[PeftConfig]</code> <p>The configuration from the <code>peft &lt;https://huggingface.co/docs/peft/index&gt;</code>_ library to use to wrap the model for parameter-efficient finetuning.</p> <code>None</code> <code>model_config_kwargs</code> <code>Optional[dict[str, Any]]</code> <p>Additional keyword arguments to pass to the model configuration.</p> <code>None</code> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If both <code>peft_config</code> and <code>freeze_layers</code> are set. The <code>peft_config</code> will override the <code>freeze_layers</code> setting.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>@store(\n    group=\"modules/encoders\",\n    provider=\"mmlearn\",\n    model_name_or_path=\"openai/clip-vit-base-patch16\",\n    hydra_convert=\"object\",  # required for `peft_config` to be converted to a `PeftConfig` object\n)\nclass HFCLIPTextEncoder(nn.Module):\n    \"\"\"Wrapper around the ``CLIPTextModel`` from HuggingFace.\n\n    Parameters\n    ----------\n    model_name_or_path : str\n        The huggingface model name or a local path from which to load the model.\n    pretrained : bool, default=True\n        Whether to load the pretrained weights or not.\n    pooling_layer : Optional[torch.nn.Module], optional, default=None\n        Pooling layer to apply to the last hidden state of the model.\n    freeze_layers : Union[int, float, list[int], bool], default=False\n        Whether to freeze layers of the model and which layers to freeze. If ``True``,\n        all model layers are frozen. If it is an integer, the first ``N`` layers of\n        the model are frozen. If it is a float, the first ``N`` percent of the layers\n        are frozen. If it is a list of integers, the layers at the indices in the\n        list are frozen.\n    freeze_layer_norm : bool, default=True\n        Whether to freeze the layer normalization layers of the model.\n    peft_config : Optional[PeftConfig], optional, default=None\n        The configuration from the `peft &lt;https://huggingface.co/docs/peft/index&gt;`_\n        library to use to wrap the model for parameter-efficient finetuning.\n    model_config_kwargs : Optional[dict[str, Any]], optional, default=None\n        Additional keyword arguments to pass to the model configuration.\n\n    Warns\n    -----\n    UserWarning\n        If both ``peft_config`` and ``freeze_layers`` are set. The ``peft_config``\n        will override the ``freeze_layers`` setting.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name_or_path: str,\n        pretrained: bool = True,\n        pooling_layer: Optional[nn.Module] = None,\n        freeze_layers: Union[int, float, list[int], bool] = False,\n        freeze_layer_norm: bool = True,\n        peft_config: Optional[\"PeftConfig\"] = None,\n        model_config_kwargs: Optional[dict[str, Any]] = None,\n    ) -&gt; None:\n        super().__init__()\n        _warn_freeze_with_peft(peft_config, freeze_layers)\n\n        model = hf_utils.load_huggingface_model(\n            transformers.CLIPTextModel,\n            model_name_or_path=model_name_or_path,\n            load_pretrained_weights=pretrained,\n            model_config_kwargs=model_config_kwargs,\n        )\n        model = _freeze_text_model(model, freeze_layers, freeze_layer_norm)\n        if peft_config is not None:\n            model = hf_utils._wrap_peft_model(model, peft_config)\n\n        self.model = model\n        self.pooling_layer = pooling_layer\n\n    def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The ``input_ids`` will be expected under the\n            ``Modalities.TEXT`` key.\n\n        Returns\n        -------\n        BaseModelOutput\n            The output of the model, including the last hidden state, all hidden\n            states, and the attention weights, if ``output_attentions`` is set\n            to ``True``.\n        \"\"\"\n        outputs = self.model(\n            input_ids=inputs[Modalities.TEXT.name],\n            attention_mask=inputs.get(\"attention_mask\")\n            or inputs.get(Modalities.TEXT.attention_mask),\n            position_ids=inputs.get(\"position_ids\"),\n            output_attentions=inputs.get(\"output_attentions\"),\n            return_dict=True,\n        )\n        last_hidden_state = outputs.hidden_states[-1]  # NOTE: no layer norm applied\n        if self.pooling_layer:\n            last_hidden_state = self.pooling_layer(last_hidden_state)\n\n        return BaseModelOutput(\n            last_hidden_state=last_hidden_state,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.clip.HFCLIPTextEncoder.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The <code>input_ids</code> will be expected under the <code>Modalities.TEXT</code> key.</p> required <p>Returns:</p> Type Description <code>BaseModelOutput</code> <p>The output of the model, including the last hidden state, all hidden states, and the attention weights, if <code>output_attentions</code> is set to <code>True</code>.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The ``input_ids`` will be expected under the\n        ``Modalities.TEXT`` key.\n\n    Returns\n    -------\n    BaseModelOutput\n        The output of the model, including the last hidden state, all hidden\n        states, and the attention weights, if ``output_attentions`` is set\n        to ``True``.\n    \"\"\"\n    outputs = self.model(\n        input_ids=inputs[Modalities.TEXT.name],\n        attention_mask=inputs.get(\"attention_mask\")\n        or inputs.get(Modalities.TEXT.attention_mask),\n        position_ids=inputs.get(\"position_ids\"),\n        output_attentions=inputs.get(\"output_attentions\"),\n        return_dict=True,\n    )\n    last_hidden_state = outputs.hidden_states[-1]  # NOTE: no layer norm applied\n    if self.pooling_layer:\n        last_hidden_state = self.pooling_layer(last_hidden_state)\n\n    return BaseModelOutput(\n        last_hidden_state=last_hidden_state,\n        hidden_states=outputs.hidden_states,\n        attentions=outputs.attentions,\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.clip.HFCLIPVisionEncoder","title":"HFCLIPVisionEncoder","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper around the <code>CLIPVisionModel</code> from HuggingFace.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>The huggingface model name or a local path from which to load the model.</p> required <code>pretrained</code> <code>bool</code> <p>Whether to load the pretrained weights or not.</p> <code>True</code> <code>pooling_layer</code> <code>Optional[Module]</code> <p>Pooling layer to apply to the last hidden state of the model.</p> <code>None</code> <code>freeze_layers</code> <code>Union[int, float, list[int], bool]</code> <p>Whether to freeze layers of the model and which layers to freeze. If <code>True</code>, all model layers are frozen. If it is an integer, the first <code>N</code> layers of the model are frozen. If it is a float, the first <code>N</code> percent of the layers are frozen. If it is a list of integers, the layers at the indices in the list are frozen.</p> <code>False</code> <code>freeze_layer_norm</code> <code>bool</code> <p>Whether to freeze the layer normalization layers of the model.</p> <code>True</code> <code>patch_dropout_rate</code> <code>float</code> <p>The proportion of patch embeddings to drop out.</p> <code>0.0</code> <code>patch_dropout_shuffle</code> <code>bool</code> <p>Whether to shuffle the patches while applying patch dropout.</p> <code>False</code> <code>patch_dropout_bias</code> <code>Optional[float]</code> <p>The bias to apply to the patch dropout mask.</p> <code>None</code> <code>peft_config</code> <code>Optional[PeftConfig]</code> <p>The configuration from the <code>peft &lt;https://huggingface.co/docs/peft/index&gt;</code>_ library to use to wrap the model for parameter-efficient finetuning.</p> <code>None</code> <code>model_config_kwargs</code> <code>Optional[dict[str, Any]]</code> <p>Additional keyword arguments to pass to the model configuration.</p> <code>None</code> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If both <code>peft_config</code> and <code>freeze_layers</code> are set. The <code>peft_config</code> will override the <code>freeze_layers</code> setting.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>@store(\n    group=\"modules/encoders\",\n    provider=\"mmlearn\",\n    model_name_or_path=\"openai/clip-vit-base-patch16\",\n    hydra_convert=\"object\",\n)\nclass HFCLIPVisionEncoder(nn.Module):\n    \"\"\"Wrapper around the ``CLIPVisionModel`` from HuggingFace.\n\n    Parameters\n    ----------\n    model_name_or_path : str\n        The huggingface model name or a local path from which to load the model.\n    pretrained : bool, default=True\n        Whether to load the pretrained weights or not.\n    pooling_layer : Optional[torch.nn.Module], optional, default=None\n        Pooling layer to apply to the last hidden state of the model.\n    freeze_layers : Union[int, float, list[int], bool], default=False\n        Whether to freeze layers of the model and which layers to freeze. If ``True``,\n        all model layers are frozen. If it is an integer, the first ``N`` layers of\n        the model are frozen. If it is a float, the first ``N`` percent of the layers\n        are frozen. If it is a list of integers, the layers at the indices in the\n        list are frozen.\n    freeze_layer_norm : bool, default=True\n        Whether to freeze the layer normalization layers of the model.\n    patch_dropout_rate : float, default=0.0\n        The proportion of patch embeddings to drop out.\n    patch_dropout_shuffle : bool, default=False\n        Whether to shuffle the patches while applying patch dropout.\n    patch_dropout_bias : Optional[float], optional, default=None\n        The bias to apply to the patch dropout mask.\n    peft_config : Optional[PeftConfig], optional, default=None\n        The configuration from the `peft &lt;https://huggingface.co/docs/peft/index&gt;`_\n        library to use to wrap the model for parameter-efficient finetuning.\n    model_config_kwargs : Optional[dict[str, Any]], optional, default=None\n        Additional keyword arguments to pass to the model configuration.\n\n    Warns\n    -----\n    UserWarning\n        If both ``peft_config`` and ``freeze_layers`` are set. The ``peft_config``\n        will override the ``freeze_layers`` setting.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name_or_path: str,\n        pretrained: bool = True,\n        pooling_layer: Optional[nn.Module] = None,\n        freeze_layers: Union[int, float, list[int], bool] = False,\n        freeze_layer_norm: bool = True,\n        patch_dropout_rate: float = 0.0,\n        patch_dropout_shuffle: bool = False,\n        patch_dropout_bias: Optional[float] = None,\n        peft_config: Optional[\"PeftConfig\"] = None,\n        model_config_kwargs: Optional[dict[str, Any]] = None,\n    ) -&gt; None:\n        super().__init__()\n        _warn_freeze_with_peft(peft_config, freeze_layers)\n\n        model = hf_utils.load_huggingface_model(\n            transformers.CLIPVisionModel,\n            model_name_or_path=model_name_or_path,\n            load_pretrained_weights=pretrained,\n            model_config_kwargs=model_config_kwargs,\n        )\n        model = _freeze_vision_model(model, freeze_layers, freeze_layer_norm)\n        if peft_config is not None:\n            model = hf_utils._wrap_peft_model(model, peft_config)\n\n        self.model = model.vision_model\n        self.pooling_layer = pooling_layer\n        self.patch_dropout = None\n        if patch_dropout_rate &gt; 0:\n            self.patch_dropout = PatchDropout(\n                keep_rate=1 - patch_dropout_rate,\n                token_shuffling=patch_dropout_shuffle,\n                bias=patch_dropout_bias,\n            )\n\n    def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The image tensor will be expected under the\n            ``Modalities.RGB`` key.\n\n        Returns\n        -------\n        BaseModelOutput\n            The output of the model, including the last hidden state, all hidden states,\n            and the attention weights, if ``output_attentions`` is set to ``True``.\n\n        \"\"\"\n        # FIXME: handle other vision modalities\n        pixel_values = inputs[Modalities.RGB.name]\n        hidden_states = self.model.embeddings(pixel_values)\n        if self.patch_dropout is not None:\n            hidden_states = self.patch_dropout(hidden_states)\n        hidden_states = self.model.pre_layrnorm(hidden_states)\n\n        encoder_outputs = self.model.encoder(\n            inputs_embeds=hidden_states,\n            output_attentions=inputs.get(\n                \"output_attentions\", self.model.config.output_attentions\n            ),\n            output_hidden_states=True,\n            return_dict=True,\n        )\n\n        last_hidden_state = encoder_outputs[0]\n        if self.pooling_layer is not None:\n            last_hidden_state = self.pooling_layer(last_hidden_state)\n\n        return BaseModelOutput(\n            last_hidden_state=last_hidden_state,\n            hidden_states=encoder_outputs.hidden_states,\n            attentions=encoder_outputs.attentions,\n        )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.clip.HFCLIPVisionEncoder.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The image tensor will be expected under the <code>Modalities.RGB</code> key.</p> required <p>Returns:</p> Type Description <code>BaseModelOutput</code> <p>The output of the model, including the last hidden state, all hidden states, and the attention weights, if <code>output_attentions</code> is set to <code>True</code>.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The image tensor will be expected under the\n        ``Modalities.RGB`` key.\n\n    Returns\n    -------\n    BaseModelOutput\n        The output of the model, including the last hidden state, all hidden states,\n        and the attention weights, if ``output_attentions`` is set to ``True``.\n\n    \"\"\"\n    # FIXME: handle other vision modalities\n    pixel_values = inputs[Modalities.RGB.name]\n    hidden_states = self.model.embeddings(pixel_values)\n    if self.patch_dropout is not None:\n        hidden_states = self.patch_dropout(hidden_states)\n    hidden_states = self.model.pre_layrnorm(hidden_states)\n\n    encoder_outputs = self.model.encoder(\n        inputs_embeds=hidden_states,\n        output_attentions=inputs.get(\n            \"output_attentions\", self.model.config.output_attentions\n        ),\n        output_hidden_states=True,\n        return_dict=True,\n    )\n\n    last_hidden_state = encoder_outputs[0]\n    if self.pooling_layer is not None:\n        last_hidden_state = self.pooling_layer(last_hidden_state)\n\n    return BaseModelOutput(\n        last_hidden_state=last_hidden_state,\n        hidden_states=encoder_outputs.hidden_states,\n        attentions=encoder_outputs.attentions,\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.clip.HFCLIPTextEncoderWithProjection","title":"HFCLIPTextEncoderWithProjection","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper around the <code>CLIPTextModelWithProjection</code> from HuggingFace.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>The huggingface model name or a local path from which to load the model.</p> required <code>pretrained</code> <code>bool</code> <p>Whether to load the pretrained weights or not.</p> <code>True</code> <code>use_all_token_embeddings</code> <code>bool</code> <p>Whether to use all token embeddings for the text. If <code>False</code> the first token embedding will be used.</p> <code>False</code> <code>freeze_layers</code> <code>Union[int, float, list[int], bool]</code> <p>Whether to freeze layers of the model and which layers to freeze. If <code>True</code>, all model layers are frozen. If it is an integer, the first <code>N</code> layers of the model are frozen. If it is a float, the first <code>N</code> percent of the layers are frozen. If it is a list of integers, the layers at the indices in the list are frozen.</p> <code>False</code> <code>freeze_layer_norm</code> <code>bool</code> <p>Whether to freeze the layer normalization layers of the model.</p> <code>True</code> <code>peft_config</code> <code>Optional[PeftConfig]</code> <p>The configuration from the <code>peft &lt;https://huggingface.co/docs/peft/index&gt;</code>_ library to use to wrap the model for parameter-efficient finetuning.</p> <code>None</code> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If both <code>peft_config</code> and <code>freeze_layers</code> are set. The <code>peft_config</code> will override the <code>freeze_layers</code> setting.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>@store(\n    group=\"modules/encoders\",\n    provider=\"mmlearn\",\n    model_name_or_path=\"openai/clip-vit-base-patch16\",\n    hydra_convert=\"object\",\n)\nclass HFCLIPTextEncoderWithProjection(nn.Module):\n    \"\"\"Wrapper around the ``CLIPTextModelWithProjection`` from HuggingFace.\n\n    Parameters\n    ----------\n    model_name_or_path : str\n        The huggingface model name or a local path from which to load the model.\n    pretrained : bool, default=True\n        Whether to load the pretrained weights or not.\n    use_all_token_embeddings : bool, default=False\n        Whether to use all token embeddings for the text. If ``False`` the first token\n        embedding will be used.\n    freeze_layers : Union[int, float, list[int], bool], default=False\n        Whether to freeze layers of the model and which layers to freeze. If ``True``,\n        all model layers are frozen. If it is an integer, the first ``N`` layers of\n        the model are frozen. If it is a float, the first ``N`` percent of the layers\n        are frozen. If it is a list of integers, the layers at the indices in the\n        list are frozen.\n    freeze_layer_norm : bool, default=True\n        Whether to freeze the layer normalization layers of the model.\n    peft_config : Optional[PeftConfig], optional, default=None\n        The configuration from the `peft &lt;https://huggingface.co/docs/peft/index&gt;`_\n        library to use to wrap the model for parameter-efficient finetuning.\n\n    Warns\n    -----\n    UserWarning\n        If both ``peft_config`` and ``freeze_layers`` are set. The ``peft_config``\n        will override the ``freeze_layers`` setting.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name_or_path: str,\n        pretrained: bool = True,\n        use_all_token_embeddings: bool = False,\n        freeze_layers: Union[int, float, list[int], bool] = False,\n        freeze_layer_norm: bool = True,\n        peft_config: Optional[\"PeftConfig\"] = None,\n        model_config_kwargs: Optional[dict[str, Any]] = None,\n    ) -&gt; None:\n        super().__init__()\n        _warn_freeze_with_peft(peft_config, freeze_layers)\n\n        self.use_all_token_embeddings = use_all_token_embeddings\n\n        model = hf_utils.load_huggingface_model(\n            transformers.CLIPTextModelWithProjection,\n            model_name_or_path,\n            load_pretrained_weights=pretrained,\n            model_config_kwargs=model_config_kwargs,\n            config_type=CLIPTextConfig,\n        )\n\n        model = _freeze_text_model(model, freeze_layers, freeze_layer_norm)\n        if peft_config is not None:\n            model = hf_utils._wrap_peft_model(model, peft_config)\n\n        self.model = model\n\n    def forward(self, inputs: dict[str, Any]) -&gt; tuple[torch.Tensor]:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The ``input_ids`` will be expected under the\n            ``Modalities.TEXT`` key.\n\n        Returns\n        -------\n        tuple[torch.Tensor]\n            The text embeddings. Will be a tuple with a single element.\n        \"\"\"\n        input_ids = inputs[Modalities.TEXT.name]\n        attention_mask: Optional[torch.Tensor] = inputs.get(\n            \"attention_mask\", inputs.get(Modalities.TEXT.attention_mask, None)\n        )\n        position_ids = inputs.get(\"position_ids\")\n\n        if self.use_all_token_embeddings:\n            text_outputs = self.model.text_model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                return_dict=True,\n            )\n            # TODO: add more options for pooling before projection\n            text_embeds = self.model.text_projection(text_outputs.last_hidden_state)\n        else:\n            text_embeds = self.model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                return_dict=True,\n            ).text_embeds\n\n        return (text_embeds,)\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.clip.HFCLIPTextEncoderWithProjection.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The <code>input_ids</code> will be expected under the <code>Modalities.TEXT</code> key.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor]</code> <p>The text embeddings. Will be a tuple with a single element.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; tuple[torch.Tensor]:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The ``input_ids`` will be expected under the\n        ``Modalities.TEXT`` key.\n\n    Returns\n    -------\n    tuple[torch.Tensor]\n        The text embeddings. Will be a tuple with a single element.\n    \"\"\"\n    input_ids = inputs[Modalities.TEXT.name]\n    attention_mask: Optional[torch.Tensor] = inputs.get(\n        \"attention_mask\", inputs.get(Modalities.TEXT.attention_mask, None)\n    )\n    position_ids = inputs.get(\"position_ids\")\n\n    if self.use_all_token_embeddings:\n        text_outputs = self.model.text_model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            return_dict=True,\n        )\n        # TODO: add more options for pooling before projection\n        text_embeds = self.model.text_projection(text_outputs.last_hidden_state)\n    else:\n        text_embeds = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            return_dict=True,\n        ).text_embeds\n\n    return (text_embeds,)\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.clip.HFCLIPVisionEncoderWithProjection","title":"HFCLIPVisionEncoderWithProjection","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper around the <code>CLIPVisionModelWithProjection</code> class from HuggingFace.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>The huggingface model name or a local path from which to load the model.</p> required <code>pretrained</code> <code>bool</code> <p>Whether to load the pretrained weights or not.</p> <code>True</code> <code>use_all_token_embeddings</code> <code>bool</code> <p>Whether to use all token embeddings for the text. If <code>False</code> the first token embedding will be used.</p> <code>False</code> <code>freeze_layers</code> <code>Union[int, float, list[int], bool]</code> <p>Whether to freeze layers of the model and which layers to freeze. If <code>True</code>, all model layers are frozen. If it is an integer, the first <code>N` layers of the model are frozen. If it is a float, the first</code>N`` percent of the layers are frozen. If it is a list of integers, the layers at the indices in the list are frozen.</p> <code>False</code> <code>freeze_layer_norm</code> <code>bool</code> <p>Whether to freeze the layer normalization layers of the model.</p> <code>True</code> <code>patch_dropout_rate</code> <code>float</code> <p>The proportion of patch embeddings to drop out.</p> <code>0.0</code> <code>patch_dropout_shuffle</code> <code>bool</code> <p>Whether to shuffle the patches while applying patch dropout.</p> <code>False</code> <code>patch_dropout_bias</code> <code>float</code> <p>The bias to apply to the patch dropout mask.</p> <code>None</code> <code>peft_config</code> <code>Optional[PeftConfig]</code> <p>The configuration from the <code>peft &lt;https://huggingface.co/docs/peft/index&gt;</code>_ library to use to wrap the model for parameter-efficient finetuning.</p> <code>None</code> <code>model_config_kwargs</code> <code>dict[str, Any]</code> <p>Additional keyword arguments to pass to the model configuration.</p> <code>None</code> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If both <code>peft_config</code> and <code>freeze_layers</code> are set. The <code>peft_config</code> will override the <code>freeze_layers</code> setting.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>@store(\n    group=\"modules/encoders\",\n    provider=\"mmlearn\",\n    model_name_or_path=\"openai/clip-vit-base-patch16\",\n    hydra_convert=\"object\",\n)\nclass HFCLIPVisionEncoderWithProjection(nn.Module):\n    \"\"\"Wrapper around the ``CLIPVisionModelWithProjection`` class from HuggingFace.\n\n    Parameters\n    ----------\n    model_name_or_path : str\n        The huggingface model name or a local path from which to load the model.\n    pretrained : bool, default=True\n        Whether to load the pretrained weights or not.\n    use_all_token_embeddings : bool, default=False\n        Whether to use all token embeddings for the text. If ``False`` the first token\n        embedding will be used.\n    freeze_layers : Union[int, float, list[int], bool], default=False\n        Whether to freeze layers of the model and which layers to freeze. If ``True``,\n        all model layers are frozen. If it is an integer, the first ``N` layers of\n        the model are frozen. If it is a float, the first ``N`` percent of the layers\n        are frozen. If it is a list of integers, the layers at the indices in the\n        list are frozen.\n    freeze_layer_norm : bool, default=True\n        Whether to freeze the layer normalization layers of the model.\n    patch_dropout_rate : float, default=0.0\n        The proportion of patch embeddings to drop out.\n    patch_dropout_shuffle : bool, default=False\n        Whether to shuffle the patches while applying patch dropout.\n    patch_dropout_bias : float, optional, default=None\n        The bias to apply to the patch dropout mask.\n    peft_config : Optional[PeftConfig], optional, default=None\n        The configuration from the `peft &lt;https://huggingface.co/docs/peft/index&gt;`_\n        library to use to wrap the model for parameter-efficient finetuning.\n    model_config_kwargs : dict[str, Any], optional, default=None\n        Additional keyword arguments to pass to the model configuration.\n\n    Warns\n    -----\n    UserWarning\n        If both ``peft_config`` and ``freeze_layers`` are set. The ``peft_config``\n        will override the ``freeze_layers`` setting.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name_or_path: str,\n        pretrained: bool = True,\n        use_all_token_embeddings: bool = False,\n        patch_dropout_rate: float = 0.0,\n        patch_dropout_shuffle: bool = False,\n        patch_dropout_bias: Optional[float] = None,\n        freeze_layers: Union[int, float, list[int], bool] = False,\n        freeze_layer_norm: bool = True,\n        peft_config: Optional[\"PeftConfig\"] = None,\n        model_config_kwargs: Optional[dict[str, Any]] = None,\n    ) -&gt; None:\n        super().__init__()\n        _warn_freeze_with_peft(peft_config, freeze_layers)\n\n        self.use_all_token_embeddings = use_all_token_embeddings\n\n        model = hf_utils.load_huggingface_model(\n            transformers.CLIPVisionModelWithProjection,\n            model_name_or_path,\n            load_pretrained_weights=pretrained,\n            model_config_kwargs=model_config_kwargs,\n            config_type=CLIPVisionConfig,\n        )\n\n        model = _freeze_vision_model(model, freeze_layers, freeze_layer_norm)\n        if peft_config is not None:\n            model = hf_utils._wrap_peft_model(model, peft_config)\n\n        self.model = model\n        self.patch_dropout = None\n        if patch_dropout_rate &gt; 0:\n            self.patch_dropout = PatchDropout(\n                keep_rate=1 - patch_dropout_rate,\n                token_shuffling=patch_dropout_shuffle,\n                bias=patch_dropout_bias,\n            )\n\n    def forward(self, inputs: dict[str, Any]) -&gt; tuple[torch.Tensor]:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The image tensor will be expected under the\n            ``Modalities.RGB`` key.\n\n        Returns\n        -------\n        tuple[torch.Tensor]\n            The image embeddings. Will be a tuple with a single element.\n        \"\"\"\n        pixel_values = inputs[Modalities.RGB.name]\n        hidden_states = self.model.vision_model.embeddings(pixel_values)\n        if self.patch_dropout is not None:\n            hidden_states = self.patch_dropout(hidden_states)\n        hidden_states = self.model.vision_model.pre_layrnorm(hidden_states)\n\n        encoder_outputs = self.model.vision_model.encoder(\n            inputs_embeds=hidden_states, return_dict=True\n        )\n\n        last_hidden_state = encoder_outputs.last_hidden_state\n        if self.use_all_token_embeddings:\n            pooled_output = last_hidden_state\n        else:\n            pooled_output = last_hidden_state[:, 0, :]\n        pooled_output = self.model.vision_model.post_layernorm(pooled_output)\n\n        return (self.model.visual_projection(pooled_output),)\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.clip.HFCLIPVisionEncoderWithProjection.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The image tensor will be expected under the <code>Modalities.RGB</code> key.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor]</code> <p>The image embeddings. Will be a tuple with a single element.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; tuple[torch.Tensor]:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The image tensor will be expected under the\n        ``Modalities.RGB`` key.\n\n    Returns\n    -------\n    tuple[torch.Tensor]\n        The image embeddings. Will be a tuple with a single element.\n    \"\"\"\n    pixel_values = inputs[Modalities.RGB.name]\n    hidden_states = self.model.vision_model.embeddings(pixel_values)\n    if self.patch_dropout is not None:\n        hidden_states = self.patch_dropout(hidden_states)\n    hidden_states = self.model.vision_model.pre_layrnorm(hidden_states)\n\n    encoder_outputs = self.model.vision_model.encoder(\n        inputs_embeds=hidden_states, return_dict=True\n    )\n\n    last_hidden_state = encoder_outputs.last_hidden_state\n    if self.use_all_token_embeddings:\n        pooled_output = last_hidden_state\n    else:\n        pooled_output = last_hidden_state[:, 0, :]\n    pooled_output = self.model.vision_model.post_layernorm(pooled_output)\n\n    return (self.model.visual_projection(pooled_output),)\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.text","title":"text","text":"<p>Huggingface text encoder model.</p>"},{"location":"api/#mmlearn.modules.encoders.text.HFTextEncoder","title":"HFTextEncoder","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper around huggingface models in the <code>AutoModelForTextEncoding</code> class.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>The huggingface model name or a local path from which to load the model.</p> required <code>pretrained</code> <code>bool</code> <p>Whether to load the pretrained weights or not.</p> <code>True</code> <code>pooling_layer</code> <code>Optional[Module]</code> <p>Pooling layer to apply to the last hidden state of the model.</p> <code>None</code> <code>freeze_layers</code> <code>Union[int, float, list[int], bool]</code> <p>Whether to freeze layers of the model and which layers to freeze. If <code>True</code>, all model layers are frozen. If it is an integer, the first <code>N</code> layers of the model are frozen. If it is a float, the first <code>N</code> percent of the layers are frozen. If it is a list of integers, the layers at the indices in the list are frozen.</p> <code>False</code> <code>freeze_layer_norm</code> <code>bool</code> <p>Whether to freeze the layer normalization layers of the model.</p> <code>True</code> <code>peft_config</code> <code>Optional[PeftConfig]</code> <p>The configuration from the <code>peft &lt;https://huggingface.co/docs/peft/index&gt;</code>_ library to use to wrap the model for parameter-efficient finetuning.</p> <code>None</code> <code>model_config_kwargs</code> <code>Optional[dict[str, Any]]</code> <p>Additional keyword arguments to pass to the model configuration.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model is a decoder model or if freezing individual layers is not supported for the model type.</p> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If both <code>peft_config</code> and <code>freeze_layers</code> are set. The <code>peft_config</code> will override the <code>freeze_layers</code> setting.</p> Source code in <code>mmlearn/modules/encoders/text.py</code> <pre><code>@store(group=\"modules/encoders\", provider=\"mmlearn\", hydra_convert=\"object\")\nclass HFTextEncoder(nn.Module):\n    \"\"\"Wrapper around huggingface models in the ``AutoModelForTextEncoding`` class.\n\n    Parameters\n    ----------\n    model_name_or_path : str\n        The huggingface model name or a local path from which to load the model.\n    pretrained : bool, default=True\n        Whether to load the pretrained weights or not.\n    pooling_layer : Optional[torch.nn.Module], optional, default=None\n        Pooling layer to apply to the last hidden state of the model.\n    freeze_layers : Union[int, float, list[int], bool], default=False\n        Whether to freeze layers of the model and which layers to freeze. If ``True``,\n        all model layers are frozen. If it is an integer, the first ``N`` layers of\n        the model are frozen. If it is a float, the first ``N`` percent of the layers\n        are frozen. If it is a list of integers, the layers at the indices in the\n        list are frozen.\n    freeze_layer_norm : bool, default=True\n        Whether to freeze the layer normalization layers of the model.\n    peft_config : Optional[PeftConfig], optional, default=None\n        The configuration from the `peft &lt;https://huggingface.co/docs/peft/index&gt;`_\n        library to use to wrap the model for parameter-efficient finetuning.\n    model_config_kwargs : Optional[dict[str, Any]], optional, default=None\n        Additional keyword arguments to pass to the model configuration.\n\n    Raises\n    ------\n    ValueError\n        If the model is a decoder model or if freezing individual layers is not\n        supported for the model type.\n\n    Warns\n    -----\n    UserWarning\n        If both ``peft_config`` and ``freeze_layers`` are set. The ``peft_config``\n        will override the ``freeze_layers`` setting.\n\n\n    \"\"\"\n\n    def __init__(  # noqa: PLR0912\n        self,\n        model_name_or_path: str,\n        pretrained: bool = True,\n        pooling_layer: Optional[nn.Module] = None,\n        freeze_layers: Union[int, float, list[int], bool] = False,\n        freeze_layer_norm: bool = True,\n        peft_config: Optional[\"PeftConfig\"] = None,\n        model_config_kwargs: Optional[dict[str, Any]] = None,\n    ):\n        super().__init__()\n        if model_config_kwargs is None:\n            model_config_kwargs = {}\n        model_config_kwargs[\"output_hidden_states\"] = True\n        model_config_kwargs[\"add_pooling_layer\"] = False\n        model = hf_utils.load_huggingface_model(\n            AutoModelForTextEncoding,\n            model_name_or_path,\n            load_pretrained_weights=pretrained,\n            model_config_kwargs=model_config_kwargs,\n        )\n        if hasattr(model.config, \"is_decoder\") and model.config.is_decoder:\n            raise ValueError(\"Model is a decoder. Only encoder models are supported.\")\n\n        if not pretrained and freeze_layers:\n            rank_zero_warn(\n                \"Freezing layers when loading a model with random weights may lead to \"\n                \"unexpected behavior. Consider setting `freeze_layers=False` if \"\n                \"`pretrained=False`.\",\n            )\n\n        if isinstance(freeze_layers, bool) and freeze_layers:\n            for name, param in model.named_parameters():\n                param.requires_grad = (\n                    (not freeze_layer_norm) if \"LayerNorm\" in name else False\n                )\n\n        if isinstance(\n            freeze_layers, (float, int, list)\n        ) and model.config.model_type in [\"flaubert\", \"xlm\"]:\n            # flaubert and xlm models have a different architecture that does not\n            # support freezing individual layers in the same way as other models\n            raise ValueError(\n                f\"Freezing individual layers is not supported for {model.config.model_type} \"\n                \"models. Please use `freeze_layers=False` or `freeze_layers=True`.\"\n            )\n\n        # get list of layers\n        embeddings = model.embeddings\n        encoder = getattr(model, \"encoder\", None) or getattr(\n            model, \"transformer\", model\n        )\n        encoder_layers = (\n            getattr(encoder, \"layer\", None)\n            or getattr(encoder, \"layers\", None)\n            or getattr(encoder, \"block\", None)\n        )\n        if encoder_layers is None and hasattr(encoder, \"albert_layer_groups\"):\n            encoder_layers = [\n                layer\n                for group in encoder.albert_layer_groups\n                for layer in group.albert_layers\n            ]\n        modules = [embeddings]\n        if encoder_layers is not None and isinstance(encoder_layers, list):\n            modules.extend(encoder_layers)\n\n        if isinstance(freeze_layers, float):\n            freeze_layers = int(freeze_layers * len(modules))\n        if isinstance(freeze_layers, int):\n            freeze_layers = list(range(freeze_layers))\n\n        if isinstance(freeze_layers, list):\n            for idx, module in enumerate(modules):\n                if idx in freeze_layers:\n                    for name, param in module.named_parameters():\n                        param.requires_grad = (\n                            (not freeze_layer_norm) if \"LayerNorm\" in name else False\n                        )\n\n        if peft_config is not None:\n            model = hf_utils._wrap_peft_model(model, peft_config)\n\n        self.model = model\n        self.pooling_layer = pooling_layer\n\n    def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The ``input_ids`` will be expected under the\n            ``Modalities.TEXT`` key.\n\n        Returns\n        -------\n        BaseModelOutput\n            The output of the model, including the last hidden state, all hidden states,\n            and the attention weights, if ``output_attentions`` is set to ``True``.\n        \"\"\"\n        outputs = self.model(\n            input_ids=inputs[Modalities.TEXT.name],\n            attention_mask=inputs.get(\n                \"attention_mask\", inputs.get(Modalities.TEXT.attention_mask, None)\n            ),\n            position_ids=inputs.get(\"position_ids\"),\n            output_attentions=inputs.get(\"output_attentions\"),\n            return_dict=True,\n        )\n        last_hidden_state = outputs.hidden_states[-1]  # NOTE: no layer norm applied\n        if self.pooling_layer:\n            last_hidden_state = self.pooling_layer(last_hidden_state)\n\n        return BaseModelOutput(\n            last_hidden_state=last_hidden_state,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.text.HFTextEncoder.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The <code>input_ids</code> will be expected under the <code>Modalities.TEXT</code> key.</p> required <p>Returns:</p> Type Description <code>BaseModelOutput</code> <p>The output of the model, including the last hidden state, all hidden states, and the attention weights, if <code>output_attentions</code> is set to <code>True</code>.</p> Source code in <code>mmlearn/modules/encoders/text.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The ``input_ids`` will be expected under the\n        ``Modalities.TEXT`` key.\n\n    Returns\n    -------\n    BaseModelOutput\n        The output of the model, including the last hidden state, all hidden states,\n        and the attention weights, if ``output_attentions`` is set to ``True``.\n    \"\"\"\n    outputs = self.model(\n        input_ids=inputs[Modalities.TEXT.name],\n        attention_mask=inputs.get(\n            \"attention_mask\", inputs.get(Modalities.TEXT.attention_mask, None)\n        ),\n        position_ids=inputs.get(\"position_ids\"),\n        output_attentions=inputs.get(\"output_attentions\"),\n        return_dict=True,\n    )\n    last_hidden_state = outputs.hidden_states[-1]  # NOTE: no layer norm applied\n    if self.pooling_layer:\n        last_hidden_state = self.pooling_layer(last_hidden_state)\n\n    return BaseModelOutput(\n        last_hidden_state=last_hidden_state,\n        hidden_states=outputs.hidden_states,\n        attentions=outputs.attentions,\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.vision","title":"vision","text":"<p>Vision encoder implementations.</p>"},{"location":"api/#mmlearn.modules.encoders.vision.TimmViT","title":"TimmViT","text":"<p>               Bases: <code>Module</code></p> <p>Vision Transformer model from timm.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the model to use.</p> required <code>modality</code> <code>str</code> <p>The modality of the input data. This allows this model to be used with different image modalities e.g. RGB, Depth, etc.</p> <code>\"RGB\"</code> <code>projection_dim</code> <code>int</code> <p>The dimension of the projection head.</p> <code>768</code> <code>pretrained</code> <code>bool</code> <p>Whether to use the pretrained weights.</p> <code>True</code> <code>freeze_layers</code> <code>Union[int, float, list[int], bool]</code> <p>Whether to freeze the layers.</p> <code>False</code> <code>freeze_layer_norm</code> <code>bool</code> <p>Whether to freeze the layer norm.</p> <code>True</code> <code>peft_config</code> <code>Optional[PeftConfig]</code> <p>The configuration from the <code>peft &lt;https://huggingface.co/docs/peft/index&gt;</code>_ library to use to wrap the model for parameter-efficient finetuning.</p> <code>None</code> <code>model_kwargs</code> <code>Optional[dict[str, Any]]</code> <p>Additional keyword arguments for the model.</p> <code>None</code> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>@store(\n    group=\"modules/encoders\",\n    provider=\"mmlearn\",\n    model_name=\"vit_base_patch16_224\",\n    hydra_convert=\"object\",\n)\nclass TimmViT(nn.Module):\n    \"\"\"Vision Transformer model from timm.\n\n    Parameters\n    ----------\n    model_name : str\n        The name of the model to use.\n    modality : str, default=\"RGB\"\n        The modality of the input data. This allows this model to be used with different\n        image modalities e.g. RGB, Depth, etc.\n    projection_dim : int, default=768\n        The dimension of the projection head.\n    pretrained : bool, default=True\n        Whether to use the pretrained weights.\n    freeze_layers : Union[int, float, list[int], bool], default=False\n        Whether to freeze the layers.\n    freeze_layer_norm : bool, default=True\n        Whether to freeze the layer norm.\n    peft_config : Optional[PeftConfig], optional, default=None\n        The configuration from the `peft &lt;https://huggingface.co/docs/peft/index&gt;`_\n        library to use to wrap the model for parameter-efficient finetuning.\n    model_kwargs : Optional[dict[str, Any]], default=None\n        Additional keyword arguments for the model.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str,\n        modality: str = \"RGB\",\n        projection_dim: int = 768,\n        pretrained: bool = True,\n        freeze_layers: Union[int, float, list[int], bool] = False,\n        freeze_layer_norm: bool = True,\n        peft_config: Optional[\"PeftConfig\"] = None,\n        model_kwargs: Optional[dict[str, Any]] = None,\n    ) -&gt; None:\n        super().__init__()\n        self.modality = Modalities.get_modality(modality)\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        self.model: TimmVisionTransformer = timm.create_model(\n            model_name,\n            pretrained=pretrained,\n            num_classes=projection_dim,\n            **model_kwargs,\n        )\n        assert isinstance(self.model, TimmVisionTransformer), (\n            f\"Model {model_name} is not a Vision Transformer. \"\n            \"Please provide a model name that corresponds to a Vision Transformer.\"\n        )\n\n        self._freeze_layers(freeze_layers, freeze_layer_norm)\n\n        if peft_config is not None:\n            self.model = hf_utils._wrap_peft_model(self.model, peft_config)\n\n    def _freeze_layers(\n        self, freeze_layers: Union[int, float, list[int], bool], freeze_layer_norm: bool\n    ) -&gt; None:\n        \"\"\"Freeze the layers of the model.\n\n        Parameters\n        ----------\n        freeze_layers : Union[int, float, list[int], bool]\n            Whether to freeze the layers.\n        freeze_layer_norm : bool\n            Whether to freeze the layer norm.\n        \"\"\"\n        if isinstance(freeze_layers, bool) and freeze_layers:\n            for name, param in self.model.named_parameters():\n                param.requires_grad = (\n                    (not freeze_layer_norm) if \"norm\" in name else False\n                )\n\n        modules = [self.model.patch_embed, *self.model.blocks, self.model.norm]\n        if isinstance(freeze_layers, float):\n            freeze_layers = int(freeze_layers * len(modules))\n        if isinstance(freeze_layers, int):\n            freeze_layers = list(range(freeze_layers))\n\n        if isinstance(freeze_layers, list):\n            for idx, module in enumerate(modules):\n                if idx in freeze_layers:\n                    for name, param in module.named_parameters():\n                        param.requires_grad = (\n                            (not freeze_layer_norm) if \"norm\" in name else False\n                        )\n\n    def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The ``image`` will be expected under the\n            ``Modalities.RGB`` key.\n\n        Returns\n        -------\n        BaseModelOutput\n            The output of the model.\n        \"\"\"\n        x = inputs[self.modality.name]\n        last_hidden_state, hidden_states = self.model.forward_intermediates(\n            x, output_fmt=\"NLC\"\n        )\n        last_hidden_state = self.model.forward_head(last_hidden_state)\n\n        return BaseModelOutput(\n            last_hidden_state=last_hidden_state, hidden_states=hidden_states\n        )\n\n    def get_intermediate_layers(\n        self, inputs: dict[str, Any], n: int = 1\n    ) -&gt; list[torch.Tensor]:\n        \"\"\"Get the output of the intermediate layers.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The ``image`` will be expected under the ``Modalities.RGB``\n            key.\n        n : int, default=1\n            The number of intermediate layers to return.\n\n        Returns\n        -------\n        list[torch.Tensor]\n            The outputs of the last n intermediate layers.\n        \"\"\"\n        return self.model.get_intermediate_layers(inputs[Modalities.RGB], n)  # type: ignore\n\n    def get_patch_info(self) -&gt; tuple[int, int]:\n        \"\"\"Get patch size and number of patches.\n\n        Returns\n        -------\n        tuple[int, int]\n            Patch size and number of patches.\n        \"\"\"\n        patch_size = self.model.patch_embed.patch_size[0]\n        num_patches = self.model.patch_embed.num_patches\n        return patch_size, num_patches\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.vision.TimmViT.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The <code>image</code> will be expected under the <code>Modalities.RGB</code> key.</p> required <p>Returns:</p> Type Description <code>BaseModelOutput</code> <p>The output of the model.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The ``image`` will be expected under the\n        ``Modalities.RGB`` key.\n\n    Returns\n    -------\n    BaseModelOutput\n        The output of the model.\n    \"\"\"\n    x = inputs[self.modality.name]\n    last_hidden_state, hidden_states = self.model.forward_intermediates(\n        x, output_fmt=\"NLC\"\n    )\n    last_hidden_state = self.model.forward_head(last_hidden_state)\n\n    return BaseModelOutput(\n        last_hidden_state=last_hidden_state, hidden_states=hidden_states\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.vision.TimmViT.get_intermediate_layers","title":"get_intermediate_layers","text":"<pre><code>get_intermediate_layers(inputs, n=1)\n</code></pre> <p>Get the output of the intermediate layers.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The <code>image</code> will be expected under the <code>Modalities.RGB</code> key.</p> required <code>n</code> <code>int</code> <p>The number of intermediate layers to return.</p> <code>1</code> <p>Returns:</p> Type Description <code>list[Tensor]</code> <p>The outputs of the last n intermediate layers.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>def get_intermediate_layers(\n    self, inputs: dict[str, Any], n: int = 1\n) -&gt; list[torch.Tensor]:\n    \"\"\"Get the output of the intermediate layers.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The ``image`` will be expected under the ``Modalities.RGB``\n        key.\n    n : int, default=1\n        The number of intermediate layers to return.\n\n    Returns\n    -------\n    list[torch.Tensor]\n        The outputs of the last n intermediate layers.\n    \"\"\"\n    return self.model.get_intermediate_layers(inputs[Modalities.RGB], n)  # type: ignore\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.vision.TimmViT.get_patch_info","title":"get_patch_info","text":"<pre><code>get_patch_info()\n</code></pre> <p>Get patch size and number of patches.</p> <p>Returns:</p> Type Description <code>tuple[int, int]</code> <p>Patch size and number of patches.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>def get_patch_info(self) -&gt; tuple[int, int]:\n    \"\"\"Get patch size and number of patches.\n\n    Returns\n    -------\n    tuple[int, int]\n        Patch size and number of patches.\n    \"\"\"\n    patch_size = self.model.patch_embed.patch_size[0]\n    num_patches = self.model.patch_embed.num_patches\n    return patch_size, num_patches\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.vision.VisionTransformer","title":"VisionTransformer","text":"<p>               Bases: <code>Module</code></p> <p>Vision Transformer.</p> <p>This module implements a Vision Transformer that processes images using a series of transformer blocks and patch embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>modality</code> <code>str</code> <p>The modality of the input data. This allows this model to be used with different image modalities e.g. RGB, Depth, etc.</p> <code>\"RGB\"</code> <code>img_size</code> <code>List[int]</code> <p>List of input image sizes.</p> <code>None</code> <code>patch_size</code> <code>int</code> <p>Size of each patch.</p> <code>16</code> <code>in_chans</code> <code>int</code> <p>Number of input channels.</p> <code>3</code> <code>embed_dim</code> <code>int</code> <p>Embedding dimension.</p> <code>768</code> <code>depth</code> <code>int</code> <p>Number of transformer blocks.</p> <code>12</code> <code>num_heads</code> <code>int</code> <p>Number of attention heads.</p> <code>12</code> <code>mlp_ratio</code> <code>float</code> <p>Ratio of hidden dimension in the MLP.</p> <code>4.0</code> <code>qkv_bias</code> <code>bool</code> <p>If True, add a learnable bias to the query, key, and value projections.</p> <code>True</code> <code>qk_scale</code> <code>Optional[float]</code> <p>Override the default qk scale factor.</p> <code>None</code> <code>drop_rate</code> <code>float</code> <p>Dropout rate for the transformer blocks.</p> <code>0.0</code> <code>attn_drop_rate</code> <code>float</code> <p>Dropout rate for the attention mechanism.</p> <code>0.0</code> <code>drop_path_rate</code> <code>float</code> <p>Dropout rate for stochastic depth.</p> <code>0.0</code> <code>norm_layer</code> <code>Callable[..., Module]</code> <p>Normalization layer to use.</p> <code>torch.nn.LayerNorm</code> <code>init_std</code> <code>float</code> <p>Standard deviation for weight initialization.</p> <code>0.02</code> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>class VisionTransformer(nn.Module):\n    \"\"\"Vision Transformer.\n\n    This module implements a Vision Transformer that processes images using a\n    series of transformer blocks and patch embeddings.\n\n    Parameters\n    ----------\n    modality : str, optional, default=\"RGB\"\n        The modality of the input data. This allows this model to be used with different\n        image modalities e.g. RGB, Depth, etc.\n    img_size : List[int], optional, default=None\n        List of input image sizes.\n    patch_size : int, optional, default=16\n        Size of each patch.\n    in_chans : int, optional, default=3\n        Number of input channels.\n    embed_dim : int, optional, default=768\n        Embedding dimension.\n    depth : int, optional, default=12\n        Number of transformer blocks.\n    num_heads : int, optional, default=12\n        Number of attention heads.\n    mlp_ratio : float, optional, default=4.0\n        Ratio of hidden dimension in the MLP.\n    qkv_bias : bool, optional, default=True\n        If True, add a learnable bias to the query, key, and value projections.\n    qk_scale : Optional[float], optional\n        Override the default qk scale factor.\n    drop_rate : float, optional, default=0.0\n        Dropout rate for the transformer blocks.\n    attn_drop_rate : float, optional, default=0.0\n        Dropout rate for the attention mechanism.\n    drop_path_rate : float, optional, default=0.0\n        Dropout rate for stochastic depth.\n    norm_layer : Callable[..., torch.nn.Module], optional, default=torch.nn.LayerNorm\n        Normalization layer to use.\n    init_std : float, optional, default=0.02\n        Standard deviation for weight initialization.\n    \"\"\"\n\n    def __init__(\n        self,\n        modality: str = \"RGB\",\n        img_size: Optional[list[int]] = None,\n        patch_size: int = 16,\n        in_chans: int = 3,\n        embed_dim: int = 768,\n        depth: int = 12,\n        num_heads: int = 12,\n        mlp_ratio: float = 4.0,\n        qkv_bias: bool = True,\n        qk_scale: Optional[float] = None,\n        global_pool: Literal[\"\", \"avg\", \"avgmax\", \"max\", \"token\"] = \"\",\n        drop_rate: float = 0.0,\n        attn_drop_rate: float = 0.0,\n        drop_path_rate: float = 0.0,\n        norm_layer: Callable[..., nn.Module] = nn.LayerNorm,\n        init_std: float = 0.02,\n    ) -&gt; None:\n        super().__init__()\n        assert global_pool in (\"\", \"avg\", \"avgmax\", \"max\", \"token\")\n\n        self.modality = Modalities.get_modality(modality)\n        self.num_features = self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        img_size = [224, 224] if img_size is None else img_size\n\n        # Patch Embedding\n        self.patch_embed = PatchEmbed(\n            img_size=img_size[0],\n            patch_size=patch_size,\n            in_chans=in_chans,\n            embed_dim=embed_dim,\n        )\n        num_patches = self.patch_embed.num_patches\n\n        # Positional Embedding\n        self.pos_embed = nn.Parameter(\n            torch.zeros(1, num_patches, embed_dim), requires_grad=False\n        )\n        pos_embed = get_2d_sincos_pos_embed(\n            self.pos_embed.shape[-1],\n            int(self.patch_embed.num_patches**0.5),\n            cls_token=False,\n        )\n        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n\n        # Transformer Blocks\n        dpr = [\n            x.item() for x in torch.linspace(0, drop_path_rate, depth)\n        ]  # stochastic depth decay rule\n        self.blocks = nn.ModuleList(\n            [\n                Block(\n                    dim=embed_dim,\n                    num_heads=num_heads,\n                    mlp_ratio=mlp_ratio,\n                    qkv_bias=qkv_bias,\n                    qk_scale=qk_scale,\n                    drop=drop_rate,\n                    attn_drop=attn_drop_rate,\n                    drop_path=dpr[i],\n                    norm_layer=norm_layer,\n                )\n                for i in range(depth)\n            ]\n        )\n        self.norm = norm_layer(embed_dim)\n\n        self.global_pool = global_pool\n\n        # Weight Initialization\n        self.init_std = init_std\n        self.apply(self._init_weights)\n\n    def fix_init_weight(self) -&gt; None:\n        \"\"\"Fix initialization of weights by rescaling them according to layer depth.\"\"\"\n\n        def rescale(param: torch.Tensor, layer_id: int) -&gt; None:\n            param.div_(math.sqrt(2.0 * layer_id))\n\n        for layer_id, layer in enumerate(self.blocks):\n            rescale(layer.attn.proj.weight.data, layer_id + 1)\n            rescale(layer.mlp[-1].weight.data, layer_id + 1)\n\n    def _init_weights(self, m: nn.Module) -&gt; None:\n        \"\"\"Initialize weights for the layers.\"\"\"\n        if isinstance(m, nn.Linear):\n            _trunc_normal(m.weight, std=self.init_std)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n        elif isinstance(m, nn.Conv2d):\n            _trunc_normal(m.weight, std=self.init_std)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n\n    def forward(\n        self, inputs: dict[str, Any], return_hidden_states: bool = False\n    ) -&gt; tuple[torch.Tensor, Optional[list[torch.Tensor]]]:\n        \"\"\"Forward pass through the Vision Transformer.\"\"\"\n        masks = inputs.get(self.modality.mask)\n        if masks is not None and not isinstance(masks, list):\n            masks = [masks]\n\n        x = inputs[self.modality.name]\n        # -- Patchify x\n        x = self.patch_embed(x)\n\n        # -- Add positional embedding to x\n        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n        x = x + pos_embed\n\n        # -- Mask x\n        if masks is not None:\n            x = apply_masks(x, masks)\n\n        # -- Initialize a list to store hidden states\n        hidden_states: Optional[list[torch.Tensor]] = (\n            [] if return_hidden_states else None\n        )\n\n        # -- Forward propagation through blocks\n        for _i, blk in enumerate(self.blocks):\n            x = blk(x)\n            if return_hidden_states and hidden_states is not None:\n                hidden_states.append(x)\n\n        # -- Apply normalization if present\n        if self.norm is not None:\n            x = self.norm(x)\n\n        # -- Apply global pooling\n        x = global_pool_nlc(x, pool_type=self.global_pool)\n\n        # -- Return both final output and hidden states if requested\n        if return_hidden_states:\n            return x, hidden_states\n        return (x, None)\n\n    def interpolate_pos_encoding(\n        self, x: torch.Tensor, pos_embed: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"Interpolate positional encoding to match the size of the input tensor.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor.\n        pos_embed : torch.Tensor\n            Positional embedding tensor.\n\n        Returns\n        -------\n        torch.Tensor\n            Interpolated positional encoding.\n        \"\"\"\n        npatch = x.shape[1] - 1\n        n = pos_embed.shape[1] - 1\n        if npatch == n:\n            return pos_embed\n        class_emb = pos_embed[:, 0]\n        pos_embed = pos_embed[:, 1:]\n        dim = x.shape[-1]\n        pos_embed = nn.functional.interpolate(\n            pos_embed.reshape(1, int(math.sqrt(n)), int(math.sqrt(n)), dim).permute(\n                0, 3, 1, 2\n            ),\n            scale_factor=math.sqrt(npatch / n),\n            mode=\"bicubic\",\n        )\n        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.vision.VisionTransformer.fix_init_weight","title":"fix_init_weight","text":"<pre><code>fix_init_weight()\n</code></pre> <p>Fix initialization of weights by rescaling them according to layer depth.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>def fix_init_weight(self) -&gt; None:\n    \"\"\"Fix initialization of weights by rescaling them according to layer depth.\"\"\"\n\n    def rescale(param: torch.Tensor, layer_id: int) -&gt; None:\n        param.div_(math.sqrt(2.0 * layer_id))\n\n    for layer_id, layer in enumerate(self.blocks):\n        rescale(layer.attn.proj.weight.data, layer_id + 1)\n        rescale(layer.mlp[-1].weight.data, layer_id + 1)\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.vision.VisionTransformer.forward","title":"forward","text":"<pre><code>forward(inputs, return_hidden_states=False)\n</code></pre> <p>Forward pass through the Vision Transformer.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>def forward(\n    self, inputs: dict[str, Any], return_hidden_states: bool = False\n) -&gt; tuple[torch.Tensor, Optional[list[torch.Tensor]]]:\n    \"\"\"Forward pass through the Vision Transformer.\"\"\"\n    masks = inputs.get(self.modality.mask)\n    if masks is not None and not isinstance(masks, list):\n        masks = [masks]\n\n    x = inputs[self.modality.name]\n    # -- Patchify x\n    x = self.patch_embed(x)\n\n    # -- Add positional embedding to x\n    pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n    x = x + pos_embed\n\n    # -- Mask x\n    if masks is not None:\n        x = apply_masks(x, masks)\n\n    # -- Initialize a list to store hidden states\n    hidden_states: Optional[list[torch.Tensor]] = (\n        [] if return_hidden_states else None\n    )\n\n    # -- Forward propagation through blocks\n    for _i, blk in enumerate(self.blocks):\n        x = blk(x)\n        if return_hidden_states and hidden_states is not None:\n            hidden_states.append(x)\n\n    # -- Apply normalization if present\n    if self.norm is not None:\n        x = self.norm(x)\n\n    # -- Apply global pooling\n    x = global_pool_nlc(x, pool_type=self.global_pool)\n\n    # -- Return both final output and hidden states if requested\n    if return_hidden_states:\n        return x, hidden_states\n    return (x, None)\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.vision.VisionTransformer.interpolate_pos_encoding","title":"interpolate_pos_encoding","text":"<pre><code>interpolate_pos_encoding(x, pos_embed)\n</code></pre> <p>Interpolate positional encoding to match the size of the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <code>pos_embed</code> <code>Tensor</code> <p>Positional embedding tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Interpolated positional encoding.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>def interpolate_pos_encoding(\n    self, x: torch.Tensor, pos_embed: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"Interpolate positional encoding to match the size of the input tensor.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor.\n    pos_embed : torch.Tensor\n        Positional embedding tensor.\n\n    Returns\n    -------\n    torch.Tensor\n        Interpolated positional encoding.\n    \"\"\"\n    npatch = x.shape[1] - 1\n    n = pos_embed.shape[1] - 1\n    if npatch == n:\n        return pos_embed\n    class_emb = pos_embed[:, 0]\n    pos_embed = pos_embed[:, 1:]\n    dim = x.shape[-1]\n    pos_embed = nn.functional.interpolate(\n        pos_embed.reshape(1, int(math.sqrt(n)), int(math.sqrt(n)), dim).permute(\n            0, 3, 1, 2\n        ),\n        scale_factor=math.sqrt(npatch / n),\n        mode=\"bicubic\",\n    )\n    pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n    return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.vision.VisionTransformerPredictor","title":"VisionTransformerPredictor","text":"<p>               Bases: <code>Module</code></p> <p>Vision Transformer Predictor.</p> <p>This module implements a Vision Transformer that predicts masked tokens using a series of transformer blocks.</p> <p>Parameters:</p> Name Type Description Default <code>num_patches</code> <code>int</code> <p>The number of patches in the input image.</p> <code>196</code> <code>embed_dim</code> <code>int</code> <p>The embedding dimension.</p> <code>768</code> <code>predictor_embed_dim</code> <code>int</code> <p>The embedding dimension for the predictor.</p> <code>384</code> <code>depth</code> <code>int</code> <p>The number of transformer blocks.</p> <code>6</code> <code>num_heads</code> <code>int</code> <p>The number of attention heads.</p> <code>12</code> <code>mlp_ratio</code> <code>float</code> <p>Ratio of the hidden dimension in the MLP.</p> <code>4.0</code> <code>qkv_bias</code> <code>bool</code> <p>If True, add a learnable bias to the query, key, and value projections.</p> <code>True</code> <code>qk_scale</code> <code>Optional[float]</code> <p>Override the default qk scale factor.</p> <code>None</code> <code>drop_rate</code> <code>float</code> <p>Dropout rate for the transformer blocks.</p> <code>0.0</code> <code>attn_drop_rate</code> <code>float</code> <p>Dropout rate for the attention mechanism.</p> <code>0.0</code> <code>drop_path_rate</code> <code>float</code> <p>Dropout rate for stochastic depth.</p> <code>0.0</code> <code>norm_layer</code> <code>Callable[..., Module]</code> <p>Normalization layer to use.</p> <code>torch.nn.LayerNorm</code> <code>init_std</code> <code>float</code> <p>Standard deviation for weight initialization.</p> <code>0.02</code> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>class VisionTransformerPredictor(nn.Module):\n    \"\"\"Vision Transformer Predictor.\n\n    This module implements a Vision Transformer that predicts masked tokens\n    using a series of transformer blocks.\n\n    Parameters\n    ----------\n    num_patches : int\n        The number of patches in the input image.\n    embed_dim : int, optional, default=768\n        The embedding dimension.\n    predictor_embed_dim : int, optional, default=384\n        The embedding dimension for the predictor.\n    depth : int, optional, default=6\n        The number of transformer blocks.\n    num_heads : int, optional, default=12\n        The number of attention heads.\n    mlp_ratio : float, optional, default=4.0\n        Ratio of the hidden dimension in the MLP.\n    qkv_bias : bool, optional, default=True\n        If True, add a learnable bias to the query, key, and value projections.\n    qk_scale : Optional[float], optional, default=None\n        Override the default qk scale factor.\n    drop_rate : float, optional, default=0.0\n        Dropout rate for the transformer blocks.\n    attn_drop_rate : float, optional, default=0.0\n        Dropout rate for the attention mechanism.\n    drop_path_rate : float, optional, default=0.0\n        Dropout rate for stochastic depth.\n    norm_layer : Callable[..., torch.nn.Module], optional, default=torch.nn.LayerNorm\n        Normalization layer to use.\n    init_std : float, optional, default=0.02\n        Standard deviation for weight initialization.\n    \"\"\"\n\n    def __init__(\n        self,\n        num_patches: int = 196,\n        embed_dim: int = 768,\n        predictor_embed_dim: int = 384,\n        depth: int = 6,\n        num_heads: int = 12,\n        mlp_ratio: float = 4.0,\n        qkv_bias: bool = True,\n        qk_scale: Optional[float] = None,\n        drop_rate: float = 0.0,\n        attn_drop_rate: float = 0.0,\n        drop_path_rate: float = 0.0,\n        norm_layer: Callable[..., nn.Module] = nn.LayerNorm,\n        init_std: float = 0.02,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__()\n        self.num_patches = num_patches\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n\n        self.predictor_embed = nn.Linear(self.embed_dim, predictor_embed_dim, bias=True)\n        self.mask_token = nn.Parameter(torch.zeros(1, 1, predictor_embed_dim))\n        dpr = [\n            x.item() for x in torch.linspace(0, drop_path_rate, depth)\n        ]  # stochastic depth decay rule\n\n        # Positional Embedding\n        self.predictor_pos_embed = nn.Parameter(\n            torch.zeros(1, self.num_patches, predictor_embed_dim), requires_grad=False\n        )\n        predictor_pos_embed = get_2d_sincos_pos_embed(\n            self.predictor_pos_embed.shape[-1],\n            int(self.num_patches**0.5),\n            cls_token=False,\n        )\n        self.predictor_pos_embed.data.copy_(\n            torch.from_numpy(predictor_pos_embed).float().unsqueeze(0)\n        )\n\n        # Transformer Blocks\n        self.predictor_blocks = nn.ModuleList(\n            [\n                Block(\n                    dim=predictor_embed_dim,\n                    num_heads=self.num_heads,\n                    mlp_ratio=mlp_ratio,\n                    qkv_bias=qkv_bias,\n                    qk_scale=qk_scale,\n                    drop=drop_rate,\n                    attn_drop=attn_drop_rate,\n                    drop_path=dpr[i],\n                    norm_layer=norm_layer,\n                )\n                for i in range(depth)\n            ]\n        )\n\n        self.predictor_norm = norm_layer(predictor_embed_dim)\n        self.predictor_proj = nn.Linear(predictor_embed_dim, embed_dim, bias=True)\n\n        # Weight Initialization\n        self.init_std = init_std\n        _trunc_normal(self.mask_token, std=self.init_std)\n        self.apply(self._init_weights)\n\n    def fix_init_weight(self) -&gt; None:\n        \"\"\"Fix initialization of weights by rescaling them according to layer depth.\"\"\"\n\n        def rescale(param: torch.Tensor, layer_id: int) -&gt; None:\n            param.div_(math.sqrt(2.0 * layer_id))\n\n        for layer_id, layer in enumerate(self.predictor_blocks):\n            rescale(layer.attn.proj.weight.data, layer_id + 1)\n            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n\n    def _init_weights(self, m: nn.Module) -&gt; None:\n        \"\"\"Initialize weights for the layers.\"\"\"\n        if isinstance(m, nn.Linear):\n            _trunc_normal(m.weight, std=self.init_std)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n        elif isinstance(m, nn.Conv2d):\n            _trunc_normal(m.weight, std=self.init_std)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        masks_x: Union[torch.Tensor, list[torch.Tensor]],\n        masks: Union[torch.Tensor, list[torch.Tensor]],\n    ) -&gt; torch.Tensor:\n        \"\"\"Forward pass through the Vision Transformer Predictor.\"\"\"\n        assert (masks is not None) and (masks_x is not None), (\n            \"Cannot run predictor without mask indices\"\n        )\n\n        if not isinstance(masks_x, list):\n            masks_x = [masks_x]\n\n        if not isinstance(masks, list):\n            masks = [masks]\n\n        # -- Batch Size\n        b = len(x) // len(masks_x)\n\n        # -- Map from encoder-dim to predictor-dim\n        x = self.predictor_embed(x)\n\n        # -- Add positional embedding to x tokens\n        x_pos_embed = self.predictor_pos_embed.repeat(b, 1, 1)\n        x += apply_masks(x_pos_embed, masks_x)\n\n        _, n_ctxt, d = x.shape\n\n        # -- Concatenate mask tokens to x\n        pos_embs = self.predictor_pos_embed.repeat(b, 1, 1)\n        pos_embs = apply_masks(pos_embs, masks)\n        pos_embs = repeat_interleave_batch(pos_embs, b, repeat=len(masks_x))\n        pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1)\n        pred_tokens += pos_embs\n        x = x.repeat(len(masks), 1, 1)\n        x = torch.cat([x, pred_tokens], dim=1)\n\n        # -- Forward propagation\n        for blk in self.predictor_blocks:\n            x = blk(x)\n        x = self.predictor_norm(x)\n\n        # -- Return predictions for mask tokens\n        x = x[:, n_ctxt:]\n        return self.predictor_proj(x)\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.vision.VisionTransformerPredictor.fix_init_weight","title":"fix_init_weight","text":"<pre><code>fix_init_weight()\n</code></pre> <p>Fix initialization of weights by rescaling them according to layer depth.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>def fix_init_weight(self) -&gt; None:\n    \"\"\"Fix initialization of weights by rescaling them according to layer depth.\"\"\"\n\n    def rescale(param: torch.Tensor, layer_id: int) -&gt; None:\n        param.div_(math.sqrt(2.0 * layer_id))\n\n    for layer_id, layer in enumerate(self.predictor_blocks):\n        rescale(layer.attn.proj.weight.data, layer_id + 1)\n        rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.vision.VisionTransformerPredictor.forward","title":"forward","text":"<pre><code>forward(x, masks_x, masks)\n</code></pre> <p>Forward pass through the Vision Transformer Predictor.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>def forward(\n    self,\n    x: torch.Tensor,\n    masks_x: Union[torch.Tensor, list[torch.Tensor]],\n    masks: Union[torch.Tensor, list[torch.Tensor]],\n) -&gt; torch.Tensor:\n    \"\"\"Forward pass through the Vision Transformer Predictor.\"\"\"\n    assert (masks is not None) and (masks_x is not None), (\n        \"Cannot run predictor without mask indices\"\n    )\n\n    if not isinstance(masks_x, list):\n        masks_x = [masks_x]\n\n    if not isinstance(masks, list):\n        masks = [masks]\n\n    # -- Batch Size\n    b = len(x) // len(masks_x)\n\n    # -- Map from encoder-dim to predictor-dim\n    x = self.predictor_embed(x)\n\n    # -- Add positional embedding to x tokens\n    x_pos_embed = self.predictor_pos_embed.repeat(b, 1, 1)\n    x += apply_masks(x_pos_embed, masks_x)\n\n    _, n_ctxt, d = x.shape\n\n    # -- Concatenate mask tokens to x\n    pos_embs = self.predictor_pos_embed.repeat(b, 1, 1)\n    pos_embs = apply_masks(pos_embs, masks)\n    pos_embs = repeat_interleave_batch(pos_embs, b, repeat=len(masks_x))\n    pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1)\n    pred_tokens += pos_embs\n    x = x.repeat(len(masks), 1, 1)\n    x = torch.cat([x, pred_tokens], dim=1)\n\n    # -- Forward propagation\n    for blk in self.predictor_blocks:\n        x = blk(x)\n    x = self.predictor_norm(x)\n\n    # -- Return predictions for mask tokens\n    x = x[:, n_ctxt:]\n    return self.predictor_proj(x)\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.vision.vit_predictor","title":"vit_predictor","text":"<pre><code>vit_predictor(kwargs=None)\n</code></pre> <p>Create a VisionTransformerPredictor model.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>dict[str, Any]</code> <p>Keyword arguments for the predictor.</p> <code>None</code> <p>Returns:</p> Type Description <code>VisionTransformerPredictor</code> <p>An instance of VisionTransformerPredictor.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>@cast(\n    VisionTransformerPredictor,\n    store(\n        group=\"modules/encoders\",\n        provider=\"mmlearn\",\n    ),\n)\ndef vit_predictor(\n    kwargs: Optional[dict[str, Any]] = None,\n) -&gt; VisionTransformerPredictor:\n    \"\"\"Create a VisionTransformerPredictor model.\n\n    Parameters\n    ----------\n    kwargs : dict[str, Any], optional, default=None\n        Keyword arguments for the predictor.\n\n    Returns\n    -------\n    VisionTransformerPredictor\n        An instance of VisionTransformerPredictor.\n    \"\"\"\n    if kwargs is None:\n        kwargs = {}\n    return VisionTransformerPredictor(\n        mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.vision.vit_tiny","title":"vit_tiny","text":"<pre><code>vit_tiny(patch_size=16, kwargs=None)\n</code></pre> <p>Create a VisionTransformer model with tiny configuration.</p> <p>Parameters:</p> Name Type Description Default <code>patch_size</code> <code>int</code> <p>Size of each patch.</p> <code>16</code> <code>kwargs</code> <code>dict[str, Any]</code> <p>Keyword arguments for the model variant.</p> <code>None</code> <p>Returns:</p> Type Description <code>VisionTransformer</code> <p>An instance of VisionTransformer.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>@cast(\n    VisionTransformer,\n    store(\n        group=\"modules/encoders\",\n        provider=\"mmlearn\",\n    ),\n)\ndef vit_tiny(\n    patch_size: int = 16, kwargs: Optional[dict[str, Any]] = None\n) -&gt; VisionTransformer:\n    \"\"\"Create a VisionTransformer model with tiny configuration.\n\n    Parameters\n    ----------\n    patch_size : int, default=16\n        Size of each patch.\n    kwargs : dict[str, Any], optional, default=None\n        Keyword arguments for the model variant.\n\n    Returns\n    -------\n    VisionTransformer\n        An instance of VisionTransformer.\n    \"\"\"\n    if kwargs is None:\n        kwargs = {}\n    return VisionTransformer(\n        patch_size=patch_size,\n        embed_dim=192,\n        depth=12,\n        num_heads=3,\n        mlp_ratio=4,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.vision.vit_small","title":"vit_small","text":"<pre><code>vit_small(patch_size=16, kwargs=None)\n</code></pre> <p>Create a VisionTransformer model with small configuration.</p> <p>Parameters:</p> Name Type Description Default <code>patch_size</code> <code>int</code> <p>Size of each patch.</p> <code>16</code> <code>kwargs</code> <code>dict[str, Any]</code> <p>Keyword arguments for the model variant.</p> <code>None</code> <p>Returns:</p> Type Description <code>VisionTransformer</code> <p>An instance of VisionTransformer.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>@cast(\n    VisionTransformer,\n    store(\n        group=\"modules/encoders\",\n        provider=\"mmlearn\",\n    ),\n)\ndef vit_small(\n    patch_size: int = 16, kwargs: Optional[dict[str, Any]] = None\n) -&gt; VisionTransformer:\n    \"\"\"Create a VisionTransformer model with small configuration.\n\n    Parameters\n    ----------\n    patch_size : int, default=16\n        Size of each patch.\n    kwargs : dict[str, Any], optional, default=None\n        Keyword arguments for the model variant.\n\n    Returns\n    -------\n    VisionTransformer\n        An instance of VisionTransformer.\n    \"\"\"\n    if kwargs is None:\n        kwargs = {}\n    return VisionTransformer(\n        patch_size=patch_size,\n        embed_dim=384,\n        depth=12,\n        num_heads=6,\n        mlp_ratio=4,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.vision.vit_base","title":"vit_base","text":"<pre><code>vit_base(patch_size=16, kwargs=None)\n</code></pre> <p>Create a VisionTransformer model with base configuration.</p> <p>Parameters:</p> Name Type Description Default <code>patch_size</code> <code>int</code> <p>Size of each patch.</p> <code>16</code> <code>kwargs</code> <code>dict[str, Any]</code> <p>Keyword arguments for the model variant.</p> <code>None</code> <p>Returns:</p> Type Description <code>VisionTransformer</code> <p>An instance of VisionTransformer.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>@cast(\n    VisionTransformer,\n    store(\n        group=\"modules/encoders\",\n        provider=\"mmlearn\",\n    ),\n)\ndef vit_base(\n    patch_size: int = 16, kwargs: Optional[dict[str, Any]] = None\n) -&gt; VisionTransformer:\n    \"\"\"Create a VisionTransformer model with base configuration.\n\n    Parameters\n    ----------\n    patch_size : int, default=16\n        Size of each patch.\n    kwargs : dict[str, Any], optional, default=None\n        Keyword arguments for the model variant.\n\n    Returns\n    -------\n    VisionTransformer\n        An instance of VisionTransformer.\n    \"\"\"\n    if kwargs is None:\n        kwargs = {}\n    return VisionTransformer(\n        patch_size=patch_size,\n        embed_dim=768,\n        depth=12,\n        num_heads=12,\n        mlp_ratio=4,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.vision.vit_large","title":"vit_large","text":"<pre><code>vit_large(patch_size=16, kwargs=None)\n</code></pre> <p>Create a VisionTransformer model with large configuration.</p> <p>Parameters:</p> Name Type Description Default <code>patch_size</code> <code>int</code> <p>Size of each patch.</p> <code>16</code> <code>kwargs</code> <code>dict[str, Any]</code> <p>Keyword arguments for the model variant.</p> <code>None</code> <p>Returns:</p> Type Description <code>VisionTransformer</code> <p>An instance of VisionTransformer.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>@cast(\n    VisionTransformer,\n    store(\n        group=\"modules/encoders\",\n        provider=\"mmlearn\",\n    ),\n)\ndef vit_large(\n    patch_size: int = 16, kwargs: Optional[dict[str, Any]] = None\n) -&gt; VisionTransformer:\n    \"\"\"Create a VisionTransformer model with large configuration.\n\n    Parameters\n    ----------\n    patch_size : int, default=16\n        Size of each patch.\n    kwargs : dict[str, Any], optional, default=None\n        Keyword arguments for the model variant.\n\n    Returns\n    -------\n    VisionTransformer\n        An instance of VisionTransformer.\n    \"\"\"\n    if kwargs is None:\n        kwargs = {}\n    return VisionTransformer(\n        patch_size=patch_size,\n        embed_dim=1024,\n        depth=24,\n        num_heads=16,\n        mlp_ratio=4,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.vision.vit_huge","title":"vit_huge","text":"<pre><code>vit_huge(patch_size=16, kwargs=None)\n</code></pre> <p>Create a VisionTransformer model with huge configuration.</p> <p>Parameters:</p> Name Type Description Default <code>patch_size</code> <code>int</code> <p>Size of each patch.</p> <code>16</code> <code>kwargs</code> <code>dict[str, Any]</code> <p>Keyword arguments for the model variant.</p> <code>None</code> <p>Returns:</p> Type Description <code>VisionTransformer</code> <p>An instance of VisionTransformer.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>@cast(\n    VisionTransformer,\n    store(\n        group=\"modules/encoders\",\n        provider=\"mmlearn\",\n    ),\n)\ndef vit_huge(\n    patch_size: int = 16, kwargs: Optional[dict[str, Any]] = None\n) -&gt; VisionTransformer:\n    \"\"\"Create a VisionTransformer model with huge configuration.\n\n    Parameters\n    ----------\n    patch_size : int, default=16\n        Size of each patch.\n    kwargs : dict[str, Any], optional, default=None\n        Keyword arguments for the model variant.\n\n    Returns\n    -------\n    VisionTransformer\n        An instance of VisionTransformer.\n    \"\"\"\n    if kwargs is None:\n        kwargs = {}\n    return VisionTransformer(\n        patch_size=patch_size,\n        embed_dim=1280,\n        depth=32,\n        num_heads=16,\n        mlp_ratio=4,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.vision.vit_giant","title":"vit_giant","text":"<pre><code>vit_giant(patch_size=16, kwargs=None)\n</code></pre> <p>Create a VisionTransformer model with giant configuration.</p> <p>Parameters:</p> Name Type Description Default <code>patch_size</code> <code>int</code> <p>Size of each patch.</p> <code>16</code> <code>kwargs</code> <code>dict[str, Any]</code> <p>Keyword arguments for the model variant.</p> <code>None</code> <p>Returns:</p> Type Description <code>VisionTransformer</code> <p>An instance of VisionTransformer.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>@cast(\n    VisionTransformer,\n    store(\n        group=\"modules/encoders\",\n        provider=\"mmlearn\",\n    ),\n)\ndef vit_giant(\n    patch_size: int = 16, kwargs: Optional[dict[str, Any]] = None\n) -&gt; VisionTransformer:\n    \"\"\"Create a VisionTransformer model with giant configuration.\n\n    Parameters\n    ----------\n    patch_size : int, default=16\n        Size of each patch.\n    kwargs : dict[str, Any], optional, default=None\n        Keyword arguments for the model variant.\n\n    Returns\n    -------\n    VisionTransformer\n        An instance of VisionTransformer.\n    \"\"\"\n    if kwargs is None:\n        kwargs = {}\n    return VisionTransformer(\n        patch_size=patch_size,\n        embed_dim=1408,\n        depth=40,\n        num_heads=16,\n        mlp_ratio=48 / 11,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.layers","title":"layers","text":"<p>Custom, reusable layers for models and tasks.</p>"},{"location":"api/#mmlearn.modules.layers.LearnableLogitScaling","title":"LearnableLogitScaling","text":"<p>               Bases: <code>Module</code></p> <p>Logit scaling layer.</p> <p>Parameters:</p> Name Type Description Default <code>init_logit_scale</code> <code>float</code> <p>Initial value of the logit scale.</p> <code>1/0.07</code> <code>learnable</code> <code>bool</code> <p>If True, the logit scale is learnable. Otherwise, it is fixed.</p> <code>True</code> <code>max_logit_scale</code> <code>float</code> <p>Maximum value of the logit scale.</p> <code>100</code> Source code in <code>mmlearn/modules/layers/logit_scaling.py</code> <pre><code>@store(group=\"modules/layers\", provider=\"mmlearn\")\nclass LearnableLogitScaling(torch.nn.Module):\n    \"\"\"Logit scaling layer.\n\n    Parameters\n    ----------\n    init_logit_scale : float, optional, default=1/0.07\n        Initial value of the logit scale.\n    learnable : bool, optional, default=True\n        If True, the logit scale is learnable. Otherwise, it is fixed.\n    max_logit_scale : float, optional, default=100\n        Maximum value of the logit scale.\n    \"\"\"\n\n    def __init__(\n        self,\n        init_logit_scale: float = 1 / 0.07,\n        max_logit_scale: float = 100,\n        learnable: bool = True,\n    ) -&gt; None:\n        super().__init__()\n        self.max_logit_scale = max_logit_scale\n        self.init_logit_scale = init_logit_scale\n        self.learnable = learnable\n        log_logit_scale = torch.ones([]) * np.log(self.init_logit_scale)\n        if learnable:\n            self.log_logit_scale = torch.nn.Parameter(log_logit_scale)\n        else:\n            self.register_buffer(\"log_logit_scale\", log_logit_scale)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Apply the logit scaling to the input tensor.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor of shape ``(batch_sz, seq_len, dim)``.\n        \"\"\"\n        return torch.clip(self.log_logit_scale.exp(), max=self.max_logit_scale) * x\n\n    def extra_repr(self) -&gt; str:\n        \"\"\"Return the string representation of the layer.\"\"\"\n        return (\n            f\"logit_scale_init={self.init_logit_scale},learnable={self.learnable},\"\n            f\" max_logit_scale={self.max_logit_scale}\"\n        )\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.LearnableLogitScaling.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Apply the logit scaling to the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape <code>(batch_sz, seq_len, dim)</code>.</p> required Source code in <code>mmlearn/modules/layers/logit_scaling.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Apply the logit scaling to the input tensor.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape ``(batch_sz, seq_len, dim)``.\n    \"\"\"\n    return torch.clip(self.log_logit_scale.exp(), max=self.max_logit_scale) * x\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.LearnableLogitScaling.extra_repr","title":"extra_repr","text":"<pre><code>extra_repr()\n</code></pre> <p>Return the string representation of the layer.</p> Source code in <code>mmlearn/modules/layers/logit_scaling.py</code> <pre><code>def extra_repr(self) -&gt; str:\n    \"\"\"Return the string representation of the layer.\"\"\"\n    return (\n        f\"logit_scale_init={self.init_logit_scale},learnable={self.learnable},\"\n        f\" max_logit_scale={self.max_logit_scale}\"\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.MLP","title":"MLP","text":"<p>               Bases: <code>Sequential</code></p> <p>Multi-layer perceptron (MLP).</p> <p>This module will create a block of <code>Linear -&gt; Normalization -&gt; Activation -&gt; Dropout</code> layers.</p> <p>Parameters:</p> Name Type Description Default <code>in_dim</code> <code>int</code> <p>The input dimension.</p> required <code>out_dim</code> <code>Optional[int]</code> <p>The output dimension. If not specified, it is set to :attr:<code>in_dim</code>.</p> <code>None</code> <code>hidden_dims</code> <code>Optional[list]</code> <p>The dimensions of the hidden layers. The length of the list determines the number of hidden layers. This parameter is mutually exclusive with :attr:<code>hidden_dims_multiplier</code>.</p> <code>None</code> <code>hidden_dims_multiplier</code> <code>Optional[list]</code> <p>The multipliers to apply to the input dimension to get the dimensions of the hidden layers. The length of the list determines the number of hidden layers. The multipliers will be used to get the dimensions of the hidden layers. This parameter is mutually exclusive with <code>hidden_dims</code>.</p> <code>None</code> <code>apply_multiplier_to_in_dim</code> <code>bool</code> <p>Whether to apply the :attr:<code>hidden_dims_multiplier</code> to :attr:<code>in_dim</code> to get the dimensions of the hidden layers. If <code>False</code>, the multipliers will be applied to the dimensions of the previous hidden layer, starting from :attr:<code>in_dim</code>. This parameter is only relevant when :attr:<code>hidden_dims_multiplier</code> is specified.</p> <code>False</code> <code>norm_layer</code> <code>Optional[Callable[..., Module]]</code> <p>The normalization layer to use. If not specified, no normalization is used. Partial functions can be used to specify the normalization layer with specific parameters.</p> <code>None</code> <code>activation_layer</code> <code>Optional[Callable[..., Module]]</code> <p>The activation layer to use. If not specified, ReLU is used. Partial functions can be used to specify the activation layer with specific parameters.</p> <code>torch.nn.ReLU</code> <code>bias</code> <code>Union[bool, list[bool]]</code> <p>Whether to use bias in the linear layers.</p> <code>True</code> <code>dropout</code> <code>Union[float, list[float]]</code> <p>The dropout probability to use.</p> <code>0.0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If both :attr:<code>hidden_dims</code> and :attr:<code>hidden_dims_multiplier</code> are specified or if the lengths of :attr:<code>bias</code> and :attr:<code>hidden_dims</code> do not match or if the lengths of :attr:<code>dropout</code> and :attr:<code>hidden_dims</code> do not match.</p> Source code in <code>mmlearn/modules/layers/mlp.py</code> <pre><code>@store(group=\"modules/layers\", provider=\"mmlearn\")\nclass MLP(torch.nn.Sequential):\n    \"\"\"Multi-layer perceptron (MLP).\n\n    This module will create a block of ``Linear -&gt; Normalization -&gt; Activation -&gt; Dropout``\n    layers.\n\n    Parameters\n    ----------\n    in_dim : int\n        The input dimension.\n    out_dim : Optional[int], optional, default=None\n        The output dimension. If not specified, it is set to :attr:`in_dim`.\n    hidden_dims : Optional[list], optional, default=None\n        The dimensions of the hidden layers. The length of the list determines the\n        number of hidden layers. This parameter is mutually exclusive with\n        :attr:`hidden_dims_multiplier`.\n    hidden_dims_multiplier : Optional[list], optional, default=None\n        The multipliers to apply to the input dimension to get the dimensions of\n        the hidden layers. The length of the list determines the number of hidden\n        layers. The multipliers will be used to get the dimensions of the hidden\n        layers. This parameter is mutually exclusive with `hidden_dims`.\n    apply_multiplier_to_in_dim : bool, optional, default=False\n        Whether to apply the :attr:`hidden_dims_multiplier` to :attr:`in_dim` to get the\n        dimensions of the hidden layers. If ``False``, the multipliers will be applied\n        to the dimensions of the previous hidden layer, starting from :attr:`in_dim`.\n        This parameter is only relevant when :attr:`hidden_dims_multiplier` is\n        specified.\n    norm_layer : Optional[Callable[..., torch.nn.Module]], optional, default=None\n        The normalization layer to use. If not specified, no normalization is used.\n        Partial functions can be used to specify the normalization layer with specific\n        parameters.\n    activation_layer : Optional[Callable[..., torch.nn.Module]], optional, default=torch.nn.ReLU\n        The activation layer to use. If not specified, ReLU is used. Partial functions\n        can be used to specify the activation layer with specific parameters.\n    bias : Union[bool, list[bool]], optional, default=True\n        Whether to use bias in the linear layers.\n    dropout : Union[float, list[float]], optional, default=0.0\n        The dropout probability to use.\n\n    Raises\n    ------\n    ValueError\n        If both :attr:`hidden_dims` and :attr:`hidden_dims_multiplier` are specified\n        or if the lengths of :attr:`bias` and :attr:`hidden_dims` do not match or if\n        the lengths of :attr:`dropout` and :attr:`hidden_dims` do not match.\n\n    \"\"\"  # noqa: W505\n\n    def __init__(  # noqa: PLR0912\n        self,\n        in_dim: int,\n        out_dim: Optional[int] = None,\n        hidden_dims: Optional[list[int]] = None,\n        hidden_dims_multiplier: Optional[list[float]] = None,\n        apply_multiplier_to_in_dim: bool = False,\n        norm_layer: Optional[Callable[..., torch.nn.Module]] = None,\n        activation_layer: Optional[Callable[..., torch.nn.Module]] = torch.nn.ReLU,\n        bias: Union[bool, list[bool]] = True,\n        dropout: Union[float, list[float]] = 0.0,\n    ) -&gt; None:\n        if hidden_dims is None and hidden_dims_multiplier is None:\n            hidden_dims = []\n        if hidden_dims is not None and hidden_dims_multiplier is not None:\n            raise ValueError(\n                \"Only one of `hidden_dims` or `hidden_dims_multiplier` must be specified.\"\n            )\n\n        if hidden_dims is None and hidden_dims_multiplier is not None:\n            if apply_multiplier_to_in_dim:\n                hidden_dims = [\n                    int(in_dim * multiplier) for multiplier in hidden_dims_multiplier\n                ]\n            else:\n                hidden_dims = [int(in_dim * hidden_dims_multiplier[0])]\n                for multiplier in hidden_dims_multiplier[1:]:\n                    hidden_dims.append(int(hidden_dims[-1] * multiplier))\n\n        if isinstance(bias, bool):\n            bias_list: list[bool] = [bias] * (len(hidden_dims) + 1)  # type: ignore[arg-type]\n        else:\n            bias_list = bias\n        if len(bias_list) != len(hidden_dims) + 1:  # type: ignore[arg-type]\n            raise ValueError(\n                \"Expected `bias` to be a boolean or a list of booleans with length \"\n                \"equal to the number of linear layers in the MLP.\"\n            )\n\n        if isinstance(dropout, float):\n            dropout_list: list[float] = [dropout] * (len(hidden_dims) + 1)  # type: ignore[arg-type]\n        else:\n            dropout_list = dropout\n        if len(dropout_list) != len(hidden_dims) + 1:  # type: ignore[arg-type]\n            raise ValueError(\n                \"Expected `dropout` to be a float or a list of floats with length \"\n                \"equal to the number of linear layers in the MLP.\"\n            )\n\n        # construct list of dimensions for the layers\n        dims = [in_dim] + hidden_dims  # type: ignore[operator]\n        layers = []\n        for layer_idx, (in_features, hidden_features) in enumerate(\n            zip(dims[:-1], dims[1:], strict=False)\n        ):\n            layers.append(\n                torch.nn.Linear(in_features, hidden_features, bias=bias_list[layer_idx])\n            )\n            if norm_layer is not None:\n                layers.append(norm_layer(hidden_features))\n            if activation_layer is not None:\n                layers.append(activation_layer())\n            layers.append(torch.nn.Dropout(dropout_list[layer_idx]))\n\n        out_dim = out_dim or in_dim\n\n        layers.append(torch.nn.Linear(dims[-1], out_dim, bias=bias_list[-1]))\n        layers.append(torch.nn.Dropout(dropout_list[-1]))\n\n        super().__init__(*layers)\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.L2Norm","title":"L2Norm","text":"<p>               Bases: <code>Module</code></p> <p>L2 normalization.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>The dimension along which to normalize.</p> required Source code in <code>mmlearn/modules/layers/normalization.py</code> <pre><code>@store(group=\"modules/layers\", provider=\"mmlearn\")\nclass L2Norm(torch.nn.Module):\n    \"\"\"L2 normalization.\n\n    Parameters\n    ----------\n    dim : int\n        The dimension along which to normalize.\n    \"\"\"\n\n    def __init__(self, dim: int) -&gt; None:\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Apply L2 normalization to the input tensor.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor of shape ``(batch_sz, seq_len, dim)``.\n\n        Returns\n        -------\n        torch.Tensor\n            Normalized tensor of shape ``(batch_sz, seq_len, dim)``.\n        \"\"\"\n        return torch.nn.functional.normalize(x, dim=self.dim, p=2)\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.L2Norm.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Apply L2 normalization to the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape <code>(batch_sz, seq_len, dim)</code>.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized tensor of shape <code>(batch_sz, seq_len, dim)</code>.</p> Source code in <code>mmlearn/modules/layers/normalization.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Apply L2 normalization to the input tensor.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape ``(batch_sz, seq_len, dim)``.\n\n    Returns\n    -------\n    torch.Tensor\n        Normalized tensor of shape ``(batch_sz, seq_len, dim)``.\n    \"\"\"\n    return torch.nn.functional.normalize(x, dim=self.dim, p=2)\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.PatchDropout","title":"PatchDropout","text":"<p>               Bases: <code>Module</code></p> <p>Patch dropout layer.</p> <p>Drops patch tokens (after embedding and adding CLS token) from the input tensor. Usually used in vision transformers to reduce the number of tokens. [1]_</p> <p>Parameters:</p> Name Type Description Default <code>keep_rate</code> <code>float</code> <p>The proportion of tokens to keep.</p> <code>0.5</code> <code>bias</code> <code>Optional[float]</code> <p>The bias to add to the random noise before sorting.</p> <code>None</code> <code>token_shuffling</code> <code>bool</code> <p>If True, the tokens are shuffled.</p> <code>False</code> References <p>.. [1] Liu, Y., Matsoukas, C., Strand, F., Azizpour, H., &amp; Smith, K. (2023).    Patchdropout: Economizing vision transformers using patch dropout. In Proceedings    of the IEEE/CVF Winter Conference on Applications of Computer Vision    (pp. 3953-3962).</p> Source code in <code>mmlearn/modules/layers/patch_dropout.py</code> <pre><code>class PatchDropout(torch.nn.Module):\n    \"\"\"Patch dropout layer.\n\n    Drops patch tokens (after embedding and adding CLS token) from the input tensor.\n    Usually used in vision transformers to reduce the number of tokens. [1]_\n\n    Parameters\n    ----------\n    keep_rate : float, optional, default=0.5\n        The proportion of tokens to keep.\n    bias : Optional[float], optional, default=None\n        The bias to add to the random noise before sorting.\n    token_shuffling : bool, optional, default=False\n        If True, the tokens are shuffled.\n\n    References\n    ----------\n    .. [1] Liu, Y., Matsoukas, C., Strand, F., Azizpour, H., &amp; Smith, K. (2023).\n       Patchdropout: Economizing vision transformers using patch dropout. In Proceedings\n       of the IEEE/CVF Winter Conference on Applications of Computer Vision\n       (pp. 3953-3962).\n    \"\"\"\n\n    def __init__(\n        self,\n        keep_rate: float = 0.5,\n        bias: Optional[float] = None,\n        token_shuffling: bool = False,\n    ):\n        super().__init__()\n        assert 0 &lt; keep_rate &lt;= 1, \"The keep_rate must be in (0,1]\"\n\n        self.bias = bias\n        self.keep_rate = keep_rate\n        self.token_shuffling = token_shuffling\n\n    def forward(self, x: torch.Tensor, force_drop: bool = False) -&gt; torch.Tensor:\n        \"\"\"Drop tokens from the input tensor.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor of shape ``(batch_sz, seq_len, dim)``.\n        force_drop : bool, optional, default=False\n            If True, the tokens are always dropped, even when the model is in\n            evaluation mode.\n\n        Returns\n        -------\n        torch.Tensor\n            Tensor of shape ``(batch_sz, keep_len, dim)`` containing the kept tokens.\n        \"\"\"\n        if (not self.training and not force_drop) or self.keep_rate == 1:\n            return x\n\n        batch_sz, _, dim = x.shape\n\n        cls_mask = torch.zeros(  # assumes that CLS is always the 1st element\n            batch_sz, 1, dtype=torch.int64, device=x.device\n        )\n        patch_mask = self.uniform_mask(x)\n        patch_mask = torch.hstack([cls_mask, patch_mask])\n\n        return torch.gather(x, dim=1, index=patch_mask.unsqueeze(-1).repeat(1, 1, dim))\n\n    def uniform_mask(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Generate token ids to keep from uniform random distribution.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor of shape ``(batch_sz, seq_len, dim)``.\n\n        Returns\n        -------\n        torch.Tensor\n            Tensor of shape ``(batch_sz, keep_len)`` containing the token ids to keep.\n\n        \"\"\"\n        batch_sz, seq_len, _ = x.shape\n        seq_len = seq_len - 1  # patch length (without CLS)\n\n        keep_len = int(seq_len * self.keep_rate)\n        noise = torch.rand(batch_sz, seq_len, device=x.device)\n        if self.bias is not None:\n            noise += self.bias\n        ids = torch.argsort(noise, dim=1)\n        keep_ids = ids[:, :keep_len]\n        if not self.token_shuffling:\n            keep_ids = keep_ids.sort(1)[0]\n        return keep_ids\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.PatchDropout.forward","title":"forward","text":"<pre><code>forward(x, force_drop=False)\n</code></pre> <p>Drop tokens from the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape <code>(batch_sz, seq_len, dim)</code>.</p> required <code>force_drop</code> <code>bool</code> <p>If True, the tokens are always dropped, even when the model is in evaluation mode.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of shape <code>(batch_sz, keep_len, dim)</code> containing the kept tokens.</p> Source code in <code>mmlearn/modules/layers/patch_dropout.py</code> <pre><code>def forward(self, x: torch.Tensor, force_drop: bool = False) -&gt; torch.Tensor:\n    \"\"\"Drop tokens from the input tensor.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape ``(batch_sz, seq_len, dim)``.\n    force_drop : bool, optional, default=False\n        If True, the tokens are always dropped, even when the model is in\n        evaluation mode.\n\n    Returns\n    -------\n    torch.Tensor\n        Tensor of shape ``(batch_sz, keep_len, dim)`` containing the kept tokens.\n    \"\"\"\n    if (not self.training and not force_drop) or self.keep_rate == 1:\n        return x\n\n    batch_sz, _, dim = x.shape\n\n    cls_mask = torch.zeros(  # assumes that CLS is always the 1st element\n        batch_sz, 1, dtype=torch.int64, device=x.device\n    )\n    patch_mask = self.uniform_mask(x)\n    patch_mask = torch.hstack([cls_mask, patch_mask])\n\n    return torch.gather(x, dim=1, index=patch_mask.unsqueeze(-1).repeat(1, 1, dim))\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.PatchDropout.uniform_mask","title":"uniform_mask","text":"<pre><code>uniform_mask(x)\n</code></pre> <p>Generate token ids to keep from uniform random distribution.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape <code>(batch_sz, seq_len, dim)</code>.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of shape <code>(batch_sz, keep_len)</code> containing the token ids to keep.</p> Source code in <code>mmlearn/modules/layers/patch_dropout.py</code> <pre><code>def uniform_mask(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Generate token ids to keep from uniform random distribution.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape ``(batch_sz, seq_len, dim)``.\n\n    Returns\n    -------\n    torch.Tensor\n        Tensor of shape ``(batch_sz, keep_len)`` containing the token ids to keep.\n\n    \"\"\"\n    batch_sz, seq_len, _ = x.shape\n    seq_len = seq_len - 1  # patch length (without CLS)\n\n    keep_len = int(seq_len * self.keep_rate)\n    noise = torch.rand(batch_sz, seq_len, device=x.device)\n    if self.bias is not None:\n        noise += self.bias\n    ids = torch.argsort(noise, dim=1)\n    keep_ids = ids[:, :keep_len]\n    if not self.token_shuffling:\n        keep_ids = keep_ids.sort(1)[0]\n    return keep_ids\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.attention","title":"attention","text":"<p>Attention modules for Vision Transformer (ViT) and related models.</p>"},{"location":"api/#mmlearn.modules.layers.attention.Attention","title":"Attention","text":"<p>               Bases: <code>Module</code></p> <p>Multi-head Self-Attention Mechanism.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>Number of input dimensions.</p> required <code>num_heads</code> <code>int</code> <p>Number of attention heads.</p> <code>8</code> <code>qkv_bias</code> <code>bool</code> <p>If True, adds a learnable bias to the query, key, value projections.</p> <code>False</code> <code>qk_scale</code> <code>Optional[float]</code> <p>Override the default scale factor for the dot-product attention.</p> <code>None</code> <code>attn_drop</code> <code>float</code> <p>Dropout probability for the attention weights.</p> <code>0.0</code> <code>proj_drop</code> <code>float</code> <p>Dropout probability for the output of the attention layer.</p> <code>0.0</code> Source code in <code>mmlearn/modules/layers/attention.py</code> <pre><code>class Attention(nn.Module):\n    \"\"\"Multi-head Self-Attention Mechanism.\n\n    Parameters\n    ----------\n    dim : int\n        Number of input dimensions.\n    num_heads : int, optional, default=8\n        Number of attention heads.\n    qkv_bias : bool, optional, default=False\n        If True, adds a learnable bias to the query, key, value projections.\n    qk_scale : Optional[float], optional, default=None\n        Override the default scale factor for the dot-product attention.\n    attn_drop : float, optional, default=0.0\n        Dropout probability for the attention weights.\n    proj_drop : float, optional, default=0.0\n        Dropout probability for the output of the attention layer.\n    \"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int = 8,\n        qkv_bias: bool = False,\n        qk_scale: Optional[float] = None,\n        attn_drop: float = 0.0,\n        proj_drop: float = 0.0,\n    ) -&gt; None:\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim**-0.5\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x: torch.Tensor) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Forward pass through the multi-head self-attention module.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor of shape ``(batch_sz, seq_len, dim)``.\n\n        Returns\n        -------\n        tuple[torch.Tensor, torch.Tensor]\n            The output tensor and the attention weights.\n        \"\"\"\n        b, n, c = x.shape\n        qkv = (\n            self.qkv(x)\n            .reshape(b, n, 3, self.num_heads, c // self.num_heads)\n            .permute(2, 0, 3, 1, 4)\n        )\n        q, k, v = qkv[0], qkv[1], qkv[2]\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(b, n, c)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x, attn\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.attention.Attention.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Forward pass through the multi-head self-attention module.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape <code>(batch_sz, seq_len, dim)</code>.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>The output tensor and the attention weights.</p> Source code in <code>mmlearn/modules/layers/attention.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Forward pass through the multi-head self-attention module.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape ``(batch_sz, seq_len, dim)``.\n\n    Returns\n    -------\n    tuple[torch.Tensor, torch.Tensor]\n        The output tensor and the attention weights.\n    \"\"\"\n    b, n, c = x.shape\n    qkv = (\n        self.qkv(x)\n        .reshape(b, n, 3, self.num_heads, c // self.num_heads)\n        .permute(2, 0, 3, 1, 4)\n    )\n    q, k, v = qkv[0], qkv[1], qkv[2]\n\n    attn = (q @ k.transpose(-2, -1)) * self.scale\n    attn = attn.softmax(dim=-1)\n    attn = self.attn_drop(attn)\n\n    x = (attn @ v).transpose(1, 2).reshape(b, n, c)\n    x = self.proj(x)\n    x = self.proj_drop(x)\n    return x, attn\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.embedding","title":"embedding","text":"<p>Embedding layers.</p>"},{"location":"api/#mmlearn.modules.layers.embedding.PatchEmbed","title":"PatchEmbed","text":"<p>               Bases: <code>Module</code></p> <p>Image to Patch Embedding.</p> <p>This module divides an image into patches and embeds them as a sequence of vectors.</p> <p>Parameters:</p> Name Type Description Default <code>img_size</code> <code>int</code> <p>Size of the input image (assumed to be square).</p> <code>224</code> <code>patch_size</code> <code>int</code> <p>Size of each image patch (assumed to be square).</p> <code>16</code> <code>in_chans</code> <code>int</code> <p>Number of input channels in the image.</p> <code>3</code> <code>embed_dim</code> <code>int</code> <p>Dimension of the output embeddings.</p> <code>768</code> Source code in <code>mmlearn/modules/layers/embedding.py</code> <pre><code>class PatchEmbed(nn.Module):\n    \"\"\"Image to Patch Embedding.\n\n    This module divides an image into patches and embeds them as a sequence of vectors.\n\n    Parameters\n    ----------\n    img_size : int, optional, default=224\n        Size of the input image (assumed to be square).\n    patch_size : int, optional, default=16\n        Size of each image patch (assumed to be square).\n    in_chans : int, optional, default=3\n        Number of input channels in the image.\n    embed_dim : int, optional, default=768\n        Dimension of the output embeddings.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        img_size: int = 224,\n        patch_size: int = 16,\n        in_chans: int = 3,\n        embed_dim: int = 768,\n    ) -&gt; None:\n        super().__init__()\n        num_patches = (img_size // patch_size) * (img_size // patch_size)\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n\n        self.proj = nn.Conv2d(\n            in_chans, embed_dim, kernel_size=patch_size, stride=patch_size\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass to convert an image into patch embeddings.\"\"\"\n        return self.proj(x).flatten(2).transpose(1, 2)\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.embedding.PatchEmbed.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Forward pass to convert an image into patch embeddings.</p> Source code in <code>mmlearn/modules/layers/embedding.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass to convert an image into patch embeddings.\"\"\"\n    return self.proj(x).flatten(2).transpose(1, 2)\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.embedding.ConvEmbed","title":"ConvEmbed","text":"<p>               Bases: <code>Module</code></p> <p>3x3 Convolution stems for ViT following ViTC models.</p> <p>This module builds convolutional stems for Vision Transformers (ViT) with intermediate batch normalization and ReLU activation.</p> <p>Parameters:</p> Name Type Description Default <code>channels</code> <code>list[int]</code> <p>list of channel sizes for each convolution layer.</p> required <code>strides</code> <code>list[int]</code> <p>list of stride sizes for each convolution layer.</p> required <code>img_size</code> <code>int</code> <p>Size of the input image (assumed to be square).</p> <code>224</code> <code>in_chans</code> <code>int</code> <p>Number of input channels in the image.</p> <code>3</code> <code>batch_norm</code> <code>bool</code> <p>Whether to include batch normalization after each convolution layer.</p> <code>True</code> Source code in <code>mmlearn/modules/layers/embedding.py</code> <pre><code>class ConvEmbed(nn.Module):\n    \"\"\"3x3 Convolution stems for ViT following ViTC models.\n\n    This module builds convolutional stems for Vision Transformers (ViT)\n    with intermediate batch normalization and ReLU activation.\n\n    Parameters\n    ----------\n    channels : list[int]\n        list of channel sizes for each convolution layer.\n    strides : list[int]\n        list of stride sizes for each convolution layer.\n    img_size : int, optional, default=224\n        Size of the input image (assumed to be square).\n    in_chans : int, optional, default=3\n        Number of input channels in the image.\n    batch_norm : bool, optional, default=True\n        Whether to include batch normalization after each convolution layer.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        channels: list[int],\n        strides: list[int],\n        img_size: int = 224,\n        in_chans: int = 3,\n        batch_norm: bool = True,\n    ) -&gt; None:\n        super().__init__()\n        # Build the stems\n        stem = []\n        channels = [in_chans] + channels\n        for i in range(len(channels) - 2):\n            stem += [\n                nn.Conv2d(\n                    channels[i],\n                    channels[i + 1],\n                    kernel_size=3,\n                    stride=strides[i],\n                    padding=1,\n                    bias=(not batch_norm),\n                )\n            ]\n            if batch_norm:\n                stem += [nn.BatchNorm2d(channels[i + 1])]\n            stem += [nn.ReLU(inplace=True)]\n        stem += [\n            nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])\n        ]\n        self.stem = nn.Sequential(*stem)\n\n        # Compute the number of patches\n        stride_prod = int(np.prod(strides))\n        self.num_patches = (img_size // stride_prod) ** 2\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass through the convolutional embedding layers.\"\"\"\n        p = self.stem(x)\n        return p.flatten(2).transpose(1, 2)\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.embedding.ConvEmbed.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Forward pass through the convolutional embedding layers.</p> Source code in <code>mmlearn/modules/layers/embedding.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass through the convolutional embedding layers.\"\"\"\n    p = self.stem(x)\n    return p.flatten(2).transpose(1, 2)\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.embedding.get_2d_sincos_pos_embed","title":"get_2d_sincos_pos_embed","text":"<pre><code>get_2d_sincos_pos_embed(\n    embed_dim, grid_size, cls_token=False\n)\n</code></pre> <p>Generate 2D sine-cosine positional embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>embed_dim</code> <code>int</code> <p>The dimension of the embeddings.</p> required <code>grid_size</code> <code>int</code> <p>The size of the grid (both height and width).</p> required <code>cls_token</code> <code>bool</code> <p>Whether to include a class token in the embeddings.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>pos_embed</code> <code>ndarray</code> <p>Positional embeddings with shape [grid_sizegrid_size, embed_dim] or [1 + grid_sizegrid_size, embed_dim] if cls_token is True.</p> Source code in <code>mmlearn/modules/layers/embedding.py</code> <pre><code>def get_2d_sincos_pos_embed(\n    embed_dim: int, grid_size: int, cls_token: bool = False\n) -&gt; np.ndarray:\n    \"\"\"\n    Generate 2D sine-cosine positional embeddings.\n\n    Parameters\n    ----------\n    embed_dim : int\n        The dimension of the embeddings.\n    grid_size : int\n        The size of the grid (both height and width).\n    cls_token : bool, optional, default=False\n        Whether to include a class token in the embeddings.\n\n    Returns\n    -------\n    pos_embed : np.ndarray\n        Positional embeddings with shape [grid_size*grid_size, embed_dim] or\n        [1 + grid_size*grid_size, embed_dim] if cls_token is True.\n    \"\"\"\n    grid_h = np.arange(grid_size, dtype=float)\n    grid_w = np.arange(grid_size, dtype=float)\n    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n    grid = np.stack(grid, axis=0)\n\n    grid = grid.reshape([2, 1, grid_size, grid_size])\n    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n    if cls_token:\n        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n    return pos_embed\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.embedding.get_2d_sincos_pos_embed_from_grid","title":"get_2d_sincos_pos_embed_from_grid","text":"<pre><code>get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n</code></pre> <p>Generate 2D sine-cosine positional embeddings from a grid.</p> <p>Parameters:</p> Name Type Description Default <code>embed_dim</code> <code>int</code> <p>The dimension of the embeddings.</p> required <code>grid</code> <code>ndarray</code> <p>The grid of positions with shape [2, 1, grid_size, grid_size].</p> required <p>Returns:</p> Name Type Description <code>emb</code> <code>ndarray</code> <p>Positional embeddings with shape [grid_size*grid_size, embed_dim].</p> Source code in <code>mmlearn/modules/layers/embedding.py</code> <pre><code>def get_2d_sincos_pos_embed_from_grid(embed_dim: int, grid: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Generate 2D sine-cosine positional embeddings from a grid.\n\n    Parameters\n    ----------\n    embed_dim : int\n        The dimension of the embeddings.\n    grid : np.ndarray\n        The grid of positions with shape [2, 1, grid_size, grid_size].\n\n    Returns\n    -------\n    emb : np.ndarray\n        Positional embeddings with shape [grid_size*grid_size, embed_dim].\n    \"\"\"\n    assert embed_dim % 2 == 0\n\n    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n\n    return np.concatenate([emb_h, emb_w], axis=1)\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.embedding.get_1d_sincos_pos_embed","title":"get_1d_sincos_pos_embed","text":"<pre><code>get_1d_sincos_pos_embed(\n    embed_dim, grid_size, cls_token=False\n)\n</code></pre> <p>Generate 1D sine-cosine positional embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>embed_dim</code> <code>int</code> <p>The dimension of the embeddings.</p> required <code>grid_size</code> <code>int</code> <p>The size of the grid.</p> required <code>cls_token</code> <code>bool</code> <p>Whether to include a class token in the embeddings.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>pos_embed</code> <code>ndarray</code> <p>Positional embeddings with shape [grid_size, embed_dim] or [1 + grid_size, embed_dim] if cls_token is True.</p> Source code in <code>mmlearn/modules/layers/embedding.py</code> <pre><code>def get_1d_sincos_pos_embed(\n    embed_dim: int, grid_size: int, cls_token: bool = False\n) -&gt; np.ndarray:\n    \"\"\"\n    Generate 1D sine-cosine positional embeddings.\n\n    Parameters\n    ----------\n    embed_dim : int\n        The dimension of the embeddings.\n    grid_size : int\n        The size of the grid.\n    cls_token : bool, optional, default=False\n        Whether to include a class token in the embeddings.\n\n    Returns\n    -------\n    pos_embed : np.ndarray\n        Positional embeddings with shape [grid_size, embed_dim] or\n        [1 + grid_size, embed_dim] if cls_token is True.\n    \"\"\"\n    grid = np.arange(grid_size, dtype=float)\n    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n    if cls_token:\n        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n    return pos_embed\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.embedding.get_1d_sincos_pos_embed_from_grid","title":"get_1d_sincos_pos_embed_from_grid","text":"<pre><code>get_1d_sincos_pos_embed_from_grid(embed_dim, pos)\n</code></pre> <p>Generate 1D sine-cosine positional embeddings from a grid.</p> <p>Parameters:</p> Name Type Description Default <code>embed_dim</code> <code>int</code> <p>The dimension of the embeddings.</p> required <code>pos</code> <code>ndarray</code> <p>A list of positions to be encoded, with shape [M,].</p> required <p>Returns:</p> Name Type Description <code>emb</code> <code>ndarray</code> <p>Positional embeddings with shape [M, embed_dim].</p> Source code in <code>mmlearn/modules/layers/embedding.py</code> <pre><code>def get_1d_sincos_pos_embed_from_grid(embed_dim: int, pos: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Generate 1D sine-cosine positional embeddings from a grid.\n\n    Parameters\n    ----------\n    embed_dim : int\n        The dimension of the embeddings.\n    pos : np.ndarray\n        A list of positions to be encoded, with shape [M,].\n\n    Returns\n    -------\n    emb : np.ndarray\n        Positional embeddings with shape [M, embed_dim].\n    \"\"\"\n    assert embed_dim % 2 == 0\n    omega = np.arange(embed_dim // 2, dtype=float)\n    omega /= embed_dim / 2.0\n    omega = 1.0 / 10000**omega  # (D/2,)\n\n    pos = pos.reshape(-1)  # (M,)\n    out = np.einsum(\"m,d-&gt;md\", pos, omega)  # (M, D/2), outer product\n\n    emb_sin = np.sin(out)  # (M, D/2)\n    emb_cos = np.cos(out)  # (M, D/2)\n\n    return np.concatenate([emb_sin, emb_cos], axis=1)\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.logit_scaling","title":"logit_scaling","text":"<p>Learnable logit scaling layer.</p>"},{"location":"api/#mmlearn.modules.layers.logit_scaling.LearnableLogitScaling","title":"LearnableLogitScaling","text":"<p>               Bases: <code>Module</code></p> <p>Logit scaling layer.</p> <p>Parameters:</p> Name Type Description Default <code>init_logit_scale</code> <code>float</code> <p>Initial value of the logit scale.</p> <code>1/0.07</code> <code>learnable</code> <code>bool</code> <p>If True, the logit scale is learnable. Otherwise, it is fixed.</p> <code>True</code> <code>max_logit_scale</code> <code>float</code> <p>Maximum value of the logit scale.</p> <code>100</code> Source code in <code>mmlearn/modules/layers/logit_scaling.py</code> <pre><code>@store(group=\"modules/layers\", provider=\"mmlearn\")\nclass LearnableLogitScaling(torch.nn.Module):\n    \"\"\"Logit scaling layer.\n\n    Parameters\n    ----------\n    init_logit_scale : float, optional, default=1/0.07\n        Initial value of the logit scale.\n    learnable : bool, optional, default=True\n        If True, the logit scale is learnable. Otherwise, it is fixed.\n    max_logit_scale : float, optional, default=100\n        Maximum value of the logit scale.\n    \"\"\"\n\n    def __init__(\n        self,\n        init_logit_scale: float = 1 / 0.07,\n        max_logit_scale: float = 100,\n        learnable: bool = True,\n    ) -&gt; None:\n        super().__init__()\n        self.max_logit_scale = max_logit_scale\n        self.init_logit_scale = init_logit_scale\n        self.learnable = learnable\n        log_logit_scale = torch.ones([]) * np.log(self.init_logit_scale)\n        if learnable:\n            self.log_logit_scale = torch.nn.Parameter(log_logit_scale)\n        else:\n            self.register_buffer(\"log_logit_scale\", log_logit_scale)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Apply the logit scaling to the input tensor.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor of shape ``(batch_sz, seq_len, dim)``.\n        \"\"\"\n        return torch.clip(self.log_logit_scale.exp(), max=self.max_logit_scale) * x\n\n    def extra_repr(self) -&gt; str:\n        \"\"\"Return the string representation of the layer.\"\"\"\n        return (\n            f\"logit_scale_init={self.init_logit_scale},learnable={self.learnable},\"\n            f\" max_logit_scale={self.max_logit_scale}\"\n        )\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.logit_scaling.LearnableLogitScaling.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Apply the logit scaling to the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape <code>(batch_sz, seq_len, dim)</code>.</p> required Source code in <code>mmlearn/modules/layers/logit_scaling.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Apply the logit scaling to the input tensor.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape ``(batch_sz, seq_len, dim)``.\n    \"\"\"\n    return torch.clip(self.log_logit_scale.exp(), max=self.max_logit_scale) * x\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.logit_scaling.LearnableLogitScaling.extra_repr","title":"extra_repr","text":"<pre><code>extra_repr()\n</code></pre> <p>Return the string representation of the layer.</p> Source code in <code>mmlearn/modules/layers/logit_scaling.py</code> <pre><code>def extra_repr(self) -&gt; str:\n    \"\"\"Return the string representation of the layer.\"\"\"\n    return (\n        f\"logit_scale_init={self.init_logit_scale},learnable={self.learnable},\"\n        f\" max_logit_scale={self.max_logit_scale}\"\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.mlp","title":"mlp","text":"<p>Multi-layer perceptron (MLP).</p>"},{"location":"api/#mmlearn.modules.layers.mlp.MLP","title":"MLP","text":"<p>               Bases: <code>Sequential</code></p> <p>Multi-layer perceptron (MLP).</p> <p>This module will create a block of <code>Linear -&gt; Normalization -&gt; Activation -&gt; Dropout</code> layers.</p> <p>Parameters:</p> Name Type Description Default <code>in_dim</code> <code>int</code> <p>The input dimension.</p> required <code>out_dim</code> <code>Optional[int]</code> <p>The output dimension. If not specified, it is set to :attr:<code>in_dim</code>.</p> <code>None</code> <code>hidden_dims</code> <code>Optional[list]</code> <p>The dimensions of the hidden layers. The length of the list determines the number of hidden layers. This parameter is mutually exclusive with :attr:<code>hidden_dims_multiplier</code>.</p> <code>None</code> <code>hidden_dims_multiplier</code> <code>Optional[list]</code> <p>The multipliers to apply to the input dimension to get the dimensions of the hidden layers. The length of the list determines the number of hidden layers. The multipliers will be used to get the dimensions of the hidden layers. This parameter is mutually exclusive with <code>hidden_dims</code>.</p> <code>None</code> <code>apply_multiplier_to_in_dim</code> <code>bool</code> <p>Whether to apply the :attr:<code>hidden_dims_multiplier</code> to :attr:<code>in_dim</code> to get the dimensions of the hidden layers. If <code>False</code>, the multipliers will be applied to the dimensions of the previous hidden layer, starting from :attr:<code>in_dim</code>. This parameter is only relevant when :attr:<code>hidden_dims_multiplier</code> is specified.</p> <code>False</code> <code>norm_layer</code> <code>Optional[Callable[..., Module]]</code> <p>The normalization layer to use. If not specified, no normalization is used. Partial functions can be used to specify the normalization layer with specific parameters.</p> <code>None</code> <code>activation_layer</code> <code>Optional[Callable[..., Module]]</code> <p>The activation layer to use. If not specified, ReLU is used. Partial functions can be used to specify the activation layer with specific parameters.</p> <code>torch.nn.ReLU</code> <code>bias</code> <code>Union[bool, list[bool]]</code> <p>Whether to use bias in the linear layers.</p> <code>True</code> <code>dropout</code> <code>Union[float, list[float]]</code> <p>The dropout probability to use.</p> <code>0.0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If both :attr:<code>hidden_dims</code> and :attr:<code>hidden_dims_multiplier</code> are specified or if the lengths of :attr:<code>bias</code> and :attr:<code>hidden_dims</code> do not match or if the lengths of :attr:<code>dropout</code> and :attr:<code>hidden_dims</code> do not match.</p> Source code in <code>mmlearn/modules/layers/mlp.py</code> <pre><code>@store(group=\"modules/layers\", provider=\"mmlearn\")\nclass MLP(torch.nn.Sequential):\n    \"\"\"Multi-layer perceptron (MLP).\n\n    This module will create a block of ``Linear -&gt; Normalization -&gt; Activation -&gt; Dropout``\n    layers.\n\n    Parameters\n    ----------\n    in_dim : int\n        The input dimension.\n    out_dim : Optional[int], optional, default=None\n        The output dimension. If not specified, it is set to :attr:`in_dim`.\n    hidden_dims : Optional[list], optional, default=None\n        The dimensions of the hidden layers. The length of the list determines the\n        number of hidden layers. This parameter is mutually exclusive with\n        :attr:`hidden_dims_multiplier`.\n    hidden_dims_multiplier : Optional[list], optional, default=None\n        The multipliers to apply to the input dimension to get the dimensions of\n        the hidden layers. The length of the list determines the number of hidden\n        layers. The multipliers will be used to get the dimensions of the hidden\n        layers. This parameter is mutually exclusive with `hidden_dims`.\n    apply_multiplier_to_in_dim : bool, optional, default=False\n        Whether to apply the :attr:`hidden_dims_multiplier` to :attr:`in_dim` to get the\n        dimensions of the hidden layers. If ``False``, the multipliers will be applied\n        to the dimensions of the previous hidden layer, starting from :attr:`in_dim`.\n        This parameter is only relevant when :attr:`hidden_dims_multiplier` is\n        specified.\n    norm_layer : Optional[Callable[..., torch.nn.Module]], optional, default=None\n        The normalization layer to use. If not specified, no normalization is used.\n        Partial functions can be used to specify the normalization layer with specific\n        parameters.\n    activation_layer : Optional[Callable[..., torch.nn.Module]], optional, default=torch.nn.ReLU\n        The activation layer to use. If not specified, ReLU is used. Partial functions\n        can be used to specify the activation layer with specific parameters.\n    bias : Union[bool, list[bool]], optional, default=True\n        Whether to use bias in the linear layers.\n    dropout : Union[float, list[float]], optional, default=0.0\n        The dropout probability to use.\n\n    Raises\n    ------\n    ValueError\n        If both :attr:`hidden_dims` and :attr:`hidden_dims_multiplier` are specified\n        or if the lengths of :attr:`bias` and :attr:`hidden_dims` do not match or if\n        the lengths of :attr:`dropout` and :attr:`hidden_dims` do not match.\n\n    \"\"\"  # noqa: W505\n\n    def __init__(  # noqa: PLR0912\n        self,\n        in_dim: int,\n        out_dim: Optional[int] = None,\n        hidden_dims: Optional[list[int]] = None,\n        hidden_dims_multiplier: Optional[list[float]] = None,\n        apply_multiplier_to_in_dim: bool = False,\n        norm_layer: Optional[Callable[..., torch.nn.Module]] = None,\n        activation_layer: Optional[Callable[..., torch.nn.Module]] = torch.nn.ReLU,\n        bias: Union[bool, list[bool]] = True,\n        dropout: Union[float, list[float]] = 0.0,\n    ) -&gt; None:\n        if hidden_dims is None and hidden_dims_multiplier is None:\n            hidden_dims = []\n        if hidden_dims is not None and hidden_dims_multiplier is not None:\n            raise ValueError(\n                \"Only one of `hidden_dims` or `hidden_dims_multiplier` must be specified.\"\n            )\n\n        if hidden_dims is None and hidden_dims_multiplier is not None:\n            if apply_multiplier_to_in_dim:\n                hidden_dims = [\n                    int(in_dim * multiplier) for multiplier in hidden_dims_multiplier\n                ]\n            else:\n                hidden_dims = [int(in_dim * hidden_dims_multiplier[0])]\n                for multiplier in hidden_dims_multiplier[1:]:\n                    hidden_dims.append(int(hidden_dims[-1] * multiplier))\n\n        if isinstance(bias, bool):\n            bias_list: list[bool] = [bias] * (len(hidden_dims) + 1)  # type: ignore[arg-type]\n        else:\n            bias_list = bias\n        if len(bias_list) != len(hidden_dims) + 1:  # type: ignore[arg-type]\n            raise ValueError(\n                \"Expected `bias` to be a boolean or a list of booleans with length \"\n                \"equal to the number of linear layers in the MLP.\"\n            )\n\n        if isinstance(dropout, float):\n            dropout_list: list[float] = [dropout] * (len(hidden_dims) + 1)  # type: ignore[arg-type]\n        else:\n            dropout_list = dropout\n        if len(dropout_list) != len(hidden_dims) + 1:  # type: ignore[arg-type]\n            raise ValueError(\n                \"Expected `dropout` to be a float or a list of floats with length \"\n                \"equal to the number of linear layers in the MLP.\"\n            )\n\n        # construct list of dimensions for the layers\n        dims = [in_dim] + hidden_dims  # type: ignore[operator]\n        layers = []\n        for layer_idx, (in_features, hidden_features) in enumerate(\n            zip(dims[:-1], dims[1:], strict=False)\n        ):\n            layers.append(\n                torch.nn.Linear(in_features, hidden_features, bias=bias_list[layer_idx])\n            )\n            if norm_layer is not None:\n                layers.append(norm_layer(hidden_features))\n            if activation_layer is not None:\n                layers.append(activation_layer())\n            layers.append(torch.nn.Dropout(dropout_list[layer_idx]))\n\n        out_dim = out_dim or in_dim\n\n        layers.append(torch.nn.Linear(dims[-1], out_dim, bias=bias_list[-1]))\n        layers.append(torch.nn.Dropout(dropout_list[-1]))\n\n        super().__init__(*layers)\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.normalization","title":"normalization","text":"<p>Normalization layers.</p>"},{"location":"api/#mmlearn.modules.layers.normalization.L2Norm","title":"L2Norm","text":"<p>               Bases: <code>Module</code></p> <p>L2 normalization.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>The dimension along which to normalize.</p> required Source code in <code>mmlearn/modules/layers/normalization.py</code> <pre><code>@store(group=\"modules/layers\", provider=\"mmlearn\")\nclass L2Norm(torch.nn.Module):\n    \"\"\"L2 normalization.\n\n    Parameters\n    ----------\n    dim : int\n        The dimension along which to normalize.\n    \"\"\"\n\n    def __init__(self, dim: int) -&gt; None:\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Apply L2 normalization to the input tensor.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor of shape ``(batch_sz, seq_len, dim)``.\n\n        Returns\n        -------\n        torch.Tensor\n            Normalized tensor of shape ``(batch_sz, seq_len, dim)``.\n        \"\"\"\n        return torch.nn.functional.normalize(x, dim=self.dim, p=2)\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.normalization.L2Norm.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Apply L2 normalization to the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape <code>(batch_sz, seq_len, dim)</code>.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized tensor of shape <code>(batch_sz, seq_len, dim)</code>.</p> Source code in <code>mmlearn/modules/layers/normalization.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Apply L2 normalization to the input tensor.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape ``(batch_sz, seq_len, dim)``.\n\n    Returns\n    -------\n    torch.Tensor\n        Normalized tensor of shape ``(batch_sz, seq_len, dim)``.\n    \"\"\"\n    return torch.nn.functional.normalize(x, dim=self.dim, p=2)\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.patch_dropout","title":"patch_dropout","text":"<p>Patch dropout layer.</p>"},{"location":"api/#mmlearn.modules.layers.patch_dropout.PatchDropout","title":"PatchDropout","text":"<p>               Bases: <code>Module</code></p> <p>Patch dropout layer.</p> <p>Drops patch tokens (after embedding and adding CLS token) from the input tensor. Usually used in vision transformers to reduce the number of tokens. [1]_</p> <p>Parameters:</p> Name Type Description Default <code>keep_rate</code> <code>float</code> <p>The proportion of tokens to keep.</p> <code>0.5</code> <code>bias</code> <code>Optional[float]</code> <p>The bias to add to the random noise before sorting.</p> <code>None</code> <code>token_shuffling</code> <code>bool</code> <p>If True, the tokens are shuffled.</p> <code>False</code> References <p>.. [1] Liu, Y., Matsoukas, C., Strand, F., Azizpour, H., &amp; Smith, K. (2023).    Patchdropout: Economizing vision transformers using patch dropout. In Proceedings    of the IEEE/CVF Winter Conference on Applications of Computer Vision    (pp. 3953-3962).</p> Source code in <code>mmlearn/modules/layers/patch_dropout.py</code> <pre><code>class PatchDropout(torch.nn.Module):\n    \"\"\"Patch dropout layer.\n\n    Drops patch tokens (after embedding and adding CLS token) from the input tensor.\n    Usually used in vision transformers to reduce the number of tokens. [1]_\n\n    Parameters\n    ----------\n    keep_rate : float, optional, default=0.5\n        The proportion of tokens to keep.\n    bias : Optional[float], optional, default=None\n        The bias to add to the random noise before sorting.\n    token_shuffling : bool, optional, default=False\n        If True, the tokens are shuffled.\n\n    References\n    ----------\n    .. [1] Liu, Y., Matsoukas, C., Strand, F., Azizpour, H., &amp; Smith, K. (2023).\n       Patchdropout: Economizing vision transformers using patch dropout. In Proceedings\n       of the IEEE/CVF Winter Conference on Applications of Computer Vision\n       (pp. 3953-3962).\n    \"\"\"\n\n    def __init__(\n        self,\n        keep_rate: float = 0.5,\n        bias: Optional[float] = None,\n        token_shuffling: bool = False,\n    ):\n        super().__init__()\n        assert 0 &lt; keep_rate &lt;= 1, \"The keep_rate must be in (0,1]\"\n\n        self.bias = bias\n        self.keep_rate = keep_rate\n        self.token_shuffling = token_shuffling\n\n    def forward(self, x: torch.Tensor, force_drop: bool = False) -&gt; torch.Tensor:\n        \"\"\"Drop tokens from the input tensor.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor of shape ``(batch_sz, seq_len, dim)``.\n        force_drop : bool, optional, default=False\n            If True, the tokens are always dropped, even when the model is in\n            evaluation mode.\n\n        Returns\n        -------\n        torch.Tensor\n            Tensor of shape ``(batch_sz, keep_len, dim)`` containing the kept tokens.\n        \"\"\"\n        if (not self.training and not force_drop) or self.keep_rate == 1:\n            return x\n\n        batch_sz, _, dim = x.shape\n\n        cls_mask = torch.zeros(  # assumes that CLS is always the 1st element\n            batch_sz, 1, dtype=torch.int64, device=x.device\n        )\n        patch_mask = self.uniform_mask(x)\n        patch_mask = torch.hstack([cls_mask, patch_mask])\n\n        return torch.gather(x, dim=1, index=patch_mask.unsqueeze(-1).repeat(1, 1, dim))\n\n    def uniform_mask(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Generate token ids to keep from uniform random distribution.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor of shape ``(batch_sz, seq_len, dim)``.\n\n        Returns\n        -------\n        torch.Tensor\n            Tensor of shape ``(batch_sz, keep_len)`` containing the token ids to keep.\n\n        \"\"\"\n        batch_sz, seq_len, _ = x.shape\n        seq_len = seq_len - 1  # patch length (without CLS)\n\n        keep_len = int(seq_len * self.keep_rate)\n        noise = torch.rand(batch_sz, seq_len, device=x.device)\n        if self.bias is not None:\n            noise += self.bias\n        ids = torch.argsort(noise, dim=1)\n        keep_ids = ids[:, :keep_len]\n        if not self.token_shuffling:\n            keep_ids = keep_ids.sort(1)[0]\n        return keep_ids\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.patch_dropout.PatchDropout.forward","title":"forward","text":"<pre><code>forward(x, force_drop=False)\n</code></pre> <p>Drop tokens from the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape <code>(batch_sz, seq_len, dim)</code>.</p> required <code>force_drop</code> <code>bool</code> <p>If True, the tokens are always dropped, even when the model is in evaluation mode.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of shape <code>(batch_sz, keep_len, dim)</code> containing the kept tokens.</p> Source code in <code>mmlearn/modules/layers/patch_dropout.py</code> <pre><code>def forward(self, x: torch.Tensor, force_drop: bool = False) -&gt; torch.Tensor:\n    \"\"\"Drop tokens from the input tensor.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape ``(batch_sz, seq_len, dim)``.\n    force_drop : bool, optional, default=False\n        If True, the tokens are always dropped, even when the model is in\n        evaluation mode.\n\n    Returns\n    -------\n    torch.Tensor\n        Tensor of shape ``(batch_sz, keep_len, dim)`` containing the kept tokens.\n    \"\"\"\n    if (not self.training and not force_drop) or self.keep_rate == 1:\n        return x\n\n    batch_sz, _, dim = x.shape\n\n    cls_mask = torch.zeros(  # assumes that CLS is always the 1st element\n        batch_sz, 1, dtype=torch.int64, device=x.device\n    )\n    patch_mask = self.uniform_mask(x)\n    patch_mask = torch.hstack([cls_mask, patch_mask])\n\n    return torch.gather(x, dim=1, index=patch_mask.unsqueeze(-1).repeat(1, 1, dim))\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.patch_dropout.PatchDropout.uniform_mask","title":"uniform_mask","text":"<pre><code>uniform_mask(x)\n</code></pre> <p>Generate token ids to keep from uniform random distribution.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape <code>(batch_sz, seq_len, dim)</code>.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of shape <code>(batch_sz, keep_len)</code> containing the token ids to keep.</p> Source code in <code>mmlearn/modules/layers/patch_dropout.py</code> <pre><code>def uniform_mask(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Generate token ids to keep from uniform random distribution.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape ``(batch_sz, seq_len, dim)``.\n\n    Returns\n    -------\n    torch.Tensor\n        Tensor of shape ``(batch_sz, keep_len)`` containing the token ids to keep.\n\n    \"\"\"\n    batch_sz, seq_len, _ = x.shape\n    seq_len = seq_len - 1  # patch length (without CLS)\n\n    keep_len = int(seq_len * self.keep_rate)\n    noise = torch.rand(batch_sz, seq_len, device=x.device)\n    if self.bias is not None:\n        noise += self.bias\n    ids = torch.argsort(noise, dim=1)\n    keep_ids = ids[:, :keep_len]\n    if not self.token_shuffling:\n        keep_ids = keep_ids.sort(1)[0]\n    return keep_ids\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.transformer_block","title":"transformer_block","text":"<p>Transformer Block and Embedding Modules for Vision Transformers (ViT).</p>"},{"location":"api/#mmlearn.modules.layers.transformer_block.DropPath","title":"DropPath","text":"<p>               Bases: <code>Module</code></p> <p>Drop paths (Stochastic Depth) per sample.</p> <p>Parameters:</p> Name Type Description Default <code>drop_prob</code> <code>float</code> <p>Probability of dropping paths.</p> <code>0.0</code> Source code in <code>mmlearn/modules/layers/transformer_block.py</code> <pre><code>class DropPath(nn.Module):\n    \"\"\"Drop paths (Stochastic Depth) per sample.\n\n    Parameters\n    ----------\n    drop_prob : float, optional, default=0.0\n        Probability of dropping paths.\n    \"\"\"\n\n    def __init__(self, drop_prob: float = 0.0) -&gt; None:\n        super().__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass through DropPath module.\"\"\"\n        return drop_path(x, self.drop_prob, self.training)\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.transformer_block.DropPath.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Forward pass through DropPath module.</p> Source code in <code>mmlearn/modules/layers/transformer_block.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass through DropPath module.\"\"\"\n    return drop_path(x, self.drop_prob, self.training)\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.transformer_block.Block","title":"Block","text":"<p>               Bases: <code>Module</code></p> <p>Transformer Block.</p> <p>This module represents a Transformer block that includes self-attention, normalization layers, and a feedforward multi-layer perceptron (MLP) network.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>The input and output dimension of the block.</p> required <code>num_heads</code> <code>int</code> <p>Number of attention heads.</p> required <code>mlp_ratio</code> <code>float</code> <p>Ratio of hidden dimension to the input dimension in the MLP.</p> <code>4.0</code> <code>qkv_bias</code> <code>bool</code> <p>If True, add a learnable bias to the query, key, value projections.</p> <code>False</code> <code>qk_scale</code> <code>Optional[float]</code> <p>Override default qk scale of head_dim ** -0.5 if set.</p> <code>None</code> <code>drop</code> <code>float</code> <p>Dropout probability for the output of attention and MLP layers.</p> <code>0.0</code> <code>attn_drop</code> <code>float</code> <p>Dropout probability for the attention scores.</p> <code>0.0</code> <code>drop_path</code> <code>float</code> <p>Stochastic depth rate, a form of layer dropout.</p> <code>0.0</code> <code>act_layer</code> <code>Callable[..., Module]</code> <p>Activation layer in the MLP.</p> <code>nn.GELU</code> <code>norm_layer</code> <code>Callable[..., Module]</code> <p>Normalization layer.</p> <code>torch.nn.LayerNorm</code> Source code in <code>mmlearn/modules/layers/transformer_block.py</code> <pre><code>class Block(nn.Module):\n    \"\"\"Transformer Block.\n\n    This module represents a Transformer block that includes self-attention,\n    normalization layers, and a feedforward multi-layer perceptron (MLP) network.\n\n    Parameters\n    ----------\n    dim : int\n        The input and output dimension of the block.\n    num_heads : int\n        Number of attention heads.\n    mlp_ratio : float, optional, default=4.0\n        Ratio of hidden dimension to the input dimension in the MLP.\n    qkv_bias : bool, optional, default=False\n        If True, add a learnable bias to the query, key, value projections.\n    qk_scale : Optional[float], optional, default=None\n        Override default qk scale of head_dim ** -0.5 if set.\n    drop : float, optional, default=0.0\n        Dropout probability for the output of attention and MLP layers.\n    attn_drop : float, optional, default=0.0\n        Dropout probability for the attention scores.\n    drop_path : float, optional, default=0.0\n        Stochastic depth rate, a form of layer dropout.\n    act_layer : Callable[..., torch.nn.Module], optional, default=nn.GELU\n        Activation layer in the MLP.\n    norm_layer : Callable[..., torch.nn.Module], optional, default=torch.nn.LayerNorm\n        Normalization layer.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int,\n        mlp_ratio: float = 4.0,\n        qkv_bias: bool = False,\n        qk_scale: Optional[float] = None,\n        drop: float = 0.0,\n        attn_drop: float = 0.0,\n        drop_path: float = 0.0,\n        act_layer: Callable[..., nn.Module] = nn.GELU,\n        norm_layer: Callable[..., nn.Module] = nn.LayerNorm,\n    ) -&gt; None:\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(\n            dim,\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            qk_scale=qk_scale,\n            attn_drop=attn_drop,\n            proj_drop=drop,\n        )\n        self.drop_path = DropPath(drop_path) if drop_path &gt; 0.0 else nn.Identity()\n        self.norm2 = norm_layer(dim)\n\n        self.mlp = MLP(\n            in_dim=dim,\n            hidden_dims_multiplier=[mlp_ratio],\n            activation_layer=act_layer,\n            bias=True,\n            dropout=drop,\n        )\n\n    def forward(\n        self, x: torch.Tensor, return_attention: bool = False\n    ) -&gt; Union[torch.Tensor, torch.Tensor]:\n        \"\"\"Forward pass through the Transformer Block.\"\"\"\n        y, attn = self.attn(self.norm1(x))\n        if return_attention:\n            return attn\n        x = x + self.drop_path(y)\n        return x + self.drop_path(self.mlp(self.norm2(x)))\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.transformer_block.Block.forward","title":"forward","text":"<pre><code>forward(x, return_attention=False)\n</code></pre> <p>Forward pass through the Transformer Block.</p> Source code in <code>mmlearn/modules/layers/transformer_block.py</code> <pre><code>def forward(\n    self, x: torch.Tensor, return_attention: bool = False\n) -&gt; Union[torch.Tensor, torch.Tensor]:\n    \"\"\"Forward pass through the Transformer Block.\"\"\"\n    y, attn = self.attn(self.norm1(x))\n    if return_attention:\n        return attn\n    x = x + self.drop_path(y)\n    return x + self.drop_path(self.mlp(self.norm2(x)))\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.transformer_block.drop_path","title":"drop_path","text":"<pre><code>drop_path(x, drop_prob=0.0, training=False)\n</code></pre> <p>Drop paths (Stochastic Depth) for regularization during training.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <code>drop_prob</code> <code>float</code> <p>Probability of dropping paths.</p> <code>0.0</code> <code>training</code> <code>bool</code> <p>Whether the model is in training mode.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>output</code> <code>Tensor</code> <p>Output tensor after applying drop path.</p> Source code in <code>mmlearn/modules/layers/transformer_block.py</code> <pre><code>def drop_path(\n    x: torch.Tensor, drop_prob: float = 0.0, training: bool = False\n) -&gt; torch.Tensor:\n    \"\"\"Drop paths (Stochastic Depth) for regularization during training.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor.\n    drop_prob : float, optional, default=0.0\n        Probability of dropping paths.\n    training : bool, optional, default=False\n        Whether the model is in training mode.\n\n    Returns\n    -------\n    output : torch.Tensor\n        Output tensor after applying drop path.\n    \"\"\"\n    if drop_prob == 0.0 or not training:\n        return x\n    keep_prob = 1 - drop_prob\n    shape = (x.shape[0],) + (1,) * (\n        x.ndim - 1\n    )  # work with diff dim tensors, not just 2D ConvNets\n    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n    random_tensor.floor_()  # binarize\n    return x.div(keep_prob) * random_tensor\n</code></pre>"},{"location":"api/#mmlearn.modules.losses","title":"losses","text":"<p>Loss functions.</p>"},{"location":"api/#mmlearn.modules.losses.ContrastiveLoss","title":"ContrastiveLoss","text":"<p>               Bases: <code>Module</code></p> <p>Contrastive Loss.</p> <p>Parameters:</p> Name Type Description Default <code>l2_normalize</code> <code>bool</code> <p>Whether to L2 normalize the features.</p> <code>False</code> <code>local_loss</code> <code>bool</code> <p>Whether to calculate the loss locally i.e. <code>local_features@global_features</code>.</p> <code>False</code> <code>gather_with_grad</code> <code>bool</code> <p>Whether to gather tensors with gradients.</p> <code>False</code> <code>modality_alignment</code> <code>bool</code> <p>Whether to include modality alignment loss. This loss considers all features from the same modality as positive pairs and all features from different modalities as negative pairs.</p> <code>False</code> <code>cache_labels</code> <code>bool</code> <p>Whether to cache the labels.</p> <code>False</code> Source code in <code>mmlearn/modules/losses/contrastive.py</code> <pre><code>@store(group=\"modules/losses\", provider=\"mmlearn\")\nclass ContrastiveLoss(nn.Module):\n    \"\"\"Contrastive Loss.\n\n    Parameters\n    ----------\n    l2_normalize : bool, optional, default=False\n        Whether to L2 normalize the features.\n    local_loss : bool, optional, default=False\n        Whether to calculate the loss locally i.e. ``local_features@global_features``.\n    gather_with_grad : bool, optional, default=False\n        Whether to gather tensors with gradients.\n    modality_alignment : bool, optional, default=False\n        Whether to include modality alignment loss. This loss considers all features\n        from the same modality as positive pairs and all features from different\n        modalities as negative pairs.\n    cache_labels : bool, optional, default=False\n        Whether to cache the labels.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        l2_normalize: bool = False,\n        local_loss: bool = False,\n        gather_with_grad: bool = False,\n        modality_alignment: bool = False,\n        cache_labels: bool = False,\n    ):\n        super().__init__()\n        self.local_loss = local_loss\n        self.gather_with_grad = gather_with_grad\n        self.cache_labels = cache_labels\n        self.l2_normalize = l2_normalize\n        self.modality_alignment = modality_alignment\n\n        # cache state\n        self._prev_num_logits = 0\n        self._labels: dict[torch.device, torch.Tensor] = {}\n\n    def forward(\n        self,\n        embeddings: dict[str, torch.Tensor],\n        example_ids: dict[str, torch.Tensor],\n        logit_scale: torch.Tensor,\n        modality_loss_pairs: list[LossPairSpec],\n    ) -&gt; torch.Tensor:\n        \"\"\"Calculate the contrastive loss.\n\n        Parameters\n        ----------\n        embeddings : dict[str, torch.Tensor]\n            Dictionary of embeddings, where the key is the modality name and the value\n            is the corresponding embedding tensor.\n        example_ids : dict[str, torch.Tensor]\n            Dictionary of example IDs, where the key is the modality name and the value\n            is a tensor tuple of the dataset index and the example index.\n        logit_scale : torch.Tensor\n            Scale factor for the logits.\n        modality_loss_pairs : list[LossPairSpec]\n            Specification of the modality pairs for which the loss should be calculated.\n\n        Returns\n        -------\n        torch.Tensor\n            The contrastive loss.\n        \"\"\"\n        world_size = dist.get_world_size() if dist.is_initialized() else 1\n        rank = dist.get_rank() if world_size &gt; 1 else 0\n\n        if self.l2_normalize:\n            embeddings = {k: F.normalize(v, p=2, dim=-1) for k, v in embeddings.items()}\n\n        if world_size &gt; 1:  # gather embeddings and example_ids across all processes\n            # NOTE: gathering dictionaries of tensors across all processes\n            # (keys + values, as opposed to just values) is especially important\n            # for the modality_alignment loss, which requires all embeddings\n            all_embeddings = _gather_dicts(\n                embeddings,\n                local_loss=self.local_loss,\n                gather_with_grad=self.gather_with_grad,\n                rank=rank,\n            )\n            all_example_ids = _gather_dicts(\n                example_ids,\n                local_loss=self.local_loss,\n                gather_with_grad=self.gather_with_grad,\n                rank=rank,\n            )\n        else:\n            all_embeddings = embeddings\n            all_example_ids = example_ids\n\n        losses = []\n        for loss_pairs in modality_loss_pairs:\n            logits_per_feature_a, logits_per_feature_b, skip_flag = self._get_logits(\n                loss_pairs.modalities,\n                per_device_embeddings=embeddings,\n                all_embeddings=all_embeddings,\n                per_device_example_ids=example_ids,\n                all_example_ids=all_example_ids,\n                logit_scale=logit_scale,\n                world_size=world_size,\n            )\n            if logits_per_feature_a is None or logits_per_feature_b is None:\n                continue\n\n            labels = self._get_ground_truth(\n                logits_per_feature_a.shape,\n                device=logits_per_feature_a.device,\n                rank=rank,\n                world_size=world_size,\n                skipped_process=skip_flag,\n            )\n\n            if labels.numel() != 0:\n                losses.append(\n                    (\n                        (\n                            F.cross_entropy(logits_per_feature_a, labels)\n                            + F.cross_entropy(logits_per_feature_b, labels)\n                        )\n                        / 2\n                    )\n                    * loss_pairs.weight\n                )\n\n        if self.modality_alignment:\n            losses.append(\n                self._compute_modality_alignment_loss(all_embeddings, logit_scale)\n            )\n\n        if not losses:  # no loss to compute (e.g. no paired data in batch)\n            losses.append(\n                torch.tensor(\n                    0.0,\n                    device=logit_scale.device,\n                    dtype=next(iter(embeddings.values())).dtype,\n                )\n            )\n\n        return torch.stack(losses).sum()\n\n    def _get_ground_truth(\n        self,\n        logits_shape: tuple[int, int],\n        device: torch.device,\n        rank: int,\n        world_size: int,\n        skipped_process: bool,\n    ) -&gt; torch.Tensor:\n        \"\"\"Return the ground-truth labels.\n\n        Parameters\n        ----------\n        logits_shape : tuple[int, int]\n            Shape of the logits tensor.\n        device : torch.device\n            Device on which the labels should be created.\n        rank : int\n            Rank of the current process.\n        world_size : int\n            Number of processes.\n        skipped_process : bool\n            Whether the current process skipped the computation due to lack of data.\n\n        Returns\n        -------\n        torch.Tensor\n            Ground-truth labels.\n        \"\"\"\n        num_logits = logits_shape[-1]\n\n        # calculate ground-truth and cache if enabled\n        if self._prev_num_logits != num_logits or device not in self._labels:\n            labels = torch.arange(num_logits, device=device, dtype=torch.long)\n\n            if world_size &gt; 1 and self.local_loss:\n                local_size = torch.tensor(\n                    0 if skipped_process else logits_shape[0], device=device\n                )\n                # NOTE: all processes must participate in the all_gather operation\n                # even if they have no data to contribute.\n                sizes = torch.stack(\n                    _simple_gather_all_tensors(\n                        local_size, group=dist.group.WORLD, world_size=world_size\n                    )\n                )\n                sizes = torch.cat(\n                    [torch.tensor([0], device=sizes.device), torch.cumsum(sizes, dim=0)]\n                )\n                labels = labels[\n                    sizes[rank] : sizes[rank + 1] if rank + 1 &lt; world_size else None\n                ]\n\n            if self.cache_labels:\n                self._labels[device] = labels\n                self._prev_num_logits = num_logits\n        else:\n            labels = self._labels[device]\n        return labels\n\n    def _get_logits(  # noqa: PLR0912\n        self,\n        modalities: tuple[str, str],\n        per_device_embeddings: dict[str, torch.Tensor],\n        all_embeddings: dict[str, torch.Tensor],\n        per_device_example_ids: dict[str, torch.Tensor],\n        all_example_ids: dict[str, torch.Tensor],\n        logit_scale: torch.Tensor,\n        world_size: int,\n    ) -&gt; tuple[Optional[torch.Tensor], Optional[torch.Tensor], bool]:\n        \"\"\"Calculate the logits for the given modalities.\n\n        Parameters\n        ----------\n        modalities : tuple[str, str]\n            Tuple of modality names.\n        per_device_embeddings : dict[str, torch.Tensor]\n            Dictionary of embeddings, where the key is the modality name and the value\n            is the corresponding embedding tensor.\n        all_embeddings : dict[str, torch.Tensor]\n            Dictionary of embeddings, where the key is the modality name and the value\n            is the corresponding embedding tensor. In distributed mode, this contains\n            embeddings from all processes.\n        per_device_example_ids : dict[str, torch.Tensor]\n            Dictionary of example IDs, where the key is the modality name and the value\n            is a tensor tuple of the dataset index and the example index.\n        all_example_ids : dict[str, torch.Tensor]\n            Dictionary of example IDs, where the key is the modality name and the value\n            is a tensor tuple of the dataset index and the example index. In distributed\n            mode, this contains example IDs from all processes.\n        logit_scale : torch.Tensor\n            Scale factor for the logits.\n        world_size : int\n            Number of processes.\n\n        Returns\n        -------\n        tuple[Optional[torch.Tensor], Optional[torch.Tensor], bool]\n            Tuple of logits for the given modalities. If embeddings for the given\n            modalities are not available, returns `None` for the logits. The last\n            element is a flag indicating whether the process skipped the computation\n            due to lack of data.\n        \"\"\"\n        modality_a = Modalities.get_modality(modalities[0])\n        modality_b = Modalities.get_modality(modalities[1])\n        skip_flag = False\n\n        if self.local_loss or world_size == 1:\n            if not (\n                modality_a.embedding in per_device_embeddings\n                and modality_b.embedding in per_device_embeddings\n            ):\n                if world_size &gt; 1:  # NOTE: not all processes exit here, hence skip_flag\n                    skip_flag = True\n                else:\n                    return None, None, skip_flag\n\n            if not skip_flag:\n                indices_a, indices_b = find_matching_indices(\n                    per_device_example_ids[modality_a.name],\n                    per_device_example_ids[modality_b.name],\n                )\n                if indices_a.numel() == 0 or indices_b.numel() == 0:\n                    if world_size &gt; 1:  # not all processes exit here\n                        skip_flag = True\n                    else:\n                        return None, None, skip_flag\n\n            if not skip_flag:\n                features_a = per_device_embeddings[modality_a.embedding][indices_a]\n                features_b = per_device_embeddings[modality_b.embedding][indices_b]\n            else:\n                # all processes must participate in the all_gather operation\n                # that follows, even if they have no data to contribute. So,\n                # we create empty tensors here.\n                features_a = torch.empty(\n                    0, device=list(per_device_embeddings.values())[0].device\n                )\n                features_b = torch.empty(\n                    0, device=list(per_device_embeddings.values())[0].device\n                )\n\n        if world_size &gt; 1:\n            if not (\n                modality_a.embedding in all_embeddings\n                and modality_b.embedding in all_embeddings\n            ):  # all processes exit here\n                return None, None, skip_flag\n\n            indices_a, indices_b = find_matching_indices(\n                all_example_ids[modality_a.name],\n                all_example_ids[modality_b.name],\n            )\n            if indices_a.numel() == 0 or indices_b.numel() == 0:\n                # all processes exit here\n                return None, None, skip_flag\n\n            all_features_a = all_embeddings[modality_a.embedding][indices_a]\n            all_features_b = all_embeddings[modality_b.embedding][indices_b]\n\n            if self.local_loss:\n                if features_a.numel() == 0:\n                    features_a = all_features_a\n                if features_b.numel() == 0:\n                    features_b = all_features_b\n\n                logits_per_feature_a = logit_scale * _safe_matmul(\n                    features_a, all_features_b\n                )\n                logits_per_feature_b = logit_scale * _safe_matmul(\n                    features_b, all_features_a\n                )\n            else:\n                logits_per_feature_a = logit_scale * _safe_matmul(\n                    all_features_a, all_features_b\n                )\n                logits_per_feature_b = logits_per_feature_a.T\n        else:\n            logits_per_feature_a = logit_scale * _safe_matmul(features_a, features_b)\n            logits_per_feature_b = logit_scale * _safe_matmul(features_b, features_a)\n\n        return logits_per_feature_a, logits_per_feature_b, skip_flag\n\n    def _compute_modality_alignment_loss(\n        self, all_embeddings: dict[str, torch.Tensor], logit_scale: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"Compute the modality alignment loss.\n\n        This loss considers all features from the same modality as positive pairs\n        and all features from different modalities as negative pairs.\n\n        Parameters\n        ----------\n        all_embeddings : dict[str, torch.Tensor]\n            Dictionary of embeddings, where the key is the modality name and the value\n            is the corresponding embedding tensor.\n        logit_scale : torch.Tensor\n            Scale factor for the logits.\n\n        Returns\n        -------\n        torch.Tensor\n            Modality alignment loss.\n\n        Notes\n        -----\n        This loss does not support `local_loss=True`.\n        \"\"\"\n        available_modalities = list(all_embeddings.keys())\n        # TODO: support local_loss for modality_alignment?\n        # if world_size == 1, all_embeddings == embeddings\n        all_features = torch.cat(list(all_embeddings.values()), dim=0)\n\n        positive_indices = torch.tensor(\n            [\n                (i, j)\n                if idx == 0\n                else (\n                    i + all_embeddings[available_modalities[idx - 1]].size(0),\n                    j + all_embeddings[available_modalities[idx - 1]].size(0),\n                )\n                for idx, k in enumerate(all_embeddings)\n                for i, j in itertools.combinations(range(all_embeddings[k].size(0)), 2)\n            ],\n            device=all_features.device,\n        )\n        logits = logit_scale * _safe_matmul(all_features, all_features)\n\n        target = torch.eye(all_features.size(0), device=all_features.device)\n        target[positive_indices[:, 0], positive_indices[:, 1]] = 1\n\n        modality_loss = torch.nn.functional.binary_cross_entropy_with_logits(\n            logits, target, reduction=\"none\"\n        )\n\n        target_pos = target.bool()\n        target_neg = ~target_pos\n\n        # loss_pos and loss_neg below contain non-zero values only for those\n        # elements that are positive pairs and negative pairs respectively.\n        loss_pos = torch.zeros(\n            logits.size(0), logits.size(0), device=target.device\n        ).masked_scatter(target_pos, modality_loss[target_pos])\n        loss_neg = torch.zeros(\n            logits.size(0), logits.size(0), device=target.device\n        ).masked_scatter(target_neg, modality_loss[target_neg])\n\n        loss_pos = loss_pos.sum(dim=1)\n        loss_neg = loss_neg.sum(dim=1)\n        num_pos = target.sum(dim=1)\n        num_neg = logits.size(0) - num_pos\n\n        return ((loss_pos / num_pos) + (loss_neg / num_neg)).mean()\n</code></pre>"},{"location":"api/#mmlearn.modules.losses.ContrastiveLoss.forward","title":"forward","text":"<pre><code>forward(\n    embeddings,\n    example_ids,\n    logit_scale,\n    modality_loss_pairs,\n)\n</code></pre> <p>Calculate the contrastive loss.</p> <p>Parameters:</p> Name Type Description Default <code>embeddings</code> <code>dict[str, Tensor]</code> <p>Dictionary of embeddings, where the key is the modality name and the value is the corresponding embedding tensor.</p> required <code>example_ids</code> <code>dict[str, Tensor]</code> <p>Dictionary of example IDs, where the key is the modality name and the value is a tensor tuple of the dataset index and the example index.</p> required <code>logit_scale</code> <code>Tensor</code> <p>Scale factor for the logits.</p> required <code>modality_loss_pairs</code> <code>list[LossPairSpec]</code> <p>Specification of the modality pairs for which the loss should be calculated.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The contrastive loss.</p> Source code in <code>mmlearn/modules/losses/contrastive.py</code> <pre><code>def forward(\n    self,\n    embeddings: dict[str, torch.Tensor],\n    example_ids: dict[str, torch.Tensor],\n    logit_scale: torch.Tensor,\n    modality_loss_pairs: list[LossPairSpec],\n) -&gt; torch.Tensor:\n    \"\"\"Calculate the contrastive loss.\n\n    Parameters\n    ----------\n    embeddings : dict[str, torch.Tensor]\n        Dictionary of embeddings, where the key is the modality name and the value\n        is the corresponding embedding tensor.\n    example_ids : dict[str, torch.Tensor]\n        Dictionary of example IDs, where the key is the modality name and the value\n        is a tensor tuple of the dataset index and the example index.\n    logit_scale : torch.Tensor\n        Scale factor for the logits.\n    modality_loss_pairs : list[LossPairSpec]\n        Specification of the modality pairs for which the loss should be calculated.\n\n    Returns\n    -------\n    torch.Tensor\n        The contrastive loss.\n    \"\"\"\n    world_size = dist.get_world_size() if dist.is_initialized() else 1\n    rank = dist.get_rank() if world_size &gt; 1 else 0\n\n    if self.l2_normalize:\n        embeddings = {k: F.normalize(v, p=2, dim=-1) for k, v in embeddings.items()}\n\n    if world_size &gt; 1:  # gather embeddings and example_ids across all processes\n        # NOTE: gathering dictionaries of tensors across all processes\n        # (keys + values, as opposed to just values) is especially important\n        # for the modality_alignment loss, which requires all embeddings\n        all_embeddings = _gather_dicts(\n            embeddings,\n            local_loss=self.local_loss,\n            gather_with_grad=self.gather_with_grad,\n            rank=rank,\n        )\n        all_example_ids = _gather_dicts(\n            example_ids,\n            local_loss=self.local_loss,\n            gather_with_grad=self.gather_with_grad,\n            rank=rank,\n        )\n    else:\n        all_embeddings = embeddings\n        all_example_ids = example_ids\n\n    losses = []\n    for loss_pairs in modality_loss_pairs:\n        logits_per_feature_a, logits_per_feature_b, skip_flag = self._get_logits(\n            loss_pairs.modalities,\n            per_device_embeddings=embeddings,\n            all_embeddings=all_embeddings,\n            per_device_example_ids=example_ids,\n            all_example_ids=all_example_ids,\n            logit_scale=logit_scale,\n            world_size=world_size,\n        )\n        if logits_per_feature_a is None or logits_per_feature_b is None:\n            continue\n\n        labels = self._get_ground_truth(\n            logits_per_feature_a.shape,\n            device=logits_per_feature_a.device,\n            rank=rank,\n            world_size=world_size,\n            skipped_process=skip_flag,\n        )\n\n        if labels.numel() != 0:\n            losses.append(\n                (\n                    (\n                        F.cross_entropy(logits_per_feature_a, labels)\n                        + F.cross_entropy(logits_per_feature_b, labels)\n                    )\n                    / 2\n                )\n                * loss_pairs.weight\n            )\n\n    if self.modality_alignment:\n        losses.append(\n            self._compute_modality_alignment_loss(all_embeddings, logit_scale)\n        )\n\n    if not losses:  # no loss to compute (e.g. no paired data in batch)\n        losses.append(\n            torch.tensor(\n                0.0,\n                device=logit_scale.device,\n                dtype=next(iter(embeddings.values())).dtype,\n            )\n        )\n\n    return torch.stack(losses).sum()\n</code></pre>"},{"location":"api/#mmlearn.modules.losses.Data2VecLoss","title":"Data2VecLoss","text":"<p>               Bases: <code>Module</code></p> <p>Data2Vec loss function.</p> <p>Parameters:</p> Name Type Description Default <code>beta</code> <code>float</code> <p>Specifies the beta parameter for smooth L1 loss. If <code>0</code>, MSE loss is used.</p> <code>0</code> <code>loss_scale</code> <code>Optional[float]</code> <p>Scaling factor for the loss. If None, uses <code>1 / sqrt(embedding_dim)</code>.</p> <code>None</code> <code>reduction</code> <code>str</code> <p>Specifies the reduction to apply to the output: <code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>.</p> <code>'none'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the reduction mode is not supported.</p> Source code in <code>mmlearn/modules/losses/data2vec.py</code> <pre><code>@store(group=\"modules/losses\", provider=\"mmlearn\")\nclass Data2VecLoss(nn.Module):\n    \"\"\"Data2Vec loss function.\n\n    Parameters\n    ----------\n    beta : float, optional, default=0\n        Specifies the beta parameter for smooth L1 loss. If ``0``, MSE loss is used.\n    loss_scale : Optional[float], optional, default=None\n        Scaling factor for the loss. If None, uses ``1 / sqrt(embedding_dim)``.\n    reduction : str, optional, default='none'\n        Specifies the reduction to apply to the output:\n        ``'none'`` | ``'mean'`` | ``'sum'``.\n\n    Raises\n    ------\n    ValueError\n        If the reduction mode is not supported.\n    \"\"\"\n\n    def __init__(\n        self,\n        beta: float = 0,\n        loss_scale: Optional[float] = None,\n        reduction: str = \"none\",\n    ) -&gt; None:\n        super().__init__()\n        self.beta = beta\n        self.loss_scale = loss_scale\n        if reduction not in [\"none\", \"mean\", \"sum\"]:\n            raise ValueError(f\"Unsupported reduction mode: {reduction}\")\n        self.reduction = reduction\n\n    def forward(self, x: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute the Data2Vec loss.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Predicted embeddings of shape ``(batch_size, num_patches, embedding_dim)``.\n        y : torch.Tensor\n            Target embeddings of shape ``(batch_size, num_patches, embedding_dim)``.\n\n        Returns\n        -------\n        torch.Tensor\n            Data2Vec loss value.\n\n        Raises\n        ------\n        ValueError\n            If the shapes of x and y do not match.\n        \"\"\"\n        if x.shape != y.shape:\n            raise ValueError(f\"Shape mismatch: x: {x.shape}, y: {y.shape}\")\n\n        x = x.view(-1, x.size(-1)).float()\n        y = y.view(-1, y.size(-1))\n\n        if self.beta == 0:\n            loss = mse_loss(x, y, reduction=\"none\")\n        else:\n            loss = smooth_l1_loss(x, y, reduction=\"none\", beta=self.beta)\n\n        if self.loss_scale is not None:\n            scale = self.loss_scale\n        else:\n            scale = 1 / math.sqrt(x.size(-1))\n\n        loss = loss * scale\n\n        if self.reduction == \"mean\":\n            return loss.mean()\n        if self.reduction == \"sum\":\n            return loss.sum()\n        # 'none'\n        return loss.view(x.size(0), -1).sum(1)\n</code></pre>"},{"location":"api/#mmlearn.modules.losses.Data2VecLoss.forward","title":"forward","text":"<pre><code>forward(x, y)\n</code></pre> <p>Compute the Data2Vec loss.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Predicted embeddings of shape <code>(batch_size, num_patches, embedding_dim)</code>.</p> required <code>y</code> <code>Tensor</code> <p>Target embeddings of shape <code>(batch_size, num_patches, embedding_dim)</code>.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Data2Vec loss value.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the shapes of x and y do not match.</p> Source code in <code>mmlearn/modules/losses/data2vec.py</code> <pre><code>def forward(self, x: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute the Data2Vec loss.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Predicted embeddings of shape ``(batch_size, num_patches, embedding_dim)``.\n    y : torch.Tensor\n        Target embeddings of shape ``(batch_size, num_patches, embedding_dim)``.\n\n    Returns\n    -------\n    torch.Tensor\n        Data2Vec loss value.\n\n    Raises\n    ------\n    ValueError\n        If the shapes of x and y do not match.\n    \"\"\"\n    if x.shape != y.shape:\n        raise ValueError(f\"Shape mismatch: x: {x.shape}, y: {y.shape}\")\n\n    x = x.view(-1, x.size(-1)).float()\n    y = y.view(-1, y.size(-1))\n\n    if self.beta == 0:\n        loss = mse_loss(x, y, reduction=\"none\")\n    else:\n        loss = smooth_l1_loss(x, y, reduction=\"none\", beta=self.beta)\n\n    if self.loss_scale is not None:\n        scale = self.loss_scale\n    else:\n        scale = 1 / math.sqrt(x.size(-1))\n\n    loss = loss * scale\n\n    if self.reduction == \"mean\":\n        return loss.mean()\n    if self.reduction == \"sum\":\n        return loss.sum()\n    # 'none'\n    return loss.view(x.size(0), -1).sum(1)\n</code></pre>"},{"location":"api/#mmlearn.modules.losses.contrastive","title":"contrastive","text":"<p>Implementations of the contrastive loss and its variants.</p>"},{"location":"api/#mmlearn.modules.losses.contrastive.ContrastiveLoss","title":"ContrastiveLoss","text":"<p>               Bases: <code>Module</code></p> <p>Contrastive Loss.</p> <p>Parameters:</p> Name Type Description Default <code>l2_normalize</code> <code>bool</code> <p>Whether to L2 normalize the features.</p> <code>False</code> <code>local_loss</code> <code>bool</code> <p>Whether to calculate the loss locally i.e. <code>local_features@global_features</code>.</p> <code>False</code> <code>gather_with_grad</code> <code>bool</code> <p>Whether to gather tensors with gradients.</p> <code>False</code> <code>modality_alignment</code> <code>bool</code> <p>Whether to include modality alignment loss. This loss considers all features from the same modality as positive pairs and all features from different modalities as negative pairs.</p> <code>False</code> <code>cache_labels</code> <code>bool</code> <p>Whether to cache the labels.</p> <code>False</code> Source code in <code>mmlearn/modules/losses/contrastive.py</code> <pre><code>@store(group=\"modules/losses\", provider=\"mmlearn\")\nclass ContrastiveLoss(nn.Module):\n    \"\"\"Contrastive Loss.\n\n    Parameters\n    ----------\n    l2_normalize : bool, optional, default=False\n        Whether to L2 normalize the features.\n    local_loss : bool, optional, default=False\n        Whether to calculate the loss locally i.e. ``local_features@global_features``.\n    gather_with_grad : bool, optional, default=False\n        Whether to gather tensors with gradients.\n    modality_alignment : bool, optional, default=False\n        Whether to include modality alignment loss. This loss considers all features\n        from the same modality as positive pairs and all features from different\n        modalities as negative pairs.\n    cache_labels : bool, optional, default=False\n        Whether to cache the labels.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        l2_normalize: bool = False,\n        local_loss: bool = False,\n        gather_with_grad: bool = False,\n        modality_alignment: bool = False,\n        cache_labels: bool = False,\n    ):\n        super().__init__()\n        self.local_loss = local_loss\n        self.gather_with_grad = gather_with_grad\n        self.cache_labels = cache_labels\n        self.l2_normalize = l2_normalize\n        self.modality_alignment = modality_alignment\n\n        # cache state\n        self._prev_num_logits = 0\n        self._labels: dict[torch.device, torch.Tensor] = {}\n\n    def forward(\n        self,\n        embeddings: dict[str, torch.Tensor],\n        example_ids: dict[str, torch.Tensor],\n        logit_scale: torch.Tensor,\n        modality_loss_pairs: list[LossPairSpec],\n    ) -&gt; torch.Tensor:\n        \"\"\"Calculate the contrastive loss.\n\n        Parameters\n        ----------\n        embeddings : dict[str, torch.Tensor]\n            Dictionary of embeddings, where the key is the modality name and the value\n            is the corresponding embedding tensor.\n        example_ids : dict[str, torch.Tensor]\n            Dictionary of example IDs, where the key is the modality name and the value\n            is a tensor tuple of the dataset index and the example index.\n        logit_scale : torch.Tensor\n            Scale factor for the logits.\n        modality_loss_pairs : list[LossPairSpec]\n            Specification of the modality pairs for which the loss should be calculated.\n\n        Returns\n        -------\n        torch.Tensor\n            The contrastive loss.\n        \"\"\"\n        world_size = dist.get_world_size() if dist.is_initialized() else 1\n        rank = dist.get_rank() if world_size &gt; 1 else 0\n\n        if self.l2_normalize:\n            embeddings = {k: F.normalize(v, p=2, dim=-1) for k, v in embeddings.items()}\n\n        if world_size &gt; 1:  # gather embeddings and example_ids across all processes\n            # NOTE: gathering dictionaries of tensors across all processes\n            # (keys + values, as opposed to just values) is especially important\n            # for the modality_alignment loss, which requires all embeddings\n            all_embeddings = _gather_dicts(\n                embeddings,\n                local_loss=self.local_loss,\n                gather_with_grad=self.gather_with_grad,\n                rank=rank,\n            )\n            all_example_ids = _gather_dicts(\n                example_ids,\n                local_loss=self.local_loss,\n                gather_with_grad=self.gather_with_grad,\n                rank=rank,\n            )\n        else:\n            all_embeddings = embeddings\n            all_example_ids = example_ids\n\n        losses = []\n        for loss_pairs in modality_loss_pairs:\n            logits_per_feature_a, logits_per_feature_b, skip_flag = self._get_logits(\n                loss_pairs.modalities,\n                per_device_embeddings=embeddings,\n                all_embeddings=all_embeddings,\n                per_device_example_ids=example_ids,\n                all_example_ids=all_example_ids,\n                logit_scale=logit_scale,\n                world_size=world_size,\n            )\n            if logits_per_feature_a is None or logits_per_feature_b is None:\n                continue\n\n            labels = self._get_ground_truth(\n                logits_per_feature_a.shape,\n                device=logits_per_feature_a.device,\n                rank=rank,\n                world_size=world_size,\n                skipped_process=skip_flag,\n            )\n\n            if labels.numel() != 0:\n                losses.append(\n                    (\n                        (\n                            F.cross_entropy(logits_per_feature_a, labels)\n                            + F.cross_entropy(logits_per_feature_b, labels)\n                        )\n                        / 2\n                    )\n                    * loss_pairs.weight\n                )\n\n        if self.modality_alignment:\n            losses.append(\n                self._compute_modality_alignment_loss(all_embeddings, logit_scale)\n            )\n\n        if not losses:  # no loss to compute (e.g. no paired data in batch)\n            losses.append(\n                torch.tensor(\n                    0.0,\n                    device=logit_scale.device,\n                    dtype=next(iter(embeddings.values())).dtype,\n                )\n            )\n\n        return torch.stack(losses).sum()\n\n    def _get_ground_truth(\n        self,\n        logits_shape: tuple[int, int],\n        device: torch.device,\n        rank: int,\n        world_size: int,\n        skipped_process: bool,\n    ) -&gt; torch.Tensor:\n        \"\"\"Return the ground-truth labels.\n\n        Parameters\n        ----------\n        logits_shape : tuple[int, int]\n            Shape of the logits tensor.\n        device : torch.device\n            Device on which the labels should be created.\n        rank : int\n            Rank of the current process.\n        world_size : int\n            Number of processes.\n        skipped_process : bool\n            Whether the current process skipped the computation due to lack of data.\n\n        Returns\n        -------\n        torch.Tensor\n            Ground-truth labels.\n        \"\"\"\n        num_logits = logits_shape[-1]\n\n        # calculate ground-truth and cache if enabled\n        if self._prev_num_logits != num_logits or device not in self._labels:\n            labels = torch.arange(num_logits, device=device, dtype=torch.long)\n\n            if world_size &gt; 1 and self.local_loss:\n                local_size = torch.tensor(\n                    0 if skipped_process else logits_shape[0], device=device\n                )\n                # NOTE: all processes must participate in the all_gather operation\n                # even if they have no data to contribute.\n                sizes = torch.stack(\n                    _simple_gather_all_tensors(\n                        local_size, group=dist.group.WORLD, world_size=world_size\n                    )\n                )\n                sizes = torch.cat(\n                    [torch.tensor([0], device=sizes.device), torch.cumsum(sizes, dim=0)]\n                )\n                labels = labels[\n                    sizes[rank] : sizes[rank + 1] if rank + 1 &lt; world_size else None\n                ]\n\n            if self.cache_labels:\n                self._labels[device] = labels\n                self._prev_num_logits = num_logits\n        else:\n            labels = self._labels[device]\n        return labels\n\n    def _get_logits(  # noqa: PLR0912\n        self,\n        modalities: tuple[str, str],\n        per_device_embeddings: dict[str, torch.Tensor],\n        all_embeddings: dict[str, torch.Tensor],\n        per_device_example_ids: dict[str, torch.Tensor],\n        all_example_ids: dict[str, torch.Tensor],\n        logit_scale: torch.Tensor,\n        world_size: int,\n    ) -&gt; tuple[Optional[torch.Tensor], Optional[torch.Tensor], bool]:\n        \"\"\"Calculate the logits for the given modalities.\n\n        Parameters\n        ----------\n        modalities : tuple[str, str]\n            Tuple of modality names.\n        per_device_embeddings : dict[str, torch.Tensor]\n            Dictionary of embeddings, where the key is the modality name and the value\n            is the corresponding embedding tensor.\n        all_embeddings : dict[str, torch.Tensor]\n            Dictionary of embeddings, where the key is the modality name and the value\n            is the corresponding embedding tensor. In distributed mode, this contains\n            embeddings from all processes.\n        per_device_example_ids : dict[str, torch.Tensor]\n            Dictionary of example IDs, where the key is the modality name and the value\n            is a tensor tuple of the dataset index and the example index.\n        all_example_ids : dict[str, torch.Tensor]\n            Dictionary of example IDs, where the key is the modality name and the value\n            is a tensor tuple of the dataset index and the example index. In distributed\n            mode, this contains example IDs from all processes.\n        logit_scale : torch.Tensor\n            Scale factor for the logits.\n        world_size : int\n            Number of processes.\n\n        Returns\n        -------\n        tuple[Optional[torch.Tensor], Optional[torch.Tensor], bool]\n            Tuple of logits for the given modalities. If embeddings for the given\n            modalities are not available, returns `None` for the logits. The last\n            element is a flag indicating whether the process skipped the computation\n            due to lack of data.\n        \"\"\"\n        modality_a = Modalities.get_modality(modalities[0])\n        modality_b = Modalities.get_modality(modalities[1])\n        skip_flag = False\n\n        if self.local_loss or world_size == 1:\n            if not (\n                modality_a.embedding in per_device_embeddings\n                and modality_b.embedding in per_device_embeddings\n            ):\n                if world_size &gt; 1:  # NOTE: not all processes exit here, hence skip_flag\n                    skip_flag = True\n                else:\n                    return None, None, skip_flag\n\n            if not skip_flag:\n                indices_a, indices_b = find_matching_indices(\n                    per_device_example_ids[modality_a.name],\n                    per_device_example_ids[modality_b.name],\n                )\n                if indices_a.numel() == 0 or indices_b.numel() == 0:\n                    if world_size &gt; 1:  # not all processes exit here\n                        skip_flag = True\n                    else:\n                        return None, None, skip_flag\n\n            if not skip_flag:\n                features_a = per_device_embeddings[modality_a.embedding][indices_a]\n                features_b = per_device_embeddings[modality_b.embedding][indices_b]\n            else:\n                # all processes must participate in the all_gather operation\n                # that follows, even if they have no data to contribute. So,\n                # we create empty tensors here.\n                features_a = torch.empty(\n                    0, device=list(per_device_embeddings.values())[0].device\n                )\n                features_b = torch.empty(\n                    0, device=list(per_device_embeddings.values())[0].device\n                )\n\n        if world_size &gt; 1:\n            if not (\n                modality_a.embedding in all_embeddings\n                and modality_b.embedding in all_embeddings\n            ):  # all processes exit here\n                return None, None, skip_flag\n\n            indices_a, indices_b = find_matching_indices(\n                all_example_ids[modality_a.name],\n                all_example_ids[modality_b.name],\n            )\n            if indices_a.numel() == 0 or indices_b.numel() == 0:\n                # all processes exit here\n                return None, None, skip_flag\n\n            all_features_a = all_embeddings[modality_a.embedding][indices_a]\n            all_features_b = all_embeddings[modality_b.embedding][indices_b]\n\n            if self.local_loss:\n                if features_a.numel() == 0:\n                    features_a = all_features_a\n                if features_b.numel() == 0:\n                    features_b = all_features_b\n\n                logits_per_feature_a = logit_scale * _safe_matmul(\n                    features_a, all_features_b\n                )\n                logits_per_feature_b = logit_scale * _safe_matmul(\n                    features_b, all_features_a\n                )\n            else:\n                logits_per_feature_a = logit_scale * _safe_matmul(\n                    all_features_a, all_features_b\n                )\n                logits_per_feature_b = logits_per_feature_a.T\n        else:\n            logits_per_feature_a = logit_scale * _safe_matmul(features_a, features_b)\n            logits_per_feature_b = logit_scale * _safe_matmul(features_b, features_a)\n\n        return logits_per_feature_a, logits_per_feature_b, skip_flag\n\n    def _compute_modality_alignment_loss(\n        self, all_embeddings: dict[str, torch.Tensor], logit_scale: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"Compute the modality alignment loss.\n\n        This loss considers all features from the same modality as positive pairs\n        and all features from different modalities as negative pairs.\n\n        Parameters\n        ----------\n        all_embeddings : dict[str, torch.Tensor]\n            Dictionary of embeddings, where the key is the modality name and the value\n            is the corresponding embedding tensor.\n        logit_scale : torch.Tensor\n            Scale factor for the logits.\n\n        Returns\n        -------\n        torch.Tensor\n            Modality alignment loss.\n\n        Notes\n        -----\n        This loss does not support `local_loss=True`.\n        \"\"\"\n        available_modalities = list(all_embeddings.keys())\n        # TODO: support local_loss for modality_alignment?\n        # if world_size == 1, all_embeddings == embeddings\n        all_features = torch.cat(list(all_embeddings.values()), dim=0)\n\n        positive_indices = torch.tensor(\n            [\n                (i, j)\n                if idx == 0\n                else (\n                    i + all_embeddings[available_modalities[idx - 1]].size(0),\n                    j + all_embeddings[available_modalities[idx - 1]].size(0),\n                )\n                for idx, k in enumerate(all_embeddings)\n                for i, j in itertools.combinations(range(all_embeddings[k].size(0)), 2)\n            ],\n            device=all_features.device,\n        )\n        logits = logit_scale * _safe_matmul(all_features, all_features)\n\n        target = torch.eye(all_features.size(0), device=all_features.device)\n        target[positive_indices[:, 0], positive_indices[:, 1]] = 1\n\n        modality_loss = torch.nn.functional.binary_cross_entropy_with_logits(\n            logits, target, reduction=\"none\"\n        )\n\n        target_pos = target.bool()\n        target_neg = ~target_pos\n\n        # loss_pos and loss_neg below contain non-zero values only for those\n        # elements that are positive pairs and negative pairs respectively.\n        loss_pos = torch.zeros(\n            logits.size(0), logits.size(0), device=target.device\n        ).masked_scatter(target_pos, modality_loss[target_pos])\n        loss_neg = torch.zeros(\n            logits.size(0), logits.size(0), device=target.device\n        ).masked_scatter(target_neg, modality_loss[target_neg])\n\n        loss_pos = loss_pos.sum(dim=1)\n        loss_neg = loss_neg.sum(dim=1)\n        num_pos = target.sum(dim=1)\n        num_neg = logits.size(0) - num_pos\n\n        return ((loss_pos / num_pos) + (loss_neg / num_neg)).mean()\n</code></pre>"},{"location":"api/#mmlearn.modules.losses.contrastive.ContrastiveLoss.forward","title":"forward","text":"<pre><code>forward(\n    embeddings,\n    example_ids,\n    logit_scale,\n    modality_loss_pairs,\n)\n</code></pre> <p>Calculate the contrastive loss.</p> <p>Parameters:</p> Name Type Description Default <code>embeddings</code> <code>dict[str, Tensor]</code> <p>Dictionary of embeddings, where the key is the modality name and the value is the corresponding embedding tensor.</p> required <code>example_ids</code> <code>dict[str, Tensor]</code> <p>Dictionary of example IDs, where the key is the modality name and the value is a tensor tuple of the dataset index and the example index.</p> required <code>logit_scale</code> <code>Tensor</code> <p>Scale factor for the logits.</p> required <code>modality_loss_pairs</code> <code>list[LossPairSpec]</code> <p>Specification of the modality pairs for which the loss should be calculated.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The contrastive loss.</p> Source code in <code>mmlearn/modules/losses/contrastive.py</code> <pre><code>def forward(\n    self,\n    embeddings: dict[str, torch.Tensor],\n    example_ids: dict[str, torch.Tensor],\n    logit_scale: torch.Tensor,\n    modality_loss_pairs: list[LossPairSpec],\n) -&gt; torch.Tensor:\n    \"\"\"Calculate the contrastive loss.\n\n    Parameters\n    ----------\n    embeddings : dict[str, torch.Tensor]\n        Dictionary of embeddings, where the key is the modality name and the value\n        is the corresponding embedding tensor.\n    example_ids : dict[str, torch.Tensor]\n        Dictionary of example IDs, where the key is the modality name and the value\n        is a tensor tuple of the dataset index and the example index.\n    logit_scale : torch.Tensor\n        Scale factor for the logits.\n    modality_loss_pairs : list[LossPairSpec]\n        Specification of the modality pairs for which the loss should be calculated.\n\n    Returns\n    -------\n    torch.Tensor\n        The contrastive loss.\n    \"\"\"\n    world_size = dist.get_world_size() if dist.is_initialized() else 1\n    rank = dist.get_rank() if world_size &gt; 1 else 0\n\n    if self.l2_normalize:\n        embeddings = {k: F.normalize(v, p=2, dim=-1) for k, v in embeddings.items()}\n\n    if world_size &gt; 1:  # gather embeddings and example_ids across all processes\n        # NOTE: gathering dictionaries of tensors across all processes\n        # (keys + values, as opposed to just values) is especially important\n        # for the modality_alignment loss, which requires all embeddings\n        all_embeddings = _gather_dicts(\n            embeddings,\n            local_loss=self.local_loss,\n            gather_with_grad=self.gather_with_grad,\n            rank=rank,\n        )\n        all_example_ids = _gather_dicts(\n            example_ids,\n            local_loss=self.local_loss,\n            gather_with_grad=self.gather_with_grad,\n            rank=rank,\n        )\n    else:\n        all_embeddings = embeddings\n        all_example_ids = example_ids\n\n    losses = []\n    for loss_pairs in modality_loss_pairs:\n        logits_per_feature_a, logits_per_feature_b, skip_flag = self._get_logits(\n            loss_pairs.modalities,\n            per_device_embeddings=embeddings,\n            all_embeddings=all_embeddings,\n            per_device_example_ids=example_ids,\n            all_example_ids=all_example_ids,\n            logit_scale=logit_scale,\n            world_size=world_size,\n        )\n        if logits_per_feature_a is None or logits_per_feature_b is None:\n            continue\n\n        labels = self._get_ground_truth(\n            logits_per_feature_a.shape,\n            device=logits_per_feature_a.device,\n            rank=rank,\n            world_size=world_size,\n            skipped_process=skip_flag,\n        )\n\n        if labels.numel() != 0:\n            losses.append(\n                (\n                    (\n                        F.cross_entropy(logits_per_feature_a, labels)\n                        + F.cross_entropy(logits_per_feature_b, labels)\n                    )\n                    / 2\n                )\n                * loss_pairs.weight\n            )\n\n    if self.modality_alignment:\n        losses.append(\n            self._compute_modality_alignment_loss(all_embeddings, logit_scale)\n        )\n\n    if not losses:  # no loss to compute (e.g. no paired data in batch)\n        losses.append(\n            torch.tensor(\n                0.0,\n                device=logit_scale.device,\n                dtype=next(iter(embeddings.values())).dtype,\n            )\n        )\n\n    return torch.stack(losses).sum()\n</code></pre>"},{"location":"api/#mmlearn.modules.losses.data2vec","title":"data2vec","text":"<p>Implementation of Data2vec loss function.</p>"},{"location":"api/#mmlearn.modules.losses.data2vec.Data2VecLoss","title":"Data2VecLoss","text":"<p>               Bases: <code>Module</code></p> <p>Data2Vec loss function.</p> <p>Parameters:</p> Name Type Description Default <code>beta</code> <code>float</code> <p>Specifies the beta parameter for smooth L1 loss. If <code>0</code>, MSE loss is used.</p> <code>0</code> <code>loss_scale</code> <code>Optional[float]</code> <p>Scaling factor for the loss. If None, uses <code>1 / sqrt(embedding_dim)</code>.</p> <code>None</code> <code>reduction</code> <code>str</code> <p>Specifies the reduction to apply to the output: <code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>.</p> <code>'none'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the reduction mode is not supported.</p> Source code in <code>mmlearn/modules/losses/data2vec.py</code> <pre><code>@store(group=\"modules/losses\", provider=\"mmlearn\")\nclass Data2VecLoss(nn.Module):\n    \"\"\"Data2Vec loss function.\n\n    Parameters\n    ----------\n    beta : float, optional, default=0\n        Specifies the beta parameter for smooth L1 loss. If ``0``, MSE loss is used.\n    loss_scale : Optional[float], optional, default=None\n        Scaling factor for the loss. If None, uses ``1 / sqrt(embedding_dim)``.\n    reduction : str, optional, default='none'\n        Specifies the reduction to apply to the output:\n        ``'none'`` | ``'mean'`` | ``'sum'``.\n\n    Raises\n    ------\n    ValueError\n        If the reduction mode is not supported.\n    \"\"\"\n\n    def __init__(\n        self,\n        beta: float = 0,\n        loss_scale: Optional[float] = None,\n        reduction: str = \"none\",\n    ) -&gt; None:\n        super().__init__()\n        self.beta = beta\n        self.loss_scale = loss_scale\n        if reduction not in [\"none\", \"mean\", \"sum\"]:\n            raise ValueError(f\"Unsupported reduction mode: {reduction}\")\n        self.reduction = reduction\n\n    def forward(self, x: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute the Data2Vec loss.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Predicted embeddings of shape ``(batch_size, num_patches, embedding_dim)``.\n        y : torch.Tensor\n            Target embeddings of shape ``(batch_size, num_patches, embedding_dim)``.\n\n        Returns\n        -------\n        torch.Tensor\n            Data2Vec loss value.\n\n        Raises\n        ------\n        ValueError\n            If the shapes of x and y do not match.\n        \"\"\"\n        if x.shape != y.shape:\n            raise ValueError(f\"Shape mismatch: x: {x.shape}, y: {y.shape}\")\n\n        x = x.view(-1, x.size(-1)).float()\n        y = y.view(-1, y.size(-1))\n\n        if self.beta == 0:\n            loss = mse_loss(x, y, reduction=\"none\")\n        else:\n            loss = smooth_l1_loss(x, y, reduction=\"none\", beta=self.beta)\n\n        if self.loss_scale is not None:\n            scale = self.loss_scale\n        else:\n            scale = 1 / math.sqrt(x.size(-1))\n\n        loss = loss * scale\n\n        if self.reduction == \"mean\":\n            return loss.mean()\n        if self.reduction == \"sum\":\n            return loss.sum()\n        # 'none'\n        return loss.view(x.size(0), -1).sum(1)\n</code></pre>"},{"location":"api/#mmlearn.modules.losses.data2vec.Data2VecLoss.forward","title":"forward","text":"<pre><code>forward(x, y)\n</code></pre> <p>Compute the Data2Vec loss.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Predicted embeddings of shape <code>(batch_size, num_patches, embedding_dim)</code>.</p> required <code>y</code> <code>Tensor</code> <p>Target embeddings of shape <code>(batch_size, num_patches, embedding_dim)</code>.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Data2Vec loss value.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the shapes of x and y do not match.</p> Source code in <code>mmlearn/modules/losses/data2vec.py</code> <pre><code>def forward(self, x: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute the Data2Vec loss.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Predicted embeddings of shape ``(batch_size, num_patches, embedding_dim)``.\n    y : torch.Tensor\n        Target embeddings of shape ``(batch_size, num_patches, embedding_dim)``.\n\n    Returns\n    -------\n    torch.Tensor\n        Data2Vec loss value.\n\n    Raises\n    ------\n    ValueError\n        If the shapes of x and y do not match.\n    \"\"\"\n    if x.shape != y.shape:\n        raise ValueError(f\"Shape mismatch: x: {x.shape}, y: {y.shape}\")\n\n    x = x.view(-1, x.size(-1)).float()\n    y = y.view(-1, y.size(-1))\n\n    if self.beta == 0:\n        loss = mse_loss(x, y, reduction=\"none\")\n    else:\n        loss = smooth_l1_loss(x, y, reduction=\"none\", beta=self.beta)\n\n    if self.loss_scale is not None:\n        scale = self.loss_scale\n    else:\n        scale = 1 / math.sqrt(x.size(-1))\n\n    loss = loss * scale\n\n    if self.reduction == \"mean\":\n        return loss.mean()\n    if self.reduction == \"sum\":\n        return loss.sum()\n    # 'none'\n    return loss.view(x.size(0), -1).sum(1)\n</code></pre>"},{"location":"api/#mmlearn.modules.lr_schedulers","title":"lr_schedulers","text":"<p>Learning rate schedulers for training models.</p>"},{"location":"api/#mmlearn.modules.lr_schedulers.linear_warmup_cosine_annealing_lr","title":"linear_warmup_cosine_annealing_lr","text":"<pre><code>linear_warmup_cosine_annealing_lr(\n    optimizer,\n    warmup_steps,\n    max_steps,\n    start_factor=1 / 3,\n    eta_min=0.0,\n    last_epoch=-1,\n)\n</code></pre> <p>Create a linear warmup cosine annealing learning rate scheduler.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>Optimizer</code> <p>The optimizer for which to schedule the learning rate.</p> required <code>warmup_steps</code> <code>int</code> <p>Maximum number of iterations for linear warmup.</p> required <code>max_steps</code> <code>int</code> <p>Maximum number of iterations.</p> required <code>start_factor</code> <code>float</code> <p>Multiplicative factor for the learning rate at the start of the warmup phase.</p> <code>1/3</code> <code>eta_min</code> <code>float</code> <p>Minimum learning rate.</p> <code>0</code> <code>last_epoch</code> <code>int</code> <p>The index of last epoch. If set to <code>-1</code>, it initializes the learning rate as the base learning rate</p> <code>-1</code> <p>Returns:</p> Type Description <code>LRScheduler</code> <p>The learning rate scheduler.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>warmup_steps</code> is greater than or equal to <code>max_steps</code> or if <code>warmup_steps</code> is less than or equal to 0.</p> Source code in <code>mmlearn/modules/lr_schedulers/linear_warmup_cosine_lr.py</code> <pre><code>@store(  # type: ignore[misc]\n    group=\"modules/lr_schedulers\",\n    provider=\"mmlearn\",\n    zen_partial=True,\n    warmup_steps=MISSING,\n    max_steps=MISSING,\n)\ndef linear_warmup_cosine_annealing_lr(\n    optimizer: Optimizer,\n    warmup_steps: int,\n    max_steps: int,\n    start_factor: float = 1 / 3,\n    eta_min: float = 0.0,\n    last_epoch: int = -1,\n) -&gt; LRScheduler:\n    \"\"\"Create a linear warmup cosine annealing learning rate scheduler.\n\n    Parameters\n    ----------\n    optimizer : Optimizer\n        The optimizer for which to schedule the learning rate.\n    warmup_steps : int\n        Maximum number of iterations for linear warmup.\n    max_steps : int\n        Maximum number of iterations.\n    start_factor : float, optional, default=1/3\n        Multiplicative factor for the learning rate at the start of the warmup phase.\n    eta_min : float, optional, default=0\n        Minimum learning rate.\n    last_epoch : int, optional, default=-1\n        The index of last epoch. If set to ``-1``, it initializes the learning rate\n        as the base learning rate\n\n    Returns\n    -------\n    LRScheduler\n        The learning rate scheduler.\n\n    Raises\n    ------\n    ValueError\n        If `warmup_steps` is greater than or equal to `max_steps` or if `warmup_steps`\n        is less than or equal to 0.\n    \"\"\"\n    if warmup_steps &gt;= max_steps:\n        raise ValueError(\n            \"Expected `warmup_steps` to be less than `max_steps` but got \"\n            f\"`warmup_steps={warmup_steps}` and `max_steps={max_steps}`.\"\n        )\n    if warmup_steps &lt;= 0:\n        raise ValueError(\n            \"Expected `warmup_steps` to be positive but got \"\n            f\"`warmup_steps={warmup_steps}`.\"\n        )\n\n    linear_lr = LinearLR(\n        optimizer,\n        start_factor=start_factor,\n        total_iters=warmup_steps,\n        last_epoch=last_epoch,\n    )\n    cosine_lr = CosineAnnealingLR(\n        optimizer,\n        T_max=max_steps - warmup_steps,\n        eta_min=eta_min,\n        last_epoch=last_epoch,\n    )\n    return SequentialLR(\n        optimizer,\n        schedulers=[linear_lr, cosine_lr],\n        milestones=[warmup_steps],\n        last_epoch=last_epoch,\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.lr_schedulers.linear_warmup_cosine_lr","title":"linear_warmup_cosine_lr","text":"<p>Linear warmup cosine annealing learning rate scheduler.</p>"},{"location":"api/#mmlearn.modules.lr_schedulers.linear_warmup_cosine_lr.linear_warmup_cosine_annealing_lr","title":"linear_warmup_cosine_annealing_lr","text":"<pre><code>linear_warmup_cosine_annealing_lr(\n    optimizer,\n    warmup_steps,\n    max_steps,\n    start_factor=1 / 3,\n    eta_min=0.0,\n    last_epoch=-1,\n)\n</code></pre> <p>Create a linear warmup cosine annealing learning rate scheduler.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>Optimizer</code> <p>The optimizer for which to schedule the learning rate.</p> required <code>warmup_steps</code> <code>int</code> <p>Maximum number of iterations for linear warmup.</p> required <code>max_steps</code> <code>int</code> <p>Maximum number of iterations.</p> required <code>start_factor</code> <code>float</code> <p>Multiplicative factor for the learning rate at the start of the warmup phase.</p> <code>1/3</code> <code>eta_min</code> <code>float</code> <p>Minimum learning rate.</p> <code>0</code> <code>last_epoch</code> <code>int</code> <p>The index of last epoch. If set to <code>-1</code>, it initializes the learning rate as the base learning rate</p> <code>-1</code> <p>Returns:</p> Type Description <code>LRScheduler</code> <p>The learning rate scheduler.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>warmup_steps</code> is greater than or equal to <code>max_steps</code> or if <code>warmup_steps</code> is less than or equal to 0.</p> Source code in <code>mmlearn/modules/lr_schedulers/linear_warmup_cosine_lr.py</code> <pre><code>@store(  # type: ignore[misc]\n    group=\"modules/lr_schedulers\",\n    provider=\"mmlearn\",\n    zen_partial=True,\n    warmup_steps=MISSING,\n    max_steps=MISSING,\n)\ndef linear_warmup_cosine_annealing_lr(\n    optimizer: Optimizer,\n    warmup_steps: int,\n    max_steps: int,\n    start_factor: float = 1 / 3,\n    eta_min: float = 0.0,\n    last_epoch: int = -1,\n) -&gt; LRScheduler:\n    \"\"\"Create a linear warmup cosine annealing learning rate scheduler.\n\n    Parameters\n    ----------\n    optimizer : Optimizer\n        The optimizer for which to schedule the learning rate.\n    warmup_steps : int\n        Maximum number of iterations for linear warmup.\n    max_steps : int\n        Maximum number of iterations.\n    start_factor : float, optional, default=1/3\n        Multiplicative factor for the learning rate at the start of the warmup phase.\n    eta_min : float, optional, default=0\n        Minimum learning rate.\n    last_epoch : int, optional, default=-1\n        The index of last epoch. If set to ``-1``, it initializes the learning rate\n        as the base learning rate\n\n    Returns\n    -------\n    LRScheduler\n        The learning rate scheduler.\n\n    Raises\n    ------\n    ValueError\n        If `warmup_steps` is greater than or equal to `max_steps` or if `warmup_steps`\n        is less than or equal to 0.\n    \"\"\"\n    if warmup_steps &gt;= max_steps:\n        raise ValueError(\n            \"Expected `warmup_steps` to be less than `max_steps` but got \"\n            f\"`warmup_steps={warmup_steps}` and `max_steps={max_steps}`.\"\n        )\n    if warmup_steps &lt;= 0:\n        raise ValueError(\n            \"Expected `warmup_steps` to be positive but got \"\n            f\"`warmup_steps={warmup_steps}`.\"\n        )\n\n    linear_lr = LinearLR(\n        optimizer,\n        start_factor=start_factor,\n        total_iters=warmup_steps,\n        last_epoch=last_epoch,\n    )\n    cosine_lr = CosineAnnealingLR(\n        optimizer,\n        T_max=max_steps - warmup_steps,\n        eta_min=eta_min,\n        last_epoch=last_epoch,\n    )\n    return SequentialLR(\n        optimizer,\n        schedulers=[linear_lr, cosine_lr],\n        milestones=[warmup_steps],\n        last_epoch=last_epoch,\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.metrics","title":"metrics","text":"<p>Metrics for evaluating models.</p>"},{"location":"api/#mmlearn.modules.metrics.RetrievalRecallAtK","title":"RetrievalRecallAtK","text":"<p>               Bases: <code>Metric</code></p> <p>Retrieval Recall@K metric.</p> <p>Computes the Recall@K for retrieval tasks. The metric is computed as follows:</p> <ol> <li>Compute the cosine similarity between the query and the database.</li> <li>For each query, sort the database in decreasing order of similarity.</li> <li>Compute the Recall@K as the number of true positives among the top K elements.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>top_k</code> <code>int</code> <p>The number of top elements to consider for computing the Recall@K.</p> required <code>reduction</code> <code>(mean, sum, none, None)</code> <p>Specifies the reduction to apply after computing the pairwise cosine similarity scores.</p> <code>\"mean\"</code> <code>aggregation</code> <code>(mean, median, min, max)</code> <p>Specifies the aggregation function to apply to the Recall@K values computed in batches. If a callable is provided, it should accept a tensor of values and a keyword argument <code>'dim'</code> and return a single scalar value.</p> <code>\"mean\"</code> <code>kwargs</code> <code>Any</code> <p>Additional arguments to be passed to the class:<code>torchmetrics.Metric</code> class.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <ul> <li>If the <code>top_k</code> is not a positive integer or None.</li> <li>If the <code>reduction</code> is not one of {\"mean\", \"sum\", \"none\", None}.</li> <li>If the <code>aggregation</code> is not one of {\"mean\", \"median\", \"min\", \"max\"} or a   custom callable function.</li> </ul> Source code in <code>mmlearn/modules/metrics/retrieval_recall.py</code> <pre><code>@store(group=\"modules/metrics\", provider=\"mmlearn\")\nclass RetrievalRecallAtK(Metric):\n    \"\"\"Retrieval Recall@K metric.\n\n    Computes the Recall@K for retrieval tasks. The metric is computed as follows:\n\n    1. Compute the cosine similarity between the query and the database.\n    2. For each query, sort the database in decreasing order of similarity.\n    3. Compute the Recall@K as the number of true positives among the top K elements.\n\n    Parameters\n    ----------\n    top_k : int\n        The number of top elements to consider for computing the Recall@K.\n    reduction : {\"mean\", \"sum\", \"none\", None}, optional, default=\"sum\"\n        Specifies the reduction to apply after computing the pairwise cosine similarity\n        scores.\n    aggregation : {\"mean\", \"median\", \"min\", \"max\"} or callable, default=\"mean\"\n        Specifies the aggregation function to apply to the Recall@K values computed\n        in batches. If a callable is provided, it should accept a tensor of values\n        and a keyword argument ``'dim'`` and return a single scalar value.\n    kwargs : Any\n        Additional arguments to be passed to the :py:class:`torchmetrics.Metric` class.\n\n    Raises\n    ------\n    ValueError\n\n        - If the `top_k` is not a positive integer or None.\n        - If the `reduction` is not one of {\"mean\", \"sum\", \"none\", None}.\n        - If the `aggregation` is not one of {\"mean\", \"median\", \"min\", \"max\"} or a\n          custom callable function.\n\n    \"\"\"\n\n    is_differentiable: bool = False\n    higher_is_better: bool = True\n    full_state_update: bool = False\n\n    indexes: list[torch.Tensor]\n    x: list[torch.Tensor]\n    y: list[torch.Tensor]\n    num_samples: torch.Tensor\n\n    def __init__(\n        self,\n        top_k: int,\n        reduction: Optional[Literal[\"mean\", \"sum\", \"none\"]] = \"sum\",\n        aggregation: Union[\n            Literal[\"mean\", \"median\", \"min\", \"max\"],\n            Callable[[torch.Tensor, int], torch.Tensor],\n        ] = \"mean\",\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Initialize the metric.\"\"\"\n        super().__init__(**kwargs)\n\n        if top_k is not None and not (isinstance(top_k, int) and top_k &gt; 0):\n            raise ValueError(\"`top_k` has to be a positive integer or None\")\n        self.top_k = top_k\n\n        allowed_reduction = (\"sum\", \"mean\", \"none\", None)\n        if reduction not in allowed_reduction:\n            raise ValueError(\n                f\"Expected argument `reduction` to be one of {allowed_reduction} but got {reduction}\"\n            )\n        self.reduction = reduction\n\n        if not (\n            aggregation in (\"mean\", \"median\", \"min\", \"max\") or callable(aggregation)\n        ):\n            raise ValueError(\n                \"Argument `aggregation` must be one of `mean`, `median`, `min`, `max` or a custom callable function\"\n                f\"which takes tensor of values, but got {aggregation}.\"\n            )\n        self.aggregation = aggregation\n\n        self.add_state(\"x\", default=[], dist_reduce_fx=\"cat\")\n        self.add_state(\"y\", default=[], dist_reduce_fx=\"cat\")\n        self.add_state(\"indexes\", default=[], dist_reduce_fx=\"cat\")\n        self.add_state(\"num_samples\", default=torch.tensor(0), dist_reduce_fx=\"cat\")\n\n        self._batch_size: int = -1\n\n        self.compute_on_cpu = True\n        self.sync_on_compute = False\n        self.dist_sync_on_step = False\n        self._to_sync = self.sync_on_compute\n        self._should_unsync = False\n\n    def update(self, x: torch.Tensor, y: torch.Tensor, indexes: torch.Tensor) -&gt; None:\n        \"\"\"Check shape, convert dtypes and add to accumulators.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Embeddings (unnormalized) of shape ``(N, D)`` where ``N`` is the number\n            of samples and `D` is the number of dimensions.\n        y : torch.Tensor\n            Embeddings (unnormalized) of shape ``(M, D)`` where ``M`` is the number\n            of samples and ``D`` is the number of dimensions.\n        indexes : torch.Tensor\n            Index tensor of shape ``(N,)`` where ``N`` is the number of samples.\n            This specifies which sample in ``y`` is the positive match for each\n            sample in ``x``.\n\n        Raises\n        ------\n        ValueError\n            If `indexes` is None.\n\n        \"\"\"\n        if indexes is None:\n            raise ValueError(\"Argument `indexes` cannot be None\")\n\n        x, y, indexes = _update_batch_inputs(x.clone(), y.clone(), indexes.clone())\n\n        # offset batch indexes by the number of samples seen so far\n        indexes += self.num_samples\n\n        local_batch_size = torch.zeros_like(self.num_samples) + x.size(0)\n        if self._is_distributed():\n            x = dim_zero_cat(gather_all_tensors(x, self.process_group))\n            y = dim_zero_cat(gather_all_tensors(y, self.process_group))\n            indexes = dim_zero_cat(\n                gather_all_tensors(indexes.clone(), self.process_group)\n            )\n\n            # offset indexes for each device\n            bsz_per_device = dim_zero_cat(\n                gather_all_tensors(local_batch_size, self.process_group)\n            )\n            cum_local_bsz = torch.cumsum(bsz_per_device, dim=0)\n            for device_idx in range(1, bsz_per_device.numel()):\n                indexes[cum_local_bsz[device_idx - 1] : cum_local_bsz[device_idx]] += (\n                    cum_local_bsz[device_idx - 1]\n                )\n\n            # update the global sample count\n            self.num_samples += cum_local_bsz[-1]\n\n            self._is_synced = True\n        else:\n            self.num_samples += x.size(0)\n\n        self.x.append(x)\n        self.y.append(y)\n        self.indexes.append(indexes)\n\n        if self._batch_size == -1:\n            self._batch_size = x.size(0)  # global batch size\n\n    def compute(self) -&gt; torch.Tensor:\n        \"\"\"Compute the metric.\n\n        Returns\n        -------\n        torch.Tensor\n            The computed metric.\n        \"\"\"\n        x = dim_zero_cat(self.x)\n        y = dim_zero_cat(self.y)\n\n        # normalize embeddings\n        x /= x.norm(dim=-1, p=2, keepdim=True)\n        y /= y.norm(dim=-1, p=2, keepdim=True)\n\n        # instantiate reduction function\n        reduction_mapping: Dict[\n            Optional[str], Callable[[torch.Tensor], torch.Tensor]\n        ] = {\n            \"sum\": partial(torch.sum, dim=-1),\n            \"mean\": partial(torch.mean, dim=-1),\n            \"none\": lambda x: x,\n            None: lambda x: x,\n        }\n\n        # concatenate indexes of true pairs\n        indexes = dim_zero_cat(self.indexes)\n\n        results: list[torch.Tensor] = []\n        with concurrent.futures.ThreadPoolExecutor(\n            max_workers=os.cpu_count() or 1  # use all available CPUs\n        ) as executor:\n            futures = [\n                executor.submit(\n                    self._process_batch,\n                    start,\n                    x,\n                    y,\n                    indexes,\n                    reduction_mapping,\n                    self.top_k,\n                )\n                for start in tqdm(\n                    range(0, len(x), self._batch_size), desc=f\"Recall@{self.top_k}\"\n                )\n            ]\n            for future in concurrent.futures.as_completed(futures):\n                results.append(future.result())\n\n        return _retrieval_aggregate(\n            (torch.cat([x.float() for x in results]) &gt; 0).float(), self.aggregation\n        )\n\n    def forward(self, *args: Any, **kwargs: Any) -&gt; Any:\n        \"\"\"Forward method is not supported.\n\n        Raises\n        ------\n        NotImplementedError\n            The forward method is not supported for this metric.\n        \"\"\"\n        raise NotImplementedError(\n            \"RetrievalRecallAtK metric does not support forward method\"\n        )\n\n    def _is_distributed(self) -&gt; bool:\n        if self.distributed_available_fn is not None:\n            distributed_available = self.distributed_available_fn\n\n        return distributed_available() if callable(distributed_available) else False\n\n    def _process_batch(\n        self,\n        start: int,\n        x_norm: torch.Tensor,\n        y_norm: torch.Tensor,\n        indexes: torch.Tensor,\n        reduction_mapping: Dict[Optional[str], Callable[[torch.Tensor], torch.Tensor]],\n        top_k: int,\n    ) -&gt; torch.Tensor:\n        \"\"\"Compute the Recall@K for a batch of samples.\"\"\"\n        end = start + self._batch_size\n        x_norm_batch = x_norm[start:end]\n        indexes_batch = indexes[start:end]\n\n        similarity = _safe_matmul(x_norm_batch, y_norm)\n        scores: torch.Tensor = reduction_mapping[self.reduction](similarity)\n\n        with torch.inference_mode():\n            positive_pairs = torch.zeros_like(scores, dtype=torch.bool)\n            positive_pairs[torch.arange(len(scores)), indexes_batch] = True\n\n        return _recall_at_k(scores, positive_pairs, top_k)\n</code></pre>"},{"location":"api/#mmlearn.modules.metrics.RetrievalRecallAtK.__init__","title":"__init__","text":"<pre><code>__init__(\n    top_k, reduction=\"sum\", aggregation=\"mean\", **kwargs\n)\n</code></pre> <p>Initialize the metric.</p> Source code in <code>mmlearn/modules/metrics/retrieval_recall.py</code> <pre><code>def __init__(\n    self,\n    top_k: int,\n    reduction: Optional[Literal[\"mean\", \"sum\", \"none\"]] = \"sum\",\n    aggregation: Union[\n        Literal[\"mean\", \"median\", \"min\", \"max\"],\n        Callable[[torch.Tensor, int], torch.Tensor],\n    ] = \"mean\",\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Initialize the metric.\"\"\"\n    super().__init__(**kwargs)\n\n    if top_k is not None and not (isinstance(top_k, int) and top_k &gt; 0):\n        raise ValueError(\"`top_k` has to be a positive integer or None\")\n    self.top_k = top_k\n\n    allowed_reduction = (\"sum\", \"mean\", \"none\", None)\n    if reduction not in allowed_reduction:\n        raise ValueError(\n            f\"Expected argument `reduction` to be one of {allowed_reduction} but got {reduction}\"\n        )\n    self.reduction = reduction\n\n    if not (\n        aggregation in (\"mean\", \"median\", \"min\", \"max\") or callable(aggregation)\n    ):\n        raise ValueError(\n            \"Argument `aggregation` must be one of `mean`, `median`, `min`, `max` or a custom callable function\"\n            f\"which takes tensor of values, but got {aggregation}.\"\n        )\n    self.aggregation = aggregation\n\n    self.add_state(\"x\", default=[], dist_reduce_fx=\"cat\")\n    self.add_state(\"y\", default=[], dist_reduce_fx=\"cat\")\n    self.add_state(\"indexes\", default=[], dist_reduce_fx=\"cat\")\n    self.add_state(\"num_samples\", default=torch.tensor(0), dist_reduce_fx=\"cat\")\n\n    self._batch_size: int = -1\n\n    self.compute_on_cpu = True\n    self.sync_on_compute = False\n    self.dist_sync_on_step = False\n    self._to_sync = self.sync_on_compute\n    self._should_unsync = False\n</code></pre>"},{"location":"api/#mmlearn.modules.metrics.RetrievalRecallAtK.update","title":"update","text":"<pre><code>update(x, y, indexes)\n</code></pre> <p>Check shape, convert dtypes and add to accumulators.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Embeddings (unnormalized) of shape <code>(N, D)</code> where <code>N</code> is the number of samples and <code>D</code> is the number of dimensions.</p> required <code>y</code> <code>Tensor</code> <p>Embeddings (unnormalized) of shape <code>(M, D)</code> where <code>M</code> is the number of samples and <code>D</code> is the number of dimensions.</p> required <code>indexes</code> <code>Tensor</code> <p>Index tensor of shape <code>(N,)</code> where <code>N</code> is the number of samples. This specifies which sample in <code>y</code> is the positive match for each sample in <code>x</code>.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>indexes</code> is None.</p> Source code in <code>mmlearn/modules/metrics/retrieval_recall.py</code> <pre><code>def update(self, x: torch.Tensor, y: torch.Tensor, indexes: torch.Tensor) -&gt; None:\n    \"\"\"Check shape, convert dtypes and add to accumulators.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Embeddings (unnormalized) of shape ``(N, D)`` where ``N`` is the number\n        of samples and `D` is the number of dimensions.\n    y : torch.Tensor\n        Embeddings (unnormalized) of shape ``(M, D)`` where ``M`` is the number\n        of samples and ``D`` is the number of dimensions.\n    indexes : torch.Tensor\n        Index tensor of shape ``(N,)`` where ``N`` is the number of samples.\n        This specifies which sample in ``y`` is the positive match for each\n        sample in ``x``.\n\n    Raises\n    ------\n    ValueError\n        If `indexes` is None.\n\n    \"\"\"\n    if indexes is None:\n        raise ValueError(\"Argument `indexes` cannot be None\")\n\n    x, y, indexes = _update_batch_inputs(x.clone(), y.clone(), indexes.clone())\n\n    # offset batch indexes by the number of samples seen so far\n    indexes += self.num_samples\n\n    local_batch_size = torch.zeros_like(self.num_samples) + x.size(0)\n    if self._is_distributed():\n        x = dim_zero_cat(gather_all_tensors(x, self.process_group))\n        y = dim_zero_cat(gather_all_tensors(y, self.process_group))\n        indexes = dim_zero_cat(\n            gather_all_tensors(indexes.clone(), self.process_group)\n        )\n\n        # offset indexes for each device\n        bsz_per_device = dim_zero_cat(\n            gather_all_tensors(local_batch_size, self.process_group)\n        )\n        cum_local_bsz = torch.cumsum(bsz_per_device, dim=0)\n        for device_idx in range(1, bsz_per_device.numel()):\n            indexes[cum_local_bsz[device_idx - 1] : cum_local_bsz[device_idx]] += (\n                cum_local_bsz[device_idx - 1]\n            )\n\n        # update the global sample count\n        self.num_samples += cum_local_bsz[-1]\n\n        self._is_synced = True\n    else:\n        self.num_samples += x.size(0)\n\n    self.x.append(x)\n    self.y.append(y)\n    self.indexes.append(indexes)\n\n    if self._batch_size == -1:\n        self._batch_size = x.size(0)  # global batch size\n</code></pre>"},{"location":"api/#mmlearn.modules.metrics.RetrievalRecallAtK.compute","title":"compute","text":"<pre><code>compute()\n</code></pre> <p>Compute the metric.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>The computed metric.</p> Source code in <code>mmlearn/modules/metrics/retrieval_recall.py</code> <pre><code>def compute(self) -&gt; torch.Tensor:\n    \"\"\"Compute the metric.\n\n    Returns\n    -------\n    torch.Tensor\n        The computed metric.\n    \"\"\"\n    x = dim_zero_cat(self.x)\n    y = dim_zero_cat(self.y)\n\n    # normalize embeddings\n    x /= x.norm(dim=-1, p=2, keepdim=True)\n    y /= y.norm(dim=-1, p=2, keepdim=True)\n\n    # instantiate reduction function\n    reduction_mapping: Dict[\n        Optional[str], Callable[[torch.Tensor], torch.Tensor]\n    ] = {\n        \"sum\": partial(torch.sum, dim=-1),\n        \"mean\": partial(torch.mean, dim=-1),\n        \"none\": lambda x: x,\n        None: lambda x: x,\n    }\n\n    # concatenate indexes of true pairs\n    indexes = dim_zero_cat(self.indexes)\n\n    results: list[torch.Tensor] = []\n    with concurrent.futures.ThreadPoolExecutor(\n        max_workers=os.cpu_count() or 1  # use all available CPUs\n    ) as executor:\n        futures = [\n            executor.submit(\n                self._process_batch,\n                start,\n                x,\n                y,\n                indexes,\n                reduction_mapping,\n                self.top_k,\n            )\n            for start in tqdm(\n                range(0, len(x), self._batch_size), desc=f\"Recall@{self.top_k}\"\n            )\n        ]\n        for future in concurrent.futures.as_completed(futures):\n            results.append(future.result())\n\n    return _retrieval_aggregate(\n        (torch.cat([x.float() for x in results]) &gt; 0).float(), self.aggregation\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.metrics.RetrievalRecallAtK.forward","title":"forward","text":"<pre><code>forward(*args, **kwargs)\n</code></pre> <p>Forward method is not supported.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>The forward method is not supported for this metric.</p> Source code in <code>mmlearn/modules/metrics/retrieval_recall.py</code> <pre><code>def forward(self, *args: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"Forward method is not supported.\n\n    Raises\n    ------\n    NotImplementedError\n        The forward method is not supported for this metric.\n    \"\"\"\n    raise NotImplementedError(\n        \"RetrievalRecallAtK metric does not support forward method\"\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.metrics.retrieval_recall","title":"retrieval_recall","text":"<p>Retrieval Recall@K metric.</p>"},{"location":"api/#mmlearn.modules.metrics.retrieval_recall.RetrievalRecallAtK","title":"RetrievalRecallAtK","text":"<p>               Bases: <code>Metric</code></p> <p>Retrieval Recall@K metric.</p> <p>Computes the Recall@K for retrieval tasks. The metric is computed as follows:</p> <ol> <li>Compute the cosine similarity between the query and the database.</li> <li>For each query, sort the database in decreasing order of similarity.</li> <li>Compute the Recall@K as the number of true positives among the top K elements.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>top_k</code> <code>int</code> <p>The number of top elements to consider for computing the Recall@K.</p> required <code>reduction</code> <code>(mean, sum, none, None)</code> <p>Specifies the reduction to apply after computing the pairwise cosine similarity scores.</p> <code>\"mean\"</code> <code>aggregation</code> <code>(mean, median, min, max)</code> <p>Specifies the aggregation function to apply to the Recall@K values computed in batches. If a callable is provided, it should accept a tensor of values and a keyword argument <code>'dim'</code> and return a single scalar value.</p> <code>\"mean\"</code> <code>kwargs</code> <code>Any</code> <p>Additional arguments to be passed to the class:<code>torchmetrics.Metric</code> class.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <ul> <li>If the <code>top_k</code> is not a positive integer or None.</li> <li>If the <code>reduction</code> is not one of {\"mean\", \"sum\", \"none\", None}.</li> <li>If the <code>aggregation</code> is not one of {\"mean\", \"median\", \"min\", \"max\"} or a   custom callable function.</li> </ul> Source code in <code>mmlearn/modules/metrics/retrieval_recall.py</code> <pre><code>@store(group=\"modules/metrics\", provider=\"mmlearn\")\nclass RetrievalRecallAtK(Metric):\n    \"\"\"Retrieval Recall@K metric.\n\n    Computes the Recall@K for retrieval tasks. The metric is computed as follows:\n\n    1. Compute the cosine similarity between the query and the database.\n    2. For each query, sort the database in decreasing order of similarity.\n    3. Compute the Recall@K as the number of true positives among the top K elements.\n\n    Parameters\n    ----------\n    top_k : int\n        The number of top elements to consider for computing the Recall@K.\n    reduction : {\"mean\", \"sum\", \"none\", None}, optional, default=\"sum\"\n        Specifies the reduction to apply after computing the pairwise cosine similarity\n        scores.\n    aggregation : {\"mean\", \"median\", \"min\", \"max\"} or callable, default=\"mean\"\n        Specifies the aggregation function to apply to the Recall@K values computed\n        in batches. If a callable is provided, it should accept a tensor of values\n        and a keyword argument ``'dim'`` and return a single scalar value.\n    kwargs : Any\n        Additional arguments to be passed to the :py:class:`torchmetrics.Metric` class.\n\n    Raises\n    ------\n    ValueError\n\n        - If the `top_k` is not a positive integer or None.\n        - If the `reduction` is not one of {\"mean\", \"sum\", \"none\", None}.\n        - If the `aggregation` is not one of {\"mean\", \"median\", \"min\", \"max\"} or a\n          custom callable function.\n\n    \"\"\"\n\n    is_differentiable: bool = False\n    higher_is_better: bool = True\n    full_state_update: bool = False\n\n    indexes: list[torch.Tensor]\n    x: list[torch.Tensor]\n    y: list[torch.Tensor]\n    num_samples: torch.Tensor\n\n    def __init__(\n        self,\n        top_k: int,\n        reduction: Optional[Literal[\"mean\", \"sum\", \"none\"]] = \"sum\",\n        aggregation: Union[\n            Literal[\"mean\", \"median\", \"min\", \"max\"],\n            Callable[[torch.Tensor, int], torch.Tensor],\n        ] = \"mean\",\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Initialize the metric.\"\"\"\n        super().__init__(**kwargs)\n\n        if top_k is not None and not (isinstance(top_k, int) and top_k &gt; 0):\n            raise ValueError(\"`top_k` has to be a positive integer or None\")\n        self.top_k = top_k\n\n        allowed_reduction = (\"sum\", \"mean\", \"none\", None)\n        if reduction not in allowed_reduction:\n            raise ValueError(\n                f\"Expected argument `reduction` to be one of {allowed_reduction} but got {reduction}\"\n            )\n        self.reduction = reduction\n\n        if not (\n            aggregation in (\"mean\", \"median\", \"min\", \"max\") or callable(aggregation)\n        ):\n            raise ValueError(\n                \"Argument `aggregation` must be one of `mean`, `median`, `min`, `max` or a custom callable function\"\n                f\"which takes tensor of values, but got {aggregation}.\"\n            )\n        self.aggregation = aggregation\n\n        self.add_state(\"x\", default=[], dist_reduce_fx=\"cat\")\n        self.add_state(\"y\", default=[], dist_reduce_fx=\"cat\")\n        self.add_state(\"indexes\", default=[], dist_reduce_fx=\"cat\")\n        self.add_state(\"num_samples\", default=torch.tensor(0), dist_reduce_fx=\"cat\")\n\n        self._batch_size: int = -1\n\n        self.compute_on_cpu = True\n        self.sync_on_compute = False\n        self.dist_sync_on_step = False\n        self._to_sync = self.sync_on_compute\n        self._should_unsync = False\n\n    def update(self, x: torch.Tensor, y: torch.Tensor, indexes: torch.Tensor) -&gt; None:\n        \"\"\"Check shape, convert dtypes and add to accumulators.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Embeddings (unnormalized) of shape ``(N, D)`` where ``N`` is the number\n            of samples and `D` is the number of dimensions.\n        y : torch.Tensor\n            Embeddings (unnormalized) of shape ``(M, D)`` where ``M`` is the number\n            of samples and ``D`` is the number of dimensions.\n        indexes : torch.Tensor\n            Index tensor of shape ``(N,)`` where ``N`` is the number of samples.\n            This specifies which sample in ``y`` is the positive match for each\n            sample in ``x``.\n\n        Raises\n        ------\n        ValueError\n            If `indexes` is None.\n\n        \"\"\"\n        if indexes is None:\n            raise ValueError(\"Argument `indexes` cannot be None\")\n\n        x, y, indexes = _update_batch_inputs(x.clone(), y.clone(), indexes.clone())\n\n        # offset batch indexes by the number of samples seen so far\n        indexes += self.num_samples\n\n        local_batch_size = torch.zeros_like(self.num_samples) + x.size(0)\n        if self._is_distributed():\n            x = dim_zero_cat(gather_all_tensors(x, self.process_group))\n            y = dim_zero_cat(gather_all_tensors(y, self.process_group))\n            indexes = dim_zero_cat(\n                gather_all_tensors(indexes.clone(), self.process_group)\n            )\n\n            # offset indexes for each device\n            bsz_per_device = dim_zero_cat(\n                gather_all_tensors(local_batch_size, self.process_group)\n            )\n            cum_local_bsz = torch.cumsum(bsz_per_device, dim=0)\n            for device_idx in range(1, bsz_per_device.numel()):\n                indexes[cum_local_bsz[device_idx - 1] : cum_local_bsz[device_idx]] += (\n                    cum_local_bsz[device_idx - 1]\n                )\n\n            # update the global sample count\n            self.num_samples += cum_local_bsz[-1]\n\n            self._is_synced = True\n        else:\n            self.num_samples += x.size(0)\n\n        self.x.append(x)\n        self.y.append(y)\n        self.indexes.append(indexes)\n\n        if self._batch_size == -1:\n            self._batch_size = x.size(0)  # global batch size\n\n    def compute(self) -&gt; torch.Tensor:\n        \"\"\"Compute the metric.\n\n        Returns\n        -------\n        torch.Tensor\n            The computed metric.\n        \"\"\"\n        x = dim_zero_cat(self.x)\n        y = dim_zero_cat(self.y)\n\n        # normalize embeddings\n        x /= x.norm(dim=-1, p=2, keepdim=True)\n        y /= y.norm(dim=-1, p=2, keepdim=True)\n\n        # instantiate reduction function\n        reduction_mapping: Dict[\n            Optional[str], Callable[[torch.Tensor], torch.Tensor]\n        ] = {\n            \"sum\": partial(torch.sum, dim=-1),\n            \"mean\": partial(torch.mean, dim=-1),\n            \"none\": lambda x: x,\n            None: lambda x: x,\n        }\n\n        # concatenate indexes of true pairs\n        indexes = dim_zero_cat(self.indexes)\n\n        results: list[torch.Tensor] = []\n        with concurrent.futures.ThreadPoolExecutor(\n            max_workers=os.cpu_count() or 1  # use all available CPUs\n        ) as executor:\n            futures = [\n                executor.submit(\n                    self._process_batch,\n                    start,\n                    x,\n                    y,\n                    indexes,\n                    reduction_mapping,\n                    self.top_k,\n                )\n                for start in tqdm(\n                    range(0, len(x), self._batch_size), desc=f\"Recall@{self.top_k}\"\n                )\n            ]\n            for future in concurrent.futures.as_completed(futures):\n                results.append(future.result())\n\n        return _retrieval_aggregate(\n            (torch.cat([x.float() for x in results]) &gt; 0).float(), self.aggregation\n        )\n\n    def forward(self, *args: Any, **kwargs: Any) -&gt; Any:\n        \"\"\"Forward method is not supported.\n\n        Raises\n        ------\n        NotImplementedError\n            The forward method is not supported for this metric.\n        \"\"\"\n        raise NotImplementedError(\n            \"RetrievalRecallAtK metric does not support forward method\"\n        )\n\n    def _is_distributed(self) -&gt; bool:\n        if self.distributed_available_fn is not None:\n            distributed_available = self.distributed_available_fn\n\n        return distributed_available() if callable(distributed_available) else False\n\n    def _process_batch(\n        self,\n        start: int,\n        x_norm: torch.Tensor,\n        y_norm: torch.Tensor,\n        indexes: torch.Tensor,\n        reduction_mapping: Dict[Optional[str], Callable[[torch.Tensor], torch.Tensor]],\n        top_k: int,\n    ) -&gt; torch.Tensor:\n        \"\"\"Compute the Recall@K for a batch of samples.\"\"\"\n        end = start + self._batch_size\n        x_norm_batch = x_norm[start:end]\n        indexes_batch = indexes[start:end]\n\n        similarity = _safe_matmul(x_norm_batch, y_norm)\n        scores: torch.Tensor = reduction_mapping[self.reduction](similarity)\n\n        with torch.inference_mode():\n            positive_pairs = torch.zeros_like(scores, dtype=torch.bool)\n            positive_pairs[torch.arange(len(scores)), indexes_batch] = True\n\n        return _recall_at_k(scores, positive_pairs, top_k)\n</code></pre>"},{"location":"api/#mmlearn.modules.metrics.retrieval_recall.RetrievalRecallAtK.__init__","title":"__init__","text":"<pre><code>__init__(\n    top_k, reduction=\"sum\", aggregation=\"mean\", **kwargs\n)\n</code></pre> <p>Initialize the metric.</p> Source code in <code>mmlearn/modules/metrics/retrieval_recall.py</code> <pre><code>def __init__(\n    self,\n    top_k: int,\n    reduction: Optional[Literal[\"mean\", \"sum\", \"none\"]] = \"sum\",\n    aggregation: Union[\n        Literal[\"mean\", \"median\", \"min\", \"max\"],\n        Callable[[torch.Tensor, int], torch.Tensor],\n    ] = \"mean\",\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Initialize the metric.\"\"\"\n    super().__init__(**kwargs)\n\n    if top_k is not None and not (isinstance(top_k, int) and top_k &gt; 0):\n        raise ValueError(\"`top_k` has to be a positive integer or None\")\n    self.top_k = top_k\n\n    allowed_reduction = (\"sum\", \"mean\", \"none\", None)\n    if reduction not in allowed_reduction:\n        raise ValueError(\n            f\"Expected argument `reduction` to be one of {allowed_reduction} but got {reduction}\"\n        )\n    self.reduction = reduction\n\n    if not (\n        aggregation in (\"mean\", \"median\", \"min\", \"max\") or callable(aggregation)\n    ):\n        raise ValueError(\n            \"Argument `aggregation` must be one of `mean`, `median`, `min`, `max` or a custom callable function\"\n            f\"which takes tensor of values, but got {aggregation}.\"\n        )\n    self.aggregation = aggregation\n\n    self.add_state(\"x\", default=[], dist_reduce_fx=\"cat\")\n    self.add_state(\"y\", default=[], dist_reduce_fx=\"cat\")\n    self.add_state(\"indexes\", default=[], dist_reduce_fx=\"cat\")\n    self.add_state(\"num_samples\", default=torch.tensor(0), dist_reduce_fx=\"cat\")\n\n    self._batch_size: int = -1\n\n    self.compute_on_cpu = True\n    self.sync_on_compute = False\n    self.dist_sync_on_step = False\n    self._to_sync = self.sync_on_compute\n    self._should_unsync = False\n</code></pre>"},{"location":"api/#mmlearn.modules.metrics.retrieval_recall.RetrievalRecallAtK.update","title":"update","text":"<pre><code>update(x, y, indexes)\n</code></pre> <p>Check shape, convert dtypes and add to accumulators.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Embeddings (unnormalized) of shape <code>(N, D)</code> where <code>N</code> is the number of samples and <code>D</code> is the number of dimensions.</p> required <code>y</code> <code>Tensor</code> <p>Embeddings (unnormalized) of shape <code>(M, D)</code> where <code>M</code> is the number of samples and <code>D</code> is the number of dimensions.</p> required <code>indexes</code> <code>Tensor</code> <p>Index tensor of shape <code>(N,)</code> where <code>N</code> is the number of samples. This specifies which sample in <code>y</code> is the positive match for each sample in <code>x</code>.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>indexes</code> is None.</p> Source code in <code>mmlearn/modules/metrics/retrieval_recall.py</code> <pre><code>def update(self, x: torch.Tensor, y: torch.Tensor, indexes: torch.Tensor) -&gt; None:\n    \"\"\"Check shape, convert dtypes and add to accumulators.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Embeddings (unnormalized) of shape ``(N, D)`` where ``N`` is the number\n        of samples and `D` is the number of dimensions.\n    y : torch.Tensor\n        Embeddings (unnormalized) of shape ``(M, D)`` where ``M`` is the number\n        of samples and ``D`` is the number of dimensions.\n    indexes : torch.Tensor\n        Index tensor of shape ``(N,)`` where ``N`` is the number of samples.\n        This specifies which sample in ``y`` is the positive match for each\n        sample in ``x``.\n\n    Raises\n    ------\n    ValueError\n        If `indexes` is None.\n\n    \"\"\"\n    if indexes is None:\n        raise ValueError(\"Argument `indexes` cannot be None\")\n\n    x, y, indexes = _update_batch_inputs(x.clone(), y.clone(), indexes.clone())\n\n    # offset batch indexes by the number of samples seen so far\n    indexes += self.num_samples\n\n    local_batch_size = torch.zeros_like(self.num_samples) + x.size(0)\n    if self._is_distributed():\n        x = dim_zero_cat(gather_all_tensors(x, self.process_group))\n        y = dim_zero_cat(gather_all_tensors(y, self.process_group))\n        indexes = dim_zero_cat(\n            gather_all_tensors(indexes.clone(), self.process_group)\n        )\n\n        # offset indexes for each device\n        bsz_per_device = dim_zero_cat(\n            gather_all_tensors(local_batch_size, self.process_group)\n        )\n        cum_local_bsz = torch.cumsum(bsz_per_device, dim=0)\n        for device_idx in range(1, bsz_per_device.numel()):\n            indexes[cum_local_bsz[device_idx - 1] : cum_local_bsz[device_idx]] += (\n                cum_local_bsz[device_idx - 1]\n            )\n\n        # update the global sample count\n        self.num_samples += cum_local_bsz[-1]\n\n        self._is_synced = True\n    else:\n        self.num_samples += x.size(0)\n\n    self.x.append(x)\n    self.y.append(y)\n    self.indexes.append(indexes)\n\n    if self._batch_size == -1:\n        self._batch_size = x.size(0)  # global batch size\n</code></pre>"},{"location":"api/#mmlearn.modules.metrics.retrieval_recall.RetrievalRecallAtK.compute","title":"compute","text":"<pre><code>compute()\n</code></pre> <p>Compute the metric.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>The computed metric.</p> Source code in <code>mmlearn/modules/metrics/retrieval_recall.py</code> <pre><code>def compute(self) -&gt; torch.Tensor:\n    \"\"\"Compute the metric.\n\n    Returns\n    -------\n    torch.Tensor\n        The computed metric.\n    \"\"\"\n    x = dim_zero_cat(self.x)\n    y = dim_zero_cat(self.y)\n\n    # normalize embeddings\n    x /= x.norm(dim=-1, p=2, keepdim=True)\n    y /= y.norm(dim=-1, p=2, keepdim=True)\n\n    # instantiate reduction function\n    reduction_mapping: Dict[\n        Optional[str], Callable[[torch.Tensor], torch.Tensor]\n    ] = {\n        \"sum\": partial(torch.sum, dim=-1),\n        \"mean\": partial(torch.mean, dim=-1),\n        \"none\": lambda x: x,\n        None: lambda x: x,\n    }\n\n    # concatenate indexes of true pairs\n    indexes = dim_zero_cat(self.indexes)\n\n    results: list[torch.Tensor] = []\n    with concurrent.futures.ThreadPoolExecutor(\n        max_workers=os.cpu_count() or 1  # use all available CPUs\n    ) as executor:\n        futures = [\n            executor.submit(\n                self._process_batch,\n                start,\n                x,\n                y,\n                indexes,\n                reduction_mapping,\n                self.top_k,\n            )\n            for start in tqdm(\n                range(0, len(x), self._batch_size), desc=f\"Recall@{self.top_k}\"\n            )\n        ]\n        for future in concurrent.futures.as_completed(futures):\n            results.append(future.result())\n\n    return _retrieval_aggregate(\n        (torch.cat([x.float() for x in results]) &gt; 0).float(), self.aggregation\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.metrics.retrieval_recall.RetrievalRecallAtK.forward","title":"forward","text":"<pre><code>forward(*args, **kwargs)\n</code></pre> <p>Forward method is not supported.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>The forward method is not supported for this metric.</p> Source code in <code>mmlearn/modules/metrics/retrieval_recall.py</code> <pre><code>def forward(self, *args: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"Forward method is not supported.\n\n    Raises\n    ------\n    NotImplementedError\n        The forward method is not supported for this metric.\n    \"\"\"\n    raise NotImplementedError(\n        \"RetrievalRecallAtK metric does not support forward method\"\n    )\n</code></pre>"},{"location":"api/#encoders","title":"Encoders","text":""},{"location":"api/#mmlearn.modules.encoders","title":"mmlearn.modules.encoders","text":"<p>Encoders.</p>"},{"location":"api/#mmlearn.modules.encoders.HFCLIPTextEncoder","title":"HFCLIPTextEncoder","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper around the <code>CLIPTextModel</code> from HuggingFace.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>The huggingface model name or a local path from which to load the model.</p> required <code>pretrained</code> <code>bool</code> <p>Whether to load the pretrained weights or not.</p> <code>True</code> <code>pooling_layer</code> <code>Optional[Module]</code> <p>Pooling layer to apply to the last hidden state of the model.</p> <code>None</code> <code>freeze_layers</code> <code>Union[int, float, list[int], bool]</code> <p>Whether to freeze layers of the model and which layers to freeze. If <code>True</code>, all model layers are frozen. If it is an integer, the first <code>N</code> layers of the model are frozen. If it is a float, the first <code>N</code> percent of the layers are frozen. If it is a list of integers, the layers at the indices in the list are frozen.</p> <code>False</code> <code>freeze_layer_norm</code> <code>bool</code> <p>Whether to freeze the layer normalization layers of the model.</p> <code>True</code> <code>peft_config</code> <code>Optional[PeftConfig]</code> <p>The configuration from the <code>peft &lt;https://huggingface.co/docs/peft/index&gt;</code>_ library to use to wrap the model for parameter-efficient finetuning.</p> <code>None</code> <code>model_config_kwargs</code> <code>Optional[dict[str, Any]]</code> <p>Additional keyword arguments to pass to the model configuration.</p> <code>None</code> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If both <code>peft_config</code> and <code>freeze_layers</code> are set. The <code>peft_config</code> will override the <code>freeze_layers</code> setting.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>@store(\n    group=\"modules/encoders\",\n    provider=\"mmlearn\",\n    model_name_or_path=\"openai/clip-vit-base-patch16\",\n    hydra_convert=\"object\",  # required for `peft_config` to be converted to a `PeftConfig` object\n)\nclass HFCLIPTextEncoder(nn.Module):\n    \"\"\"Wrapper around the ``CLIPTextModel`` from HuggingFace.\n\n    Parameters\n    ----------\n    model_name_or_path : str\n        The huggingface model name or a local path from which to load the model.\n    pretrained : bool, default=True\n        Whether to load the pretrained weights or not.\n    pooling_layer : Optional[torch.nn.Module], optional, default=None\n        Pooling layer to apply to the last hidden state of the model.\n    freeze_layers : Union[int, float, list[int], bool], default=False\n        Whether to freeze layers of the model and which layers to freeze. If ``True``,\n        all model layers are frozen. If it is an integer, the first ``N`` layers of\n        the model are frozen. If it is a float, the first ``N`` percent of the layers\n        are frozen. If it is a list of integers, the layers at the indices in the\n        list are frozen.\n    freeze_layer_norm : bool, default=True\n        Whether to freeze the layer normalization layers of the model.\n    peft_config : Optional[PeftConfig], optional, default=None\n        The configuration from the `peft &lt;https://huggingface.co/docs/peft/index&gt;`_\n        library to use to wrap the model for parameter-efficient finetuning.\n    model_config_kwargs : Optional[dict[str, Any]], optional, default=None\n        Additional keyword arguments to pass to the model configuration.\n\n    Warns\n    -----\n    UserWarning\n        If both ``peft_config`` and ``freeze_layers`` are set. The ``peft_config``\n        will override the ``freeze_layers`` setting.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name_or_path: str,\n        pretrained: bool = True,\n        pooling_layer: Optional[nn.Module] = None,\n        freeze_layers: Union[int, float, list[int], bool] = False,\n        freeze_layer_norm: bool = True,\n        peft_config: Optional[\"PeftConfig\"] = None,\n        model_config_kwargs: Optional[dict[str, Any]] = None,\n    ) -&gt; None:\n        super().__init__()\n        _warn_freeze_with_peft(peft_config, freeze_layers)\n\n        model = hf_utils.load_huggingface_model(\n            transformers.CLIPTextModel,\n            model_name_or_path=model_name_or_path,\n            load_pretrained_weights=pretrained,\n            model_config_kwargs=model_config_kwargs,\n        )\n        model = _freeze_text_model(model, freeze_layers, freeze_layer_norm)\n        if peft_config is not None:\n            model = hf_utils._wrap_peft_model(model, peft_config)\n\n        self.model = model\n        self.pooling_layer = pooling_layer\n\n    def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The ``input_ids`` will be expected under the\n            ``Modalities.TEXT`` key.\n\n        Returns\n        -------\n        BaseModelOutput\n            The output of the model, including the last hidden state, all hidden\n            states, and the attention weights, if ``output_attentions`` is set\n            to ``True``.\n        \"\"\"\n        outputs = self.model(\n            input_ids=inputs[Modalities.TEXT.name],\n            attention_mask=inputs.get(\"attention_mask\")\n            or inputs.get(Modalities.TEXT.attention_mask),\n            position_ids=inputs.get(\"position_ids\"),\n            output_attentions=inputs.get(\"output_attentions\"),\n            return_dict=True,\n        )\n        last_hidden_state = outputs.hidden_states[-1]  # NOTE: no layer norm applied\n        if self.pooling_layer:\n            last_hidden_state = self.pooling_layer(last_hidden_state)\n\n        return BaseModelOutput(\n            last_hidden_state=last_hidden_state,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.HFCLIPTextEncoder.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The <code>input_ids</code> will be expected under the <code>Modalities.TEXT</code> key.</p> required <p>Returns:</p> Type Description <code>BaseModelOutput</code> <p>The output of the model, including the last hidden state, all hidden states, and the attention weights, if <code>output_attentions</code> is set to <code>True</code>.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The ``input_ids`` will be expected under the\n        ``Modalities.TEXT`` key.\n\n    Returns\n    -------\n    BaseModelOutput\n        The output of the model, including the last hidden state, all hidden\n        states, and the attention weights, if ``output_attentions`` is set\n        to ``True``.\n    \"\"\"\n    outputs = self.model(\n        input_ids=inputs[Modalities.TEXT.name],\n        attention_mask=inputs.get(\"attention_mask\")\n        or inputs.get(Modalities.TEXT.attention_mask),\n        position_ids=inputs.get(\"position_ids\"),\n        output_attentions=inputs.get(\"output_attentions\"),\n        return_dict=True,\n    )\n    last_hidden_state = outputs.hidden_states[-1]  # NOTE: no layer norm applied\n    if self.pooling_layer:\n        last_hidden_state = self.pooling_layer(last_hidden_state)\n\n    return BaseModelOutput(\n        last_hidden_state=last_hidden_state,\n        hidden_states=outputs.hidden_states,\n        attentions=outputs.attentions,\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.HFCLIPTextEncoderWithProjection","title":"HFCLIPTextEncoderWithProjection","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper around the <code>CLIPTextModelWithProjection</code> from HuggingFace.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>The huggingface model name or a local path from which to load the model.</p> required <code>pretrained</code> <code>bool</code> <p>Whether to load the pretrained weights or not.</p> <code>True</code> <code>use_all_token_embeddings</code> <code>bool</code> <p>Whether to use all token embeddings for the text. If <code>False</code> the first token embedding will be used.</p> <code>False</code> <code>freeze_layers</code> <code>Union[int, float, list[int], bool]</code> <p>Whether to freeze layers of the model and which layers to freeze. If <code>True</code>, all model layers are frozen. If it is an integer, the first <code>N</code> layers of the model are frozen. If it is a float, the first <code>N</code> percent of the layers are frozen. If it is a list of integers, the layers at the indices in the list are frozen.</p> <code>False</code> <code>freeze_layer_norm</code> <code>bool</code> <p>Whether to freeze the layer normalization layers of the model.</p> <code>True</code> <code>peft_config</code> <code>Optional[PeftConfig]</code> <p>The configuration from the <code>peft &lt;https://huggingface.co/docs/peft/index&gt;</code>_ library to use to wrap the model for parameter-efficient finetuning.</p> <code>None</code> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If both <code>peft_config</code> and <code>freeze_layers</code> are set. The <code>peft_config</code> will override the <code>freeze_layers</code> setting.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>@store(\n    group=\"modules/encoders\",\n    provider=\"mmlearn\",\n    model_name_or_path=\"openai/clip-vit-base-patch16\",\n    hydra_convert=\"object\",\n)\nclass HFCLIPTextEncoderWithProjection(nn.Module):\n    \"\"\"Wrapper around the ``CLIPTextModelWithProjection`` from HuggingFace.\n\n    Parameters\n    ----------\n    model_name_or_path : str\n        The huggingface model name or a local path from which to load the model.\n    pretrained : bool, default=True\n        Whether to load the pretrained weights or not.\n    use_all_token_embeddings : bool, default=False\n        Whether to use all token embeddings for the text. If ``False`` the first token\n        embedding will be used.\n    freeze_layers : Union[int, float, list[int], bool], default=False\n        Whether to freeze layers of the model and which layers to freeze. If ``True``,\n        all model layers are frozen. If it is an integer, the first ``N`` layers of\n        the model are frozen. If it is a float, the first ``N`` percent of the layers\n        are frozen. If it is a list of integers, the layers at the indices in the\n        list are frozen.\n    freeze_layer_norm : bool, default=True\n        Whether to freeze the layer normalization layers of the model.\n    peft_config : Optional[PeftConfig], optional, default=None\n        The configuration from the `peft &lt;https://huggingface.co/docs/peft/index&gt;`_\n        library to use to wrap the model for parameter-efficient finetuning.\n\n    Warns\n    -----\n    UserWarning\n        If both ``peft_config`` and ``freeze_layers`` are set. The ``peft_config``\n        will override the ``freeze_layers`` setting.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name_or_path: str,\n        pretrained: bool = True,\n        use_all_token_embeddings: bool = False,\n        freeze_layers: Union[int, float, list[int], bool] = False,\n        freeze_layer_norm: bool = True,\n        peft_config: Optional[\"PeftConfig\"] = None,\n        model_config_kwargs: Optional[dict[str, Any]] = None,\n    ) -&gt; None:\n        super().__init__()\n        _warn_freeze_with_peft(peft_config, freeze_layers)\n\n        self.use_all_token_embeddings = use_all_token_embeddings\n\n        model = hf_utils.load_huggingface_model(\n            transformers.CLIPTextModelWithProjection,\n            model_name_or_path,\n            load_pretrained_weights=pretrained,\n            model_config_kwargs=model_config_kwargs,\n            config_type=CLIPTextConfig,\n        )\n\n        model = _freeze_text_model(model, freeze_layers, freeze_layer_norm)\n        if peft_config is not None:\n            model = hf_utils._wrap_peft_model(model, peft_config)\n\n        self.model = model\n\n    def forward(self, inputs: dict[str, Any]) -&gt; tuple[torch.Tensor]:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The ``input_ids`` will be expected under the\n            ``Modalities.TEXT`` key.\n\n        Returns\n        -------\n        tuple[torch.Tensor]\n            The text embeddings. Will be a tuple with a single element.\n        \"\"\"\n        input_ids = inputs[Modalities.TEXT.name]\n        attention_mask: Optional[torch.Tensor] = inputs.get(\n            \"attention_mask\", inputs.get(Modalities.TEXT.attention_mask, None)\n        )\n        position_ids = inputs.get(\"position_ids\")\n\n        if self.use_all_token_embeddings:\n            text_outputs = self.model.text_model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                return_dict=True,\n            )\n            # TODO: add more options for pooling before projection\n            text_embeds = self.model.text_projection(text_outputs.last_hidden_state)\n        else:\n            text_embeds = self.model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                return_dict=True,\n            ).text_embeds\n\n        return (text_embeds,)\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.HFCLIPTextEncoderWithProjection.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The <code>input_ids</code> will be expected under the <code>Modalities.TEXT</code> key.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor]</code> <p>The text embeddings. Will be a tuple with a single element.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; tuple[torch.Tensor]:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The ``input_ids`` will be expected under the\n        ``Modalities.TEXT`` key.\n\n    Returns\n    -------\n    tuple[torch.Tensor]\n        The text embeddings. Will be a tuple with a single element.\n    \"\"\"\n    input_ids = inputs[Modalities.TEXT.name]\n    attention_mask: Optional[torch.Tensor] = inputs.get(\n        \"attention_mask\", inputs.get(Modalities.TEXT.attention_mask, None)\n    )\n    position_ids = inputs.get(\"position_ids\")\n\n    if self.use_all_token_embeddings:\n        text_outputs = self.model.text_model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            return_dict=True,\n        )\n        # TODO: add more options for pooling before projection\n        text_embeds = self.model.text_projection(text_outputs.last_hidden_state)\n    else:\n        text_embeds = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            return_dict=True,\n        ).text_embeds\n\n    return (text_embeds,)\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.HFCLIPVisionEncoder","title":"HFCLIPVisionEncoder","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper around the <code>CLIPVisionModel</code> from HuggingFace.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>The huggingface model name or a local path from which to load the model.</p> required <code>pretrained</code> <code>bool</code> <p>Whether to load the pretrained weights or not.</p> <code>True</code> <code>pooling_layer</code> <code>Optional[Module]</code> <p>Pooling layer to apply to the last hidden state of the model.</p> <code>None</code> <code>freeze_layers</code> <code>Union[int, float, list[int], bool]</code> <p>Whether to freeze layers of the model and which layers to freeze. If <code>True</code>, all model layers are frozen. If it is an integer, the first <code>N</code> layers of the model are frozen. If it is a float, the first <code>N</code> percent of the layers are frozen. If it is a list of integers, the layers at the indices in the list are frozen.</p> <code>False</code> <code>freeze_layer_norm</code> <code>bool</code> <p>Whether to freeze the layer normalization layers of the model.</p> <code>True</code> <code>patch_dropout_rate</code> <code>float</code> <p>The proportion of patch embeddings to drop out.</p> <code>0.0</code> <code>patch_dropout_shuffle</code> <code>bool</code> <p>Whether to shuffle the patches while applying patch dropout.</p> <code>False</code> <code>patch_dropout_bias</code> <code>Optional[float]</code> <p>The bias to apply to the patch dropout mask.</p> <code>None</code> <code>peft_config</code> <code>Optional[PeftConfig]</code> <p>The configuration from the <code>peft &lt;https://huggingface.co/docs/peft/index&gt;</code>_ library to use to wrap the model for parameter-efficient finetuning.</p> <code>None</code> <code>model_config_kwargs</code> <code>Optional[dict[str, Any]]</code> <p>Additional keyword arguments to pass to the model configuration.</p> <code>None</code> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If both <code>peft_config</code> and <code>freeze_layers</code> are set. The <code>peft_config</code> will override the <code>freeze_layers</code> setting.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>@store(\n    group=\"modules/encoders\",\n    provider=\"mmlearn\",\n    model_name_or_path=\"openai/clip-vit-base-patch16\",\n    hydra_convert=\"object\",\n)\nclass HFCLIPVisionEncoder(nn.Module):\n    \"\"\"Wrapper around the ``CLIPVisionModel`` from HuggingFace.\n\n    Parameters\n    ----------\n    model_name_or_path : str\n        The huggingface model name or a local path from which to load the model.\n    pretrained : bool, default=True\n        Whether to load the pretrained weights or not.\n    pooling_layer : Optional[torch.nn.Module], optional, default=None\n        Pooling layer to apply to the last hidden state of the model.\n    freeze_layers : Union[int, float, list[int], bool], default=False\n        Whether to freeze layers of the model and which layers to freeze. If ``True``,\n        all model layers are frozen. If it is an integer, the first ``N`` layers of\n        the model are frozen. If it is a float, the first ``N`` percent of the layers\n        are frozen. If it is a list of integers, the layers at the indices in the\n        list are frozen.\n    freeze_layer_norm : bool, default=True\n        Whether to freeze the layer normalization layers of the model.\n    patch_dropout_rate : float, default=0.0\n        The proportion of patch embeddings to drop out.\n    patch_dropout_shuffle : bool, default=False\n        Whether to shuffle the patches while applying patch dropout.\n    patch_dropout_bias : Optional[float], optional, default=None\n        The bias to apply to the patch dropout mask.\n    peft_config : Optional[PeftConfig], optional, default=None\n        The configuration from the `peft &lt;https://huggingface.co/docs/peft/index&gt;`_\n        library to use to wrap the model for parameter-efficient finetuning.\n    model_config_kwargs : Optional[dict[str, Any]], optional, default=None\n        Additional keyword arguments to pass to the model configuration.\n\n    Warns\n    -----\n    UserWarning\n        If both ``peft_config`` and ``freeze_layers`` are set. The ``peft_config``\n        will override the ``freeze_layers`` setting.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name_or_path: str,\n        pretrained: bool = True,\n        pooling_layer: Optional[nn.Module] = None,\n        freeze_layers: Union[int, float, list[int], bool] = False,\n        freeze_layer_norm: bool = True,\n        patch_dropout_rate: float = 0.0,\n        patch_dropout_shuffle: bool = False,\n        patch_dropout_bias: Optional[float] = None,\n        peft_config: Optional[\"PeftConfig\"] = None,\n        model_config_kwargs: Optional[dict[str, Any]] = None,\n    ) -&gt; None:\n        super().__init__()\n        _warn_freeze_with_peft(peft_config, freeze_layers)\n\n        model = hf_utils.load_huggingface_model(\n            transformers.CLIPVisionModel,\n            model_name_or_path=model_name_or_path,\n            load_pretrained_weights=pretrained,\n            model_config_kwargs=model_config_kwargs,\n        )\n        model = _freeze_vision_model(model, freeze_layers, freeze_layer_norm)\n        if peft_config is not None:\n            model = hf_utils._wrap_peft_model(model, peft_config)\n\n        self.model = model.vision_model\n        self.pooling_layer = pooling_layer\n        self.patch_dropout = None\n        if patch_dropout_rate &gt; 0:\n            self.patch_dropout = PatchDropout(\n                keep_rate=1 - patch_dropout_rate,\n                token_shuffling=patch_dropout_shuffle,\n                bias=patch_dropout_bias,\n            )\n\n    def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The image tensor will be expected under the\n            ``Modalities.RGB`` key.\n\n        Returns\n        -------\n        BaseModelOutput\n            The output of the model, including the last hidden state, all hidden states,\n            and the attention weights, if ``output_attentions`` is set to ``True``.\n\n        \"\"\"\n        # FIXME: handle other vision modalities\n        pixel_values = inputs[Modalities.RGB.name]\n        hidden_states = self.model.embeddings(pixel_values)\n        if self.patch_dropout is not None:\n            hidden_states = self.patch_dropout(hidden_states)\n        hidden_states = self.model.pre_layrnorm(hidden_states)\n\n        encoder_outputs = self.model.encoder(\n            inputs_embeds=hidden_states,\n            output_attentions=inputs.get(\n                \"output_attentions\", self.model.config.output_attentions\n            ),\n            output_hidden_states=True,\n            return_dict=True,\n        )\n\n        last_hidden_state = encoder_outputs[0]\n        if self.pooling_layer is not None:\n            last_hidden_state = self.pooling_layer(last_hidden_state)\n\n        return BaseModelOutput(\n            last_hidden_state=last_hidden_state,\n            hidden_states=encoder_outputs.hidden_states,\n            attentions=encoder_outputs.attentions,\n        )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.HFCLIPVisionEncoder.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The image tensor will be expected under the <code>Modalities.RGB</code> key.</p> required <p>Returns:</p> Type Description <code>BaseModelOutput</code> <p>The output of the model, including the last hidden state, all hidden states, and the attention weights, if <code>output_attentions</code> is set to <code>True</code>.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The image tensor will be expected under the\n        ``Modalities.RGB`` key.\n\n    Returns\n    -------\n    BaseModelOutput\n        The output of the model, including the last hidden state, all hidden states,\n        and the attention weights, if ``output_attentions`` is set to ``True``.\n\n    \"\"\"\n    # FIXME: handle other vision modalities\n    pixel_values = inputs[Modalities.RGB.name]\n    hidden_states = self.model.embeddings(pixel_values)\n    if self.patch_dropout is not None:\n        hidden_states = self.patch_dropout(hidden_states)\n    hidden_states = self.model.pre_layrnorm(hidden_states)\n\n    encoder_outputs = self.model.encoder(\n        inputs_embeds=hidden_states,\n        output_attentions=inputs.get(\n            \"output_attentions\", self.model.config.output_attentions\n        ),\n        output_hidden_states=True,\n        return_dict=True,\n    )\n\n    last_hidden_state = encoder_outputs[0]\n    if self.pooling_layer is not None:\n        last_hidden_state = self.pooling_layer(last_hidden_state)\n\n    return BaseModelOutput(\n        last_hidden_state=last_hidden_state,\n        hidden_states=encoder_outputs.hidden_states,\n        attentions=encoder_outputs.attentions,\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.HFCLIPVisionEncoderWithProjection","title":"HFCLIPVisionEncoderWithProjection","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper around the <code>CLIPVisionModelWithProjection</code> class from HuggingFace.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>The huggingface model name or a local path from which to load the model.</p> required <code>pretrained</code> <code>bool</code> <p>Whether to load the pretrained weights or not.</p> <code>True</code> <code>use_all_token_embeddings</code> <code>bool</code> <p>Whether to use all token embeddings for the text. If <code>False</code> the first token embedding will be used.</p> <code>False</code> <code>freeze_layers</code> <code>Union[int, float, list[int], bool]</code> <p>Whether to freeze layers of the model and which layers to freeze. If <code>True</code>, all model layers are frozen. If it is an integer, the first <code>N` layers of the model are frozen. If it is a float, the first</code>N`` percent of the layers are frozen. If it is a list of integers, the layers at the indices in the list are frozen.</p> <code>False</code> <code>freeze_layer_norm</code> <code>bool</code> <p>Whether to freeze the layer normalization layers of the model.</p> <code>True</code> <code>patch_dropout_rate</code> <code>float</code> <p>The proportion of patch embeddings to drop out.</p> <code>0.0</code> <code>patch_dropout_shuffle</code> <code>bool</code> <p>Whether to shuffle the patches while applying patch dropout.</p> <code>False</code> <code>patch_dropout_bias</code> <code>float</code> <p>The bias to apply to the patch dropout mask.</p> <code>None</code> <code>peft_config</code> <code>Optional[PeftConfig]</code> <p>The configuration from the <code>peft &lt;https://huggingface.co/docs/peft/index&gt;</code>_ library to use to wrap the model for parameter-efficient finetuning.</p> <code>None</code> <code>model_config_kwargs</code> <code>dict[str, Any]</code> <p>Additional keyword arguments to pass to the model configuration.</p> <code>None</code> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If both <code>peft_config</code> and <code>freeze_layers</code> are set. The <code>peft_config</code> will override the <code>freeze_layers</code> setting.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>@store(\n    group=\"modules/encoders\",\n    provider=\"mmlearn\",\n    model_name_or_path=\"openai/clip-vit-base-patch16\",\n    hydra_convert=\"object\",\n)\nclass HFCLIPVisionEncoderWithProjection(nn.Module):\n    \"\"\"Wrapper around the ``CLIPVisionModelWithProjection`` class from HuggingFace.\n\n    Parameters\n    ----------\n    model_name_or_path : str\n        The huggingface model name or a local path from which to load the model.\n    pretrained : bool, default=True\n        Whether to load the pretrained weights or not.\n    use_all_token_embeddings : bool, default=False\n        Whether to use all token embeddings for the text. If ``False`` the first token\n        embedding will be used.\n    freeze_layers : Union[int, float, list[int], bool], default=False\n        Whether to freeze layers of the model and which layers to freeze. If ``True``,\n        all model layers are frozen. If it is an integer, the first ``N` layers of\n        the model are frozen. If it is a float, the first ``N`` percent of the layers\n        are frozen. If it is a list of integers, the layers at the indices in the\n        list are frozen.\n    freeze_layer_norm : bool, default=True\n        Whether to freeze the layer normalization layers of the model.\n    patch_dropout_rate : float, default=0.0\n        The proportion of patch embeddings to drop out.\n    patch_dropout_shuffle : bool, default=False\n        Whether to shuffle the patches while applying patch dropout.\n    patch_dropout_bias : float, optional, default=None\n        The bias to apply to the patch dropout mask.\n    peft_config : Optional[PeftConfig], optional, default=None\n        The configuration from the `peft &lt;https://huggingface.co/docs/peft/index&gt;`_\n        library to use to wrap the model for parameter-efficient finetuning.\n    model_config_kwargs : dict[str, Any], optional, default=None\n        Additional keyword arguments to pass to the model configuration.\n\n    Warns\n    -----\n    UserWarning\n        If both ``peft_config`` and ``freeze_layers`` are set. The ``peft_config``\n        will override the ``freeze_layers`` setting.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name_or_path: str,\n        pretrained: bool = True,\n        use_all_token_embeddings: bool = False,\n        patch_dropout_rate: float = 0.0,\n        patch_dropout_shuffle: bool = False,\n        patch_dropout_bias: Optional[float] = None,\n        freeze_layers: Union[int, float, list[int], bool] = False,\n        freeze_layer_norm: bool = True,\n        peft_config: Optional[\"PeftConfig\"] = None,\n        model_config_kwargs: Optional[dict[str, Any]] = None,\n    ) -&gt; None:\n        super().__init__()\n        _warn_freeze_with_peft(peft_config, freeze_layers)\n\n        self.use_all_token_embeddings = use_all_token_embeddings\n\n        model = hf_utils.load_huggingface_model(\n            transformers.CLIPVisionModelWithProjection,\n            model_name_or_path,\n            load_pretrained_weights=pretrained,\n            model_config_kwargs=model_config_kwargs,\n            config_type=CLIPVisionConfig,\n        )\n\n        model = _freeze_vision_model(model, freeze_layers, freeze_layer_norm)\n        if peft_config is not None:\n            model = hf_utils._wrap_peft_model(model, peft_config)\n\n        self.model = model\n        self.patch_dropout = None\n        if patch_dropout_rate &gt; 0:\n            self.patch_dropout = PatchDropout(\n                keep_rate=1 - patch_dropout_rate,\n                token_shuffling=patch_dropout_shuffle,\n                bias=patch_dropout_bias,\n            )\n\n    def forward(self, inputs: dict[str, Any]) -&gt; tuple[torch.Tensor]:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The image tensor will be expected under the\n            ``Modalities.RGB`` key.\n\n        Returns\n        -------\n        tuple[torch.Tensor]\n            The image embeddings. Will be a tuple with a single element.\n        \"\"\"\n        pixel_values = inputs[Modalities.RGB.name]\n        hidden_states = self.model.vision_model.embeddings(pixel_values)\n        if self.patch_dropout is not None:\n            hidden_states = self.patch_dropout(hidden_states)\n        hidden_states = self.model.vision_model.pre_layrnorm(hidden_states)\n\n        encoder_outputs = self.model.vision_model.encoder(\n            inputs_embeds=hidden_states, return_dict=True\n        )\n\n        last_hidden_state = encoder_outputs.last_hidden_state\n        if self.use_all_token_embeddings:\n            pooled_output = last_hidden_state\n        else:\n            pooled_output = last_hidden_state[:, 0, :]\n        pooled_output = self.model.vision_model.post_layernorm(pooled_output)\n\n        return (self.model.visual_projection(pooled_output),)\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.HFCLIPVisionEncoderWithProjection.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The image tensor will be expected under the <code>Modalities.RGB</code> key.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor]</code> <p>The image embeddings. Will be a tuple with a single element.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; tuple[torch.Tensor]:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The image tensor will be expected under the\n        ``Modalities.RGB`` key.\n\n    Returns\n    -------\n    tuple[torch.Tensor]\n        The image embeddings. Will be a tuple with a single element.\n    \"\"\"\n    pixel_values = inputs[Modalities.RGB.name]\n    hidden_states = self.model.vision_model.embeddings(pixel_values)\n    if self.patch_dropout is not None:\n        hidden_states = self.patch_dropout(hidden_states)\n    hidden_states = self.model.vision_model.pre_layrnorm(hidden_states)\n\n    encoder_outputs = self.model.vision_model.encoder(\n        inputs_embeds=hidden_states, return_dict=True\n    )\n\n    last_hidden_state = encoder_outputs.last_hidden_state\n    if self.use_all_token_embeddings:\n        pooled_output = last_hidden_state\n    else:\n        pooled_output = last_hidden_state[:, 0, :]\n    pooled_output = self.model.vision_model.post_layernorm(pooled_output)\n\n    return (self.model.visual_projection(pooled_output),)\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.HFTextEncoder","title":"HFTextEncoder","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper around huggingface models in the <code>AutoModelForTextEncoding</code> class.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>The huggingface model name or a local path from which to load the model.</p> required <code>pretrained</code> <code>bool</code> <p>Whether to load the pretrained weights or not.</p> <code>True</code> <code>pooling_layer</code> <code>Optional[Module]</code> <p>Pooling layer to apply to the last hidden state of the model.</p> <code>None</code> <code>freeze_layers</code> <code>Union[int, float, list[int], bool]</code> <p>Whether to freeze layers of the model and which layers to freeze. If <code>True</code>, all model layers are frozen. If it is an integer, the first <code>N</code> layers of the model are frozen. If it is a float, the first <code>N</code> percent of the layers are frozen. If it is a list of integers, the layers at the indices in the list are frozen.</p> <code>False</code> <code>freeze_layer_norm</code> <code>bool</code> <p>Whether to freeze the layer normalization layers of the model.</p> <code>True</code> <code>peft_config</code> <code>Optional[PeftConfig]</code> <p>The configuration from the <code>peft &lt;https://huggingface.co/docs/peft/index&gt;</code>_ library to use to wrap the model for parameter-efficient finetuning.</p> <code>None</code> <code>model_config_kwargs</code> <code>Optional[dict[str, Any]]</code> <p>Additional keyword arguments to pass to the model configuration.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model is a decoder model or if freezing individual layers is not supported for the model type.</p> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If both <code>peft_config</code> and <code>freeze_layers</code> are set. The <code>peft_config</code> will override the <code>freeze_layers</code> setting.</p> Source code in <code>mmlearn/modules/encoders/text.py</code> <pre><code>@store(group=\"modules/encoders\", provider=\"mmlearn\", hydra_convert=\"object\")\nclass HFTextEncoder(nn.Module):\n    \"\"\"Wrapper around huggingface models in the ``AutoModelForTextEncoding`` class.\n\n    Parameters\n    ----------\n    model_name_or_path : str\n        The huggingface model name or a local path from which to load the model.\n    pretrained : bool, default=True\n        Whether to load the pretrained weights or not.\n    pooling_layer : Optional[torch.nn.Module], optional, default=None\n        Pooling layer to apply to the last hidden state of the model.\n    freeze_layers : Union[int, float, list[int], bool], default=False\n        Whether to freeze layers of the model and which layers to freeze. If ``True``,\n        all model layers are frozen. If it is an integer, the first ``N`` layers of\n        the model are frozen. If it is a float, the first ``N`` percent of the layers\n        are frozen. If it is a list of integers, the layers at the indices in the\n        list are frozen.\n    freeze_layer_norm : bool, default=True\n        Whether to freeze the layer normalization layers of the model.\n    peft_config : Optional[PeftConfig], optional, default=None\n        The configuration from the `peft &lt;https://huggingface.co/docs/peft/index&gt;`_\n        library to use to wrap the model for parameter-efficient finetuning.\n    model_config_kwargs : Optional[dict[str, Any]], optional, default=None\n        Additional keyword arguments to pass to the model configuration.\n\n    Raises\n    ------\n    ValueError\n        If the model is a decoder model or if freezing individual layers is not\n        supported for the model type.\n\n    Warns\n    -----\n    UserWarning\n        If both ``peft_config`` and ``freeze_layers`` are set. The ``peft_config``\n        will override the ``freeze_layers`` setting.\n\n\n    \"\"\"\n\n    def __init__(  # noqa: PLR0912\n        self,\n        model_name_or_path: str,\n        pretrained: bool = True,\n        pooling_layer: Optional[nn.Module] = None,\n        freeze_layers: Union[int, float, list[int], bool] = False,\n        freeze_layer_norm: bool = True,\n        peft_config: Optional[\"PeftConfig\"] = None,\n        model_config_kwargs: Optional[dict[str, Any]] = None,\n    ):\n        super().__init__()\n        if model_config_kwargs is None:\n            model_config_kwargs = {}\n        model_config_kwargs[\"output_hidden_states\"] = True\n        model_config_kwargs[\"add_pooling_layer\"] = False\n        model = hf_utils.load_huggingface_model(\n            AutoModelForTextEncoding,\n            model_name_or_path,\n            load_pretrained_weights=pretrained,\n            model_config_kwargs=model_config_kwargs,\n        )\n        if hasattr(model.config, \"is_decoder\") and model.config.is_decoder:\n            raise ValueError(\"Model is a decoder. Only encoder models are supported.\")\n\n        if not pretrained and freeze_layers:\n            rank_zero_warn(\n                \"Freezing layers when loading a model with random weights may lead to \"\n                \"unexpected behavior. Consider setting `freeze_layers=False` if \"\n                \"`pretrained=False`.\",\n            )\n\n        if isinstance(freeze_layers, bool) and freeze_layers:\n            for name, param in model.named_parameters():\n                param.requires_grad = (\n                    (not freeze_layer_norm) if \"LayerNorm\" in name else False\n                )\n\n        if isinstance(\n            freeze_layers, (float, int, list)\n        ) and model.config.model_type in [\"flaubert\", \"xlm\"]:\n            # flaubert and xlm models have a different architecture that does not\n            # support freezing individual layers in the same way as other models\n            raise ValueError(\n                f\"Freezing individual layers is not supported for {model.config.model_type} \"\n                \"models. Please use `freeze_layers=False` or `freeze_layers=True`.\"\n            )\n\n        # get list of layers\n        embeddings = model.embeddings\n        encoder = getattr(model, \"encoder\", None) or getattr(\n            model, \"transformer\", model\n        )\n        encoder_layers = (\n            getattr(encoder, \"layer\", None)\n            or getattr(encoder, \"layers\", None)\n            or getattr(encoder, \"block\", None)\n        )\n        if encoder_layers is None and hasattr(encoder, \"albert_layer_groups\"):\n            encoder_layers = [\n                layer\n                for group in encoder.albert_layer_groups\n                for layer in group.albert_layers\n            ]\n        modules = [embeddings]\n        if encoder_layers is not None and isinstance(encoder_layers, list):\n            modules.extend(encoder_layers)\n\n        if isinstance(freeze_layers, float):\n            freeze_layers = int(freeze_layers * len(modules))\n        if isinstance(freeze_layers, int):\n            freeze_layers = list(range(freeze_layers))\n\n        if isinstance(freeze_layers, list):\n            for idx, module in enumerate(modules):\n                if idx in freeze_layers:\n                    for name, param in module.named_parameters():\n                        param.requires_grad = (\n                            (not freeze_layer_norm) if \"LayerNorm\" in name else False\n                        )\n\n        if peft_config is not None:\n            model = hf_utils._wrap_peft_model(model, peft_config)\n\n        self.model = model\n        self.pooling_layer = pooling_layer\n\n    def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The ``input_ids`` will be expected under the\n            ``Modalities.TEXT`` key.\n\n        Returns\n        -------\n        BaseModelOutput\n            The output of the model, including the last hidden state, all hidden states,\n            and the attention weights, if ``output_attentions`` is set to ``True``.\n        \"\"\"\n        outputs = self.model(\n            input_ids=inputs[Modalities.TEXT.name],\n            attention_mask=inputs.get(\n                \"attention_mask\", inputs.get(Modalities.TEXT.attention_mask, None)\n            ),\n            position_ids=inputs.get(\"position_ids\"),\n            output_attentions=inputs.get(\"output_attentions\"),\n            return_dict=True,\n        )\n        last_hidden_state = outputs.hidden_states[-1]  # NOTE: no layer norm applied\n        if self.pooling_layer:\n            last_hidden_state = self.pooling_layer(last_hidden_state)\n\n        return BaseModelOutput(\n            last_hidden_state=last_hidden_state,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.HFTextEncoder.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The <code>input_ids</code> will be expected under the <code>Modalities.TEXT</code> key.</p> required <p>Returns:</p> Type Description <code>BaseModelOutput</code> <p>The output of the model, including the last hidden state, all hidden states, and the attention weights, if <code>output_attentions</code> is set to <code>True</code>.</p> Source code in <code>mmlearn/modules/encoders/text.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The ``input_ids`` will be expected under the\n        ``Modalities.TEXT`` key.\n\n    Returns\n    -------\n    BaseModelOutput\n        The output of the model, including the last hidden state, all hidden states,\n        and the attention weights, if ``output_attentions`` is set to ``True``.\n    \"\"\"\n    outputs = self.model(\n        input_ids=inputs[Modalities.TEXT.name],\n        attention_mask=inputs.get(\n            \"attention_mask\", inputs.get(Modalities.TEXT.attention_mask, None)\n        ),\n        position_ids=inputs.get(\"position_ids\"),\n        output_attentions=inputs.get(\"output_attentions\"),\n        return_dict=True,\n    )\n    last_hidden_state = outputs.hidden_states[-1]  # NOTE: no layer norm applied\n    if self.pooling_layer:\n        last_hidden_state = self.pooling_layer(last_hidden_state)\n\n    return BaseModelOutput(\n        last_hidden_state=last_hidden_state,\n        hidden_states=outputs.hidden_states,\n        attentions=outputs.attentions,\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.TimmViT","title":"TimmViT","text":"<p>               Bases: <code>Module</code></p> <p>Vision Transformer model from timm.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the model to use.</p> required <code>modality</code> <code>str</code> <p>The modality of the input data. This allows this model to be used with different image modalities e.g. RGB, Depth, etc.</p> <code>\"RGB\"</code> <code>projection_dim</code> <code>int</code> <p>The dimension of the projection head.</p> <code>768</code> <code>pretrained</code> <code>bool</code> <p>Whether to use the pretrained weights.</p> <code>True</code> <code>freeze_layers</code> <code>Union[int, float, list[int], bool]</code> <p>Whether to freeze the layers.</p> <code>False</code> <code>freeze_layer_norm</code> <code>bool</code> <p>Whether to freeze the layer norm.</p> <code>True</code> <code>peft_config</code> <code>Optional[PeftConfig]</code> <p>The configuration from the <code>peft &lt;https://huggingface.co/docs/peft/index&gt;</code>_ library to use to wrap the model for parameter-efficient finetuning.</p> <code>None</code> <code>model_kwargs</code> <code>Optional[dict[str, Any]]</code> <p>Additional keyword arguments for the model.</p> <code>None</code> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>@store(\n    group=\"modules/encoders\",\n    provider=\"mmlearn\",\n    model_name=\"vit_base_patch16_224\",\n    hydra_convert=\"object\",\n)\nclass TimmViT(nn.Module):\n    \"\"\"Vision Transformer model from timm.\n\n    Parameters\n    ----------\n    model_name : str\n        The name of the model to use.\n    modality : str, default=\"RGB\"\n        The modality of the input data. This allows this model to be used with different\n        image modalities e.g. RGB, Depth, etc.\n    projection_dim : int, default=768\n        The dimension of the projection head.\n    pretrained : bool, default=True\n        Whether to use the pretrained weights.\n    freeze_layers : Union[int, float, list[int], bool], default=False\n        Whether to freeze the layers.\n    freeze_layer_norm : bool, default=True\n        Whether to freeze the layer norm.\n    peft_config : Optional[PeftConfig], optional, default=None\n        The configuration from the `peft &lt;https://huggingface.co/docs/peft/index&gt;`_\n        library to use to wrap the model for parameter-efficient finetuning.\n    model_kwargs : Optional[dict[str, Any]], default=None\n        Additional keyword arguments for the model.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str,\n        modality: str = \"RGB\",\n        projection_dim: int = 768,\n        pretrained: bool = True,\n        freeze_layers: Union[int, float, list[int], bool] = False,\n        freeze_layer_norm: bool = True,\n        peft_config: Optional[\"PeftConfig\"] = None,\n        model_kwargs: Optional[dict[str, Any]] = None,\n    ) -&gt; None:\n        super().__init__()\n        self.modality = Modalities.get_modality(modality)\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        self.model: TimmVisionTransformer = timm.create_model(\n            model_name,\n            pretrained=pretrained,\n            num_classes=projection_dim,\n            **model_kwargs,\n        )\n        assert isinstance(self.model, TimmVisionTransformer), (\n            f\"Model {model_name} is not a Vision Transformer. \"\n            \"Please provide a model name that corresponds to a Vision Transformer.\"\n        )\n\n        self._freeze_layers(freeze_layers, freeze_layer_norm)\n\n        if peft_config is not None:\n            self.model = hf_utils._wrap_peft_model(self.model, peft_config)\n\n    def _freeze_layers(\n        self, freeze_layers: Union[int, float, list[int], bool], freeze_layer_norm: bool\n    ) -&gt; None:\n        \"\"\"Freeze the layers of the model.\n\n        Parameters\n        ----------\n        freeze_layers : Union[int, float, list[int], bool]\n            Whether to freeze the layers.\n        freeze_layer_norm : bool\n            Whether to freeze the layer norm.\n        \"\"\"\n        if isinstance(freeze_layers, bool) and freeze_layers:\n            for name, param in self.model.named_parameters():\n                param.requires_grad = (\n                    (not freeze_layer_norm) if \"norm\" in name else False\n                )\n\n        modules = [self.model.patch_embed, *self.model.blocks, self.model.norm]\n        if isinstance(freeze_layers, float):\n            freeze_layers = int(freeze_layers * len(modules))\n        if isinstance(freeze_layers, int):\n            freeze_layers = list(range(freeze_layers))\n\n        if isinstance(freeze_layers, list):\n            for idx, module in enumerate(modules):\n                if idx in freeze_layers:\n                    for name, param in module.named_parameters():\n                        param.requires_grad = (\n                            (not freeze_layer_norm) if \"norm\" in name else False\n                        )\n\n    def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The ``image`` will be expected under the\n            ``Modalities.RGB`` key.\n\n        Returns\n        -------\n        BaseModelOutput\n            The output of the model.\n        \"\"\"\n        x = inputs[self.modality.name]\n        last_hidden_state, hidden_states = self.model.forward_intermediates(\n            x, output_fmt=\"NLC\"\n        )\n        last_hidden_state = self.model.forward_head(last_hidden_state)\n\n        return BaseModelOutput(\n            last_hidden_state=last_hidden_state, hidden_states=hidden_states\n        )\n\n    def get_intermediate_layers(\n        self, inputs: dict[str, Any], n: int = 1\n    ) -&gt; list[torch.Tensor]:\n        \"\"\"Get the output of the intermediate layers.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The ``image`` will be expected under the ``Modalities.RGB``\n            key.\n        n : int, default=1\n            The number of intermediate layers to return.\n\n        Returns\n        -------\n        list[torch.Tensor]\n            The outputs of the last n intermediate layers.\n        \"\"\"\n        return self.model.get_intermediate_layers(inputs[Modalities.RGB], n)  # type: ignore\n\n    def get_patch_info(self) -&gt; tuple[int, int]:\n        \"\"\"Get patch size and number of patches.\n\n        Returns\n        -------\n        tuple[int, int]\n            Patch size and number of patches.\n        \"\"\"\n        patch_size = self.model.patch_embed.patch_size[0]\n        num_patches = self.model.patch_embed.num_patches\n        return patch_size, num_patches\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.TimmViT.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The <code>image</code> will be expected under the <code>Modalities.RGB</code> key.</p> required <p>Returns:</p> Type Description <code>BaseModelOutput</code> <p>The output of the model.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The ``image`` will be expected under the\n        ``Modalities.RGB`` key.\n\n    Returns\n    -------\n    BaseModelOutput\n        The output of the model.\n    \"\"\"\n    x = inputs[self.modality.name]\n    last_hidden_state, hidden_states = self.model.forward_intermediates(\n        x, output_fmt=\"NLC\"\n    )\n    last_hidden_state = self.model.forward_head(last_hidden_state)\n\n    return BaseModelOutput(\n        last_hidden_state=last_hidden_state, hidden_states=hidden_states\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.TimmViT.get_intermediate_layers","title":"get_intermediate_layers","text":"<pre><code>get_intermediate_layers(inputs, n=1)\n</code></pre> <p>Get the output of the intermediate layers.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The <code>image</code> will be expected under the <code>Modalities.RGB</code> key.</p> required <code>n</code> <code>int</code> <p>The number of intermediate layers to return.</p> <code>1</code> <p>Returns:</p> Type Description <code>list[Tensor]</code> <p>The outputs of the last n intermediate layers.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>def get_intermediate_layers(\n    self, inputs: dict[str, Any], n: int = 1\n) -&gt; list[torch.Tensor]:\n    \"\"\"Get the output of the intermediate layers.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The ``image`` will be expected under the ``Modalities.RGB``\n        key.\n    n : int, default=1\n        The number of intermediate layers to return.\n\n    Returns\n    -------\n    list[torch.Tensor]\n        The outputs of the last n intermediate layers.\n    \"\"\"\n    return self.model.get_intermediate_layers(inputs[Modalities.RGB], n)  # type: ignore\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.TimmViT.get_patch_info","title":"get_patch_info","text":"<pre><code>get_patch_info()\n</code></pre> <p>Get patch size and number of patches.</p> <p>Returns:</p> Type Description <code>tuple[int, int]</code> <p>Patch size and number of patches.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>def get_patch_info(self) -&gt; tuple[int, int]:\n    \"\"\"Get patch size and number of patches.\n\n    Returns\n    -------\n    tuple[int, int]\n        Patch size and number of patches.\n    \"\"\"\n    patch_size = self.model.patch_embed.patch_size[0]\n    num_patches = self.model.patch_embed.num_patches\n    return patch_size, num_patches\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.clip","title":"clip","text":"<p>Wrappers and interfaces for CLIP models.</p>"},{"location":"api/#mmlearn.modules.encoders.clip.HFCLIPTextEncoder","title":"HFCLIPTextEncoder","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper around the <code>CLIPTextModel</code> from HuggingFace.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>The huggingface model name or a local path from which to load the model.</p> required <code>pretrained</code> <code>bool</code> <p>Whether to load the pretrained weights or not.</p> <code>True</code> <code>pooling_layer</code> <code>Optional[Module]</code> <p>Pooling layer to apply to the last hidden state of the model.</p> <code>None</code> <code>freeze_layers</code> <code>Union[int, float, list[int], bool]</code> <p>Whether to freeze layers of the model and which layers to freeze. If <code>True</code>, all model layers are frozen. If it is an integer, the first <code>N</code> layers of the model are frozen. If it is a float, the first <code>N</code> percent of the layers are frozen. If it is a list of integers, the layers at the indices in the list are frozen.</p> <code>False</code> <code>freeze_layer_norm</code> <code>bool</code> <p>Whether to freeze the layer normalization layers of the model.</p> <code>True</code> <code>peft_config</code> <code>Optional[PeftConfig]</code> <p>The configuration from the <code>peft &lt;https://huggingface.co/docs/peft/index&gt;</code>_ library to use to wrap the model for parameter-efficient finetuning.</p> <code>None</code> <code>model_config_kwargs</code> <code>Optional[dict[str, Any]]</code> <p>Additional keyword arguments to pass to the model configuration.</p> <code>None</code> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If both <code>peft_config</code> and <code>freeze_layers</code> are set. The <code>peft_config</code> will override the <code>freeze_layers</code> setting.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>@store(\n    group=\"modules/encoders\",\n    provider=\"mmlearn\",\n    model_name_or_path=\"openai/clip-vit-base-patch16\",\n    hydra_convert=\"object\",  # required for `peft_config` to be converted to a `PeftConfig` object\n)\nclass HFCLIPTextEncoder(nn.Module):\n    \"\"\"Wrapper around the ``CLIPTextModel`` from HuggingFace.\n\n    Parameters\n    ----------\n    model_name_or_path : str\n        The huggingface model name or a local path from which to load the model.\n    pretrained : bool, default=True\n        Whether to load the pretrained weights or not.\n    pooling_layer : Optional[torch.nn.Module], optional, default=None\n        Pooling layer to apply to the last hidden state of the model.\n    freeze_layers : Union[int, float, list[int], bool], default=False\n        Whether to freeze layers of the model and which layers to freeze. If ``True``,\n        all model layers are frozen. If it is an integer, the first ``N`` layers of\n        the model are frozen. If it is a float, the first ``N`` percent of the layers\n        are frozen. If it is a list of integers, the layers at the indices in the\n        list are frozen.\n    freeze_layer_norm : bool, default=True\n        Whether to freeze the layer normalization layers of the model.\n    peft_config : Optional[PeftConfig], optional, default=None\n        The configuration from the `peft &lt;https://huggingface.co/docs/peft/index&gt;`_\n        library to use to wrap the model for parameter-efficient finetuning.\n    model_config_kwargs : Optional[dict[str, Any]], optional, default=None\n        Additional keyword arguments to pass to the model configuration.\n\n    Warns\n    -----\n    UserWarning\n        If both ``peft_config`` and ``freeze_layers`` are set. The ``peft_config``\n        will override the ``freeze_layers`` setting.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name_or_path: str,\n        pretrained: bool = True,\n        pooling_layer: Optional[nn.Module] = None,\n        freeze_layers: Union[int, float, list[int], bool] = False,\n        freeze_layer_norm: bool = True,\n        peft_config: Optional[\"PeftConfig\"] = None,\n        model_config_kwargs: Optional[dict[str, Any]] = None,\n    ) -&gt; None:\n        super().__init__()\n        _warn_freeze_with_peft(peft_config, freeze_layers)\n\n        model = hf_utils.load_huggingface_model(\n            transformers.CLIPTextModel,\n            model_name_or_path=model_name_or_path,\n            load_pretrained_weights=pretrained,\n            model_config_kwargs=model_config_kwargs,\n        )\n        model = _freeze_text_model(model, freeze_layers, freeze_layer_norm)\n        if peft_config is not None:\n            model = hf_utils._wrap_peft_model(model, peft_config)\n\n        self.model = model\n        self.pooling_layer = pooling_layer\n\n    def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The ``input_ids`` will be expected under the\n            ``Modalities.TEXT`` key.\n\n        Returns\n        -------\n        BaseModelOutput\n            The output of the model, including the last hidden state, all hidden\n            states, and the attention weights, if ``output_attentions`` is set\n            to ``True``.\n        \"\"\"\n        outputs = self.model(\n            input_ids=inputs[Modalities.TEXT.name],\n            attention_mask=inputs.get(\"attention_mask\")\n            or inputs.get(Modalities.TEXT.attention_mask),\n            position_ids=inputs.get(\"position_ids\"),\n            output_attentions=inputs.get(\"output_attentions\"),\n            return_dict=True,\n        )\n        last_hidden_state = outputs.hidden_states[-1]  # NOTE: no layer norm applied\n        if self.pooling_layer:\n            last_hidden_state = self.pooling_layer(last_hidden_state)\n\n        return BaseModelOutput(\n            last_hidden_state=last_hidden_state,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.clip.HFCLIPTextEncoder.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The <code>input_ids</code> will be expected under the <code>Modalities.TEXT</code> key.</p> required <p>Returns:</p> Type Description <code>BaseModelOutput</code> <p>The output of the model, including the last hidden state, all hidden states, and the attention weights, if <code>output_attentions</code> is set to <code>True</code>.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The ``input_ids`` will be expected under the\n        ``Modalities.TEXT`` key.\n\n    Returns\n    -------\n    BaseModelOutput\n        The output of the model, including the last hidden state, all hidden\n        states, and the attention weights, if ``output_attentions`` is set\n        to ``True``.\n    \"\"\"\n    outputs = self.model(\n        input_ids=inputs[Modalities.TEXT.name],\n        attention_mask=inputs.get(\"attention_mask\")\n        or inputs.get(Modalities.TEXT.attention_mask),\n        position_ids=inputs.get(\"position_ids\"),\n        output_attentions=inputs.get(\"output_attentions\"),\n        return_dict=True,\n    )\n    last_hidden_state = outputs.hidden_states[-1]  # NOTE: no layer norm applied\n    if self.pooling_layer:\n        last_hidden_state = self.pooling_layer(last_hidden_state)\n\n    return BaseModelOutput(\n        last_hidden_state=last_hidden_state,\n        hidden_states=outputs.hidden_states,\n        attentions=outputs.attentions,\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.clip.HFCLIPVisionEncoder","title":"HFCLIPVisionEncoder","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper around the <code>CLIPVisionModel</code> from HuggingFace.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>The huggingface model name or a local path from which to load the model.</p> required <code>pretrained</code> <code>bool</code> <p>Whether to load the pretrained weights or not.</p> <code>True</code> <code>pooling_layer</code> <code>Optional[Module]</code> <p>Pooling layer to apply to the last hidden state of the model.</p> <code>None</code> <code>freeze_layers</code> <code>Union[int, float, list[int], bool]</code> <p>Whether to freeze layers of the model and which layers to freeze. If <code>True</code>, all model layers are frozen. If it is an integer, the first <code>N</code> layers of the model are frozen. If it is a float, the first <code>N</code> percent of the layers are frozen. If it is a list of integers, the layers at the indices in the list are frozen.</p> <code>False</code> <code>freeze_layer_norm</code> <code>bool</code> <p>Whether to freeze the layer normalization layers of the model.</p> <code>True</code> <code>patch_dropout_rate</code> <code>float</code> <p>The proportion of patch embeddings to drop out.</p> <code>0.0</code> <code>patch_dropout_shuffle</code> <code>bool</code> <p>Whether to shuffle the patches while applying patch dropout.</p> <code>False</code> <code>patch_dropout_bias</code> <code>Optional[float]</code> <p>The bias to apply to the patch dropout mask.</p> <code>None</code> <code>peft_config</code> <code>Optional[PeftConfig]</code> <p>The configuration from the <code>peft &lt;https://huggingface.co/docs/peft/index&gt;</code>_ library to use to wrap the model for parameter-efficient finetuning.</p> <code>None</code> <code>model_config_kwargs</code> <code>Optional[dict[str, Any]]</code> <p>Additional keyword arguments to pass to the model configuration.</p> <code>None</code> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If both <code>peft_config</code> and <code>freeze_layers</code> are set. The <code>peft_config</code> will override the <code>freeze_layers</code> setting.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>@store(\n    group=\"modules/encoders\",\n    provider=\"mmlearn\",\n    model_name_or_path=\"openai/clip-vit-base-patch16\",\n    hydra_convert=\"object\",\n)\nclass HFCLIPVisionEncoder(nn.Module):\n    \"\"\"Wrapper around the ``CLIPVisionModel`` from HuggingFace.\n\n    Parameters\n    ----------\n    model_name_or_path : str\n        The huggingface model name or a local path from which to load the model.\n    pretrained : bool, default=True\n        Whether to load the pretrained weights or not.\n    pooling_layer : Optional[torch.nn.Module], optional, default=None\n        Pooling layer to apply to the last hidden state of the model.\n    freeze_layers : Union[int, float, list[int], bool], default=False\n        Whether to freeze layers of the model and which layers to freeze. If ``True``,\n        all model layers are frozen. If it is an integer, the first ``N`` layers of\n        the model are frozen. If it is a float, the first ``N`` percent of the layers\n        are frozen. If it is a list of integers, the layers at the indices in the\n        list are frozen.\n    freeze_layer_norm : bool, default=True\n        Whether to freeze the layer normalization layers of the model.\n    patch_dropout_rate : float, default=0.0\n        The proportion of patch embeddings to drop out.\n    patch_dropout_shuffle : bool, default=False\n        Whether to shuffle the patches while applying patch dropout.\n    patch_dropout_bias : Optional[float], optional, default=None\n        The bias to apply to the patch dropout mask.\n    peft_config : Optional[PeftConfig], optional, default=None\n        The configuration from the `peft &lt;https://huggingface.co/docs/peft/index&gt;`_\n        library to use to wrap the model for parameter-efficient finetuning.\n    model_config_kwargs : Optional[dict[str, Any]], optional, default=None\n        Additional keyword arguments to pass to the model configuration.\n\n    Warns\n    -----\n    UserWarning\n        If both ``peft_config`` and ``freeze_layers`` are set. The ``peft_config``\n        will override the ``freeze_layers`` setting.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name_or_path: str,\n        pretrained: bool = True,\n        pooling_layer: Optional[nn.Module] = None,\n        freeze_layers: Union[int, float, list[int], bool] = False,\n        freeze_layer_norm: bool = True,\n        patch_dropout_rate: float = 0.0,\n        patch_dropout_shuffle: bool = False,\n        patch_dropout_bias: Optional[float] = None,\n        peft_config: Optional[\"PeftConfig\"] = None,\n        model_config_kwargs: Optional[dict[str, Any]] = None,\n    ) -&gt; None:\n        super().__init__()\n        _warn_freeze_with_peft(peft_config, freeze_layers)\n\n        model = hf_utils.load_huggingface_model(\n            transformers.CLIPVisionModel,\n            model_name_or_path=model_name_or_path,\n            load_pretrained_weights=pretrained,\n            model_config_kwargs=model_config_kwargs,\n        )\n        model = _freeze_vision_model(model, freeze_layers, freeze_layer_norm)\n        if peft_config is not None:\n            model = hf_utils._wrap_peft_model(model, peft_config)\n\n        self.model = model.vision_model\n        self.pooling_layer = pooling_layer\n        self.patch_dropout = None\n        if patch_dropout_rate &gt; 0:\n            self.patch_dropout = PatchDropout(\n                keep_rate=1 - patch_dropout_rate,\n                token_shuffling=patch_dropout_shuffle,\n                bias=patch_dropout_bias,\n            )\n\n    def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The image tensor will be expected under the\n            ``Modalities.RGB`` key.\n\n        Returns\n        -------\n        BaseModelOutput\n            The output of the model, including the last hidden state, all hidden states,\n            and the attention weights, if ``output_attentions`` is set to ``True``.\n\n        \"\"\"\n        # FIXME: handle other vision modalities\n        pixel_values = inputs[Modalities.RGB.name]\n        hidden_states = self.model.embeddings(pixel_values)\n        if self.patch_dropout is not None:\n            hidden_states = self.patch_dropout(hidden_states)\n        hidden_states = self.model.pre_layrnorm(hidden_states)\n\n        encoder_outputs = self.model.encoder(\n            inputs_embeds=hidden_states,\n            output_attentions=inputs.get(\n                \"output_attentions\", self.model.config.output_attentions\n            ),\n            output_hidden_states=True,\n            return_dict=True,\n        )\n\n        last_hidden_state = encoder_outputs[0]\n        if self.pooling_layer is not None:\n            last_hidden_state = self.pooling_layer(last_hidden_state)\n\n        return BaseModelOutput(\n            last_hidden_state=last_hidden_state,\n            hidden_states=encoder_outputs.hidden_states,\n            attentions=encoder_outputs.attentions,\n        )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.clip.HFCLIPVisionEncoder.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The image tensor will be expected under the <code>Modalities.RGB</code> key.</p> required <p>Returns:</p> Type Description <code>BaseModelOutput</code> <p>The output of the model, including the last hidden state, all hidden states, and the attention weights, if <code>output_attentions</code> is set to <code>True</code>.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The image tensor will be expected under the\n        ``Modalities.RGB`` key.\n\n    Returns\n    -------\n    BaseModelOutput\n        The output of the model, including the last hidden state, all hidden states,\n        and the attention weights, if ``output_attentions`` is set to ``True``.\n\n    \"\"\"\n    # FIXME: handle other vision modalities\n    pixel_values = inputs[Modalities.RGB.name]\n    hidden_states = self.model.embeddings(pixel_values)\n    if self.patch_dropout is not None:\n        hidden_states = self.patch_dropout(hidden_states)\n    hidden_states = self.model.pre_layrnorm(hidden_states)\n\n    encoder_outputs = self.model.encoder(\n        inputs_embeds=hidden_states,\n        output_attentions=inputs.get(\n            \"output_attentions\", self.model.config.output_attentions\n        ),\n        output_hidden_states=True,\n        return_dict=True,\n    )\n\n    last_hidden_state = encoder_outputs[0]\n    if self.pooling_layer is not None:\n        last_hidden_state = self.pooling_layer(last_hidden_state)\n\n    return BaseModelOutput(\n        last_hidden_state=last_hidden_state,\n        hidden_states=encoder_outputs.hidden_states,\n        attentions=encoder_outputs.attentions,\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.clip.HFCLIPTextEncoderWithProjection","title":"HFCLIPTextEncoderWithProjection","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper around the <code>CLIPTextModelWithProjection</code> from HuggingFace.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>The huggingface model name or a local path from which to load the model.</p> required <code>pretrained</code> <code>bool</code> <p>Whether to load the pretrained weights or not.</p> <code>True</code> <code>use_all_token_embeddings</code> <code>bool</code> <p>Whether to use all token embeddings for the text. If <code>False</code> the first token embedding will be used.</p> <code>False</code> <code>freeze_layers</code> <code>Union[int, float, list[int], bool]</code> <p>Whether to freeze layers of the model and which layers to freeze. If <code>True</code>, all model layers are frozen. If it is an integer, the first <code>N</code> layers of the model are frozen. If it is a float, the first <code>N</code> percent of the layers are frozen. If it is a list of integers, the layers at the indices in the list are frozen.</p> <code>False</code> <code>freeze_layer_norm</code> <code>bool</code> <p>Whether to freeze the layer normalization layers of the model.</p> <code>True</code> <code>peft_config</code> <code>Optional[PeftConfig]</code> <p>The configuration from the <code>peft &lt;https://huggingface.co/docs/peft/index&gt;</code>_ library to use to wrap the model for parameter-efficient finetuning.</p> <code>None</code> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If both <code>peft_config</code> and <code>freeze_layers</code> are set. The <code>peft_config</code> will override the <code>freeze_layers</code> setting.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>@store(\n    group=\"modules/encoders\",\n    provider=\"mmlearn\",\n    model_name_or_path=\"openai/clip-vit-base-patch16\",\n    hydra_convert=\"object\",\n)\nclass HFCLIPTextEncoderWithProjection(nn.Module):\n    \"\"\"Wrapper around the ``CLIPTextModelWithProjection`` from HuggingFace.\n\n    Parameters\n    ----------\n    model_name_or_path : str\n        The huggingface model name or a local path from which to load the model.\n    pretrained : bool, default=True\n        Whether to load the pretrained weights or not.\n    use_all_token_embeddings : bool, default=False\n        Whether to use all token embeddings for the text. If ``False`` the first token\n        embedding will be used.\n    freeze_layers : Union[int, float, list[int], bool], default=False\n        Whether to freeze layers of the model and which layers to freeze. If ``True``,\n        all model layers are frozen. If it is an integer, the first ``N`` layers of\n        the model are frozen. If it is a float, the first ``N`` percent of the layers\n        are frozen. If it is a list of integers, the layers at the indices in the\n        list are frozen.\n    freeze_layer_norm : bool, default=True\n        Whether to freeze the layer normalization layers of the model.\n    peft_config : Optional[PeftConfig], optional, default=None\n        The configuration from the `peft &lt;https://huggingface.co/docs/peft/index&gt;`_\n        library to use to wrap the model for parameter-efficient finetuning.\n\n    Warns\n    -----\n    UserWarning\n        If both ``peft_config`` and ``freeze_layers`` are set. The ``peft_config``\n        will override the ``freeze_layers`` setting.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name_or_path: str,\n        pretrained: bool = True,\n        use_all_token_embeddings: bool = False,\n        freeze_layers: Union[int, float, list[int], bool] = False,\n        freeze_layer_norm: bool = True,\n        peft_config: Optional[\"PeftConfig\"] = None,\n        model_config_kwargs: Optional[dict[str, Any]] = None,\n    ) -&gt; None:\n        super().__init__()\n        _warn_freeze_with_peft(peft_config, freeze_layers)\n\n        self.use_all_token_embeddings = use_all_token_embeddings\n\n        model = hf_utils.load_huggingface_model(\n            transformers.CLIPTextModelWithProjection,\n            model_name_or_path,\n            load_pretrained_weights=pretrained,\n            model_config_kwargs=model_config_kwargs,\n            config_type=CLIPTextConfig,\n        )\n\n        model = _freeze_text_model(model, freeze_layers, freeze_layer_norm)\n        if peft_config is not None:\n            model = hf_utils._wrap_peft_model(model, peft_config)\n\n        self.model = model\n\n    def forward(self, inputs: dict[str, Any]) -&gt; tuple[torch.Tensor]:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The ``input_ids`` will be expected under the\n            ``Modalities.TEXT`` key.\n\n        Returns\n        -------\n        tuple[torch.Tensor]\n            The text embeddings. Will be a tuple with a single element.\n        \"\"\"\n        input_ids = inputs[Modalities.TEXT.name]\n        attention_mask: Optional[torch.Tensor] = inputs.get(\n            \"attention_mask\", inputs.get(Modalities.TEXT.attention_mask, None)\n        )\n        position_ids = inputs.get(\"position_ids\")\n\n        if self.use_all_token_embeddings:\n            text_outputs = self.model.text_model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                return_dict=True,\n            )\n            # TODO: add more options for pooling before projection\n            text_embeds = self.model.text_projection(text_outputs.last_hidden_state)\n        else:\n            text_embeds = self.model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                return_dict=True,\n            ).text_embeds\n\n        return (text_embeds,)\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.clip.HFCLIPTextEncoderWithProjection.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The <code>input_ids</code> will be expected under the <code>Modalities.TEXT</code> key.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor]</code> <p>The text embeddings. Will be a tuple with a single element.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; tuple[torch.Tensor]:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The ``input_ids`` will be expected under the\n        ``Modalities.TEXT`` key.\n\n    Returns\n    -------\n    tuple[torch.Tensor]\n        The text embeddings. Will be a tuple with a single element.\n    \"\"\"\n    input_ids = inputs[Modalities.TEXT.name]\n    attention_mask: Optional[torch.Tensor] = inputs.get(\n        \"attention_mask\", inputs.get(Modalities.TEXT.attention_mask, None)\n    )\n    position_ids = inputs.get(\"position_ids\")\n\n    if self.use_all_token_embeddings:\n        text_outputs = self.model.text_model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            return_dict=True,\n        )\n        # TODO: add more options for pooling before projection\n        text_embeds = self.model.text_projection(text_outputs.last_hidden_state)\n    else:\n        text_embeds = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            return_dict=True,\n        ).text_embeds\n\n    return (text_embeds,)\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.clip.HFCLIPVisionEncoderWithProjection","title":"HFCLIPVisionEncoderWithProjection","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper around the <code>CLIPVisionModelWithProjection</code> class from HuggingFace.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>The huggingface model name or a local path from which to load the model.</p> required <code>pretrained</code> <code>bool</code> <p>Whether to load the pretrained weights or not.</p> <code>True</code> <code>use_all_token_embeddings</code> <code>bool</code> <p>Whether to use all token embeddings for the text. If <code>False</code> the first token embedding will be used.</p> <code>False</code> <code>freeze_layers</code> <code>Union[int, float, list[int], bool]</code> <p>Whether to freeze layers of the model and which layers to freeze. If <code>True</code>, all model layers are frozen. If it is an integer, the first <code>N` layers of the model are frozen. If it is a float, the first</code>N`` percent of the layers are frozen. If it is a list of integers, the layers at the indices in the list are frozen.</p> <code>False</code> <code>freeze_layer_norm</code> <code>bool</code> <p>Whether to freeze the layer normalization layers of the model.</p> <code>True</code> <code>patch_dropout_rate</code> <code>float</code> <p>The proportion of patch embeddings to drop out.</p> <code>0.0</code> <code>patch_dropout_shuffle</code> <code>bool</code> <p>Whether to shuffle the patches while applying patch dropout.</p> <code>False</code> <code>patch_dropout_bias</code> <code>float</code> <p>The bias to apply to the patch dropout mask.</p> <code>None</code> <code>peft_config</code> <code>Optional[PeftConfig]</code> <p>The configuration from the <code>peft &lt;https://huggingface.co/docs/peft/index&gt;</code>_ library to use to wrap the model for parameter-efficient finetuning.</p> <code>None</code> <code>model_config_kwargs</code> <code>dict[str, Any]</code> <p>Additional keyword arguments to pass to the model configuration.</p> <code>None</code> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If both <code>peft_config</code> and <code>freeze_layers</code> are set. The <code>peft_config</code> will override the <code>freeze_layers</code> setting.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>@store(\n    group=\"modules/encoders\",\n    provider=\"mmlearn\",\n    model_name_or_path=\"openai/clip-vit-base-patch16\",\n    hydra_convert=\"object\",\n)\nclass HFCLIPVisionEncoderWithProjection(nn.Module):\n    \"\"\"Wrapper around the ``CLIPVisionModelWithProjection`` class from HuggingFace.\n\n    Parameters\n    ----------\n    model_name_or_path : str\n        The huggingface model name or a local path from which to load the model.\n    pretrained : bool, default=True\n        Whether to load the pretrained weights or not.\n    use_all_token_embeddings : bool, default=False\n        Whether to use all token embeddings for the text. If ``False`` the first token\n        embedding will be used.\n    freeze_layers : Union[int, float, list[int], bool], default=False\n        Whether to freeze layers of the model and which layers to freeze. If ``True``,\n        all model layers are frozen. If it is an integer, the first ``N` layers of\n        the model are frozen. If it is a float, the first ``N`` percent of the layers\n        are frozen. If it is a list of integers, the layers at the indices in the\n        list are frozen.\n    freeze_layer_norm : bool, default=True\n        Whether to freeze the layer normalization layers of the model.\n    patch_dropout_rate : float, default=0.0\n        The proportion of patch embeddings to drop out.\n    patch_dropout_shuffle : bool, default=False\n        Whether to shuffle the patches while applying patch dropout.\n    patch_dropout_bias : float, optional, default=None\n        The bias to apply to the patch dropout mask.\n    peft_config : Optional[PeftConfig], optional, default=None\n        The configuration from the `peft &lt;https://huggingface.co/docs/peft/index&gt;`_\n        library to use to wrap the model for parameter-efficient finetuning.\n    model_config_kwargs : dict[str, Any], optional, default=None\n        Additional keyword arguments to pass to the model configuration.\n\n    Warns\n    -----\n    UserWarning\n        If both ``peft_config`` and ``freeze_layers`` are set. The ``peft_config``\n        will override the ``freeze_layers`` setting.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name_or_path: str,\n        pretrained: bool = True,\n        use_all_token_embeddings: bool = False,\n        patch_dropout_rate: float = 0.0,\n        patch_dropout_shuffle: bool = False,\n        patch_dropout_bias: Optional[float] = None,\n        freeze_layers: Union[int, float, list[int], bool] = False,\n        freeze_layer_norm: bool = True,\n        peft_config: Optional[\"PeftConfig\"] = None,\n        model_config_kwargs: Optional[dict[str, Any]] = None,\n    ) -&gt; None:\n        super().__init__()\n        _warn_freeze_with_peft(peft_config, freeze_layers)\n\n        self.use_all_token_embeddings = use_all_token_embeddings\n\n        model = hf_utils.load_huggingface_model(\n            transformers.CLIPVisionModelWithProjection,\n            model_name_or_path,\n            load_pretrained_weights=pretrained,\n            model_config_kwargs=model_config_kwargs,\n            config_type=CLIPVisionConfig,\n        )\n\n        model = _freeze_vision_model(model, freeze_layers, freeze_layer_norm)\n        if peft_config is not None:\n            model = hf_utils._wrap_peft_model(model, peft_config)\n\n        self.model = model\n        self.patch_dropout = None\n        if patch_dropout_rate &gt; 0:\n            self.patch_dropout = PatchDropout(\n                keep_rate=1 - patch_dropout_rate,\n                token_shuffling=patch_dropout_shuffle,\n                bias=patch_dropout_bias,\n            )\n\n    def forward(self, inputs: dict[str, Any]) -&gt; tuple[torch.Tensor]:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The image tensor will be expected under the\n            ``Modalities.RGB`` key.\n\n        Returns\n        -------\n        tuple[torch.Tensor]\n            The image embeddings. Will be a tuple with a single element.\n        \"\"\"\n        pixel_values = inputs[Modalities.RGB.name]\n        hidden_states = self.model.vision_model.embeddings(pixel_values)\n        if self.patch_dropout is not None:\n            hidden_states = self.patch_dropout(hidden_states)\n        hidden_states = self.model.vision_model.pre_layrnorm(hidden_states)\n\n        encoder_outputs = self.model.vision_model.encoder(\n            inputs_embeds=hidden_states, return_dict=True\n        )\n\n        last_hidden_state = encoder_outputs.last_hidden_state\n        if self.use_all_token_embeddings:\n            pooled_output = last_hidden_state\n        else:\n            pooled_output = last_hidden_state[:, 0, :]\n        pooled_output = self.model.vision_model.post_layernorm(pooled_output)\n\n        return (self.model.visual_projection(pooled_output),)\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.clip.HFCLIPVisionEncoderWithProjection.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The image tensor will be expected under the <code>Modalities.RGB</code> key.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor]</code> <p>The image embeddings. Will be a tuple with a single element.</p> Source code in <code>mmlearn/modules/encoders/clip.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; tuple[torch.Tensor]:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The image tensor will be expected under the\n        ``Modalities.RGB`` key.\n\n    Returns\n    -------\n    tuple[torch.Tensor]\n        The image embeddings. Will be a tuple with a single element.\n    \"\"\"\n    pixel_values = inputs[Modalities.RGB.name]\n    hidden_states = self.model.vision_model.embeddings(pixel_values)\n    if self.patch_dropout is not None:\n        hidden_states = self.patch_dropout(hidden_states)\n    hidden_states = self.model.vision_model.pre_layrnorm(hidden_states)\n\n    encoder_outputs = self.model.vision_model.encoder(\n        inputs_embeds=hidden_states, return_dict=True\n    )\n\n    last_hidden_state = encoder_outputs.last_hidden_state\n    if self.use_all_token_embeddings:\n        pooled_output = last_hidden_state\n    else:\n        pooled_output = last_hidden_state[:, 0, :]\n    pooled_output = self.model.vision_model.post_layernorm(pooled_output)\n\n    return (self.model.visual_projection(pooled_output),)\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.text","title":"text","text":"<p>Huggingface text encoder model.</p>"},{"location":"api/#mmlearn.modules.encoders.text.HFTextEncoder","title":"HFTextEncoder","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper around huggingface models in the <code>AutoModelForTextEncoding</code> class.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>The huggingface model name or a local path from which to load the model.</p> required <code>pretrained</code> <code>bool</code> <p>Whether to load the pretrained weights or not.</p> <code>True</code> <code>pooling_layer</code> <code>Optional[Module]</code> <p>Pooling layer to apply to the last hidden state of the model.</p> <code>None</code> <code>freeze_layers</code> <code>Union[int, float, list[int], bool]</code> <p>Whether to freeze layers of the model and which layers to freeze. If <code>True</code>, all model layers are frozen. If it is an integer, the first <code>N</code> layers of the model are frozen. If it is a float, the first <code>N</code> percent of the layers are frozen. If it is a list of integers, the layers at the indices in the list are frozen.</p> <code>False</code> <code>freeze_layer_norm</code> <code>bool</code> <p>Whether to freeze the layer normalization layers of the model.</p> <code>True</code> <code>peft_config</code> <code>Optional[PeftConfig]</code> <p>The configuration from the <code>peft &lt;https://huggingface.co/docs/peft/index&gt;</code>_ library to use to wrap the model for parameter-efficient finetuning.</p> <code>None</code> <code>model_config_kwargs</code> <code>Optional[dict[str, Any]]</code> <p>Additional keyword arguments to pass to the model configuration.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model is a decoder model or if freezing individual layers is not supported for the model type.</p> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If both <code>peft_config</code> and <code>freeze_layers</code> are set. The <code>peft_config</code> will override the <code>freeze_layers</code> setting.</p> Source code in <code>mmlearn/modules/encoders/text.py</code> <pre><code>@store(group=\"modules/encoders\", provider=\"mmlearn\", hydra_convert=\"object\")\nclass HFTextEncoder(nn.Module):\n    \"\"\"Wrapper around huggingface models in the ``AutoModelForTextEncoding`` class.\n\n    Parameters\n    ----------\n    model_name_or_path : str\n        The huggingface model name or a local path from which to load the model.\n    pretrained : bool, default=True\n        Whether to load the pretrained weights or not.\n    pooling_layer : Optional[torch.nn.Module], optional, default=None\n        Pooling layer to apply to the last hidden state of the model.\n    freeze_layers : Union[int, float, list[int], bool], default=False\n        Whether to freeze layers of the model and which layers to freeze. If ``True``,\n        all model layers are frozen. If it is an integer, the first ``N`` layers of\n        the model are frozen. If it is a float, the first ``N`` percent of the layers\n        are frozen. If it is a list of integers, the layers at the indices in the\n        list are frozen.\n    freeze_layer_norm : bool, default=True\n        Whether to freeze the layer normalization layers of the model.\n    peft_config : Optional[PeftConfig], optional, default=None\n        The configuration from the `peft &lt;https://huggingface.co/docs/peft/index&gt;`_\n        library to use to wrap the model for parameter-efficient finetuning.\n    model_config_kwargs : Optional[dict[str, Any]], optional, default=None\n        Additional keyword arguments to pass to the model configuration.\n\n    Raises\n    ------\n    ValueError\n        If the model is a decoder model or if freezing individual layers is not\n        supported for the model type.\n\n    Warns\n    -----\n    UserWarning\n        If both ``peft_config`` and ``freeze_layers`` are set. The ``peft_config``\n        will override the ``freeze_layers`` setting.\n\n\n    \"\"\"\n\n    def __init__(  # noqa: PLR0912\n        self,\n        model_name_or_path: str,\n        pretrained: bool = True,\n        pooling_layer: Optional[nn.Module] = None,\n        freeze_layers: Union[int, float, list[int], bool] = False,\n        freeze_layer_norm: bool = True,\n        peft_config: Optional[\"PeftConfig\"] = None,\n        model_config_kwargs: Optional[dict[str, Any]] = None,\n    ):\n        super().__init__()\n        if model_config_kwargs is None:\n            model_config_kwargs = {}\n        model_config_kwargs[\"output_hidden_states\"] = True\n        model_config_kwargs[\"add_pooling_layer\"] = False\n        model = hf_utils.load_huggingface_model(\n            AutoModelForTextEncoding,\n            model_name_or_path,\n            load_pretrained_weights=pretrained,\n            model_config_kwargs=model_config_kwargs,\n        )\n        if hasattr(model.config, \"is_decoder\") and model.config.is_decoder:\n            raise ValueError(\"Model is a decoder. Only encoder models are supported.\")\n\n        if not pretrained and freeze_layers:\n            rank_zero_warn(\n                \"Freezing layers when loading a model with random weights may lead to \"\n                \"unexpected behavior. Consider setting `freeze_layers=False` if \"\n                \"`pretrained=False`.\",\n            )\n\n        if isinstance(freeze_layers, bool) and freeze_layers:\n            for name, param in model.named_parameters():\n                param.requires_grad = (\n                    (not freeze_layer_norm) if \"LayerNorm\" in name else False\n                )\n\n        if isinstance(\n            freeze_layers, (float, int, list)\n        ) and model.config.model_type in [\"flaubert\", \"xlm\"]:\n            # flaubert and xlm models have a different architecture that does not\n            # support freezing individual layers in the same way as other models\n            raise ValueError(\n                f\"Freezing individual layers is not supported for {model.config.model_type} \"\n                \"models. Please use `freeze_layers=False` or `freeze_layers=True`.\"\n            )\n\n        # get list of layers\n        embeddings = model.embeddings\n        encoder = getattr(model, \"encoder\", None) or getattr(\n            model, \"transformer\", model\n        )\n        encoder_layers = (\n            getattr(encoder, \"layer\", None)\n            or getattr(encoder, \"layers\", None)\n            or getattr(encoder, \"block\", None)\n        )\n        if encoder_layers is None and hasattr(encoder, \"albert_layer_groups\"):\n            encoder_layers = [\n                layer\n                for group in encoder.albert_layer_groups\n                for layer in group.albert_layers\n            ]\n        modules = [embeddings]\n        if encoder_layers is not None and isinstance(encoder_layers, list):\n            modules.extend(encoder_layers)\n\n        if isinstance(freeze_layers, float):\n            freeze_layers = int(freeze_layers * len(modules))\n        if isinstance(freeze_layers, int):\n            freeze_layers = list(range(freeze_layers))\n\n        if isinstance(freeze_layers, list):\n            for idx, module in enumerate(modules):\n                if idx in freeze_layers:\n                    for name, param in module.named_parameters():\n                        param.requires_grad = (\n                            (not freeze_layer_norm) if \"LayerNorm\" in name else False\n                        )\n\n        if peft_config is not None:\n            model = hf_utils._wrap_peft_model(model, peft_config)\n\n        self.model = model\n        self.pooling_layer = pooling_layer\n\n    def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The ``input_ids`` will be expected under the\n            ``Modalities.TEXT`` key.\n\n        Returns\n        -------\n        BaseModelOutput\n            The output of the model, including the last hidden state, all hidden states,\n            and the attention weights, if ``output_attentions`` is set to ``True``.\n        \"\"\"\n        outputs = self.model(\n            input_ids=inputs[Modalities.TEXT.name],\n            attention_mask=inputs.get(\n                \"attention_mask\", inputs.get(Modalities.TEXT.attention_mask, None)\n            ),\n            position_ids=inputs.get(\"position_ids\"),\n            output_attentions=inputs.get(\"output_attentions\"),\n            return_dict=True,\n        )\n        last_hidden_state = outputs.hidden_states[-1]  # NOTE: no layer norm applied\n        if self.pooling_layer:\n            last_hidden_state = self.pooling_layer(last_hidden_state)\n\n        return BaseModelOutput(\n            last_hidden_state=last_hidden_state,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.text.HFTextEncoder.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The <code>input_ids</code> will be expected under the <code>Modalities.TEXT</code> key.</p> required <p>Returns:</p> Type Description <code>BaseModelOutput</code> <p>The output of the model, including the last hidden state, all hidden states, and the attention weights, if <code>output_attentions</code> is set to <code>True</code>.</p> Source code in <code>mmlearn/modules/encoders/text.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The ``input_ids`` will be expected under the\n        ``Modalities.TEXT`` key.\n\n    Returns\n    -------\n    BaseModelOutput\n        The output of the model, including the last hidden state, all hidden states,\n        and the attention weights, if ``output_attentions`` is set to ``True``.\n    \"\"\"\n    outputs = self.model(\n        input_ids=inputs[Modalities.TEXT.name],\n        attention_mask=inputs.get(\n            \"attention_mask\", inputs.get(Modalities.TEXT.attention_mask, None)\n        ),\n        position_ids=inputs.get(\"position_ids\"),\n        output_attentions=inputs.get(\"output_attentions\"),\n        return_dict=True,\n    )\n    last_hidden_state = outputs.hidden_states[-1]  # NOTE: no layer norm applied\n    if self.pooling_layer:\n        last_hidden_state = self.pooling_layer(last_hidden_state)\n\n    return BaseModelOutput(\n        last_hidden_state=last_hidden_state,\n        hidden_states=outputs.hidden_states,\n        attentions=outputs.attentions,\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.vision","title":"vision","text":"<p>Vision encoder implementations.</p>"},{"location":"api/#mmlearn.modules.encoders.vision.TimmViT","title":"TimmViT","text":"<p>               Bases: <code>Module</code></p> <p>Vision Transformer model from timm.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the model to use.</p> required <code>modality</code> <code>str</code> <p>The modality of the input data. This allows this model to be used with different image modalities e.g. RGB, Depth, etc.</p> <code>\"RGB\"</code> <code>projection_dim</code> <code>int</code> <p>The dimension of the projection head.</p> <code>768</code> <code>pretrained</code> <code>bool</code> <p>Whether to use the pretrained weights.</p> <code>True</code> <code>freeze_layers</code> <code>Union[int, float, list[int], bool]</code> <p>Whether to freeze the layers.</p> <code>False</code> <code>freeze_layer_norm</code> <code>bool</code> <p>Whether to freeze the layer norm.</p> <code>True</code> <code>peft_config</code> <code>Optional[PeftConfig]</code> <p>The configuration from the <code>peft &lt;https://huggingface.co/docs/peft/index&gt;</code>_ library to use to wrap the model for parameter-efficient finetuning.</p> <code>None</code> <code>model_kwargs</code> <code>Optional[dict[str, Any]]</code> <p>Additional keyword arguments for the model.</p> <code>None</code> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>@store(\n    group=\"modules/encoders\",\n    provider=\"mmlearn\",\n    model_name=\"vit_base_patch16_224\",\n    hydra_convert=\"object\",\n)\nclass TimmViT(nn.Module):\n    \"\"\"Vision Transformer model from timm.\n\n    Parameters\n    ----------\n    model_name : str\n        The name of the model to use.\n    modality : str, default=\"RGB\"\n        The modality of the input data. This allows this model to be used with different\n        image modalities e.g. RGB, Depth, etc.\n    projection_dim : int, default=768\n        The dimension of the projection head.\n    pretrained : bool, default=True\n        Whether to use the pretrained weights.\n    freeze_layers : Union[int, float, list[int], bool], default=False\n        Whether to freeze the layers.\n    freeze_layer_norm : bool, default=True\n        Whether to freeze the layer norm.\n    peft_config : Optional[PeftConfig], optional, default=None\n        The configuration from the `peft &lt;https://huggingface.co/docs/peft/index&gt;`_\n        library to use to wrap the model for parameter-efficient finetuning.\n    model_kwargs : Optional[dict[str, Any]], default=None\n        Additional keyword arguments for the model.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str,\n        modality: str = \"RGB\",\n        projection_dim: int = 768,\n        pretrained: bool = True,\n        freeze_layers: Union[int, float, list[int], bool] = False,\n        freeze_layer_norm: bool = True,\n        peft_config: Optional[\"PeftConfig\"] = None,\n        model_kwargs: Optional[dict[str, Any]] = None,\n    ) -&gt; None:\n        super().__init__()\n        self.modality = Modalities.get_modality(modality)\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        self.model: TimmVisionTransformer = timm.create_model(\n            model_name,\n            pretrained=pretrained,\n            num_classes=projection_dim,\n            **model_kwargs,\n        )\n        assert isinstance(self.model, TimmVisionTransformer), (\n            f\"Model {model_name} is not a Vision Transformer. \"\n            \"Please provide a model name that corresponds to a Vision Transformer.\"\n        )\n\n        self._freeze_layers(freeze_layers, freeze_layer_norm)\n\n        if peft_config is not None:\n            self.model = hf_utils._wrap_peft_model(self.model, peft_config)\n\n    def _freeze_layers(\n        self, freeze_layers: Union[int, float, list[int], bool], freeze_layer_norm: bool\n    ) -&gt; None:\n        \"\"\"Freeze the layers of the model.\n\n        Parameters\n        ----------\n        freeze_layers : Union[int, float, list[int], bool]\n            Whether to freeze the layers.\n        freeze_layer_norm : bool\n            Whether to freeze the layer norm.\n        \"\"\"\n        if isinstance(freeze_layers, bool) and freeze_layers:\n            for name, param in self.model.named_parameters():\n                param.requires_grad = (\n                    (not freeze_layer_norm) if \"norm\" in name else False\n                )\n\n        modules = [self.model.patch_embed, *self.model.blocks, self.model.norm]\n        if isinstance(freeze_layers, float):\n            freeze_layers = int(freeze_layers * len(modules))\n        if isinstance(freeze_layers, int):\n            freeze_layers = list(range(freeze_layers))\n\n        if isinstance(freeze_layers, list):\n            for idx, module in enumerate(modules):\n                if idx in freeze_layers:\n                    for name, param in module.named_parameters():\n                        param.requires_grad = (\n                            (not freeze_layer_norm) if \"norm\" in name else False\n                        )\n\n    def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The ``image`` will be expected under the\n            ``Modalities.RGB`` key.\n\n        Returns\n        -------\n        BaseModelOutput\n            The output of the model.\n        \"\"\"\n        x = inputs[self.modality.name]\n        last_hidden_state, hidden_states = self.model.forward_intermediates(\n            x, output_fmt=\"NLC\"\n        )\n        last_hidden_state = self.model.forward_head(last_hidden_state)\n\n        return BaseModelOutput(\n            last_hidden_state=last_hidden_state, hidden_states=hidden_states\n        )\n\n    def get_intermediate_layers(\n        self, inputs: dict[str, Any], n: int = 1\n    ) -&gt; list[torch.Tensor]:\n        \"\"\"Get the output of the intermediate layers.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input data. The ``image`` will be expected under the ``Modalities.RGB``\n            key.\n        n : int, default=1\n            The number of intermediate layers to return.\n\n        Returns\n        -------\n        list[torch.Tensor]\n            The outputs of the last n intermediate layers.\n        \"\"\"\n        return self.model.get_intermediate_layers(inputs[Modalities.RGB], n)  # type: ignore\n\n    def get_patch_info(self) -&gt; tuple[int, int]:\n        \"\"\"Get patch size and number of patches.\n\n        Returns\n        -------\n        tuple[int, int]\n            Patch size and number of patches.\n        \"\"\"\n        patch_size = self.model.patch_embed.patch_size[0]\n        num_patches = self.model.patch_embed.num_patches\n        return patch_size, num_patches\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.vision.TimmViT.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The <code>image</code> will be expected under the <code>Modalities.RGB</code> key.</p> required <p>Returns:</p> Type Description <code>BaseModelOutput</code> <p>The output of the model.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; BaseModelOutput:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The ``image`` will be expected under the\n        ``Modalities.RGB`` key.\n\n    Returns\n    -------\n    BaseModelOutput\n        The output of the model.\n    \"\"\"\n    x = inputs[self.modality.name]\n    last_hidden_state, hidden_states = self.model.forward_intermediates(\n        x, output_fmt=\"NLC\"\n    )\n    last_hidden_state = self.model.forward_head(last_hidden_state)\n\n    return BaseModelOutput(\n        last_hidden_state=last_hidden_state, hidden_states=hidden_states\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.vision.TimmViT.get_intermediate_layers","title":"get_intermediate_layers","text":"<pre><code>get_intermediate_layers(inputs, n=1)\n</code></pre> <p>Get the output of the intermediate layers.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input data. The <code>image</code> will be expected under the <code>Modalities.RGB</code> key.</p> required <code>n</code> <code>int</code> <p>The number of intermediate layers to return.</p> <code>1</code> <p>Returns:</p> Type Description <code>list[Tensor]</code> <p>The outputs of the last n intermediate layers.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>def get_intermediate_layers(\n    self, inputs: dict[str, Any], n: int = 1\n) -&gt; list[torch.Tensor]:\n    \"\"\"Get the output of the intermediate layers.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input data. The ``image`` will be expected under the ``Modalities.RGB``\n        key.\n    n : int, default=1\n        The number of intermediate layers to return.\n\n    Returns\n    -------\n    list[torch.Tensor]\n        The outputs of the last n intermediate layers.\n    \"\"\"\n    return self.model.get_intermediate_layers(inputs[Modalities.RGB], n)  # type: ignore\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.vision.TimmViT.get_patch_info","title":"get_patch_info","text":"<pre><code>get_patch_info()\n</code></pre> <p>Get patch size and number of patches.</p> <p>Returns:</p> Type Description <code>tuple[int, int]</code> <p>Patch size and number of patches.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>def get_patch_info(self) -&gt; tuple[int, int]:\n    \"\"\"Get patch size and number of patches.\n\n    Returns\n    -------\n    tuple[int, int]\n        Patch size and number of patches.\n    \"\"\"\n    patch_size = self.model.patch_embed.patch_size[0]\n    num_patches = self.model.patch_embed.num_patches\n    return patch_size, num_patches\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.vision.VisionTransformer","title":"VisionTransformer","text":"<p>               Bases: <code>Module</code></p> <p>Vision Transformer.</p> <p>This module implements a Vision Transformer that processes images using a series of transformer blocks and patch embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>modality</code> <code>str</code> <p>The modality of the input data. This allows this model to be used with different image modalities e.g. RGB, Depth, etc.</p> <code>\"RGB\"</code> <code>img_size</code> <code>List[int]</code> <p>List of input image sizes.</p> <code>None</code> <code>patch_size</code> <code>int</code> <p>Size of each patch.</p> <code>16</code> <code>in_chans</code> <code>int</code> <p>Number of input channels.</p> <code>3</code> <code>embed_dim</code> <code>int</code> <p>Embedding dimension.</p> <code>768</code> <code>depth</code> <code>int</code> <p>Number of transformer blocks.</p> <code>12</code> <code>num_heads</code> <code>int</code> <p>Number of attention heads.</p> <code>12</code> <code>mlp_ratio</code> <code>float</code> <p>Ratio of hidden dimension in the MLP.</p> <code>4.0</code> <code>qkv_bias</code> <code>bool</code> <p>If True, add a learnable bias to the query, key, and value projections.</p> <code>True</code> <code>qk_scale</code> <code>Optional[float]</code> <p>Override the default qk scale factor.</p> <code>None</code> <code>drop_rate</code> <code>float</code> <p>Dropout rate for the transformer blocks.</p> <code>0.0</code> <code>attn_drop_rate</code> <code>float</code> <p>Dropout rate for the attention mechanism.</p> <code>0.0</code> <code>drop_path_rate</code> <code>float</code> <p>Dropout rate for stochastic depth.</p> <code>0.0</code> <code>norm_layer</code> <code>Callable[..., Module]</code> <p>Normalization layer to use.</p> <code>torch.nn.LayerNorm</code> <code>init_std</code> <code>float</code> <p>Standard deviation for weight initialization.</p> <code>0.02</code> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>class VisionTransformer(nn.Module):\n    \"\"\"Vision Transformer.\n\n    This module implements a Vision Transformer that processes images using a\n    series of transformer blocks and patch embeddings.\n\n    Parameters\n    ----------\n    modality : str, optional, default=\"RGB\"\n        The modality of the input data. This allows this model to be used with different\n        image modalities e.g. RGB, Depth, etc.\n    img_size : List[int], optional, default=None\n        List of input image sizes.\n    patch_size : int, optional, default=16\n        Size of each patch.\n    in_chans : int, optional, default=3\n        Number of input channels.\n    embed_dim : int, optional, default=768\n        Embedding dimension.\n    depth : int, optional, default=12\n        Number of transformer blocks.\n    num_heads : int, optional, default=12\n        Number of attention heads.\n    mlp_ratio : float, optional, default=4.0\n        Ratio of hidden dimension in the MLP.\n    qkv_bias : bool, optional, default=True\n        If True, add a learnable bias to the query, key, and value projections.\n    qk_scale : Optional[float], optional\n        Override the default qk scale factor.\n    drop_rate : float, optional, default=0.0\n        Dropout rate for the transformer blocks.\n    attn_drop_rate : float, optional, default=0.0\n        Dropout rate for the attention mechanism.\n    drop_path_rate : float, optional, default=0.0\n        Dropout rate for stochastic depth.\n    norm_layer : Callable[..., torch.nn.Module], optional, default=torch.nn.LayerNorm\n        Normalization layer to use.\n    init_std : float, optional, default=0.02\n        Standard deviation for weight initialization.\n    \"\"\"\n\n    def __init__(\n        self,\n        modality: str = \"RGB\",\n        img_size: Optional[list[int]] = None,\n        patch_size: int = 16,\n        in_chans: int = 3,\n        embed_dim: int = 768,\n        depth: int = 12,\n        num_heads: int = 12,\n        mlp_ratio: float = 4.0,\n        qkv_bias: bool = True,\n        qk_scale: Optional[float] = None,\n        global_pool: Literal[\"\", \"avg\", \"avgmax\", \"max\", \"token\"] = \"\",\n        drop_rate: float = 0.0,\n        attn_drop_rate: float = 0.0,\n        drop_path_rate: float = 0.0,\n        norm_layer: Callable[..., nn.Module] = nn.LayerNorm,\n        init_std: float = 0.02,\n    ) -&gt; None:\n        super().__init__()\n        assert global_pool in (\"\", \"avg\", \"avgmax\", \"max\", \"token\")\n\n        self.modality = Modalities.get_modality(modality)\n        self.num_features = self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        img_size = [224, 224] if img_size is None else img_size\n\n        # Patch Embedding\n        self.patch_embed = PatchEmbed(\n            img_size=img_size[0],\n            patch_size=patch_size,\n            in_chans=in_chans,\n            embed_dim=embed_dim,\n        )\n        num_patches = self.patch_embed.num_patches\n\n        # Positional Embedding\n        self.pos_embed = nn.Parameter(\n            torch.zeros(1, num_patches, embed_dim), requires_grad=False\n        )\n        pos_embed = get_2d_sincos_pos_embed(\n            self.pos_embed.shape[-1],\n            int(self.patch_embed.num_patches**0.5),\n            cls_token=False,\n        )\n        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n\n        # Transformer Blocks\n        dpr = [\n            x.item() for x in torch.linspace(0, drop_path_rate, depth)\n        ]  # stochastic depth decay rule\n        self.blocks = nn.ModuleList(\n            [\n                Block(\n                    dim=embed_dim,\n                    num_heads=num_heads,\n                    mlp_ratio=mlp_ratio,\n                    qkv_bias=qkv_bias,\n                    qk_scale=qk_scale,\n                    drop=drop_rate,\n                    attn_drop=attn_drop_rate,\n                    drop_path=dpr[i],\n                    norm_layer=norm_layer,\n                )\n                for i in range(depth)\n            ]\n        )\n        self.norm = norm_layer(embed_dim)\n\n        self.global_pool = global_pool\n\n        # Weight Initialization\n        self.init_std = init_std\n        self.apply(self._init_weights)\n\n    def fix_init_weight(self) -&gt; None:\n        \"\"\"Fix initialization of weights by rescaling them according to layer depth.\"\"\"\n\n        def rescale(param: torch.Tensor, layer_id: int) -&gt; None:\n            param.div_(math.sqrt(2.0 * layer_id))\n\n        for layer_id, layer in enumerate(self.blocks):\n            rescale(layer.attn.proj.weight.data, layer_id + 1)\n            rescale(layer.mlp[-1].weight.data, layer_id + 1)\n\n    def _init_weights(self, m: nn.Module) -&gt; None:\n        \"\"\"Initialize weights for the layers.\"\"\"\n        if isinstance(m, nn.Linear):\n            _trunc_normal(m.weight, std=self.init_std)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n        elif isinstance(m, nn.Conv2d):\n            _trunc_normal(m.weight, std=self.init_std)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n\n    def forward(\n        self, inputs: dict[str, Any], return_hidden_states: bool = False\n    ) -&gt; tuple[torch.Tensor, Optional[list[torch.Tensor]]]:\n        \"\"\"Forward pass through the Vision Transformer.\"\"\"\n        masks = inputs.get(self.modality.mask)\n        if masks is not None and not isinstance(masks, list):\n            masks = [masks]\n\n        x = inputs[self.modality.name]\n        # -- Patchify x\n        x = self.patch_embed(x)\n\n        # -- Add positional embedding to x\n        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n        x = x + pos_embed\n\n        # -- Mask x\n        if masks is not None:\n            x = apply_masks(x, masks)\n\n        # -- Initialize a list to store hidden states\n        hidden_states: Optional[list[torch.Tensor]] = (\n            [] if return_hidden_states else None\n        )\n\n        # -- Forward propagation through blocks\n        for _i, blk in enumerate(self.blocks):\n            x = blk(x)\n            if return_hidden_states and hidden_states is not None:\n                hidden_states.append(x)\n\n        # -- Apply normalization if present\n        if self.norm is not None:\n            x = self.norm(x)\n\n        # -- Apply global pooling\n        x = global_pool_nlc(x, pool_type=self.global_pool)\n\n        # -- Return both final output and hidden states if requested\n        if return_hidden_states:\n            return x, hidden_states\n        return (x, None)\n\n    def interpolate_pos_encoding(\n        self, x: torch.Tensor, pos_embed: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"Interpolate positional encoding to match the size of the input tensor.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor.\n        pos_embed : torch.Tensor\n            Positional embedding tensor.\n\n        Returns\n        -------\n        torch.Tensor\n            Interpolated positional encoding.\n        \"\"\"\n        npatch = x.shape[1] - 1\n        n = pos_embed.shape[1] - 1\n        if npatch == n:\n            return pos_embed\n        class_emb = pos_embed[:, 0]\n        pos_embed = pos_embed[:, 1:]\n        dim = x.shape[-1]\n        pos_embed = nn.functional.interpolate(\n            pos_embed.reshape(1, int(math.sqrt(n)), int(math.sqrt(n)), dim).permute(\n                0, 3, 1, 2\n            ),\n            scale_factor=math.sqrt(npatch / n),\n            mode=\"bicubic\",\n        )\n        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.vision.VisionTransformer.fix_init_weight","title":"fix_init_weight","text":"<pre><code>fix_init_weight()\n</code></pre> <p>Fix initialization of weights by rescaling them according to layer depth.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>def fix_init_weight(self) -&gt; None:\n    \"\"\"Fix initialization of weights by rescaling them according to layer depth.\"\"\"\n\n    def rescale(param: torch.Tensor, layer_id: int) -&gt; None:\n        param.div_(math.sqrt(2.0 * layer_id))\n\n    for layer_id, layer in enumerate(self.blocks):\n        rescale(layer.attn.proj.weight.data, layer_id + 1)\n        rescale(layer.mlp[-1].weight.data, layer_id + 1)\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.vision.VisionTransformer.forward","title":"forward","text":"<pre><code>forward(inputs, return_hidden_states=False)\n</code></pre> <p>Forward pass through the Vision Transformer.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>def forward(\n    self, inputs: dict[str, Any], return_hidden_states: bool = False\n) -&gt; tuple[torch.Tensor, Optional[list[torch.Tensor]]]:\n    \"\"\"Forward pass through the Vision Transformer.\"\"\"\n    masks = inputs.get(self.modality.mask)\n    if masks is not None and not isinstance(masks, list):\n        masks = [masks]\n\n    x = inputs[self.modality.name]\n    # -- Patchify x\n    x = self.patch_embed(x)\n\n    # -- Add positional embedding to x\n    pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n    x = x + pos_embed\n\n    # -- Mask x\n    if masks is not None:\n        x = apply_masks(x, masks)\n\n    # -- Initialize a list to store hidden states\n    hidden_states: Optional[list[torch.Tensor]] = (\n        [] if return_hidden_states else None\n    )\n\n    # -- Forward propagation through blocks\n    for _i, blk in enumerate(self.blocks):\n        x = blk(x)\n        if return_hidden_states and hidden_states is not None:\n            hidden_states.append(x)\n\n    # -- Apply normalization if present\n    if self.norm is not None:\n        x = self.norm(x)\n\n    # -- Apply global pooling\n    x = global_pool_nlc(x, pool_type=self.global_pool)\n\n    # -- Return both final output and hidden states if requested\n    if return_hidden_states:\n        return x, hidden_states\n    return (x, None)\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.vision.VisionTransformer.interpolate_pos_encoding","title":"interpolate_pos_encoding","text":"<pre><code>interpolate_pos_encoding(x, pos_embed)\n</code></pre> <p>Interpolate positional encoding to match the size of the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <code>pos_embed</code> <code>Tensor</code> <p>Positional embedding tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Interpolated positional encoding.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>def interpolate_pos_encoding(\n    self, x: torch.Tensor, pos_embed: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"Interpolate positional encoding to match the size of the input tensor.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor.\n    pos_embed : torch.Tensor\n        Positional embedding tensor.\n\n    Returns\n    -------\n    torch.Tensor\n        Interpolated positional encoding.\n    \"\"\"\n    npatch = x.shape[1] - 1\n    n = pos_embed.shape[1] - 1\n    if npatch == n:\n        return pos_embed\n    class_emb = pos_embed[:, 0]\n    pos_embed = pos_embed[:, 1:]\n    dim = x.shape[-1]\n    pos_embed = nn.functional.interpolate(\n        pos_embed.reshape(1, int(math.sqrt(n)), int(math.sqrt(n)), dim).permute(\n            0, 3, 1, 2\n        ),\n        scale_factor=math.sqrt(npatch / n),\n        mode=\"bicubic\",\n    )\n    pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n    return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.vision.VisionTransformerPredictor","title":"VisionTransformerPredictor","text":"<p>               Bases: <code>Module</code></p> <p>Vision Transformer Predictor.</p> <p>This module implements a Vision Transformer that predicts masked tokens using a series of transformer blocks.</p> <p>Parameters:</p> Name Type Description Default <code>num_patches</code> <code>int</code> <p>The number of patches in the input image.</p> <code>196</code> <code>embed_dim</code> <code>int</code> <p>The embedding dimension.</p> <code>768</code> <code>predictor_embed_dim</code> <code>int</code> <p>The embedding dimension for the predictor.</p> <code>384</code> <code>depth</code> <code>int</code> <p>The number of transformer blocks.</p> <code>6</code> <code>num_heads</code> <code>int</code> <p>The number of attention heads.</p> <code>12</code> <code>mlp_ratio</code> <code>float</code> <p>Ratio of the hidden dimension in the MLP.</p> <code>4.0</code> <code>qkv_bias</code> <code>bool</code> <p>If True, add a learnable bias to the query, key, and value projections.</p> <code>True</code> <code>qk_scale</code> <code>Optional[float]</code> <p>Override the default qk scale factor.</p> <code>None</code> <code>drop_rate</code> <code>float</code> <p>Dropout rate for the transformer blocks.</p> <code>0.0</code> <code>attn_drop_rate</code> <code>float</code> <p>Dropout rate for the attention mechanism.</p> <code>0.0</code> <code>drop_path_rate</code> <code>float</code> <p>Dropout rate for stochastic depth.</p> <code>0.0</code> <code>norm_layer</code> <code>Callable[..., Module]</code> <p>Normalization layer to use.</p> <code>torch.nn.LayerNorm</code> <code>init_std</code> <code>float</code> <p>Standard deviation for weight initialization.</p> <code>0.02</code> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>class VisionTransformerPredictor(nn.Module):\n    \"\"\"Vision Transformer Predictor.\n\n    This module implements a Vision Transformer that predicts masked tokens\n    using a series of transformer blocks.\n\n    Parameters\n    ----------\n    num_patches : int\n        The number of patches in the input image.\n    embed_dim : int, optional, default=768\n        The embedding dimension.\n    predictor_embed_dim : int, optional, default=384\n        The embedding dimension for the predictor.\n    depth : int, optional, default=6\n        The number of transformer blocks.\n    num_heads : int, optional, default=12\n        The number of attention heads.\n    mlp_ratio : float, optional, default=4.0\n        Ratio of the hidden dimension in the MLP.\n    qkv_bias : bool, optional, default=True\n        If True, add a learnable bias to the query, key, and value projections.\n    qk_scale : Optional[float], optional, default=None\n        Override the default qk scale factor.\n    drop_rate : float, optional, default=0.0\n        Dropout rate for the transformer blocks.\n    attn_drop_rate : float, optional, default=0.0\n        Dropout rate for the attention mechanism.\n    drop_path_rate : float, optional, default=0.0\n        Dropout rate for stochastic depth.\n    norm_layer : Callable[..., torch.nn.Module], optional, default=torch.nn.LayerNorm\n        Normalization layer to use.\n    init_std : float, optional, default=0.02\n        Standard deviation for weight initialization.\n    \"\"\"\n\n    def __init__(\n        self,\n        num_patches: int = 196,\n        embed_dim: int = 768,\n        predictor_embed_dim: int = 384,\n        depth: int = 6,\n        num_heads: int = 12,\n        mlp_ratio: float = 4.0,\n        qkv_bias: bool = True,\n        qk_scale: Optional[float] = None,\n        drop_rate: float = 0.0,\n        attn_drop_rate: float = 0.0,\n        drop_path_rate: float = 0.0,\n        norm_layer: Callable[..., nn.Module] = nn.LayerNorm,\n        init_std: float = 0.02,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__()\n        self.num_patches = num_patches\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n\n        self.predictor_embed = nn.Linear(self.embed_dim, predictor_embed_dim, bias=True)\n        self.mask_token = nn.Parameter(torch.zeros(1, 1, predictor_embed_dim))\n        dpr = [\n            x.item() for x in torch.linspace(0, drop_path_rate, depth)\n        ]  # stochastic depth decay rule\n\n        # Positional Embedding\n        self.predictor_pos_embed = nn.Parameter(\n            torch.zeros(1, self.num_patches, predictor_embed_dim), requires_grad=False\n        )\n        predictor_pos_embed = get_2d_sincos_pos_embed(\n            self.predictor_pos_embed.shape[-1],\n            int(self.num_patches**0.5),\n            cls_token=False,\n        )\n        self.predictor_pos_embed.data.copy_(\n            torch.from_numpy(predictor_pos_embed).float().unsqueeze(0)\n        )\n\n        # Transformer Blocks\n        self.predictor_blocks = nn.ModuleList(\n            [\n                Block(\n                    dim=predictor_embed_dim,\n                    num_heads=self.num_heads,\n                    mlp_ratio=mlp_ratio,\n                    qkv_bias=qkv_bias,\n                    qk_scale=qk_scale,\n                    drop=drop_rate,\n                    attn_drop=attn_drop_rate,\n                    drop_path=dpr[i],\n                    norm_layer=norm_layer,\n                )\n                for i in range(depth)\n            ]\n        )\n\n        self.predictor_norm = norm_layer(predictor_embed_dim)\n        self.predictor_proj = nn.Linear(predictor_embed_dim, embed_dim, bias=True)\n\n        # Weight Initialization\n        self.init_std = init_std\n        _trunc_normal(self.mask_token, std=self.init_std)\n        self.apply(self._init_weights)\n\n    def fix_init_weight(self) -&gt; None:\n        \"\"\"Fix initialization of weights by rescaling them according to layer depth.\"\"\"\n\n        def rescale(param: torch.Tensor, layer_id: int) -&gt; None:\n            param.div_(math.sqrt(2.0 * layer_id))\n\n        for layer_id, layer in enumerate(self.predictor_blocks):\n            rescale(layer.attn.proj.weight.data, layer_id + 1)\n            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n\n    def _init_weights(self, m: nn.Module) -&gt; None:\n        \"\"\"Initialize weights for the layers.\"\"\"\n        if isinstance(m, nn.Linear):\n            _trunc_normal(m.weight, std=self.init_std)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n        elif isinstance(m, nn.Conv2d):\n            _trunc_normal(m.weight, std=self.init_std)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        masks_x: Union[torch.Tensor, list[torch.Tensor]],\n        masks: Union[torch.Tensor, list[torch.Tensor]],\n    ) -&gt; torch.Tensor:\n        \"\"\"Forward pass through the Vision Transformer Predictor.\"\"\"\n        assert (masks is not None) and (masks_x is not None), (\n            \"Cannot run predictor without mask indices\"\n        )\n\n        if not isinstance(masks_x, list):\n            masks_x = [masks_x]\n\n        if not isinstance(masks, list):\n            masks = [masks]\n\n        # -- Batch Size\n        b = len(x) // len(masks_x)\n\n        # -- Map from encoder-dim to predictor-dim\n        x = self.predictor_embed(x)\n\n        # -- Add positional embedding to x tokens\n        x_pos_embed = self.predictor_pos_embed.repeat(b, 1, 1)\n        x += apply_masks(x_pos_embed, masks_x)\n\n        _, n_ctxt, d = x.shape\n\n        # -- Concatenate mask tokens to x\n        pos_embs = self.predictor_pos_embed.repeat(b, 1, 1)\n        pos_embs = apply_masks(pos_embs, masks)\n        pos_embs = repeat_interleave_batch(pos_embs, b, repeat=len(masks_x))\n        pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1)\n        pred_tokens += pos_embs\n        x = x.repeat(len(masks), 1, 1)\n        x = torch.cat([x, pred_tokens], dim=1)\n\n        # -- Forward propagation\n        for blk in self.predictor_blocks:\n            x = blk(x)\n        x = self.predictor_norm(x)\n\n        # -- Return predictions for mask tokens\n        x = x[:, n_ctxt:]\n        return self.predictor_proj(x)\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.vision.VisionTransformerPredictor.fix_init_weight","title":"fix_init_weight","text":"<pre><code>fix_init_weight()\n</code></pre> <p>Fix initialization of weights by rescaling them according to layer depth.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>def fix_init_weight(self) -&gt; None:\n    \"\"\"Fix initialization of weights by rescaling them according to layer depth.\"\"\"\n\n    def rescale(param: torch.Tensor, layer_id: int) -&gt; None:\n        param.div_(math.sqrt(2.0 * layer_id))\n\n    for layer_id, layer in enumerate(self.predictor_blocks):\n        rescale(layer.attn.proj.weight.data, layer_id + 1)\n        rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.vision.VisionTransformerPredictor.forward","title":"forward","text":"<pre><code>forward(x, masks_x, masks)\n</code></pre> <p>Forward pass through the Vision Transformer Predictor.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>def forward(\n    self,\n    x: torch.Tensor,\n    masks_x: Union[torch.Tensor, list[torch.Tensor]],\n    masks: Union[torch.Tensor, list[torch.Tensor]],\n) -&gt; torch.Tensor:\n    \"\"\"Forward pass through the Vision Transformer Predictor.\"\"\"\n    assert (masks is not None) and (masks_x is not None), (\n        \"Cannot run predictor without mask indices\"\n    )\n\n    if not isinstance(masks_x, list):\n        masks_x = [masks_x]\n\n    if not isinstance(masks, list):\n        masks = [masks]\n\n    # -- Batch Size\n    b = len(x) // len(masks_x)\n\n    # -- Map from encoder-dim to predictor-dim\n    x = self.predictor_embed(x)\n\n    # -- Add positional embedding to x tokens\n    x_pos_embed = self.predictor_pos_embed.repeat(b, 1, 1)\n    x += apply_masks(x_pos_embed, masks_x)\n\n    _, n_ctxt, d = x.shape\n\n    # -- Concatenate mask tokens to x\n    pos_embs = self.predictor_pos_embed.repeat(b, 1, 1)\n    pos_embs = apply_masks(pos_embs, masks)\n    pos_embs = repeat_interleave_batch(pos_embs, b, repeat=len(masks_x))\n    pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1)\n    pred_tokens += pos_embs\n    x = x.repeat(len(masks), 1, 1)\n    x = torch.cat([x, pred_tokens], dim=1)\n\n    # -- Forward propagation\n    for blk in self.predictor_blocks:\n        x = blk(x)\n    x = self.predictor_norm(x)\n\n    # -- Return predictions for mask tokens\n    x = x[:, n_ctxt:]\n    return self.predictor_proj(x)\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.vision.vit_predictor","title":"vit_predictor","text":"<pre><code>vit_predictor(kwargs=None)\n</code></pre> <p>Create a VisionTransformerPredictor model.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>dict[str, Any]</code> <p>Keyword arguments for the predictor.</p> <code>None</code> <p>Returns:</p> Type Description <code>VisionTransformerPredictor</code> <p>An instance of VisionTransformerPredictor.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>@cast(\n    VisionTransformerPredictor,\n    store(\n        group=\"modules/encoders\",\n        provider=\"mmlearn\",\n    ),\n)\ndef vit_predictor(\n    kwargs: Optional[dict[str, Any]] = None,\n) -&gt; VisionTransformerPredictor:\n    \"\"\"Create a VisionTransformerPredictor model.\n\n    Parameters\n    ----------\n    kwargs : dict[str, Any], optional, default=None\n        Keyword arguments for the predictor.\n\n    Returns\n    -------\n    VisionTransformerPredictor\n        An instance of VisionTransformerPredictor.\n    \"\"\"\n    if kwargs is None:\n        kwargs = {}\n    return VisionTransformerPredictor(\n        mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.vision.vit_tiny","title":"vit_tiny","text":"<pre><code>vit_tiny(patch_size=16, kwargs=None)\n</code></pre> <p>Create a VisionTransformer model with tiny configuration.</p> <p>Parameters:</p> Name Type Description Default <code>patch_size</code> <code>int</code> <p>Size of each patch.</p> <code>16</code> <code>kwargs</code> <code>dict[str, Any]</code> <p>Keyword arguments for the model variant.</p> <code>None</code> <p>Returns:</p> Type Description <code>VisionTransformer</code> <p>An instance of VisionTransformer.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>@cast(\n    VisionTransformer,\n    store(\n        group=\"modules/encoders\",\n        provider=\"mmlearn\",\n    ),\n)\ndef vit_tiny(\n    patch_size: int = 16, kwargs: Optional[dict[str, Any]] = None\n) -&gt; VisionTransformer:\n    \"\"\"Create a VisionTransformer model with tiny configuration.\n\n    Parameters\n    ----------\n    patch_size : int, default=16\n        Size of each patch.\n    kwargs : dict[str, Any], optional, default=None\n        Keyword arguments for the model variant.\n\n    Returns\n    -------\n    VisionTransformer\n        An instance of VisionTransformer.\n    \"\"\"\n    if kwargs is None:\n        kwargs = {}\n    return VisionTransformer(\n        patch_size=patch_size,\n        embed_dim=192,\n        depth=12,\n        num_heads=3,\n        mlp_ratio=4,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.vision.vit_small","title":"vit_small","text":"<pre><code>vit_small(patch_size=16, kwargs=None)\n</code></pre> <p>Create a VisionTransformer model with small configuration.</p> <p>Parameters:</p> Name Type Description Default <code>patch_size</code> <code>int</code> <p>Size of each patch.</p> <code>16</code> <code>kwargs</code> <code>dict[str, Any]</code> <p>Keyword arguments for the model variant.</p> <code>None</code> <p>Returns:</p> Type Description <code>VisionTransformer</code> <p>An instance of VisionTransformer.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>@cast(\n    VisionTransformer,\n    store(\n        group=\"modules/encoders\",\n        provider=\"mmlearn\",\n    ),\n)\ndef vit_small(\n    patch_size: int = 16, kwargs: Optional[dict[str, Any]] = None\n) -&gt; VisionTransformer:\n    \"\"\"Create a VisionTransformer model with small configuration.\n\n    Parameters\n    ----------\n    patch_size : int, default=16\n        Size of each patch.\n    kwargs : dict[str, Any], optional, default=None\n        Keyword arguments for the model variant.\n\n    Returns\n    -------\n    VisionTransformer\n        An instance of VisionTransformer.\n    \"\"\"\n    if kwargs is None:\n        kwargs = {}\n    return VisionTransformer(\n        patch_size=patch_size,\n        embed_dim=384,\n        depth=12,\n        num_heads=6,\n        mlp_ratio=4,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.vision.vit_base","title":"vit_base","text":"<pre><code>vit_base(patch_size=16, kwargs=None)\n</code></pre> <p>Create a VisionTransformer model with base configuration.</p> <p>Parameters:</p> Name Type Description Default <code>patch_size</code> <code>int</code> <p>Size of each patch.</p> <code>16</code> <code>kwargs</code> <code>dict[str, Any]</code> <p>Keyword arguments for the model variant.</p> <code>None</code> <p>Returns:</p> Type Description <code>VisionTransformer</code> <p>An instance of VisionTransformer.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>@cast(\n    VisionTransformer,\n    store(\n        group=\"modules/encoders\",\n        provider=\"mmlearn\",\n    ),\n)\ndef vit_base(\n    patch_size: int = 16, kwargs: Optional[dict[str, Any]] = None\n) -&gt; VisionTransformer:\n    \"\"\"Create a VisionTransformer model with base configuration.\n\n    Parameters\n    ----------\n    patch_size : int, default=16\n        Size of each patch.\n    kwargs : dict[str, Any], optional, default=None\n        Keyword arguments for the model variant.\n\n    Returns\n    -------\n    VisionTransformer\n        An instance of VisionTransformer.\n    \"\"\"\n    if kwargs is None:\n        kwargs = {}\n    return VisionTransformer(\n        patch_size=patch_size,\n        embed_dim=768,\n        depth=12,\n        num_heads=12,\n        mlp_ratio=4,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.vision.vit_large","title":"vit_large","text":"<pre><code>vit_large(patch_size=16, kwargs=None)\n</code></pre> <p>Create a VisionTransformer model with large configuration.</p> <p>Parameters:</p> Name Type Description Default <code>patch_size</code> <code>int</code> <p>Size of each patch.</p> <code>16</code> <code>kwargs</code> <code>dict[str, Any]</code> <p>Keyword arguments for the model variant.</p> <code>None</code> <p>Returns:</p> Type Description <code>VisionTransformer</code> <p>An instance of VisionTransformer.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>@cast(\n    VisionTransformer,\n    store(\n        group=\"modules/encoders\",\n        provider=\"mmlearn\",\n    ),\n)\ndef vit_large(\n    patch_size: int = 16, kwargs: Optional[dict[str, Any]] = None\n) -&gt; VisionTransformer:\n    \"\"\"Create a VisionTransformer model with large configuration.\n\n    Parameters\n    ----------\n    patch_size : int, default=16\n        Size of each patch.\n    kwargs : dict[str, Any], optional, default=None\n        Keyword arguments for the model variant.\n\n    Returns\n    -------\n    VisionTransformer\n        An instance of VisionTransformer.\n    \"\"\"\n    if kwargs is None:\n        kwargs = {}\n    return VisionTransformer(\n        patch_size=patch_size,\n        embed_dim=1024,\n        depth=24,\n        num_heads=16,\n        mlp_ratio=4,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.vision.vit_huge","title":"vit_huge","text":"<pre><code>vit_huge(patch_size=16, kwargs=None)\n</code></pre> <p>Create a VisionTransformer model with huge configuration.</p> <p>Parameters:</p> Name Type Description Default <code>patch_size</code> <code>int</code> <p>Size of each patch.</p> <code>16</code> <code>kwargs</code> <code>dict[str, Any]</code> <p>Keyword arguments for the model variant.</p> <code>None</code> <p>Returns:</p> Type Description <code>VisionTransformer</code> <p>An instance of VisionTransformer.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>@cast(\n    VisionTransformer,\n    store(\n        group=\"modules/encoders\",\n        provider=\"mmlearn\",\n    ),\n)\ndef vit_huge(\n    patch_size: int = 16, kwargs: Optional[dict[str, Any]] = None\n) -&gt; VisionTransformer:\n    \"\"\"Create a VisionTransformer model with huge configuration.\n\n    Parameters\n    ----------\n    patch_size : int, default=16\n        Size of each patch.\n    kwargs : dict[str, Any], optional, default=None\n        Keyword arguments for the model variant.\n\n    Returns\n    -------\n    VisionTransformer\n        An instance of VisionTransformer.\n    \"\"\"\n    if kwargs is None:\n        kwargs = {}\n    return VisionTransformer(\n        patch_size=patch_size,\n        embed_dim=1280,\n        depth=32,\n        num_heads=16,\n        mlp_ratio=4,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.encoders.vision.vit_giant","title":"vit_giant","text":"<pre><code>vit_giant(patch_size=16, kwargs=None)\n</code></pre> <p>Create a VisionTransformer model with giant configuration.</p> <p>Parameters:</p> Name Type Description Default <code>patch_size</code> <code>int</code> <p>Size of each patch.</p> <code>16</code> <code>kwargs</code> <code>dict[str, Any]</code> <p>Keyword arguments for the model variant.</p> <code>None</code> <p>Returns:</p> Type Description <code>VisionTransformer</code> <p>An instance of VisionTransformer.</p> Source code in <code>mmlearn/modules/encoders/vision.py</code> <pre><code>@cast(\n    VisionTransformer,\n    store(\n        group=\"modules/encoders\",\n        provider=\"mmlearn\",\n    ),\n)\ndef vit_giant(\n    patch_size: int = 16, kwargs: Optional[dict[str, Any]] = None\n) -&gt; VisionTransformer:\n    \"\"\"Create a VisionTransformer model with giant configuration.\n\n    Parameters\n    ----------\n    patch_size : int, default=16\n        Size of each patch.\n    kwargs : dict[str, Any], optional, default=None\n        Keyword arguments for the model variant.\n\n    Returns\n    -------\n    VisionTransformer\n        An instance of VisionTransformer.\n    \"\"\"\n    if kwargs is None:\n        kwargs = {}\n    return VisionTransformer(\n        patch_size=patch_size,\n        embed_dim=1408,\n        depth=40,\n        num_heads=16,\n        mlp_ratio=48 / 11,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/#layers","title":"Layers","text":""},{"location":"api/#mmlearn.modules.layers","title":"mmlearn.modules.layers","text":"<p>Custom, reusable layers for models and tasks.</p>"},{"location":"api/#mmlearn.modules.layers.LearnableLogitScaling","title":"LearnableLogitScaling","text":"<p>               Bases: <code>Module</code></p> <p>Logit scaling layer.</p> <p>Parameters:</p> Name Type Description Default <code>init_logit_scale</code> <code>float</code> <p>Initial value of the logit scale.</p> <code>1/0.07</code> <code>learnable</code> <code>bool</code> <p>If True, the logit scale is learnable. Otherwise, it is fixed.</p> <code>True</code> <code>max_logit_scale</code> <code>float</code> <p>Maximum value of the logit scale.</p> <code>100</code> Source code in <code>mmlearn/modules/layers/logit_scaling.py</code> <pre><code>@store(group=\"modules/layers\", provider=\"mmlearn\")\nclass LearnableLogitScaling(torch.nn.Module):\n    \"\"\"Logit scaling layer.\n\n    Parameters\n    ----------\n    init_logit_scale : float, optional, default=1/0.07\n        Initial value of the logit scale.\n    learnable : bool, optional, default=True\n        If True, the logit scale is learnable. Otherwise, it is fixed.\n    max_logit_scale : float, optional, default=100\n        Maximum value of the logit scale.\n    \"\"\"\n\n    def __init__(\n        self,\n        init_logit_scale: float = 1 / 0.07,\n        max_logit_scale: float = 100,\n        learnable: bool = True,\n    ) -&gt; None:\n        super().__init__()\n        self.max_logit_scale = max_logit_scale\n        self.init_logit_scale = init_logit_scale\n        self.learnable = learnable\n        log_logit_scale = torch.ones([]) * np.log(self.init_logit_scale)\n        if learnable:\n            self.log_logit_scale = torch.nn.Parameter(log_logit_scale)\n        else:\n            self.register_buffer(\"log_logit_scale\", log_logit_scale)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Apply the logit scaling to the input tensor.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor of shape ``(batch_sz, seq_len, dim)``.\n        \"\"\"\n        return torch.clip(self.log_logit_scale.exp(), max=self.max_logit_scale) * x\n\n    def extra_repr(self) -&gt; str:\n        \"\"\"Return the string representation of the layer.\"\"\"\n        return (\n            f\"logit_scale_init={self.init_logit_scale},learnable={self.learnable},\"\n            f\" max_logit_scale={self.max_logit_scale}\"\n        )\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.LearnableLogitScaling.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Apply the logit scaling to the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape <code>(batch_sz, seq_len, dim)</code>.</p> required Source code in <code>mmlearn/modules/layers/logit_scaling.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Apply the logit scaling to the input tensor.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape ``(batch_sz, seq_len, dim)``.\n    \"\"\"\n    return torch.clip(self.log_logit_scale.exp(), max=self.max_logit_scale) * x\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.LearnableLogitScaling.extra_repr","title":"extra_repr","text":"<pre><code>extra_repr()\n</code></pre> <p>Return the string representation of the layer.</p> Source code in <code>mmlearn/modules/layers/logit_scaling.py</code> <pre><code>def extra_repr(self) -&gt; str:\n    \"\"\"Return the string representation of the layer.\"\"\"\n    return (\n        f\"logit_scale_init={self.init_logit_scale},learnable={self.learnable},\"\n        f\" max_logit_scale={self.max_logit_scale}\"\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.MLP","title":"MLP","text":"<p>               Bases: <code>Sequential</code></p> <p>Multi-layer perceptron (MLP).</p> <p>This module will create a block of <code>Linear -&gt; Normalization -&gt; Activation -&gt; Dropout</code> layers.</p> <p>Parameters:</p> Name Type Description Default <code>in_dim</code> <code>int</code> <p>The input dimension.</p> required <code>out_dim</code> <code>Optional[int]</code> <p>The output dimension. If not specified, it is set to :attr:<code>in_dim</code>.</p> <code>None</code> <code>hidden_dims</code> <code>Optional[list]</code> <p>The dimensions of the hidden layers. The length of the list determines the number of hidden layers. This parameter is mutually exclusive with :attr:<code>hidden_dims_multiplier</code>.</p> <code>None</code> <code>hidden_dims_multiplier</code> <code>Optional[list]</code> <p>The multipliers to apply to the input dimension to get the dimensions of the hidden layers. The length of the list determines the number of hidden layers. The multipliers will be used to get the dimensions of the hidden layers. This parameter is mutually exclusive with <code>hidden_dims</code>.</p> <code>None</code> <code>apply_multiplier_to_in_dim</code> <code>bool</code> <p>Whether to apply the :attr:<code>hidden_dims_multiplier</code> to :attr:<code>in_dim</code> to get the dimensions of the hidden layers. If <code>False</code>, the multipliers will be applied to the dimensions of the previous hidden layer, starting from :attr:<code>in_dim</code>. This parameter is only relevant when :attr:<code>hidden_dims_multiplier</code> is specified.</p> <code>False</code> <code>norm_layer</code> <code>Optional[Callable[..., Module]]</code> <p>The normalization layer to use. If not specified, no normalization is used. Partial functions can be used to specify the normalization layer with specific parameters.</p> <code>None</code> <code>activation_layer</code> <code>Optional[Callable[..., Module]]</code> <p>The activation layer to use. If not specified, ReLU is used. Partial functions can be used to specify the activation layer with specific parameters.</p> <code>torch.nn.ReLU</code> <code>bias</code> <code>Union[bool, list[bool]]</code> <p>Whether to use bias in the linear layers.</p> <code>True</code> <code>dropout</code> <code>Union[float, list[float]]</code> <p>The dropout probability to use.</p> <code>0.0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If both :attr:<code>hidden_dims</code> and :attr:<code>hidden_dims_multiplier</code> are specified or if the lengths of :attr:<code>bias</code> and :attr:<code>hidden_dims</code> do not match or if the lengths of :attr:<code>dropout</code> and :attr:<code>hidden_dims</code> do not match.</p> Source code in <code>mmlearn/modules/layers/mlp.py</code> <pre><code>@store(group=\"modules/layers\", provider=\"mmlearn\")\nclass MLP(torch.nn.Sequential):\n    \"\"\"Multi-layer perceptron (MLP).\n\n    This module will create a block of ``Linear -&gt; Normalization -&gt; Activation -&gt; Dropout``\n    layers.\n\n    Parameters\n    ----------\n    in_dim : int\n        The input dimension.\n    out_dim : Optional[int], optional, default=None\n        The output dimension. If not specified, it is set to :attr:`in_dim`.\n    hidden_dims : Optional[list], optional, default=None\n        The dimensions of the hidden layers. The length of the list determines the\n        number of hidden layers. This parameter is mutually exclusive with\n        :attr:`hidden_dims_multiplier`.\n    hidden_dims_multiplier : Optional[list], optional, default=None\n        The multipliers to apply to the input dimension to get the dimensions of\n        the hidden layers. The length of the list determines the number of hidden\n        layers. The multipliers will be used to get the dimensions of the hidden\n        layers. This parameter is mutually exclusive with `hidden_dims`.\n    apply_multiplier_to_in_dim : bool, optional, default=False\n        Whether to apply the :attr:`hidden_dims_multiplier` to :attr:`in_dim` to get the\n        dimensions of the hidden layers. If ``False``, the multipliers will be applied\n        to the dimensions of the previous hidden layer, starting from :attr:`in_dim`.\n        This parameter is only relevant when :attr:`hidden_dims_multiplier` is\n        specified.\n    norm_layer : Optional[Callable[..., torch.nn.Module]], optional, default=None\n        The normalization layer to use. If not specified, no normalization is used.\n        Partial functions can be used to specify the normalization layer with specific\n        parameters.\n    activation_layer : Optional[Callable[..., torch.nn.Module]], optional, default=torch.nn.ReLU\n        The activation layer to use. If not specified, ReLU is used. Partial functions\n        can be used to specify the activation layer with specific parameters.\n    bias : Union[bool, list[bool]], optional, default=True\n        Whether to use bias in the linear layers.\n    dropout : Union[float, list[float]], optional, default=0.0\n        The dropout probability to use.\n\n    Raises\n    ------\n    ValueError\n        If both :attr:`hidden_dims` and :attr:`hidden_dims_multiplier` are specified\n        or if the lengths of :attr:`bias` and :attr:`hidden_dims` do not match or if\n        the lengths of :attr:`dropout` and :attr:`hidden_dims` do not match.\n\n    \"\"\"  # noqa: W505\n\n    def __init__(  # noqa: PLR0912\n        self,\n        in_dim: int,\n        out_dim: Optional[int] = None,\n        hidden_dims: Optional[list[int]] = None,\n        hidden_dims_multiplier: Optional[list[float]] = None,\n        apply_multiplier_to_in_dim: bool = False,\n        norm_layer: Optional[Callable[..., torch.nn.Module]] = None,\n        activation_layer: Optional[Callable[..., torch.nn.Module]] = torch.nn.ReLU,\n        bias: Union[bool, list[bool]] = True,\n        dropout: Union[float, list[float]] = 0.0,\n    ) -&gt; None:\n        if hidden_dims is None and hidden_dims_multiplier is None:\n            hidden_dims = []\n        if hidden_dims is not None and hidden_dims_multiplier is not None:\n            raise ValueError(\n                \"Only one of `hidden_dims` or `hidden_dims_multiplier` must be specified.\"\n            )\n\n        if hidden_dims is None and hidden_dims_multiplier is not None:\n            if apply_multiplier_to_in_dim:\n                hidden_dims = [\n                    int(in_dim * multiplier) for multiplier in hidden_dims_multiplier\n                ]\n            else:\n                hidden_dims = [int(in_dim * hidden_dims_multiplier[0])]\n                for multiplier in hidden_dims_multiplier[1:]:\n                    hidden_dims.append(int(hidden_dims[-1] * multiplier))\n\n        if isinstance(bias, bool):\n            bias_list: list[bool] = [bias] * (len(hidden_dims) + 1)  # type: ignore[arg-type]\n        else:\n            bias_list = bias\n        if len(bias_list) != len(hidden_dims) + 1:  # type: ignore[arg-type]\n            raise ValueError(\n                \"Expected `bias` to be a boolean or a list of booleans with length \"\n                \"equal to the number of linear layers in the MLP.\"\n            )\n\n        if isinstance(dropout, float):\n            dropout_list: list[float] = [dropout] * (len(hidden_dims) + 1)  # type: ignore[arg-type]\n        else:\n            dropout_list = dropout\n        if len(dropout_list) != len(hidden_dims) + 1:  # type: ignore[arg-type]\n            raise ValueError(\n                \"Expected `dropout` to be a float or a list of floats with length \"\n                \"equal to the number of linear layers in the MLP.\"\n            )\n\n        # construct list of dimensions for the layers\n        dims = [in_dim] + hidden_dims  # type: ignore[operator]\n        layers = []\n        for layer_idx, (in_features, hidden_features) in enumerate(\n            zip(dims[:-1], dims[1:], strict=False)\n        ):\n            layers.append(\n                torch.nn.Linear(in_features, hidden_features, bias=bias_list[layer_idx])\n            )\n            if norm_layer is not None:\n                layers.append(norm_layer(hidden_features))\n            if activation_layer is not None:\n                layers.append(activation_layer())\n            layers.append(torch.nn.Dropout(dropout_list[layer_idx]))\n\n        out_dim = out_dim or in_dim\n\n        layers.append(torch.nn.Linear(dims[-1], out_dim, bias=bias_list[-1]))\n        layers.append(torch.nn.Dropout(dropout_list[-1]))\n\n        super().__init__(*layers)\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.L2Norm","title":"L2Norm","text":"<p>               Bases: <code>Module</code></p> <p>L2 normalization.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>The dimension along which to normalize.</p> required Source code in <code>mmlearn/modules/layers/normalization.py</code> <pre><code>@store(group=\"modules/layers\", provider=\"mmlearn\")\nclass L2Norm(torch.nn.Module):\n    \"\"\"L2 normalization.\n\n    Parameters\n    ----------\n    dim : int\n        The dimension along which to normalize.\n    \"\"\"\n\n    def __init__(self, dim: int) -&gt; None:\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Apply L2 normalization to the input tensor.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor of shape ``(batch_sz, seq_len, dim)``.\n\n        Returns\n        -------\n        torch.Tensor\n            Normalized tensor of shape ``(batch_sz, seq_len, dim)``.\n        \"\"\"\n        return torch.nn.functional.normalize(x, dim=self.dim, p=2)\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.L2Norm.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Apply L2 normalization to the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape <code>(batch_sz, seq_len, dim)</code>.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized tensor of shape <code>(batch_sz, seq_len, dim)</code>.</p> Source code in <code>mmlearn/modules/layers/normalization.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Apply L2 normalization to the input tensor.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape ``(batch_sz, seq_len, dim)``.\n\n    Returns\n    -------\n    torch.Tensor\n        Normalized tensor of shape ``(batch_sz, seq_len, dim)``.\n    \"\"\"\n    return torch.nn.functional.normalize(x, dim=self.dim, p=2)\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.PatchDropout","title":"PatchDropout","text":"<p>               Bases: <code>Module</code></p> <p>Patch dropout layer.</p> <p>Drops patch tokens (after embedding and adding CLS token) from the input tensor. Usually used in vision transformers to reduce the number of tokens. [1]_</p> <p>Parameters:</p> Name Type Description Default <code>keep_rate</code> <code>float</code> <p>The proportion of tokens to keep.</p> <code>0.5</code> <code>bias</code> <code>Optional[float]</code> <p>The bias to add to the random noise before sorting.</p> <code>None</code> <code>token_shuffling</code> <code>bool</code> <p>If True, the tokens are shuffled.</p> <code>False</code> References <p>.. [1] Liu, Y., Matsoukas, C., Strand, F., Azizpour, H., &amp; Smith, K. (2023).    Patchdropout: Economizing vision transformers using patch dropout. In Proceedings    of the IEEE/CVF Winter Conference on Applications of Computer Vision    (pp. 3953-3962).</p> Source code in <code>mmlearn/modules/layers/patch_dropout.py</code> <pre><code>class PatchDropout(torch.nn.Module):\n    \"\"\"Patch dropout layer.\n\n    Drops patch tokens (after embedding and adding CLS token) from the input tensor.\n    Usually used in vision transformers to reduce the number of tokens. [1]_\n\n    Parameters\n    ----------\n    keep_rate : float, optional, default=0.5\n        The proportion of tokens to keep.\n    bias : Optional[float], optional, default=None\n        The bias to add to the random noise before sorting.\n    token_shuffling : bool, optional, default=False\n        If True, the tokens are shuffled.\n\n    References\n    ----------\n    .. [1] Liu, Y., Matsoukas, C., Strand, F., Azizpour, H., &amp; Smith, K. (2023).\n       Patchdropout: Economizing vision transformers using patch dropout. In Proceedings\n       of the IEEE/CVF Winter Conference on Applications of Computer Vision\n       (pp. 3953-3962).\n    \"\"\"\n\n    def __init__(\n        self,\n        keep_rate: float = 0.5,\n        bias: Optional[float] = None,\n        token_shuffling: bool = False,\n    ):\n        super().__init__()\n        assert 0 &lt; keep_rate &lt;= 1, \"The keep_rate must be in (0,1]\"\n\n        self.bias = bias\n        self.keep_rate = keep_rate\n        self.token_shuffling = token_shuffling\n\n    def forward(self, x: torch.Tensor, force_drop: bool = False) -&gt; torch.Tensor:\n        \"\"\"Drop tokens from the input tensor.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor of shape ``(batch_sz, seq_len, dim)``.\n        force_drop : bool, optional, default=False\n            If True, the tokens are always dropped, even when the model is in\n            evaluation mode.\n\n        Returns\n        -------\n        torch.Tensor\n            Tensor of shape ``(batch_sz, keep_len, dim)`` containing the kept tokens.\n        \"\"\"\n        if (not self.training and not force_drop) or self.keep_rate == 1:\n            return x\n\n        batch_sz, _, dim = x.shape\n\n        cls_mask = torch.zeros(  # assumes that CLS is always the 1st element\n            batch_sz, 1, dtype=torch.int64, device=x.device\n        )\n        patch_mask = self.uniform_mask(x)\n        patch_mask = torch.hstack([cls_mask, patch_mask])\n\n        return torch.gather(x, dim=1, index=patch_mask.unsqueeze(-1).repeat(1, 1, dim))\n\n    def uniform_mask(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Generate token ids to keep from uniform random distribution.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor of shape ``(batch_sz, seq_len, dim)``.\n\n        Returns\n        -------\n        torch.Tensor\n            Tensor of shape ``(batch_sz, keep_len)`` containing the token ids to keep.\n\n        \"\"\"\n        batch_sz, seq_len, _ = x.shape\n        seq_len = seq_len - 1  # patch length (without CLS)\n\n        keep_len = int(seq_len * self.keep_rate)\n        noise = torch.rand(batch_sz, seq_len, device=x.device)\n        if self.bias is not None:\n            noise += self.bias\n        ids = torch.argsort(noise, dim=1)\n        keep_ids = ids[:, :keep_len]\n        if not self.token_shuffling:\n            keep_ids = keep_ids.sort(1)[0]\n        return keep_ids\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.PatchDropout.forward","title":"forward","text":"<pre><code>forward(x, force_drop=False)\n</code></pre> <p>Drop tokens from the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape <code>(batch_sz, seq_len, dim)</code>.</p> required <code>force_drop</code> <code>bool</code> <p>If True, the tokens are always dropped, even when the model is in evaluation mode.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of shape <code>(batch_sz, keep_len, dim)</code> containing the kept tokens.</p> Source code in <code>mmlearn/modules/layers/patch_dropout.py</code> <pre><code>def forward(self, x: torch.Tensor, force_drop: bool = False) -&gt; torch.Tensor:\n    \"\"\"Drop tokens from the input tensor.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape ``(batch_sz, seq_len, dim)``.\n    force_drop : bool, optional, default=False\n        If True, the tokens are always dropped, even when the model is in\n        evaluation mode.\n\n    Returns\n    -------\n    torch.Tensor\n        Tensor of shape ``(batch_sz, keep_len, dim)`` containing the kept tokens.\n    \"\"\"\n    if (not self.training and not force_drop) or self.keep_rate == 1:\n        return x\n\n    batch_sz, _, dim = x.shape\n\n    cls_mask = torch.zeros(  # assumes that CLS is always the 1st element\n        batch_sz, 1, dtype=torch.int64, device=x.device\n    )\n    patch_mask = self.uniform_mask(x)\n    patch_mask = torch.hstack([cls_mask, patch_mask])\n\n    return torch.gather(x, dim=1, index=patch_mask.unsqueeze(-1).repeat(1, 1, dim))\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.PatchDropout.uniform_mask","title":"uniform_mask","text":"<pre><code>uniform_mask(x)\n</code></pre> <p>Generate token ids to keep from uniform random distribution.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape <code>(batch_sz, seq_len, dim)</code>.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of shape <code>(batch_sz, keep_len)</code> containing the token ids to keep.</p> Source code in <code>mmlearn/modules/layers/patch_dropout.py</code> <pre><code>def uniform_mask(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Generate token ids to keep from uniform random distribution.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape ``(batch_sz, seq_len, dim)``.\n\n    Returns\n    -------\n    torch.Tensor\n        Tensor of shape ``(batch_sz, keep_len)`` containing the token ids to keep.\n\n    \"\"\"\n    batch_sz, seq_len, _ = x.shape\n    seq_len = seq_len - 1  # patch length (without CLS)\n\n    keep_len = int(seq_len * self.keep_rate)\n    noise = torch.rand(batch_sz, seq_len, device=x.device)\n    if self.bias is not None:\n        noise += self.bias\n    ids = torch.argsort(noise, dim=1)\n    keep_ids = ids[:, :keep_len]\n    if not self.token_shuffling:\n        keep_ids = keep_ids.sort(1)[0]\n    return keep_ids\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.attention","title":"attention","text":"<p>Attention modules for Vision Transformer (ViT) and related models.</p>"},{"location":"api/#mmlearn.modules.layers.attention.Attention","title":"Attention","text":"<p>               Bases: <code>Module</code></p> <p>Multi-head Self-Attention Mechanism.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>Number of input dimensions.</p> required <code>num_heads</code> <code>int</code> <p>Number of attention heads.</p> <code>8</code> <code>qkv_bias</code> <code>bool</code> <p>If True, adds a learnable bias to the query, key, value projections.</p> <code>False</code> <code>qk_scale</code> <code>Optional[float]</code> <p>Override the default scale factor for the dot-product attention.</p> <code>None</code> <code>attn_drop</code> <code>float</code> <p>Dropout probability for the attention weights.</p> <code>0.0</code> <code>proj_drop</code> <code>float</code> <p>Dropout probability for the output of the attention layer.</p> <code>0.0</code> Source code in <code>mmlearn/modules/layers/attention.py</code> <pre><code>class Attention(nn.Module):\n    \"\"\"Multi-head Self-Attention Mechanism.\n\n    Parameters\n    ----------\n    dim : int\n        Number of input dimensions.\n    num_heads : int, optional, default=8\n        Number of attention heads.\n    qkv_bias : bool, optional, default=False\n        If True, adds a learnable bias to the query, key, value projections.\n    qk_scale : Optional[float], optional, default=None\n        Override the default scale factor for the dot-product attention.\n    attn_drop : float, optional, default=0.0\n        Dropout probability for the attention weights.\n    proj_drop : float, optional, default=0.0\n        Dropout probability for the output of the attention layer.\n    \"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int = 8,\n        qkv_bias: bool = False,\n        qk_scale: Optional[float] = None,\n        attn_drop: float = 0.0,\n        proj_drop: float = 0.0,\n    ) -&gt; None:\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim**-0.5\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x: torch.Tensor) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Forward pass through the multi-head self-attention module.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor of shape ``(batch_sz, seq_len, dim)``.\n\n        Returns\n        -------\n        tuple[torch.Tensor, torch.Tensor]\n            The output tensor and the attention weights.\n        \"\"\"\n        b, n, c = x.shape\n        qkv = (\n            self.qkv(x)\n            .reshape(b, n, 3, self.num_heads, c // self.num_heads)\n            .permute(2, 0, 3, 1, 4)\n        )\n        q, k, v = qkv[0], qkv[1], qkv[2]\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(b, n, c)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x, attn\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.attention.Attention.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Forward pass through the multi-head self-attention module.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape <code>(batch_sz, seq_len, dim)</code>.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>The output tensor and the attention weights.</p> Source code in <code>mmlearn/modules/layers/attention.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Forward pass through the multi-head self-attention module.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape ``(batch_sz, seq_len, dim)``.\n\n    Returns\n    -------\n    tuple[torch.Tensor, torch.Tensor]\n        The output tensor and the attention weights.\n    \"\"\"\n    b, n, c = x.shape\n    qkv = (\n        self.qkv(x)\n        .reshape(b, n, 3, self.num_heads, c // self.num_heads)\n        .permute(2, 0, 3, 1, 4)\n    )\n    q, k, v = qkv[0], qkv[1], qkv[2]\n\n    attn = (q @ k.transpose(-2, -1)) * self.scale\n    attn = attn.softmax(dim=-1)\n    attn = self.attn_drop(attn)\n\n    x = (attn @ v).transpose(1, 2).reshape(b, n, c)\n    x = self.proj(x)\n    x = self.proj_drop(x)\n    return x, attn\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.embedding","title":"embedding","text":"<p>Embedding layers.</p>"},{"location":"api/#mmlearn.modules.layers.embedding.PatchEmbed","title":"PatchEmbed","text":"<p>               Bases: <code>Module</code></p> <p>Image to Patch Embedding.</p> <p>This module divides an image into patches and embeds them as a sequence of vectors.</p> <p>Parameters:</p> Name Type Description Default <code>img_size</code> <code>int</code> <p>Size of the input image (assumed to be square).</p> <code>224</code> <code>patch_size</code> <code>int</code> <p>Size of each image patch (assumed to be square).</p> <code>16</code> <code>in_chans</code> <code>int</code> <p>Number of input channels in the image.</p> <code>3</code> <code>embed_dim</code> <code>int</code> <p>Dimension of the output embeddings.</p> <code>768</code> Source code in <code>mmlearn/modules/layers/embedding.py</code> <pre><code>class PatchEmbed(nn.Module):\n    \"\"\"Image to Patch Embedding.\n\n    This module divides an image into patches and embeds them as a sequence of vectors.\n\n    Parameters\n    ----------\n    img_size : int, optional, default=224\n        Size of the input image (assumed to be square).\n    patch_size : int, optional, default=16\n        Size of each image patch (assumed to be square).\n    in_chans : int, optional, default=3\n        Number of input channels in the image.\n    embed_dim : int, optional, default=768\n        Dimension of the output embeddings.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        img_size: int = 224,\n        patch_size: int = 16,\n        in_chans: int = 3,\n        embed_dim: int = 768,\n    ) -&gt; None:\n        super().__init__()\n        num_patches = (img_size // patch_size) * (img_size // patch_size)\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n\n        self.proj = nn.Conv2d(\n            in_chans, embed_dim, kernel_size=patch_size, stride=patch_size\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass to convert an image into patch embeddings.\"\"\"\n        return self.proj(x).flatten(2).transpose(1, 2)\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.embedding.PatchEmbed.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Forward pass to convert an image into patch embeddings.</p> Source code in <code>mmlearn/modules/layers/embedding.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass to convert an image into patch embeddings.\"\"\"\n    return self.proj(x).flatten(2).transpose(1, 2)\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.embedding.ConvEmbed","title":"ConvEmbed","text":"<p>               Bases: <code>Module</code></p> <p>3x3 Convolution stems for ViT following ViTC models.</p> <p>This module builds convolutional stems for Vision Transformers (ViT) with intermediate batch normalization and ReLU activation.</p> <p>Parameters:</p> Name Type Description Default <code>channels</code> <code>list[int]</code> <p>list of channel sizes for each convolution layer.</p> required <code>strides</code> <code>list[int]</code> <p>list of stride sizes for each convolution layer.</p> required <code>img_size</code> <code>int</code> <p>Size of the input image (assumed to be square).</p> <code>224</code> <code>in_chans</code> <code>int</code> <p>Number of input channels in the image.</p> <code>3</code> <code>batch_norm</code> <code>bool</code> <p>Whether to include batch normalization after each convolution layer.</p> <code>True</code> Source code in <code>mmlearn/modules/layers/embedding.py</code> <pre><code>class ConvEmbed(nn.Module):\n    \"\"\"3x3 Convolution stems for ViT following ViTC models.\n\n    This module builds convolutional stems for Vision Transformers (ViT)\n    with intermediate batch normalization and ReLU activation.\n\n    Parameters\n    ----------\n    channels : list[int]\n        list of channel sizes for each convolution layer.\n    strides : list[int]\n        list of stride sizes for each convolution layer.\n    img_size : int, optional, default=224\n        Size of the input image (assumed to be square).\n    in_chans : int, optional, default=3\n        Number of input channels in the image.\n    batch_norm : bool, optional, default=True\n        Whether to include batch normalization after each convolution layer.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        channels: list[int],\n        strides: list[int],\n        img_size: int = 224,\n        in_chans: int = 3,\n        batch_norm: bool = True,\n    ) -&gt; None:\n        super().__init__()\n        # Build the stems\n        stem = []\n        channels = [in_chans] + channels\n        for i in range(len(channels) - 2):\n            stem += [\n                nn.Conv2d(\n                    channels[i],\n                    channels[i + 1],\n                    kernel_size=3,\n                    stride=strides[i],\n                    padding=1,\n                    bias=(not batch_norm),\n                )\n            ]\n            if batch_norm:\n                stem += [nn.BatchNorm2d(channels[i + 1])]\n            stem += [nn.ReLU(inplace=True)]\n        stem += [\n            nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])\n        ]\n        self.stem = nn.Sequential(*stem)\n\n        # Compute the number of patches\n        stride_prod = int(np.prod(strides))\n        self.num_patches = (img_size // stride_prod) ** 2\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass through the convolutional embedding layers.\"\"\"\n        p = self.stem(x)\n        return p.flatten(2).transpose(1, 2)\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.embedding.ConvEmbed.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Forward pass through the convolutional embedding layers.</p> Source code in <code>mmlearn/modules/layers/embedding.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass through the convolutional embedding layers.\"\"\"\n    p = self.stem(x)\n    return p.flatten(2).transpose(1, 2)\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.embedding.get_2d_sincos_pos_embed","title":"get_2d_sincos_pos_embed","text":"<pre><code>get_2d_sincos_pos_embed(\n    embed_dim, grid_size, cls_token=False\n)\n</code></pre> <p>Generate 2D sine-cosine positional embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>embed_dim</code> <code>int</code> <p>The dimension of the embeddings.</p> required <code>grid_size</code> <code>int</code> <p>The size of the grid (both height and width).</p> required <code>cls_token</code> <code>bool</code> <p>Whether to include a class token in the embeddings.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>pos_embed</code> <code>ndarray</code> <p>Positional embeddings with shape [grid_sizegrid_size, embed_dim] or [1 + grid_sizegrid_size, embed_dim] if cls_token is True.</p> Source code in <code>mmlearn/modules/layers/embedding.py</code> <pre><code>def get_2d_sincos_pos_embed(\n    embed_dim: int, grid_size: int, cls_token: bool = False\n) -&gt; np.ndarray:\n    \"\"\"\n    Generate 2D sine-cosine positional embeddings.\n\n    Parameters\n    ----------\n    embed_dim : int\n        The dimension of the embeddings.\n    grid_size : int\n        The size of the grid (both height and width).\n    cls_token : bool, optional, default=False\n        Whether to include a class token in the embeddings.\n\n    Returns\n    -------\n    pos_embed : np.ndarray\n        Positional embeddings with shape [grid_size*grid_size, embed_dim] or\n        [1 + grid_size*grid_size, embed_dim] if cls_token is True.\n    \"\"\"\n    grid_h = np.arange(grid_size, dtype=float)\n    grid_w = np.arange(grid_size, dtype=float)\n    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n    grid = np.stack(grid, axis=0)\n\n    grid = grid.reshape([2, 1, grid_size, grid_size])\n    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n    if cls_token:\n        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n    return pos_embed\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.embedding.get_2d_sincos_pos_embed_from_grid","title":"get_2d_sincos_pos_embed_from_grid","text":"<pre><code>get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n</code></pre> <p>Generate 2D sine-cosine positional embeddings from a grid.</p> <p>Parameters:</p> Name Type Description Default <code>embed_dim</code> <code>int</code> <p>The dimension of the embeddings.</p> required <code>grid</code> <code>ndarray</code> <p>The grid of positions with shape [2, 1, grid_size, grid_size].</p> required <p>Returns:</p> Name Type Description <code>emb</code> <code>ndarray</code> <p>Positional embeddings with shape [grid_size*grid_size, embed_dim].</p> Source code in <code>mmlearn/modules/layers/embedding.py</code> <pre><code>def get_2d_sincos_pos_embed_from_grid(embed_dim: int, grid: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Generate 2D sine-cosine positional embeddings from a grid.\n\n    Parameters\n    ----------\n    embed_dim : int\n        The dimension of the embeddings.\n    grid : np.ndarray\n        The grid of positions with shape [2, 1, grid_size, grid_size].\n\n    Returns\n    -------\n    emb : np.ndarray\n        Positional embeddings with shape [grid_size*grid_size, embed_dim].\n    \"\"\"\n    assert embed_dim % 2 == 0\n\n    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n\n    return np.concatenate([emb_h, emb_w], axis=1)\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.embedding.get_1d_sincos_pos_embed","title":"get_1d_sincos_pos_embed","text":"<pre><code>get_1d_sincos_pos_embed(\n    embed_dim, grid_size, cls_token=False\n)\n</code></pre> <p>Generate 1D sine-cosine positional embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>embed_dim</code> <code>int</code> <p>The dimension of the embeddings.</p> required <code>grid_size</code> <code>int</code> <p>The size of the grid.</p> required <code>cls_token</code> <code>bool</code> <p>Whether to include a class token in the embeddings.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>pos_embed</code> <code>ndarray</code> <p>Positional embeddings with shape [grid_size, embed_dim] or [1 + grid_size, embed_dim] if cls_token is True.</p> Source code in <code>mmlearn/modules/layers/embedding.py</code> <pre><code>def get_1d_sincos_pos_embed(\n    embed_dim: int, grid_size: int, cls_token: bool = False\n) -&gt; np.ndarray:\n    \"\"\"\n    Generate 1D sine-cosine positional embeddings.\n\n    Parameters\n    ----------\n    embed_dim : int\n        The dimension of the embeddings.\n    grid_size : int\n        The size of the grid.\n    cls_token : bool, optional, default=False\n        Whether to include a class token in the embeddings.\n\n    Returns\n    -------\n    pos_embed : np.ndarray\n        Positional embeddings with shape [grid_size, embed_dim] or\n        [1 + grid_size, embed_dim] if cls_token is True.\n    \"\"\"\n    grid = np.arange(grid_size, dtype=float)\n    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n    if cls_token:\n        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n    return pos_embed\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.embedding.get_1d_sincos_pos_embed_from_grid","title":"get_1d_sincos_pos_embed_from_grid","text":"<pre><code>get_1d_sincos_pos_embed_from_grid(embed_dim, pos)\n</code></pre> <p>Generate 1D sine-cosine positional embeddings from a grid.</p> <p>Parameters:</p> Name Type Description Default <code>embed_dim</code> <code>int</code> <p>The dimension of the embeddings.</p> required <code>pos</code> <code>ndarray</code> <p>A list of positions to be encoded, with shape [M,].</p> required <p>Returns:</p> Name Type Description <code>emb</code> <code>ndarray</code> <p>Positional embeddings with shape [M, embed_dim].</p> Source code in <code>mmlearn/modules/layers/embedding.py</code> <pre><code>def get_1d_sincos_pos_embed_from_grid(embed_dim: int, pos: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Generate 1D sine-cosine positional embeddings from a grid.\n\n    Parameters\n    ----------\n    embed_dim : int\n        The dimension of the embeddings.\n    pos : np.ndarray\n        A list of positions to be encoded, with shape [M,].\n\n    Returns\n    -------\n    emb : np.ndarray\n        Positional embeddings with shape [M, embed_dim].\n    \"\"\"\n    assert embed_dim % 2 == 0\n    omega = np.arange(embed_dim // 2, dtype=float)\n    omega /= embed_dim / 2.0\n    omega = 1.0 / 10000**omega  # (D/2,)\n\n    pos = pos.reshape(-1)  # (M,)\n    out = np.einsum(\"m,d-&gt;md\", pos, omega)  # (M, D/2), outer product\n\n    emb_sin = np.sin(out)  # (M, D/2)\n    emb_cos = np.cos(out)  # (M, D/2)\n\n    return np.concatenate([emb_sin, emb_cos], axis=1)\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.logit_scaling","title":"logit_scaling","text":"<p>Learnable logit scaling layer.</p>"},{"location":"api/#mmlearn.modules.layers.logit_scaling.LearnableLogitScaling","title":"LearnableLogitScaling","text":"<p>               Bases: <code>Module</code></p> <p>Logit scaling layer.</p> <p>Parameters:</p> Name Type Description Default <code>init_logit_scale</code> <code>float</code> <p>Initial value of the logit scale.</p> <code>1/0.07</code> <code>learnable</code> <code>bool</code> <p>If True, the logit scale is learnable. Otherwise, it is fixed.</p> <code>True</code> <code>max_logit_scale</code> <code>float</code> <p>Maximum value of the logit scale.</p> <code>100</code> Source code in <code>mmlearn/modules/layers/logit_scaling.py</code> <pre><code>@store(group=\"modules/layers\", provider=\"mmlearn\")\nclass LearnableLogitScaling(torch.nn.Module):\n    \"\"\"Logit scaling layer.\n\n    Parameters\n    ----------\n    init_logit_scale : float, optional, default=1/0.07\n        Initial value of the logit scale.\n    learnable : bool, optional, default=True\n        If True, the logit scale is learnable. Otherwise, it is fixed.\n    max_logit_scale : float, optional, default=100\n        Maximum value of the logit scale.\n    \"\"\"\n\n    def __init__(\n        self,\n        init_logit_scale: float = 1 / 0.07,\n        max_logit_scale: float = 100,\n        learnable: bool = True,\n    ) -&gt; None:\n        super().__init__()\n        self.max_logit_scale = max_logit_scale\n        self.init_logit_scale = init_logit_scale\n        self.learnable = learnable\n        log_logit_scale = torch.ones([]) * np.log(self.init_logit_scale)\n        if learnable:\n            self.log_logit_scale = torch.nn.Parameter(log_logit_scale)\n        else:\n            self.register_buffer(\"log_logit_scale\", log_logit_scale)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Apply the logit scaling to the input tensor.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor of shape ``(batch_sz, seq_len, dim)``.\n        \"\"\"\n        return torch.clip(self.log_logit_scale.exp(), max=self.max_logit_scale) * x\n\n    def extra_repr(self) -&gt; str:\n        \"\"\"Return the string representation of the layer.\"\"\"\n        return (\n            f\"logit_scale_init={self.init_logit_scale},learnable={self.learnable},\"\n            f\" max_logit_scale={self.max_logit_scale}\"\n        )\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.logit_scaling.LearnableLogitScaling.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Apply the logit scaling to the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape <code>(batch_sz, seq_len, dim)</code>.</p> required Source code in <code>mmlearn/modules/layers/logit_scaling.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Apply the logit scaling to the input tensor.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape ``(batch_sz, seq_len, dim)``.\n    \"\"\"\n    return torch.clip(self.log_logit_scale.exp(), max=self.max_logit_scale) * x\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.logit_scaling.LearnableLogitScaling.extra_repr","title":"extra_repr","text":"<pre><code>extra_repr()\n</code></pre> <p>Return the string representation of the layer.</p> Source code in <code>mmlearn/modules/layers/logit_scaling.py</code> <pre><code>def extra_repr(self) -&gt; str:\n    \"\"\"Return the string representation of the layer.\"\"\"\n    return (\n        f\"logit_scale_init={self.init_logit_scale},learnable={self.learnable},\"\n        f\" max_logit_scale={self.max_logit_scale}\"\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.mlp","title":"mlp","text":"<p>Multi-layer perceptron (MLP).</p>"},{"location":"api/#mmlearn.modules.layers.mlp.MLP","title":"MLP","text":"<p>               Bases: <code>Sequential</code></p> <p>Multi-layer perceptron (MLP).</p> <p>This module will create a block of <code>Linear -&gt; Normalization -&gt; Activation -&gt; Dropout</code> layers.</p> <p>Parameters:</p> Name Type Description Default <code>in_dim</code> <code>int</code> <p>The input dimension.</p> required <code>out_dim</code> <code>Optional[int]</code> <p>The output dimension. If not specified, it is set to :attr:<code>in_dim</code>.</p> <code>None</code> <code>hidden_dims</code> <code>Optional[list]</code> <p>The dimensions of the hidden layers. The length of the list determines the number of hidden layers. This parameter is mutually exclusive with :attr:<code>hidden_dims_multiplier</code>.</p> <code>None</code> <code>hidden_dims_multiplier</code> <code>Optional[list]</code> <p>The multipliers to apply to the input dimension to get the dimensions of the hidden layers. The length of the list determines the number of hidden layers. The multipliers will be used to get the dimensions of the hidden layers. This parameter is mutually exclusive with <code>hidden_dims</code>.</p> <code>None</code> <code>apply_multiplier_to_in_dim</code> <code>bool</code> <p>Whether to apply the :attr:<code>hidden_dims_multiplier</code> to :attr:<code>in_dim</code> to get the dimensions of the hidden layers. If <code>False</code>, the multipliers will be applied to the dimensions of the previous hidden layer, starting from :attr:<code>in_dim</code>. This parameter is only relevant when :attr:<code>hidden_dims_multiplier</code> is specified.</p> <code>False</code> <code>norm_layer</code> <code>Optional[Callable[..., Module]]</code> <p>The normalization layer to use. If not specified, no normalization is used. Partial functions can be used to specify the normalization layer with specific parameters.</p> <code>None</code> <code>activation_layer</code> <code>Optional[Callable[..., Module]]</code> <p>The activation layer to use. If not specified, ReLU is used. Partial functions can be used to specify the activation layer with specific parameters.</p> <code>torch.nn.ReLU</code> <code>bias</code> <code>Union[bool, list[bool]]</code> <p>Whether to use bias in the linear layers.</p> <code>True</code> <code>dropout</code> <code>Union[float, list[float]]</code> <p>The dropout probability to use.</p> <code>0.0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If both :attr:<code>hidden_dims</code> and :attr:<code>hidden_dims_multiplier</code> are specified or if the lengths of :attr:<code>bias</code> and :attr:<code>hidden_dims</code> do not match or if the lengths of :attr:<code>dropout</code> and :attr:<code>hidden_dims</code> do not match.</p> Source code in <code>mmlearn/modules/layers/mlp.py</code> <pre><code>@store(group=\"modules/layers\", provider=\"mmlearn\")\nclass MLP(torch.nn.Sequential):\n    \"\"\"Multi-layer perceptron (MLP).\n\n    This module will create a block of ``Linear -&gt; Normalization -&gt; Activation -&gt; Dropout``\n    layers.\n\n    Parameters\n    ----------\n    in_dim : int\n        The input dimension.\n    out_dim : Optional[int], optional, default=None\n        The output dimension. If not specified, it is set to :attr:`in_dim`.\n    hidden_dims : Optional[list], optional, default=None\n        The dimensions of the hidden layers. The length of the list determines the\n        number of hidden layers. This parameter is mutually exclusive with\n        :attr:`hidden_dims_multiplier`.\n    hidden_dims_multiplier : Optional[list], optional, default=None\n        The multipliers to apply to the input dimension to get the dimensions of\n        the hidden layers. The length of the list determines the number of hidden\n        layers. The multipliers will be used to get the dimensions of the hidden\n        layers. This parameter is mutually exclusive with `hidden_dims`.\n    apply_multiplier_to_in_dim : bool, optional, default=False\n        Whether to apply the :attr:`hidden_dims_multiplier` to :attr:`in_dim` to get the\n        dimensions of the hidden layers. If ``False``, the multipliers will be applied\n        to the dimensions of the previous hidden layer, starting from :attr:`in_dim`.\n        This parameter is only relevant when :attr:`hidden_dims_multiplier` is\n        specified.\n    norm_layer : Optional[Callable[..., torch.nn.Module]], optional, default=None\n        The normalization layer to use. If not specified, no normalization is used.\n        Partial functions can be used to specify the normalization layer with specific\n        parameters.\n    activation_layer : Optional[Callable[..., torch.nn.Module]], optional, default=torch.nn.ReLU\n        The activation layer to use. If not specified, ReLU is used. Partial functions\n        can be used to specify the activation layer with specific parameters.\n    bias : Union[bool, list[bool]], optional, default=True\n        Whether to use bias in the linear layers.\n    dropout : Union[float, list[float]], optional, default=0.0\n        The dropout probability to use.\n\n    Raises\n    ------\n    ValueError\n        If both :attr:`hidden_dims` and :attr:`hidden_dims_multiplier` are specified\n        or if the lengths of :attr:`bias` and :attr:`hidden_dims` do not match or if\n        the lengths of :attr:`dropout` and :attr:`hidden_dims` do not match.\n\n    \"\"\"  # noqa: W505\n\n    def __init__(  # noqa: PLR0912\n        self,\n        in_dim: int,\n        out_dim: Optional[int] = None,\n        hidden_dims: Optional[list[int]] = None,\n        hidden_dims_multiplier: Optional[list[float]] = None,\n        apply_multiplier_to_in_dim: bool = False,\n        norm_layer: Optional[Callable[..., torch.nn.Module]] = None,\n        activation_layer: Optional[Callable[..., torch.nn.Module]] = torch.nn.ReLU,\n        bias: Union[bool, list[bool]] = True,\n        dropout: Union[float, list[float]] = 0.0,\n    ) -&gt; None:\n        if hidden_dims is None and hidden_dims_multiplier is None:\n            hidden_dims = []\n        if hidden_dims is not None and hidden_dims_multiplier is not None:\n            raise ValueError(\n                \"Only one of `hidden_dims` or `hidden_dims_multiplier` must be specified.\"\n            )\n\n        if hidden_dims is None and hidden_dims_multiplier is not None:\n            if apply_multiplier_to_in_dim:\n                hidden_dims = [\n                    int(in_dim * multiplier) for multiplier in hidden_dims_multiplier\n                ]\n            else:\n                hidden_dims = [int(in_dim * hidden_dims_multiplier[0])]\n                for multiplier in hidden_dims_multiplier[1:]:\n                    hidden_dims.append(int(hidden_dims[-1] * multiplier))\n\n        if isinstance(bias, bool):\n            bias_list: list[bool] = [bias] * (len(hidden_dims) + 1)  # type: ignore[arg-type]\n        else:\n            bias_list = bias\n        if len(bias_list) != len(hidden_dims) + 1:  # type: ignore[arg-type]\n            raise ValueError(\n                \"Expected `bias` to be a boolean or a list of booleans with length \"\n                \"equal to the number of linear layers in the MLP.\"\n            )\n\n        if isinstance(dropout, float):\n            dropout_list: list[float] = [dropout] * (len(hidden_dims) + 1)  # type: ignore[arg-type]\n        else:\n            dropout_list = dropout\n        if len(dropout_list) != len(hidden_dims) + 1:  # type: ignore[arg-type]\n            raise ValueError(\n                \"Expected `dropout` to be a float or a list of floats with length \"\n                \"equal to the number of linear layers in the MLP.\"\n            )\n\n        # construct list of dimensions for the layers\n        dims = [in_dim] + hidden_dims  # type: ignore[operator]\n        layers = []\n        for layer_idx, (in_features, hidden_features) in enumerate(\n            zip(dims[:-1], dims[1:], strict=False)\n        ):\n            layers.append(\n                torch.nn.Linear(in_features, hidden_features, bias=bias_list[layer_idx])\n            )\n            if norm_layer is not None:\n                layers.append(norm_layer(hidden_features))\n            if activation_layer is not None:\n                layers.append(activation_layer())\n            layers.append(torch.nn.Dropout(dropout_list[layer_idx]))\n\n        out_dim = out_dim or in_dim\n\n        layers.append(torch.nn.Linear(dims[-1], out_dim, bias=bias_list[-1]))\n        layers.append(torch.nn.Dropout(dropout_list[-1]))\n\n        super().__init__(*layers)\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.normalization","title":"normalization","text":"<p>Normalization layers.</p>"},{"location":"api/#mmlearn.modules.layers.normalization.L2Norm","title":"L2Norm","text":"<p>               Bases: <code>Module</code></p> <p>L2 normalization.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>The dimension along which to normalize.</p> required Source code in <code>mmlearn/modules/layers/normalization.py</code> <pre><code>@store(group=\"modules/layers\", provider=\"mmlearn\")\nclass L2Norm(torch.nn.Module):\n    \"\"\"L2 normalization.\n\n    Parameters\n    ----------\n    dim : int\n        The dimension along which to normalize.\n    \"\"\"\n\n    def __init__(self, dim: int) -&gt; None:\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Apply L2 normalization to the input tensor.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor of shape ``(batch_sz, seq_len, dim)``.\n\n        Returns\n        -------\n        torch.Tensor\n            Normalized tensor of shape ``(batch_sz, seq_len, dim)``.\n        \"\"\"\n        return torch.nn.functional.normalize(x, dim=self.dim, p=2)\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.normalization.L2Norm.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Apply L2 normalization to the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape <code>(batch_sz, seq_len, dim)</code>.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized tensor of shape <code>(batch_sz, seq_len, dim)</code>.</p> Source code in <code>mmlearn/modules/layers/normalization.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Apply L2 normalization to the input tensor.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape ``(batch_sz, seq_len, dim)``.\n\n    Returns\n    -------\n    torch.Tensor\n        Normalized tensor of shape ``(batch_sz, seq_len, dim)``.\n    \"\"\"\n    return torch.nn.functional.normalize(x, dim=self.dim, p=2)\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.patch_dropout","title":"patch_dropout","text":"<p>Patch dropout layer.</p>"},{"location":"api/#mmlearn.modules.layers.patch_dropout.PatchDropout","title":"PatchDropout","text":"<p>               Bases: <code>Module</code></p> <p>Patch dropout layer.</p> <p>Drops patch tokens (after embedding and adding CLS token) from the input tensor. Usually used in vision transformers to reduce the number of tokens. [1]_</p> <p>Parameters:</p> Name Type Description Default <code>keep_rate</code> <code>float</code> <p>The proportion of tokens to keep.</p> <code>0.5</code> <code>bias</code> <code>Optional[float]</code> <p>The bias to add to the random noise before sorting.</p> <code>None</code> <code>token_shuffling</code> <code>bool</code> <p>If True, the tokens are shuffled.</p> <code>False</code> References <p>.. [1] Liu, Y., Matsoukas, C., Strand, F., Azizpour, H., &amp; Smith, K. (2023).    Patchdropout: Economizing vision transformers using patch dropout. In Proceedings    of the IEEE/CVF Winter Conference on Applications of Computer Vision    (pp. 3953-3962).</p> Source code in <code>mmlearn/modules/layers/patch_dropout.py</code> <pre><code>class PatchDropout(torch.nn.Module):\n    \"\"\"Patch dropout layer.\n\n    Drops patch tokens (after embedding and adding CLS token) from the input tensor.\n    Usually used in vision transformers to reduce the number of tokens. [1]_\n\n    Parameters\n    ----------\n    keep_rate : float, optional, default=0.5\n        The proportion of tokens to keep.\n    bias : Optional[float], optional, default=None\n        The bias to add to the random noise before sorting.\n    token_shuffling : bool, optional, default=False\n        If True, the tokens are shuffled.\n\n    References\n    ----------\n    .. [1] Liu, Y., Matsoukas, C., Strand, F., Azizpour, H., &amp; Smith, K. (2023).\n       Patchdropout: Economizing vision transformers using patch dropout. In Proceedings\n       of the IEEE/CVF Winter Conference on Applications of Computer Vision\n       (pp. 3953-3962).\n    \"\"\"\n\n    def __init__(\n        self,\n        keep_rate: float = 0.5,\n        bias: Optional[float] = None,\n        token_shuffling: bool = False,\n    ):\n        super().__init__()\n        assert 0 &lt; keep_rate &lt;= 1, \"The keep_rate must be in (0,1]\"\n\n        self.bias = bias\n        self.keep_rate = keep_rate\n        self.token_shuffling = token_shuffling\n\n    def forward(self, x: torch.Tensor, force_drop: bool = False) -&gt; torch.Tensor:\n        \"\"\"Drop tokens from the input tensor.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor of shape ``(batch_sz, seq_len, dim)``.\n        force_drop : bool, optional, default=False\n            If True, the tokens are always dropped, even when the model is in\n            evaluation mode.\n\n        Returns\n        -------\n        torch.Tensor\n            Tensor of shape ``(batch_sz, keep_len, dim)`` containing the kept tokens.\n        \"\"\"\n        if (not self.training and not force_drop) or self.keep_rate == 1:\n            return x\n\n        batch_sz, _, dim = x.shape\n\n        cls_mask = torch.zeros(  # assumes that CLS is always the 1st element\n            batch_sz, 1, dtype=torch.int64, device=x.device\n        )\n        patch_mask = self.uniform_mask(x)\n        patch_mask = torch.hstack([cls_mask, patch_mask])\n\n        return torch.gather(x, dim=1, index=patch_mask.unsqueeze(-1).repeat(1, 1, dim))\n\n    def uniform_mask(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Generate token ids to keep from uniform random distribution.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor of shape ``(batch_sz, seq_len, dim)``.\n\n        Returns\n        -------\n        torch.Tensor\n            Tensor of shape ``(batch_sz, keep_len)`` containing the token ids to keep.\n\n        \"\"\"\n        batch_sz, seq_len, _ = x.shape\n        seq_len = seq_len - 1  # patch length (without CLS)\n\n        keep_len = int(seq_len * self.keep_rate)\n        noise = torch.rand(batch_sz, seq_len, device=x.device)\n        if self.bias is not None:\n            noise += self.bias\n        ids = torch.argsort(noise, dim=1)\n        keep_ids = ids[:, :keep_len]\n        if not self.token_shuffling:\n            keep_ids = keep_ids.sort(1)[0]\n        return keep_ids\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.patch_dropout.PatchDropout.forward","title":"forward","text":"<pre><code>forward(x, force_drop=False)\n</code></pre> <p>Drop tokens from the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape <code>(batch_sz, seq_len, dim)</code>.</p> required <code>force_drop</code> <code>bool</code> <p>If True, the tokens are always dropped, even when the model is in evaluation mode.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of shape <code>(batch_sz, keep_len, dim)</code> containing the kept tokens.</p> Source code in <code>mmlearn/modules/layers/patch_dropout.py</code> <pre><code>def forward(self, x: torch.Tensor, force_drop: bool = False) -&gt; torch.Tensor:\n    \"\"\"Drop tokens from the input tensor.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape ``(batch_sz, seq_len, dim)``.\n    force_drop : bool, optional, default=False\n        If True, the tokens are always dropped, even when the model is in\n        evaluation mode.\n\n    Returns\n    -------\n    torch.Tensor\n        Tensor of shape ``(batch_sz, keep_len, dim)`` containing the kept tokens.\n    \"\"\"\n    if (not self.training and not force_drop) or self.keep_rate == 1:\n        return x\n\n    batch_sz, _, dim = x.shape\n\n    cls_mask = torch.zeros(  # assumes that CLS is always the 1st element\n        batch_sz, 1, dtype=torch.int64, device=x.device\n    )\n    patch_mask = self.uniform_mask(x)\n    patch_mask = torch.hstack([cls_mask, patch_mask])\n\n    return torch.gather(x, dim=1, index=patch_mask.unsqueeze(-1).repeat(1, 1, dim))\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.patch_dropout.PatchDropout.uniform_mask","title":"uniform_mask","text":"<pre><code>uniform_mask(x)\n</code></pre> <p>Generate token ids to keep from uniform random distribution.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape <code>(batch_sz, seq_len, dim)</code>.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of shape <code>(batch_sz, keep_len)</code> containing the token ids to keep.</p> Source code in <code>mmlearn/modules/layers/patch_dropout.py</code> <pre><code>def uniform_mask(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Generate token ids to keep from uniform random distribution.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape ``(batch_sz, seq_len, dim)``.\n\n    Returns\n    -------\n    torch.Tensor\n        Tensor of shape ``(batch_sz, keep_len)`` containing the token ids to keep.\n\n    \"\"\"\n    batch_sz, seq_len, _ = x.shape\n    seq_len = seq_len - 1  # patch length (without CLS)\n\n    keep_len = int(seq_len * self.keep_rate)\n    noise = torch.rand(batch_sz, seq_len, device=x.device)\n    if self.bias is not None:\n        noise += self.bias\n    ids = torch.argsort(noise, dim=1)\n    keep_ids = ids[:, :keep_len]\n    if not self.token_shuffling:\n        keep_ids = keep_ids.sort(1)[0]\n    return keep_ids\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.transformer_block","title":"transformer_block","text":"<p>Transformer Block and Embedding Modules for Vision Transformers (ViT).</p>"},{"location":"api/#mmlearn.modules.layers.transformer_block.DropPath","title":"DropPath","text":"<p>               Bases: <code>Module</code></p> <p>Drop paths (Stochastic Depth) per sample.</p> <p>Parameters:</p> Name Type Description Default <code>drop_prob</code> <code>float</code> <p>Probability of dropping paths.</p> <code>0.0</code> Source code in <code>mmlearn/modules/layers/transformer_block.py</code> <pre><code>class DropPath(nn.Module):\n    \"\"\"Drop paths (Stochastic Depth) per sample.\n\n    Parameters\n    ----------\n    drop_prob : float, optional, default=0.0\n        Probability of dropping paths.\n    \"\"\"\n\n    def __init__(self, drop_prob: float = 0.0) -&gt; None:\n        super().__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass through DropPath module.\"\"\"\n        return drop_path(x, self.drop_prob, self.training)\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.transformer_block.DropPath.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Forward pass through DropPath module.</p> Source code in <code>mmlearn/modules/layers/transformer_block.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass through DropPath module.\"\"\"\n    return drop_path(x, self.drop_prob, self.training)\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.transformer_block.Block","title":"Block","text":"<p>               Bases: <code>Module</code></p> <p>Transformer Block.</p> <p>This module represents a Transformer block that includes self-attention, normalization layers, and a feedforward multi-layer perceptron (MLP) network.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>The input and output dimension of the block.</p> required <code>num_heads</code> <code>int</code> <p>Number of attention heads.</p> required <code>mlp_ratio</code> <code>float</code> <p>Ratio of hidden dimension to the input dimension in the MLP.</p> <code>4.0</code> <code>qkv_bias</code> <code>bool</code> <p>If True, add a learnable bias to the query, key, value projections.</p> <code>False</code> <code>qk_scale</code> <code>Optional[float]</code> <p>Override default qk scale of head_dim ** -0.5 if set.</p> <code>None</code> <code>drop</code> <code>float</code> <p>Dropout probability for the output of attention and MLP layers.</p> <code>0.0</code> <code>attn_drop</code> <code>float</code> <p>Dropout probability for the attention scores.</p> <code>0.0</code> <code>drop_path</code> <code>float</code> <p>Stochastic depth rate, a form of layer dropout.</p> <code>0.0</code> <code>act_layer</code> <code>Callable[..., Module]</code> <p>Activation layer in the MLP.</p> <code>nn.GELU</code> <code>norm_layer</code> <code>Callable[..., Module]</code> <p>Normalization layer.</p> <code>torch.nn.LayerNorm</code> Source code in <code>mmlearn/modules/layers/transformer_block.py</code> <pre><code>class Block(nn.Module):\n    \"\"\"Transformer Block.\n\n    This module represents a Transformer block that includes self-attention,\n    normalization layers, and a feedforward multi-layer perceptron (MLP) network.\n\n    Parameters\n    ----------\n    dim : int\n        The input and output dimension of the block.\n    num_heads : int\n        Number of attention heads.\n    mlp_ratio : float, optional, default=4.0\n        Ratio of hidden dimension to the input dimension in the MLP.\n    qkv_bias : bool, optional, default=False\n        If True, add a learnable bias to the query, key, value projections.\n    qk_scale : Optional[float], optional, default=None\n        Override default qk scale of head_dim ** -0.5 if set.\n    drop : float, optional, default=0.0\n        Dropout probability for the output of attention and MLP layers.\n    attn_drop : float, optional, default=0.0\n        Dropout probability for the attention scores.\n    drop_path : float, optional, default=0.0\n        Stochastic depth rate, a form of layer dropout.\n    act_layer : Callable[..., torch.nn.Module], optional, default=nn.GELU\n        Activation layer in the MLP.\n    norm_layer : Callable[..., torch.nn.Module], optional, default=torch.nn.LayerNorm\n        Normalization layer.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int,\n        mlp_ratio: float = 4.0,\n        qkv_bias: bool = False,\n        qk_scale: Optional[float] = None,\n        drop: float = 0.0,\n        attn_drop: float = 0.0,\n        drop_path: float = 0.0,\n        act_layer: Callable[..., nn.Module] = nn.GELU,\n        norm_layer: Callable[..., nn.Module] = nn.LayerNorm,\n    ) -&gt; None:\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(\n            dim,\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            qk_scale=qk_scale,\n            attn_drop=attn_drop,\n            proj_drop=drop,\n        )\n        self.drop_path = DropPath(drop_path) if drop_path &gt; 0.0 else nn.Identity()\n        self.norm2 = norm_layer(dim)\n\n        self.mlp = MLP(\n            in_dim=dim,\n            hidden_dims_multiplier=[mlp_ratio],\n            activation_layer=act_layer,\n            bias=True,\n            dropout=drop,\n        )\n\n    def forward(\n        self, x: torch.Tensor, return_attention: bool = False\n    ) -&gt; Union[torch.Tensor, torch.Tensor]:\n        \"\"\"Forward pass through the Transformer Block.\"\"\"\n        y, attn = self.attn(self.norm1(x))\n        if return_attention:\n            return attn\n        x = x + self.drop_path(y)\n        return x + self.drop_path(self.mlp(self.norm2(x)))\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.transformer_block.Block.forward","title":"forward","text":"<pre><code>forward(x, return_attention=False)\n</code></pre> <p>Forward pass through the Transformer Block.</p> Source code in <code>mmlearn/modules/layers/transformer_block.py</code> <pre><code>def forward(\n    self, x: torch.Tensor, return_attention: bool = False\n) -&gt; Union[torch.Tensor, torch.Tensor]:\n    \"\"\"Forward pass through the Transformer Block.\"\"\"\n    y, attn = self.attn(self.norm1(x))\n    if return_attention:\n        return attn\n    x = x + self.drop_path(y)\n    return x + self.drop_path(self.mlp(self.norm2(x)))\n</code></pre>"},{"location":"api/#mmlearn.modules.layers.transformer_block.drop_path","title":"drop_path","text":"<pre><code>drop_path(x, drop_prob=0.0, training=False)\n</code></pre> <p>Drop paths (Stochastic Depth) for regularization during training.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <code>drop_prob</code> <code>float</code> <p>Probability of dropping paths.</p> <code>0.0</code> <code>training</code> <code>bool</code> <p>Whether the model is in training mode.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>output</code> <code>Tensor</code> <p>Output tensor after applying drop path.</p> Source code in <code>mmlearn/modules/layers/transformer_block.py</code> <pre><code>def drop_path(\n    x: torch.Tensor, drop_prob: float = 0.0, training: bool = False\n) -&gt; torch.Tensor:\n    \"\"\"Drop paths (Stochastic Depth) for regularization during training.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor.\n    drop_prob : float, optional, default=0.0\n        Probability of dropping paths.\n    training : bool, optional, default=False\n        Whether the model is in training mode.\n\n    Returns\n    -------\n    output : torch.Tensor\n        Output tensor after applying drop path.\n    \"\"\"\n    if drop_prob == 0.0 or not training:\n        return x\n    keep_prob = 1 - drop_prob\n    shape = (x.shape[0],) + (1,) * (\n        x.ndim - 1\n    )  # work with diff dim tensors, not just 2D ConvNets\n    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n    random_tensor.floor_()  # binarize\n    return x.div(keep_prob) * random_tensor\n</code></pre>"},{"location":"api/#losses","title":"Losses","text":""},{"location":"api/#mmlearn.modules.losses","title":"mmlearn.modules.losses","text":"<p>Loss functions.</p>"},{"location":"api/#mmlearn.modules.losses.ContrastiveLoss","title":"ContrastiveLoss","text":"<p>               Bases: <code>Module</code></p> <p>Contrastive Loss.</p> <p>Parameters:</p> Name Type Description Default <code>l2_normalize</code> <code>bool</code> <p>Whether to L2 normalize the features.</p> <code>False</code> <code>local_loss</code> <code>bool</code> <p>Whether to calculate the loss locally i.e. <code>local_features@global_features</code>.</p> <code>False</code> <code>gather_with_grad</code> <code>bool</code> <p>Whether to gather tensors with gradients.</p> <code>False</code> <code>modality_alignment</code> <code>bool</code> <p>Whether to include modality alignment loss. This loss considers all features from the same modality as positive pairs and all features from different modalities as negative pairs.</p> <code>False</code> <code>cache_labels</code> <code>bool</code> <p>Whether to cache the labels.</p> <code>False</code> Source code in <code>mmlearn/modules/losses/contrastive.py</code> <pre><code>@store(group=\"modules/losses\", provider=\"mmlearn\")\nclass ContrastiveLoss(nn.Module):\n    \"\"\"Contrastive Loss.\n\n    Parameters\n    ----------\n    l2_normalize : bool, optional, default=False\n        Whether to L2 normalize the features.\n    local_loss : bool, optional, default=False\n        Whether to calculate the loss locally i.e. ``local_features@global_features``.\n    gather_with_grad : bool, optional, default=False\n        Whether to gather tensors with gradients.\n    modality_alignment : bool, optional, default=False\n        Whether to include modality alignment loss. This loss considers all features\n        from the same modality as positive pairs and all features from different\n        modalities as negative pairs.\n    cache_labels : bool, optional, default=False\n        Whether to cache the labels.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        l2_normalize: bool = False,\n        local_loss: bool = False,\n        gather_with_grad: bool = False,\n        modality_alignment: bool = False,\n        cache_labels: bool = False,\n    ):\n        super().__init__()\n        self.local_loss = local_loss\n        self.gather_with_grad = gather_with_grad\n        self.cache_labels = cache_labels\n        self.l2_normalize = l2_normalize\n        self.modality_alignment = modality_alignment\n\n        # cache state\n        self._prev_num_logits = 0\n        self._labels: dict[torch.device, torch.Tensor] = {}\n\n    def forward(\n        self,\n        embeddings: dict[str, torch.Tensor],\n        example_ids: dict[str, torch.Tensor],\n        logit_scale: torch.Tensor,\n        modality_loss_pairs: list[LossPairSpec],\n    ) -&gt; torch.Tensor:\n        \"\"\"Calculate the contrastive loss.\n\n        Parameters\n        ----------\n        embeddings : dict[str, torch.Tensor]\n            Dictionary of embeddings, where the key is the modality name and the value\n            is the corresponding embedding tensor.\n        example_ids : dict[str, torch.Tensor]\n            Dictionary of example IDs, where the key is the modality name and the value\n            is a tensor tuple of the dataset index and the example index.\n        logit_scale : torch.Tensor\n            Scale factor for the logits.\n        modality_loss_pairs : list[LossPairSpec]\n            Specification of the modality pairs for which the loss should be calculated.\n\n        Returns\n        -------\n        torch.Tensor\n            The contrastive loss.\n        \"\"\"\n        world_size = dist.get_world_size() if dist.is_initialized() else 1\n        rank = dist.get_rank() if world_size &gt; 1 else 0\n\n        if self.l2_normalize:\n            embeddings = {k: F.normalize(v, p=2, dim=-1) for k, v in embeddings.items()}\n\n        if world_size &gt; 1:  # gather embeddings and example_ids across all processes\n            # NOTE: gathering dictionaries of tensors across all processes\n            # (keys + values, as opposed to just values) is especially important\n            # for the modality_alignment loss, which requires all embeddings\n            all_embeddings = _gather_dicts(\n                embeddings,\n                local_loss=self.local_loss,\n                gather_with_grad=self.gather_with_grad,\n                rank=rank,\n            )\n            all_example_ids = _gather_dicts(\n                example_ids,\n                local_loss=self.local_loss,\n                gather_with_grad=self.gather_with_grad,\n                rank=rank,\n            )\n        else:\n            all_embeddings = embeddings\n            all_example_ids = example_ids\n\n        losses = []\n        for loss_pairs in modality_loss_pairs:\n            logits_per_feature_a, logits_per_feature_b, skip_flag = self._get_logits(\n                loss_pairs.modalities,\n                per_device_embeddings=embeddings,\n                all_embeddings=all_embeddings,\n                per_device_example_ids=example_ids,\n                all_example_ids=all_example_ids,\n                logit_scale=logit_scale,\n                world_size=world_size,\n            )\n            if logits_per_feature_a is None or logits_per_feature_b is None:\n                continue\n\n            labels = self._get_ground_truth(\n                logits_per_feature_a.shape,\n                device=logits_per_feature_a.device,\n                rank=rank,\n                world_size=world_size,\n                skipped_process=skip_flag,\n            )\n\n            if labels.numel() != 0:\n                losses.append(\n                    (\n                        (\n                            F.cross_entropy(logits_per_feature_a, labels)\n                            + F.cross_entropy(logits_per_feature_b, labels)\n                        )\n                        / 2\n                    )\n                    * loss_pairs.weight\n                )\n\n        if self.modality_alignment:\n            losses.append(\n                self._compute_modality_alignment_loss(all_embeddings, logit_scale)\n            )\n\n        if not losses:  # no loss to compute (e.g. no paired data in batch)\n            losses.append(\n                torch.tensor(\n                    0.0,\n                    device=logit_scale.device,\n                    dtype=next(iter(embeddings.values())).dtype,\n                )\n            )\n\n        return torch.stack(losses).sum()\n\n    def _get_ground_truth(\n        self,\n        logits_shape: tuple[int, int],\n        device: torch.device,\n        rank: int,\n        world_size: int,\n        skipped_process: bool,\n    ) -&gt; torch.Tensor:\n        \"\"\"Return the ground-truth labels.\n\n        Parameters\n        ----------\n        logits_shape : tuple[int, int]\n            Shape of the logits tensor.\n        device : torch.device\n            Device on which the labels should be created.\n        rank : int\n            Rank of the current process.\n        world_size : int\n            Number of processes.\n        skipped_process : bool\n            Whether the current process skipped the computation due to lack of data.\n\n        Returns\n        -------\n        torch.Tensor\n            Ground-truth labels.\n        \"\"\"\n        num_logits = logits_shape[-1]\n\n        # calculate ground-truth and cache if enabled\n        if self._prev_num_logits != num_logits or device not in self._labels:\n            labels = torch.arange(num_logits, device=device, dtype=torch.long)\n\n            if world_size &gt; 1 and self.local_loss:\n                local_size = torch.tensor(\n                    0 if skipped_process else logits_shape[0], device=device\n                )\n                # NOTE: all processes must participate in the all_gather operation\n                # even if they have no data to contribute.\n                sizes = torch.stack(\n                    _simple_gather_all_tensors(\n                        local_size, group=dist.group.WORLD, world_size=world_size\n                    )\n                )\n                sizes = torch.cat(\n                    [torch.tensor([0], device=sizes.device), torch.cumsum(sizes, dim=0)]\n                )\n                labels = labels[\n                    sizes[rank] : sizes[rank + 1] if rank + 1 &lt; world_size else None\n                ]\n\n            if self.cache_labels:\n                self._labels[device] = labels\n                self._prev_num_logits = num_logits\n        else:\n            labels = self._labels[device]\n        return labels\n\n    def _get_logits(  # noqa: PLR0912\n        self,\n        modalities: tuple[str, str],\n        per_device_embeddings: dict[str, torch.Tensor],\n        all_embeddings: dict[str, torch.Tensor],\n        per_device_example_ids: dict[str, torch.Tensor],\n        all_example_ids: dict[str, torch.Tensor],\n        logit_scale: torch.Tensor,\n        world_size: int,\n    ) -&gt; tuple[Optional[torch.Tensor], Optional[torch.Tensor], bool]:\n        \"\"\"Calculate the logits for the given modalities.\n\n        Parameters\n        ----------\n        modalities : tuple[str, str]\n            Tuple of modality names.\n        per_device_embeddings : dict[str, torch.Tensor]\n            Dictionary of embeddings, where the key is the modality name and the value\n            is the corresponding embedding tensor.\n        all_embeddings : dict[str, torch.Tensor]\n            Dictionary of embeddings, where the key is the modality name and the value\n            is the corresponding embedding tensor. In distributed mode, this contains\n            embeddings from all processes.\n        per_device_example_ids : dict[str, torch.Tensor]\n            Dictionary of example IDs, where the key is the modality name and the value\n            is a tensor tuple of the dataset index and the example index.\n        all_example_ids : dict[str, torch.Tensor]\n            Dictionary of example IDs, where the key is the modality name and the value\n            is a tensor tuple of the dataset index and the example index. In distributed\n            mode, this contains example IDs from all processes.\n        logit_scale : torch.Tensor\n            Scale factor for the logits.\n        world_size : int\n            Number of processes.\n\n        Returns\n        -------\n        tuple[Optional[torch.Tensor], Optional[torch.Tensor], bool]\n            Tuple of logits for the given modalities. If embeddings for the given\n            modalities are not available, returns `None` for the logits. The last\n            element is a flag indicating whether the process skipped the computation\n            due to lack of data.\n        \"\"\"\n        modality_a = Modalities.get_modality(modalities[0])\n        modality_b = Modalities.get_modality(modalities[1])\n        skip_flag = False\n\n        if self.local_loss or world_size == 1:\n            if not (\n                modality_a.embedding in per_device_embeddings\n                and modality_b.embedding in per_device_embeddings\n            ):\n                if world_size &gt; 1:  # NOTE: not all processes exit here, hence skip_flag\n                    skip_flag = True\n                else:\n                    return None, None, skip_flag\n\n            if not skip_flag:\n                indices_a, indices_b = find_matching_indices(\n                    per_device_example_ids[modality_a.name],\n                    per_device_example_ids[modality_b.name],\n                )\n                if indices_a.numel() == 0 or indices_b.numel() == 0:\n                    if world_size &gt; 1:  # not all processes exit here\n                        skip_flag = True\n                    else:\n                        return None, None, skip_flag\n\n            if not skip_flag:\n                features_a = per_device_embeddings[modality_a.embedding][indices_a]\n                features_b = per_device_embeddings[modality_b.embedding][indices_b]\n            else:\n                # all processes must participate in the all_gather operation\n                # that follows, even if they have no data to contribute. So,\n                # we create empty tensors here.\n                features_a = torch.empty(\n                    0, device=list(per_device_embeddings.values())[0].device\n                )\n                features_b = torch.empty(\n                    0, device=list(per_device_embeddings.values())[0].device\n                )\n\n        if world_size &gt; 1:\n            if not (\n                modality_a.embedding in all_embeddings\n                and modality_b.embedding in all_embeddings\n            ):  # all processes exit here\n                return None, None, skip_flag\n\n            indices_a, indices_b = find_matching_indices(\n                all_example_ids[modality_a.name],\n                all_example_ids[modality_b.name],\n            )\n            if indices_a.numel() == 0 or indices_b.numel() == 0:\n                # all processes exit here\n                return None, None, skip_flag\n\n            all_features_a = all_embeddings[modality_a.embedding][indices_a]\n            all_features_b = all_embeddings[modality_b.embedding][indices_b]\n\n            if self.local_loss:\n                if features_a.numel() == 0:\n                    features_a = all_features_a\n                if features_b.numel() == 0:\n                    features_b = all_features_b\n\n                logits_per_feature_a = logit_scale * _safe_matmul(\n                    features_a, all_features_b\n                )\n                logits_per_feature_b = logit_scale * _safe_matmul(\n                    features_b, all_features_a\n                )\n            else:\n                logits_per_feature_a = logit_scale * _safe_matmul(\n                    all_features_a, all_features_b\n                )\n                logits_per_feature_b = logits_per_feature_a.T\n        else:\n            logits_per_feature_a = logit_scale * _safe_matmul(features_a, features_b)\n            logits_per_feature_b = logit_scale * _safe_matmul(features_b, features_a)\n\n        return logits_per_feature_a, logits_per_feature_b, skip_flag\n\n    def _compute_modality_alignment_loss(\n        self, all_embeddings: dict[str, torch.Tensor], logit_scale: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"Compute the modality alignment loss.\n\n        This loss considers all features from the same modality as positive pairs\n        and all features from different modalities as negative pairs.\n\n        Parameters\n        ----------\n        all_embeddings : dict[str, torch.Tensor]\n            Dictionary of embeddings, where the key is the modality name and the value\n            is the corresponding embedding tensor.\n        logit_scale : torch.Tensor\n            Scale factor for the logits.\n\n        Returns\n        -------\n        torch.Tensor\n            Modality alignment loss.\n\n        Notes\n        -----\n        This loss does not support `local_loss=True`.\n        \"\"\"\n        available_modalities = list(all_embeddings.keys())\n        # TODO: support local_loss for modality_alignment?\n        # if world_size == 1, all_embeddings == embeddings\n        all_features = torch.cat(list(all_embeddings.values()), dim=0)\n\n        positive_indices = torch.tensor(\n            [\n                (i, j)\n                if idx == 0\n                else (\n                    i + all_embeddings[available_modalities[idx - 1]].size(0),\n                    j + all_embeddings[available_modalities[idx - 1]].size(0),\n                )\n                for idx, k in enumerate(all_embeddings)\n                for i, j in itertools.combinations(range(all_embeddings[k].size(0)), 2)\n            ],\n            device=all_features.device,\n        )\n        logits = logit_scale * _safe_matmul(all_features, all_features)\n\n        target = torch.eye(all_features.size(0), device=all_features.device)\n        target[positive_indices[:, 0], positive_indices[:, 1]] = 1\n\n        modality_loss = torch.nn.functional.binary_cross_entropy_with_logits(\n            logits, target, reduction=\"none\"\n        )\n\n        target_pos = target.bool()\n        target_neg = ~target_pos\n\n        # loss_pos and loss_neg below contain non-zero values only for those\n        # elements that are positive pairs and negative pairs respectively.\n        loss_pos = torch.zeros(\n            logits.size(0), logits.size(0), device=target.device\n        ).masked_scatter(target_pos, modality_loss[target_pos])\n        loss_neg = torch.zeros(\n            logits.size(0), logits.size(0), device=target.device\n        ).masked_scatter(target_neg, modality_loss[target_neg])\n\n        loss_pos = loss_pos.sum(dim=1)\n        loss_neg = loss_neg.sum(dim=1)\n        num_pos = target.sum(dim=1)\n        num_neg = logits.size(0) - num_pos\n\n        return ((loss_pos / num_pos) + (loss_neg / num_neg)).mean()\n</code></pre>"},{"location":"api/#mmlearn.modules.losses.ContrastiveLoss.forward","title":"forward","text":"<pre><code>forward(\n    embeddings,\n    example_ids,\n    logit_scale,\n    modality_loss_pairs,\n)\n</code></pre> <p>Calculate the contrastive loss.</p> <p>Parameters:</p> Name Type Description Default <code>embeddings</code> <code>dict[str, Tensor]</code> <p>Dictionary of embeddings, where the key is the modality name and the value is the corresponding embedding tensor.</p> required <code>example_ids</code> <code>dict[str, Tensor]</code> <p>Dictionary of example IDs, where the key is the modality name and the value is a tensor tuple of the dataset index and the example index.</p> required <code>logit_scale</code> <code>Tensor</code> <p>Scale factor for the logits.</p> required <code>modality_loss_pairs</code> <code>list[LossPairSpec]</code> <p>Specification of the modality pairs for which the loss should be calculated.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The contrastive loss.</p> Source code in <code>mmlearn/modules/losses/contrastive.py</code> <pre><code>def forward(\n    self,\n    embeddings: dict[str, torch.Tensor],\n    example_ids: dict[str, torch.Tensor],\n    logit_scale: torch.Tensor,\n    modality_loss_pairs: list[LossPairSpec],\n) -&gt; torch.Tensor:\n    \"\"\"Calculate the contrastive loss.\n\n    Parameters\n    ----------\n    embeddings : dict[str, torch.Tensor]\n        Dictionary of embeddings, where the key is the modality name and the value\n        is the corresponding embedding tensor.\n    example_ids : dict[str, torch.Tensor]\n        Dictionary of example IDs, where the key is the modality name and the value\n        is a tensor tuple of the dataset index and the example index.\n    logit_scale : torch.Tensor\n        Scale factor for the logits.\n    modality_loss_pairs : list[LossPairSpec]\n        Specification of the modality pairs for which the loss should be calculated.\n\n    Returns\n    -------\n    torch.Tensor\n        The contrastive loss.\n    \"\"\"\n    world_size = dist.get_world_size() if dist.is_initialized() else 1\n    rank = dist.get_rank() if world_size &gt; 1 else 0\n\n    if self.l2_normalize:\n        embeddings = {k: F.normalize(v, p=2, dim=-1) for k, v in embeddings.items()}\n\n    if world_size &gt; 1:  # gather embeddings and example_ids across all processes\n        # NOTE: gathering dictionaries of tensors across all processes\n        # (keys + values, as opposed to just values) is especially important\n        # for the modality_alignment loss, which requires all embeddings\n        all_embeddings = _gather_dicts(\n            embeddings,\n            local_loss=self.local_loss,\n            gather_with_grad=self.gather_with_grad,\n            rank=rank,\n        )\n        all_example_ids = _gather_dicts(\n            example_ids,\n            local_loss=self.local_loss,\n            gather_with_grad=self.gather_with_grad,\n            rank=rank,\n        )\n    else:\n        all_embeddings = embeddings\n        all_example_ids = example_ids\n\n    losses = []\n    for loss_pairs in modality_loss_pairs:\n        logits_per_feature_a, logits_per_feature_b, skip_flag = self._get_logits(\n            loss_pairs.modalities,\n            per_device_embeddings=embeddings,\n            all_embeddings=all_embeddings,\n            per_device_example_ids=example_ids,\n            all_example_ids=all_example_ids,\n            logit_scale=logit_scale,\n            world_size=world_size,\n        )\n        if logits_per_feature_a is None or logits_per_feature_b is None:\n            continue\n\n        labels = self._get_ground_truth(\n            logits_per_feature_a.shape,\n            device=logits_per_feature_a.device,\n            rank=rank,\n            world_size=world_size,\n            skipped_process=skip_flag,\n        )\n\n        if labels.numel() != 0:\n            losses.append(\n                (\n                    (\n                        F.cross_entropy(logits_per_feature_a, labels)\n                        + F.cross_entropy(logits_per_feature_b, labels)\n                    )\n                    / 2\n                )\n                * loss_pairs.weight\n            )\n\n    if self.modality_alignment:\n        losses.append(\n            self._compute_modality_alignment_loss(all_embeddings, logit_scale)\n        )\n\n    if not losses:  # no loss to compute (e.g. no paired data in batch)\n        losses.append(\n            torch.tensor(\n                0.0,\n                device=logit_scale.device,\n                dtype=next(iter(embeddings.values())).dtype,\n            )\n        )\n\n    return torch.stack(losses).sum()\n</code></pre>"},{"location":"api/#mmlearn.modules.losses.Data2VecLoss","title":"Data2VecLoss","text":"<p>               Bases: <code>Module</code></p> <p>Data2Vec loss function.</p> <p>Parameters:</p> Name Type Description Default <code>beta</code> <code>float</code> <p>Specifies the beta parameter for smooth L1 loss. If <code>0</code>, MSE loss is used.</p> <code>0</code> <code>loss_scale</code> <code>Optional[float]</code> <p>Scaling factor for the loss. If None, uses <code>1 / sqrt(embedding_dim)</code>.</p> <code>None</code> <code>reduction</code> <code>str</code> <p>Specifies the reduction to apply to the output: <code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>.</p> <code>'none'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the reduction mode is not supported.</p> Source code in <code>mmlearn/modules/losses/data2vec.py</code> <pre><code>@store(group=\"modules/losses\", provider=\"mmlearn\")\nclass Data2VecLoss(nn.Module):\n    \"\"\"Data2Vec loss function.\n\n    Parameters\n    ----------\n    beta : float, optional, default=0\n        Specifies the beta parameter for smooth L1 loss. If ``0``, MSE loss is used.\n    loss_scale : Optional[float], optional, default=None\n        Scaling factor for the loss. If None, uses ``1 / sqrt(embedding_dim)``.\n    reduction : str, optional, default='none'\n        Specifies the reduction to apply to the output:\n        ``'none'`` | ``'mean'`` | ``'sum'``.\n\n    Raises\n    ------\n    ValueError\n        If the reduction mode is not supported.\n    \"\"\"\n\n    def __init__(\n        self,\n        beta: float = 0,\n        loss_scale: Optional[float] = None,\n        reduction: str = \"none\",\n    ) -&gt; None:\n        super().__init__()\n        self.beta = beta\n        self.loss_scale = loss_scale\n        if reduction not in [\"none\", \"mean\", \"sum\"]:\n            raise ValueError(f\"Unsupported reduction mode: {reduction}\")\n        self.reduction = reduction\n\n    def forward(self, x: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute the Data2Vec loss.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Predicted embeddings of shape ``(batch_size, num_patches, embedding_dim)``.\n        y : torch.Tensor\n            Target embeddings of shape ``(batch_size, num_patches, embedding_dim)``.\n\n        Returns\n        -------\n        torch.Tensor\n            Data2Vec loss value.\n\n        Raises\n        ------\n        ValueError\n            If the shapes of x and y do not match.\n        \"\"\"\n        if x.shape != y.shape:\n            raise ValueError(f\"Shape mismatch: x: {x.shape}, y: {y.shape}\")\n\n        x = x.view(-1, x.size(-1)).float()\n        y = y.view(-1, y.size(-1))\n\n        if self.beta == 0:\n            loss = mse_loss(x, y, reduction=\"none\")\n        else:\n            loss = smooth_l1_loss(x, y, reduction=\"none\", beta=self.beta)\n\n        if self.loss_scale is not None:\n            scale = self.loss_scale\n        else:\n            scale = 1 / math.sqrt(x.size(-1))\n\n        loss = loss * scale\n\n        if self.reduction == \"mean\":\n            return loss.mean()\n        if self.reduction == \"sum\":\n            return loss.sum()\n        # 'none'\n        return loss.view(x.size(0), -1).sum(1)\n</code></pre>"},{"location":"api/#mmlearn.modules.losses.Data2VecLoss.forward","title":"forward","text":"<pre><code>forward(x, y)\n</code></pre> <p>Compute the Data2Vec loss.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Predicted embeddings of shape <code>(batch_size, num_patches, embedding_dim)</code>.</p> required <code>y</code> <code>Tensor</code> <p>Target embeddings of shape <code>(batch_size, num_patches, embedding_dim)</code>.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Data2Vec loss value.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the shapes of x and y do not match.</p> Source code in <code>mmlearn/modules/losses/data2vec.py</code> <pre><code>def forward(self, x: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute the Data2Vec loss.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Predicted embeddings of shape ``(batch_size, num_patches, embedding_dim)``.\n    y : torch.Tensor\n        Target embeddings of shape ``(batch_size, num_patches, embedding_dim)``.\n\n    Returns\n    -------\n    torch.Tensor\n        Data2Vec loss value.\n\n    Raises\n    ------\n    ValueError\n        If the shapes of x and y do not match.\n    \"\"\"\n    if x.shape != y.shape:\n        raise ValueError(f\"Shape mismatch: x: {x.shape}, y: {y.shape}\")\n\n    x = x.view(-1, x.size(-1)).float()\n    y = y.view(-1, y.size(-1))\n\n    if self.beta == 0:\n        loss = mse_loss(x, y, reduction=\"none\")\n    else:\n        loss = smooth_l1_loss(x, y, reduction=\"none\", beta=self.beta)\n\n    if self.loss_scale is not None:\n        scale = self.loss_scale\n    else:\n        scale = 1 / math.sqrt(x.size(-1))\n\n    loss = loss * scale\n\n    if self.reduction == \"mean\":\n        return loss.mean()\n    if self.reduction == \"sum\":\n        return loss.sum()\n    # 'none'\n    return loss.view(x.size(0), -1).sum(1)\n</code></pre>"},{"location":"api/#mmlearn.modules.losses.contrastive","title":"contrastive","text":"<p>Implementations of the contrastive loss and its variants.</p>"},{"location":"api/#mmlearn.modules.losses.contrastive.ContrastiveLoss","title":"ContrastiveLoss","text":"<p>               Bases: <code>Module</code></p> <p>Contrastive Loss.</p> <p>Parameters:</p> Name Type Description Default <code>l2_normalize</code> <code>bool</code> <p>Whether to L2 normalize the features.</p> <code>False</code> <code>local_loss</code> <code>bool</code> <p>Whether to calculate the loss locally i.e. <code>local_features@global_features</code>.</p> <code>False</code> <code>gather_with_grad</code> <code>bool</code> <p>Whether to gather tensors with gradients.</p> <code>False</code> <code>modality_alignment</code> <code>bool</code> <p>Whether to include modality alignment loss. This loss considers all features from the same modality as positive pairs and all features from different modalities as negative pairs.</p> <code>False</code> <code>cache_labels</code> <code>bool</code> <p>Whether to cache the labels.</p> <code>False</code> Source code in <code>mmlearn/modules/losses/contrastive.py</code> <pre><code>@store(group=\"modules/losses\", provider=\"mmlearn\")\nclass ContrastiveLoss(nn.Module):\n    \"\"\"Contrastive Loss.\n\n    Parameters\n    ----------\n    l2_normalize : bool, optional, default=False\n        Whether to L2 normalize the features.\n    local_loss : bool, optional, default=False\n        Whether to calculate the loss locally i.e. ``local_features@global_features``.\n    gather_with_grad : bool, optional, default=False\n        Whether to gather tensors with gradients.\n    modality_alignment : bool, optional, default=False\n        Whether to include modality alignment loss. This loss considers all features\n        from the same modality as positive pairs and all features from different\n        modalities as negative pairs.\n    cache_labels : bool, optional, default=False\n        Whether to cache the labels.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        l2_normalize: bool = False,\n        local_loss: bool = False,\n        gather_with_grad: bool = False,\n        modality_alignment: bool = False,\n        cache_labels: bool = False,\n    ):\n        super().__init__()\n        self.local_loss = local_loss\n        self.gather_with_grad = gather_with_grad\n        self.cache_labels = cache_labels\n        self.l2_normalize = l2_normalize\n        self.modality_alignment = modality_alignment\n\n        # cache state\n        self._prev_num_logits = 0\n        self._labels: dict[torch.device, torch.Tensor] = {}\n\n    def forward(\n        self,\n        embeddings: dict[str, torch.Tensor],\n        example_ids: dict[str, torch.Tensor],\n        logit_scale: torch.Tensor,\n        modality_loss_pairs: list[LossPairSpec],\n    ) -&gt; torch.Tensor:\n        \"\"\"Calculate the contrastive loss.\n\n        Parameters\n        ----------\n        embeddings : dict[str, torch.Tensor]\n            Dictionary of embeddings, where the key is the modality name and the value\n            is the corresponding embedding tensor.\n        example_ids : dict[str, torch.Tensor]\n            Dictionary of example IDs, where the key is the modality name and the value\n            is a tensor tuple of the dataset index and the example index.\n        logit_scale : torch.Tensor\n            Scale factor for the logits.\n        modality_loss_pairs : list[LossPairSpec]\n            Specification of the modality pairs for which the loss should be calculated.\n\n        Returns\n        -------\n        torch.Tensor\n            The contrastive loss.\n        \"\"\"\n        world_size = dist.get_world_size() if dist.is_initialized() else 1\n        rank = dist.get_rank() if world_size &gt; 1 else 0\n\n        if self.l2_normalize:\n            embeddings = {k: F.normalize(v, p=2, dim=-1) for k, v in embeddings.items()}\n\n        if world_size &gt; 1:  # gather embeddings and example_ids across all processes\n            # NOTE: gathering dictionaries of tensors across all processes\n            # (keys + values, as opposed to just values) is especially important\n            # for the modality_alignment loss, which requires all embeddings\n            all_embeddings = _gather_dicts(\n                embeddings,\n                local_loss=self.local_loss,\n                gather_with_grad=self.gather_with_grad,\n                rank=rank,\n            )\n            all_example_ids = _gather_dicts(\n                example_ids,\n                local_loss=self.local_loss,\n                gather_with_grad=self.gather_with_grad,\n                rank=rank,\n            )\n        else:\n            all_embeddings = embeddings\n            all_example_ids = example_ids\n\n        losses = []\n        for loss_pairs in modality_loss_pairs:\n            logits_per_feature_a, logits_per_feature_b, skip_flag = self._get_logits(\n                loss_pairs.modalities,\n                per_device_embeddings=embeddings,\n                all_embeddings=all_embeddings,\n                per_device_example_ids=example_ids,\n                all_example_ids=all_example_ids,\n                logit_scale=logit_scale,\n                world_size=world_size,\n            )\n            if logits_per_feature_a is None or logits_per_feature_b is None:\n                continue\n\n            labels = self._get_ground_truth(\n                logits_per_feature_a.shape,\n                device=logits_per_feature_a.device,\n                rank=rank,\n                world_size=world_size,\n                skipped_process=skip_flag,\n            )\n\n            if labels.numel() != 0:\n                losses.append(\n                    (\n                        (\n                            F.cross_entropy(logits_per_feature_a, labels)\n                            + F.cross_entropy(logits_per_feature_b, labels)\n                        )\n                        / 2\n                    )\n                    * loss_pairs.weight\n                )\n\n        if self.modality_alignment:\n            losses.append(\n                self._compute_modality_alignment_loss(all_embeddings, logit_scale)\n            )\n\n        if not losses:  # no loss to compute (e.g. no paired data in batch)\n            losses.append(\n                torch.tensor(\n                    0.0,\n                    device=logit_scale.device,\n                    dtype=next(iter(embeddings.values())).dtype,\n                )\n            )\n\n        return torch.stack(losses).sum()\n\n    def _get_ground_truth(\n        self,\n        logits_shape: tuple[int, int],\n        device: torch.device,\n        rank: int,\n        world_size: int,\n        skipped_process: bool,\n    ) -&gt; torch.Tensor:\n        \"\"\"Return the ground-truth labels.\n\n        Parameters\n        ----------\n        logits_shape : tuple[int, int]\n            Shape of the logits tensor.\n        device : torch.device\n            Device on which the labels should be created.\n        rank : int\n            Rank of the current process.\n        world_size : int\n            Number of processes.\n        skipped_process : bool\n            Whether the current process skipped the computation due to lack of data.\n\n        Returns\n        -------\n        torch.Tensor\n            Ground-truth labels.\n        \"\"\"\n        num_logits = logits_shape[-1]\n\n        # calculate ground-truth and cache if enabled\n        if self._prev_num_logits != num_logits or device not in self._labels:\n            labels = torch.arange(num_logits, device=device, dtype=torch.long)\n\n            if world_size &gt; 1 and self.local_loss:\n                local_size = torch.tensor(\n                    0 if skipped_process else logits_shape[0], device=device\n                )\n                # NOTE: all processes must participate in the all_gather operation\n                # even if they have no data to contribute.\n                sizes = torch.stack(\n                    _simple_gather_all_tensors(\n                        local_size, group=dist.group.WORLD, world_size=world_size\n                    )\n                )\n                sizes = torch.cat(\n                    [torch.tensor([0], device=sizes.device), torch.cumsum(sizes, dim=0)]\n                )\n                labels = labels[\n                    sizes[rank] : sizes[rank + 1] if rank + 1 &lt; world_size else None\n                ]\n\n            if self.cache_labels:\n                self._labels[device] = labels\n                self._prev_num_logits = num_logits\n        else:\n            labels = self._labels[device]\n        return labels\n\n    def _get_logits(  # noqa: PLR0912\n        self,\n        modalities: tuple[str, str],\n        per_device_embeddings: dict[str, torch.Tensor],\n        all_embeddings: dict[str, torch.Tensor],\n        per_device_example_ids: dict[str, torch.Tensor],\n        all_example_ids: dict[str, torch.Tensor],\n        logit_scale: torch.Tensor,\n        world_size: int,\n    ) -&gt; tuple[Optional[torch.Tensor], Optional[torch.Tensor], bool]:\n        \"\"\"Calculate the logits for the given modalities.\n\n        Parameters\n        ----------\n        modalities : tuple[str, str]\n            Tuple of modality names.\n        per_device_embeddings : dict[str, torch.Tensor]\n            Dictionary of embeddings, where the key is the modality name and the value\n            is the corresponding embedding tensor.\n        all_embeddings : dict[str, torch.Tensor]\n            Dictionary of embeddings, where the key is the modality name and the value\n            is the corresponding embedding tensor. In distributed mode, this contains\n            embeddings from all processes.\n        per_device_example_ids : dict[str, torch.Tensor]\n            Dictionary of example IDs, where the key is the modality name and the value\n            is a tensor tuple of the dataset index and the example index.\n        all_example_ids : dict[str, torch.Tensor]\n            Dictionary of example IDs, where the key is the modality name and the value\n            is a tensor tuple of the dataset index and the example index. In distributed\n            mode, this contains example IDs from all processes.\n        logit_scale : torch.Tensor\n            Scale factor for the logits.\n        world_size : int\n            Number of processes.\n\n        Returns\n        -------\n        tuple[Optional[torch.Tensor], Optional[torch.Tensor], bool]\n            Tuple of logits for the given modalities. If embeddings for the given\n            modalities are not available, returns `None` for the logits. The last\n            element is a flag indicating whether the process skipped the computation\n            due to lack of data.\n        \"\"\"\n        modality_a = Modalities.get_modality(modalities[0])\n        modality_b = Modalities.get_modality(modalities[1])\n        skip_flag = False\n\n        if self.local_loss or world_size == 1:\n            if not (\n                modality_a.embedding in per_device_embeddings\n                and modality_b.embedding in per_device_embeddings\n            ):\n                if world_size &gt; 1:  # NOTE: not all processes exit here, hence skip_flag\n                    skip_flag = True\n                else:\n                    return None, None, skip_flag\n\n            if not skip_flag:\n                indices_a, indices_b = find_matching_indices(\n                    per_device_example_ids[modality_a.name],\n                    per_device_example_ids[modality_b.name],\n                )\n                if indices_a.numel() == 0 or indices_b.numel() == 0:\n                    if world_size &gt; 1:  # not all processes exit here\n                        skip_flag = True\n                    else:\n                        return None, None, skip_flag\n\n            if not skip_flag:\n                features_a = per_device_embeddings[modality_a.embedding][indices_a]\n                features_b = per_device_embeddings[modality_b.embedding][indices_b]\n            else:\n                # all processes must participate in the all_gather operation\n                # that follows, even if they have no data to contribute. So,\n                # we create empty tensors here.\n                features_a = torch.empty(\n                    0, device=list(per_device_embeddings.values())[0].device\n                )\n                features_b = torch.empty(\n                    0, device=list(per_device_embeddings.values())[0].device\n                )\n\n        if world_size &gt; 1:\n            if not (\n                modality_a.embedding in all_embeddings\n                and modality_b.embedding in all_embeddings\n            ):  # all processes exit here\n                return None, None, skip_flag\n\n            indices_a, indices_b = find_matching_indices(\n                all_example_ids[modality_a.name],\n                all_example_ids[modality_b.name],\n            )\n            if indices_a.numel() == 0 or indices_b.numel() == 0:\n                # all processes exit here\n                return None, None, skip_flag\n\n            all_features_a = all_embeddings[modality_a.embedding][indices_a]\n            all_features_b = all_embeddings[modality_b.embedding][indices_b]\n\n            if self.local_loss:\n                if features_a.numel() == 0:\n                    features_a = all_features_a\n                if features_b.numel() == 0:\n                    features_b = all_features_b\n\n                logits_per_feature_a = logit_scale * _safe_matmul(\n                    features_a, all_features_b\n                )\n                logits_per_feature_b = logit_scale * _safe_matmul(\n                    features_b, all_features_a\n                )\n            else:\n                logits_per_feature_a = logit_scale * _safe_matmul(\n                    all_features_a, all_features_b\n                )\n                logits_per_feature_b = logits_per_feature_a.T\n        else:\n            logits_per_feature_a = logit_scale * _safe_matmul(features_a, features_b)\n            logits_per_feature_b = logit_scale * _safe_matmul(features_b, features_a)\n\n        return logits_per_feature_a, logits_per_feature_b, skip_flag\n\n    def _compute_modality_alignment_loss(\n        self, all_embeddings: dict[str, torch.Tensor], logit_scale: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"Compute the modality alignment loss.\n\n        This loss considers all features from the same modality as positive pairs\n        and all features from different modalities as negative pairs.\n\n        Parameters\n        ----------\n        all_embeddings : dict[str, torch.Tensor]\n            Dictionary of embeddings, where the key is the modality name and the value\n            is the corresponding embedding tensor.\n        logit_scale : torch.Tensor\n            Scale factor for the logits.\n\n        Returns\n        -------\n        torch.Tensor\n            Modality alignment loss.\n\n        Notes\n        -----\n        This loss does not support `local_loss=True`.\n        \"\"\"\n        available_modalities = list(all_embeddings.keys())\n        # TODO: support local_loss for modality_alignment?\n        # if world_size == 1, all_embeddings == embeddings\n        all_features = torch.cat(list(all_embeddings.values()), dim=0)\n\n        positive_indices = torch.tensor(\n            [\n                (i, j)\n                if idx == 0\n                else (\n                    i + all_embeddings[available_modalities[idx - 1]].size(0),\n                    j + all_embeddings[available_modalities[idx - 1]].size(0),\n                )\n                for idx, k in enumerate(all_embeddings)\n                for i, j in itertools.combinations(range(all_embeddings[k].size(0)), 2)\n            ],\n            device=all_features.device,\n        )\n        logits = logit_scale * _safe_matmul(all_features, all_features)\n\n        target = torch.eye(all_features.size(0), device=all_features.device)\n        target[positive_indices[:, 0], positive_indices[:, 1]] = 1\n\n        modality_loss = torch.nn.functional.binary_cross_entropy_with_logits(\n            logits, target, reduction=\"none\"\n        )\n\n        target_pos = target.bool()\n        target_neg = ~target_pos\n\n        # loss_pos and loss_neg below contain non-zero values only for those\n        # elements that are positive pairs and negative pairs respectively.\n        loss_pos = torch.zeros(\n            logits.size(0), logits.size(0), device=target.device\n        ).masked_scatter(target_pos, modality_loss[target_pos])\n        loss_neg = torch.zeros(\n            logits.size(0), logits.size(0), device=target.device\n        ).masked_scatter(target_neg, modality_loss[target_neg])\n\n        loss_pos = loss_pos.sum(dim=1)\n        loss_neg = loss_neg.sum(dim=1)\n        num_pos = target.sum(dim=1)\n        num_neg = logits.size(0) - num_pos\n\n        return ((loss_pos / num_pos) + (loss_neg / num_neg)).mean()\n</code></pre>"},{"location":"api/#mmlearn.modules.losses.contrastive.ContrastiveLoss.forward","title":"forward","text":"<pre><code>forward(\n    embeddings,\n    example_ids,\n    logit_scale,\n    modality_loss_pairs,\n)\n</code></pre> <p>Calculate the contrastive loss.</p> <p>Parameters:</p> Name Type Description Default <code>embeddings</code> <code>dict[str, Tensor]</code> <p>Dictionary of embeddings, where the key is the modality name and the value is the corresponding embedding tensor.</p> required <code>example_ids</code> <code>dict[str, Tensor]</code> <p>Dictionary of example IDs, where the key is the modality name and the value is a tensor tuple of the dataset index and the example index.</p> required <code>logit_scale</code> <code>Tensor</code> <p>Scale factor for the logits.</p> required <code>modality_loss_pairs</code> <code>list[LossPairSpec]</code> <p>Specification of the modality pairs for which the loss should be calculated.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The contrastive loss.</p> Source code in <code>mmlearn/modules/losses/contrastive.py</code> <pre><code>def forward(\n    self,\n    embeddings: dict[str, torch.Tensor],\n    example_ids: dict[str, torch.Tensor],\n    logit_scale: torch.Tensor,\n    modality_loss_pairs: list[LossPairSpec],\n) -&gt; torch.Tensor:\n    \"\"\"Calculate the contrastive loss.\n\n    Parameters\n    ----------\n    embeddings : dict[str, torch.Tensor]\n        Dictionary of embeddings, where the key is the modality name and the value\n        is the corresponding embedding tensor.\n    example_ids : dict[str, torch.Tensor]\n        Dictionary of example IDs, where the key is the modality name and the value\n        is a tensor tuple of the dataset index and the example index.\n    logit_scale : torch.Tensor\n        Scale factor for the logits.\n    modality_loss_pairs : list[LossPairSpec]\n        Specification of the modality pairs for which the loss should be calculated.\n\n    Returns\n    -------\n    torch.Tensor\n        The contrastive loss.\n    \"\"\"\n    world_size = dist.get_world_size() if dist.is_initialized() else 1\n    rank = dist.get_rank() if world_size &gt; 1 else 0\n\n    if self.l2_normalize:\n        embeddings = {k: F.normalize(v, p=2, dim=-1) for k, v in embeddings.items()}\n\n    if world_size &gt; 1:  # gather embeddings and example_ids across all processes\n        # NOTE: gathering dictionaries of tensors across all processes\n        # (keys + values, as opposed to just values) is especially important\n        # for the modality_alignment loss, which requires all embeddings\n        all_embeddings = _gather_dicts(\n            embeddings,\n            local_loss=self.local_loss,\n            gather_with_grad=self.gather_with_grad,\n            rank=rank,\n        )\n        all_example_ids = _gather_dicts(\n            example_ids,\n            local_loss=self.local_loss,\n            gather_with_grad=self.gather_with_grad,\n            rank=rank,\n        )\n    else:\n        all_embeddings = embeddings\n        all_example_ids = example_ids\n\n    losses = []\n    for loss_pairs in modality_loss_pairs:\n        logits_per_feature_a, logits_per_feature_b, skip_flag = self._get_logits(\n            loss_pairs.modalities,\n            per_device_embeddings=embeddings,\n            all_embeddings=all_embeddings,\n            per_device_example_ids=example_ids,\n            all_example_ids=all_example_ids,\n            logit_scale=logit_scale,\n            world_size=world_size,\n        )\n        if logits_per_feature_a is None or logits_per_feature_b is None:\n            continue\n\n        labels = self._get_ground_truth(\n            logits_per_feature_a.shape,\n            device=logits_per_feature_a.device,\n            rank=rank,\n            world_size=world_size,\n            skipped_process=skip_flag,\n        )\n\n        if labels.numel() != 0:\n            losses.append(\n                (\n                    (\n                        F.cross_entropy(logits_per_feature_a, labels)\n                        + F.cross_entropy(logits_per_feature_b, labels)\n                    )\n                    / 2\n                )\n                * loss_pairs.weight\n            )\n\n    if self.modality_alignment:\n        losses.append(\n            self._compute_modality_alignment_loss(all_embeddings, logit_scale)\n        )\n\n    if not losses:  # no loss to compute (e.g. no paired data in batch)\n        losses.append(\n            torch.tensor(\n                0.0,\n                device=logit_scale.device,\n                dtype=next(iter(embeddings.values())).dtype,\n            )\n        )\n\n    return torch.stack(losses).sum()\n</code></pre>"},{"location":"api/#mmlearn.modules.losses.data2vec","title":"data2vec","text":"<p>Implementation of Data2vec loss function.</p>"},{"location":"api/#mmlearn.modules.losses.data2vec.Data2VecLoss","title":"Data2VecLoss","text":"<p>               Bases: <code>Module</code></p> <p>Data2Vec loss function.</p> <p>Parameters:</p> Name Type Description Default <code>beta</code> <code>float</code> <p>Specifies the beta parameter for smooth L1 loss. If <code>0</code>, MSE loss is used.</p> <code>0</code> <code>loss_scale</code> <code>Optional[float]</code> <p>Scaling factor for the loss. If None, uses <code>1 / sqrt(embedding_dim)</code>.</p> <code>None</code> <code>reduction</code> <code>str</code> <p>Specifies the reduction to apply to the output: <code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>.</p> <code>'none'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the reduction mode is not supported.</p> Source code in <code>mmlearn/modules/losses/data2vec.py</code> <pre><code>@store(group=\"modules/losses\", provider=\"mmlearn\")\nclass Data2VecLoss(nn.Module):\n    \"\"\"Data2Vec loss function.\n\n    Parameters\n    ----------\n    beta : float, optional, default=0\n        Specifies the beta parameter for smooth L1 loss. If ``0``, MSE loss is used.\n    loss_scale : Optional[float], optional, default=None\n        Scaling factor for the loss. If None, uses ``1 / sqrt(embedding_dim)``.\n    reduction : str, optional, default='none'\n        Specifies the reduction to apply to the output:\n        ``'none'`` | ``'mean'`` | ``'sum'``.\n\n    Raises\n    ------\n    ValueError\n        If the reduction mode is not supported.\n    \"\"\"\n\n    def __init__(\n        self,\n        beta: float = 0,\n        loss_scale: Optional[float] = None,\n        reduction: str = \"none\",\n    ) -&gt; None:\n        super().__init__()\n        self.beta = beta\n        self.loss_scale = loss_scale\n        if reduction not in [\"none\", \"mean\", \"sum\"]:\n            raise ValueError(f\"Unsupported reduction mode: {reduction}\")\n        self.reduction = reduction\n\n    def forward(self, x: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute the Data2Vec loss.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Predicted embeddings of shape ``(batch_size, num_patches, embedding_dim)``.\n        y : torch.Tensor\n            Target embeddings of shape ``(batch_size, num_patches, embedding_dim)``.\n\n        Returns\n        -------\n        torch.Tensor\n            Data2Vec loss value.\n\n        Raises\n        ------\n        ValueError\n            If the shapes of x and y do not match.\n        \"\"\"\n        if x.shape != y.shape:\n            raise ValueError(f\"Shape mismatch: x: {x.shape}, y: {y.shape}\")\n\n        x = x.view(-1, x.size(-1)).float()\n        y = y.view(-1, y.size(-1))\n\n        if self.beta == 0:\n            loss = mse_loss(x, y, reduction=\"none\")\n        else:\n            loss = smooth_l1_loss(x, y, reduction=\"none\", beta=self.beta)\n\n        if self.loss_scale is not None:\n            scale = self.loss_scale\n        else:\n            scale = 1 / math.sqrt(x.size(-1))\n\n        loss = loss * scale\n\n        if self.reduction == \"mean\":\n            return loss.mean()\n        if self.reduction == \"sum\":\n            return loss.sum()\n        # 'none'\n        return loss.view(x.size(0), -1).sum(1)\n</code></pre>"},{"location":"api/#mmlearn.modules.losses.data2vec.Data2VecLoss.forward","title":"forward","text":"<pre><code>forward(x, y)\n</code></pre> <p>Compute the Data2Vec loss.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Predicted embeddings of shape <code>(batch_size, num_patches, embedding_dim)</code>.</p> required <code>y</code> <code>Tensor</code> <p>Target embeddings of shape <code>(batch_size, num_patches, embedding_dim)</code>.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Data2Vec loss value.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the shapes of x and y do not match.</p> Source code in <code>mmlearn/modules/losses/data2vec.py</code> <pre><code>def forward(self, x: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute the Data2Vec loss.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Predicted embeddings of shape ``(batch_size, num_patches, embedding_dim)``.\n    y : torch.Tensor\n        Target embeddings of shape ``(batch_size, num_patches, embedding_dim)``.\n\n    Returns\n    -------\n    torch.Tensor\n        Data2Vec loss value.\n\n    Raises\n    ------\n    ValueError\n        If the shapes of x and y do not match.\n    \"\"\"\n    if x.shape != y.shape:\n        raise ValueError(f\"Shape mismatch: x: {x.shape}, y: {y.shape}\")\n\n    x = x.view(-1, x.size(-1)).float()\n    y = y.view(-1, y.size(-1))\n\n    if self.beta == 0:\n        loss = mse_loss(x, y, reduction=\"none\")\n    else:\n        loss = smooth_l1_loss(x, y, reduction=\"none\", beta=self.beta)\n\n    if self.loss_scale is not None:\n        scale = self.loss_scale\n    else:\n        scale = 1 / math.sqrt(x.size(-1))\n\n    loss = loss * scale\n\n    if self.reduction == \"mean\":\n        return loss.mean()\n    if self.reduction == \"sum\":\n        return loss.sum()\n    # 'none'\n    return loss.view(x.size(0), -1).sum(1)\n</code></pre>"},{"location":"api/#learning-rate-schedulers","title":"Learning Rate Schedulers","text":""},{"location":"api/#mmlearn.modules.lr_schedulers","title":"mmlearn.modules.lr_schedulers","text":"<p>Learning rate schedulers for training models.</p>"},{"location":"api/#mmlearn.modules.lr_schedulers.linear_warmup_cosine_annealing_lr","title":"linear_warmup_cosine_annealing_lr","text":"<pre><code>linear_warmup_cosine_annealing_lr(\n    optimizer,\n    warmup_steps,\n    max_steps,\n    start_factor=1 / 3,\n    eta_min=0.0,\n    last_epoch=-1,\n)\n</code></pre> <p>Create a linear warmup cosine annealing learning rate scheduler.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>Optimizer</code> <p>The optimizer for which to schedule the learning rate.</p> required <code>warmup_steps</code> <code>int</code> <p>Maximum number of iterations for linear warmup.</p> required <code>max_steps</code> <code>int</code> <p>Maximum number of iterations.</p> required <code>start_factor</code> <code>float</code> <p>Multiplicative factor for the learning rate at the start of the warmup phase.</p> <code>1/3</code> <code>eta_min</code> <code>float</code> <p>Minimum learning rate.</p> <code>0</code> <code>last_epoch</code> <code>int</code> <p>The index of last epoch. If set to <code>-1</code>, it initializes the learning rate as the base learning rate</p> <code>-1</code> <p>Returns:</p> Type Description <code>LRScheduler</code> <p>The learning rate scheduler.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>warmup_steps</code> is greater than or equal to <code>max_steps</code> or if <code>warmup_steps</code> is less than or equal to 0.</p> Source code in <code>mmlearn/modules/lr_schedulers/linear_warmup_cosine_lr.py</code> <pre><code>@store(  # type: ignore[misc]\n    group=\"modules/lr_schedulers\",\n    provider=\"mmlearn\",\n    zen_partial=True,\n    warmup_steps=MISSING,\n    max_steps=MISSING,\n)\ndef linear_warmup_cosine_annealing_lr(\n    optimizer: Optimizer,\n    warmup_steps: int,\n    max_steps: int,\n    start_factor: float = 1 / 3,\n    eta_min: float = 0.0,\n    last_epoch: int = -1,\n) -&gt; LRScheduler:\n    \"\"\"Create a linear warmup cosine annealing learning rate scheduler.\n\n    Parameters\n    ----------\n    optimizer : Optimizer\n        The optimizer for which to schedule the learning rate.\n    warmup_steps : int\n        Maximum number of iterations for linear warmup.\n    max_steps : int\n        Maximum number of iterations.\n    start_factor : float, optional, default=1/3\n        Multiplicative factor for the learning rate at the start of the warmup phase.\n    eta_min : float, optional, default=0\n        Minimum learning rate.\n    last_epoch : int, optional, default=-1\n        The index of last epoch. If set to ``-1``, it initializes the learning rate\n        as the base learning rate\n\n    Returns\n    -------\n    LRScheduler\n        The learning rate scheduler.\n\n    Raises\n    ------\n    ValueError\n        If `warmup_steps` is greater than or equal to `max_steps` or if `warmup_steps`\n        is less than or equal to 0.\n    \"\"\"\n    if warmup_steps &gt;= max_steps:\n        raise ValueError(\n            \"Expected `warmup_steps` to be less than `max_steps` but got \"\n            f\"`warmup_steps={warmup_steps}` and `max_steps={max_steps}`.\"\n        )\n    if warmup_steps &lt;= 0:\n        raise ValueError(\n            \"Expected `warmup_steps` to be positive but got \"\n            f\"`warmup_steps={warmup_steps}`.\"\n        )\n\n    linear_lr = LinearLR(\n        optimizer,\n        start_factor=start_factor,\n        total_iters=warmup_steps,\n        last_epoch=last_epoch,\n    )\n    cosine_lr = CosineAnnealingLR(\n        optimizer,\n        T_max=max_steps - warmup_steps,\n        eta_min=eta_min,\n        last_epoch=last_epoch,\n    )\n    return SequentialLR(\n        optimizer,\n        schedulers=[linear_lr, cosine_lr],\n        milestones=[warmup_steps],\n        last_epoch=last_epoch,\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.lr_schedulers.linear_warmup_cosine_lr","title":"linear_warmup_cosine_lr","text":"<p>Linear warmup cosine annealing learning rate scheduler.</p>"},{"location":"api/#mmlearn.modules.lr_schedulers.linear_warmup_cosine_lr.linear_warmup_cosine_annealing_lr","title":"linear_warmup_cosine_annealing_lr","text":"<pre><code>linear_warmup_cosine_annealing_lr(\n    optimizer,\n    warmup_steps,\n    max_steps,\n    start_factor=1 / 3,\n    eta_min=0.0,\n    last_epoch=-1,\n)\n</code></pre> <p>Create a linear warmup cosine annealing learning rate scheduler.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>Optimizer</code> <p>The optimizer for which to schedule the learning rate.</p> required <code>warmup_steps</code> <code>int</code> <p>Maximum number of iterations for linear warmup.</p> required <code>max_steps</code> <code>int</code> <p>Maximum number of iterations.</p> required <code>start_factor</code> <code>float</code> <p>Multiplicative factor for the learning rate at the start of the warmup phase.</p> <code>1/3</code> <code>eta_min</code> <code>float</code> <p>Minimum learning rate.</p> <code>0</code> <code>last_epoch</code> <code>int</code> <p>The index of last epoch. If set to <code>-1</code>, it initializes the learning rate as the base learning rate</p> <code>-1</code> <p>Returns:</p> Type Description <code>LRScheduler</code> <p>The learning rate scheduler.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>warmup_steps</code> is greater than or equal to <code>max_steps</code> or if <code>warmup_steps</code> is less than or equal to 0.</p> Source code in <code>mmlearn/modules/lr_schedulers/linear_warmup_cosine_lr.py</code> <pre><code>@store(  # type: ignore[misc]\n    group=\"modules/lr_schedulers\",\n    provider=\"mmlearn\",\n    zen_partial=True,\n    warmup_steps=MISSING,\n    max_steps=MISSING,\n)\ndef linear_warmup_cosine_annealing_lr(\n    optimizer: Optimizer,\n    warmup_steps: int,\n    max_steps: int,\n    start_factor: float = 1 / 3,\n    eta_min: float = 0.0,\n    last_epoch: int = -1,\n) -&gt; LRScheduler:\n    \"\"\"Create a linear warmup cosine annealing learning rate scheduler.\n\n    Parameters\n    ----------\n    optimizer : Optimizer\n        The optimizer for which to schedule the learning rate.\n    warmup_steps : int\n        Maximum number of iterations for linear warmup.\n    max_steps : int\n        Maximum number of iterations.\n    start_factor : float, optional, default=1/3\n        Multiplicative factor for the learning rate at the start of the warmup phase.\n    eta_min : float, optional, default=0\n        Minimum learning rate.\n    last_epoch : int, optional, default=-1\n        The index of last epoch. If set to ``-1``, it initializes the learning rate\n        as the base learning rate\n\n    Returns\n    -------\n    LRScheduler\n        The learning rate scheduler.\n\n    Raises\n    ------\n    ValueError\n        If `warmup_steps` is greater than or equal to `max_steps` or if `warmup_steps`\n        is less than or equal to 0.\n    \"\"\"\n    if warmup_steps &gt;= max_steps:\n        raise ValueError(\n            \"Expected `warmup_steps` to be less than `max_steps` but got \"\n            f\"`warmup_steps={warmup_steps}` and `max_steps={max_steps}`.\"\n        )\n    if warmup_steps &lt;= 0:\n        raise ValueError(\n            \"Expected `warmup_steps` to be positive but got \"\n            f\"`warmup_steps={warmup_steps}`.\"\n        )\n\n    linear_lr = LinearLR(\n        optimizer,\n        start_factor=start_factor,\n        total_iters=warmup_steps,\n        last_epoch=last_epoch,\n    )\n    cosine_lr = CosineAnnealingLR(\n        optimizer,\n        T_max=max_steps - warmup_steps,\n        eta_min=eta_min,\n        last_epoch=last_epoch,\n    )\n    return SequentialLR(\n        optimizer,\n        schedulers=[linear_lr, cosine_lr],\n        milestones=[warmup_steps],\n        last_epoch=last_epoch,\n    )\n</code></pre>"},{"location":"api/#metrics","title":"Metrics","text":""},{"location":"api/#mmlearn.modules.metrics","title":"mmlearn.modules.metrics","text":"<p>Metrics for evaluating models.</p>"},{"location":"api/#mmlearn.modules.metrics.RetrievalRecallAtK","title":"RetrievalRecallAtK","text":"<p>               Bases: <code>Metric</code></p> <p>Retrieval Recall@K metric.</p> <p>Computes the Recall@K for retrieval tasks. The metric is computed as follows:</p> <ol> <li>Compute the cosine similarity between the query and the database.</li> <li>For each query, sort the database in decreasing order of similarity.</li> <li>Compute the Recall@K as the number of true positives among the top K elements.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>top_k</code> <code>int</code> <p>The number of top elements to consider for computing the Recall@K.</p> required <code>reduction</code> <code>(mean, sum, none, None)</code> <p>Specifies the reduction to apply after computing the pairwise cosine similarity scores.</p> <code>\"mean\"</code> <code>aggregation</code> <code>(mean, median, min, max)</code> <p>Specifies the aggregation function to apply to the Recall@K values computed in batches. If a callable is provided, it should accept a tensor of values and a keyword argument <code>'dim'</code> and return a single scalar value.</p> <code>\"mean\"</code> <code>kwargs</code> <code>Any</code> <p>Additional arguments to be passed to the class:<code>torchmetrics.Metric</code> class.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <ul> <li>If the <code>top_k</code> is not a positive integer or None.</li> <li>If the <code>reduction</code> is not one of {\"mean\", \"sum\", \"none\", None}.</li> <li>If the <code>aggregation</code> is not one of {\"mean\", \"median\", \"min\", \"max\"} or a   custom callable function.</li> </ul> Source code in <code>mmlearn/modules/metrics/retrieval_recall.py</code> <pre><code>@store(group=\"modules/metrics\", provider=\"mmlearn\")\nclass RetrievalRecallAtK(Metric):\n    \"\"\"Retrieval Recall@K metric.\n\n    Computes the Recall@K for retrieval tasks. The metric is computed as follows:\n\n    1. Compute the cosine similarity between the query and the database.\n    2. For each query, sort the database in decreasing order of similarity.\n    3. Compute the Recall@K as the number of true positives among the top K elements.\n\n    Parameters\n    ----------\n    top_k : int\n        The number of top elements to consider for computing the Recall@K.\n    reduction : {\"mean\", \"sum\", \"none\", None}, optional, default=\"sum\"\n        Specifies the reduction to apply after computing the pairwise cosine similarity\n        scores.\n    aggregation : {\"mean\", \"median\", \"min\", \"max\"} or callable, default=\"mean\"\n        Specifies the aggregation function to apply to the Recall@K values computed\n        in batches. If a callable is provided, it should accept a tensor of values\n        and a keyword argument ``'dim'`` and return a single scalar value.\n    kwargs : Any\n        Additional arguments to be passed to the :py:class:`torchmetrics.Metric` class.\n\n    Raises\n    ------\n    ValueError\n\n        - If the `top_k` is not a positive integer or None.\n        - If the `reduction` is not one of {\"mean\", \"sum\", \"none\", None}.\n        - If the `aggregation` is not one of {\"mean\", \"median\", \"min\", \"max\"} or a\n          custom callable function.\n\n    \"\"\"\n\n    is_differentiable: bool = False\n    higher_is_better: bool = True\n    full_state_update: bool = False\n\n    indexes: list[torch.Tensor]\n    x: list[torch.Tensor]\n    y: list[torch.Tensor]\n    num_samples: torch.Tensor\n\n    def __init__(\n        self,\n        top_k: int,\n        reduction: Optional[Literal[\"mean\", \"sum\", \"none\"]] = \"sum\",\n        aggregation: Union[\n            Literal[\"mean\", \"median\", \"min\", \"max\"],\n            Callable[[torch.Tensor, int], torch.Tensor],\n        ] = \"mean\",\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Initialize the metric.\"\"\"\n        super().__init__(**kwargs)\n\n        if top_k is not None and not (isinstance(top_k, int) and top_k &gt; 0):\n            raise ValueError(\"`top_k` has to be a positive integer or None\")\n        self.top_k = top_k\n\n        allowed_reduction = (\"sum\", \"mean\", \"none\", None)\n        if reduction not in allowed_reduction:\n            raise ValueError(\n                f\"Expected argument `reduction` to be one of {allowed_reduction} but got {reduction}\"\n            )\n        self.reduction = reduction\n\n        if not (\n            aggregation in (\"mean\", \"median\", \"min\", \"max\") or callable(aggregation)\n        ):\n            raise ValueError(\n                \"Argument `aggregation` must be one of `mean`, `median`, `min`, `max` or a custom callable function\"\n                f\"which takes tensor of values, but got {aggregation}.\"\n            )\n        self.aggregation = aggregation\n\n        self.add_state(\"x\", default=[], dist_reduce_fx=\"cat\")\n        self.add_state(\"y\", default=[], dist_reduce_fx=\"cat\")\n        self.add_state(\"indexes\", default=[], dist_reduce_fx=\"cat\")\n        self.add_state(\"num_samples\", default=torch.tensor(0), dist_reduce_fx=\"cat\")\n\n        self._batch_size: int = -1\n\n        self.compute_on_cpu = True\n        self.sync_on_compute = False\n        self.dist_sync_on_step = False\n        self._to_sync = self.sync_on_compute\n        self._should_unsync = False\n\n    def update(self, x: torch.Tensor, y: torch.Tensor, indexes: torch.Tensor) -&gt; None:\n        \"\"\"Check shape, convert dtypes and add to accumulators.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Embeddings (unnormalized) of shape ``(N, D)`` where ``N`` is the number\n            of samples and `D` is the number of dimensions.\n        y : torch.Tensor\n            Embeddings (unnormalized) of shape ``(M, D)`` where ``M`` is the number\n            of samples and ``D`` is the number of dimensions.\n        indexes : torch.Tensor\n            Index tensor of shape ``(N,)`` where ``N`` is the number of samples.\n            This specifies which sample in ``y`` is the positive match for each\n            sample in ``x``.\n\n        Raises\n        ------\n        ValueError\n            If `indexes` is None.\n\n        \"\"\"\n        if indexes is None:\n            raise ValueError(\"Argument `indexes` cannot be None\")\n\n        x, y, indexes = _update_batch_inputs(x.clone(), y.clone(), indexes.clone())\n\n        # offset batch indexes by the number of samples seen so far\n        indexes += self.num_samples\n\n        local_batch_size = torch.zeros_like(self.num_samples) + x.size(0)\n        if self._is_distributed():\n            x = dim_zero_cat(gather_all_tensors(x, self.process_group))\n            y = dim_zero_cat(gather_all_tensors(y, self.process_group))\n            indexes = dim_zero_cat(\n                gather_all_tensors(indexes.clone(), self.process_group)\n            )\n\n            # offset indexes for each device\n            bsz_per_device = dim_zero_cat(\n                gather_all_tensors(local_batch_size, self.process_group)\n            )\n            cum_local_bsz = torch.cumsum(bsz_per_device, dim=0)\n            for device_idx in range(1, bsz_per_device.numel()):\n                indexes[cum_local_bsz[device_idx - 1] : cum_local_bsz[device_idx]] += (\n                    cum_local_bsz[device_idx - 1]\n                )\n\n            # update the global sample count\n            self.num_samples += cum_local_bsz[-1]\n\n            self._is_synced = True\n        else:\n            self.num_samples += x.size(0)\n\n        self.x.append(x)\n        self.y.append(y)\n        self.indexes.append(indexes)\n\n        if self._batch_size == -1:\n            self._batch_size = x.size(0)  # global batch size\n\n    def compute(self) -&gt; torch.Tensor:\n        \"\"\"Compute the metric.\n\n        Returns\n        -------\n        torch.Tensor\n            The computed metric.\n        \"\"\"\n        x = dim_zero_cat(self.x)\n        y = dim_zero_cat(self.y)\n\n        # normalize embeddings\n        x /= x.norm(dim=-1, p=2, keepdim=True)\n        y /= y.norm(dim=-1, p=2, keepdim=True)\n\n        # instantiate reduction function\n        reduction_mapping: Dict[\n            Optional[str], Callable[[torch.Tensor], torch.Tensor]\n        ] = {\n            \"sum\": partial(torch.sum, dim=-1),\n            \"mean\": partial(torch.mean, dim=-1),\n            \"none\": lambda x: x,\n            None: lambda x: x,\n        }\n\n        # concatenate indexes of true pairs\n        indexes = dim_zero_cat(self.indexes)\n\n        results: list[torch.Tensor] = []\n        with concurrent.futures.ThreadPoolExecutor(\n            max_workers=os.cpu_count() or 1  # use all available CPUs\n        ) as executor:\n            futures = [\n                executor.submit(\n                    self._process_batch,\n                    start,\n                    x,\n                    y,\n                    indexes,\n                    reduction_mapping,\n                    self.top_k,\n                )\n                for start in tqdm(\n                    range(0, len(x), self._batch_size), desc=f\"Recall@{self.top_k}\"\n                )\n            ]\n            for future in concurrent.futures.as_completed(futures):\n                results.append(future.result())\n\n        return _retrieval_aggregate(\n            (torch.cat([x.float() for x in results]) &gt; 0).float(), self.aggregation\n        )\n\n    def forward(self, *args: Any, **kwargs: Any) -&gt; Any:\n        \"\"\"Forward method is not supported.\n\n        Raises\n        ------\n        NotImplementedError\n            The forward method is not supported for this metric.\n        \"\"\"\n        raise NotImplementedError(\n            \"RetrievalRecallAtK metric does not support forward method\"\n        )\n\n    def _is_distributed(self) -&gt; bool:\n        if self.distributed_available_fn is not None:\n            distributed_available = self.distributed_available_fn\n\n        return distributed_available() if callable(distributed_available) else False\n\n    def _process_batch(\n        self,\n        start: int,\n        x_norm: torch.Tensor,\n        y_norm: torch.Tensor,\n        indexes: torch.Tensor,\n        reduction_mapping: Dict[Optional[str], Callable[[torch.Tensor], torch.Tensor]],\n        top_k: int,\n    ) -&gt; torch.Tensor:\n        \"\"\"Compute the Recall@K for a batch of samples.\"\"\"\n        end = start + self._batch_size\n        x_norm_batch = x_norm[start:end]\n        indexes_batch = indexes[start:end]\n\n        similarity = _safe_matmul(x_norm_batch, y_norm)\n        scores: torch.Tensor = reduction_mapping[self.reduction](similarity)\n\n        with torch.inference_mode():\n            positive_pairs = torch.zeros_like(scores, dtype=torch.bool)\n            positive_pairs[torch.arange(len(scores)), indexes_batch] = True\n\n        return _recall_at_k(scores, positive_pairs, top_k)\n</code></pre>"},{"location":"api/#mmlearn.modules.metrics.RetrievalRecallAtK.__init__","title":"__init__","text":"<pre><code>__init__(\n    top_k, reduction=\"sum\", aggregation=\"mean\", **kwargs\n)\n</code></pre> <p>Initialize the metric.</p> Source code in <code>mmlearn/modules/metrics/retrieval_recall.py</code> <pre><code>def __init__(\n    self,\n    top_k: int,\n    reduction: Optional[Literal[\"mean\", \"sum\", \"none\"]] = \"sum\",\n    aggregation: Union[\n        Literal[\"mean\", \"median\", \"min\", \"max\"],\n        Callable[[torch.Tensor, int], torch.Tensor],\n    ] = \"mean\",\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Initialize the metric.\"\"\"\n    super().__init__(**kwargs)\n\n    if top_k is not None and not (isinstance(top_k, int) and top_k &gt; 0):\n        raise ValueError(\"`top_k` has to be a positive integer or None\")\n    self.top_k = top_k\n\n    allowed_reduction = (\"sum\", \"mean\", \"none\", None)\n    if reduction not in allowed_reduction:\n        raise ValueError(\n            f\"Expected argument `reduction` to be one of {allowed_reduction} but got {reduction}\"\n        )\n    self.reduction = reduction\n\n    if not (\n        aggregation in (\"mean\", \"median\", \"min\", \"max\") or callable(aggregation)\n    ):\n        raise ValueError(\n            \"Argument `aggregation` must be one of `mean`, `median`, `min`, `max` or a custom callable function\"\n            f\"which takes tensor of values, but got {aggregation}.\"\n        )\n    self.aggregation = aggregation\n\n    self.add_state(\"x\", default=[], dist_reduce_fx=\"cat\")\n    self.add_state(\"y\", default=[], dist_reduce_fx=\"cat\")\n    self.add_state(\"indexes\", default=[], dist_reduce_fx=\"cat\")\n    self.add_state(\"num_samples\", default=torch.tensor(0), dist_reduce_fx=\"cat\")\n\n    self._batch_size: int = -1\n\n    self.compute_on_cpu = True\n    self.sync_on_compute = False\n    self.dist_sync_on_step = False\n    self._to_sync = self.sync_on_compute\n    self._should_unsync = False\n</code></pre>"},{"location":"api/#mmlearn.modules.metrics.RetrievalRecallAtK.update","title":"update","text":"<pre><code>update(x, y, indexes)\n</code></pre> <p>Check shape, convert dtypes and add to accumulators.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Embeddings (unnormalized) of shape <code>(N, D)</code> where <code>N</code> is the number of samples and <code>D</code> is the number of dimensions.</p> required <code>y</code> <code>Tensor</code> <p>Embeddings (unnormalized) of shape <code>(M, D)</code> where <code>M</code> is the number of samples and <code>D</code> is the number of dimensions.</p> required <code>indexes</code> <code>Tensor</code> <p>Index tensor of shape <code>(N,)</code> where <code>N</code> is the number of samples. This specifies which sample in <code>y</code> is the positive match for each sample in <code>x</code>.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>indexes</code> is None.</p> Source code in <code>mmlearn/modules/metrics/retrieval_recall.py</code> <pre><code>def update(self, x: torch.Tensor, y: torch.Tensor, indexes: torch.Tensor) -&gt; None:\n    \"\"\"Check shape, convert dtypes and add to accumulators.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Embeddings (unnormalized) of shape ``(N, D)`` where ``N`` is the number\n        of samples and `D` is the number of dimensions.\n    y : torch.Tensor\n        Embeddings (unnormalized) of shape ``(M, D)`` where ``M`` is the number\n        of samples and ``D`` is the number of dimensions.\n    indexes : torch.Tensor\n        Index tensor of shape ``(N,)`` where ``N`` is the number of samples.\n        This specifies which sample in ``y`` is the positive match for each\n        sample in ``x``.\n\n    Raises\n    ------\n    ValueError\n        If `indexes` is None.\n\n    \"\"\"\n    if indexes is None:\n        raise ValueError(\"Argument `indexes` cannot be None\")\n\n    x, y, indexes = _update_batch_inputs(x.clone(), y.clone(), indexes.clone())\n\n    # offset batch indexes by the number of samples seen so far\n    indexes += self.num_samples\n\n    local_batch_size = torch.zeros_like(self.num_samples) + x.size(0)\n    if self._is_distributed():\n        x = dim_zero_cat(gather_all_tensors(x, self.process_group))\n        y = dim_zero_cat(gather_all_tensors(y, self.process_group))\n        indexes = dim_zero_cat(\n            gather_all_tensors(indexes.clone(), self.process_group)\n        )\n\n        # offset indexes for each device\n        bsz_per_device = dim_zero_cat(\n            gather_all_tensors(local_batch_size, self.process_group)\n        )\n        cum_local_bsz = torch.cumsum(bsz_per_device, dim=0)\n        for device_idx in range(1, bsz_per_device.numel()):\n            indexes[cum_local_bsz[device_idx - 1] : cum_local_bsz[device_idx]] += (\n                cum_local_bsz[device_idx - 1]\n            )\n\n        # update the global sample count\n        self.num_samples += cum_local_bsz[-1]\n\n        self._is_synced = True\n    else:\n        self.num_samples += x.size(0)\n\n    self.x.append(x)\n    self.y.append(y)\n    self.indexes.append(indexes)\n\n    if self._batch_size == -1:\n        self._batch_size = x.size(0)  # global batch size\n</code></pre>"},{"location":"api/#mmlearn.modules.metrics.RetrievalRecallAtK.compute","title":"compute","text":"<pre><code>compute()\n</code></pre> <p>Compute the metric.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>The computed metric.</p> Source code in <code>mmlearn/modules/metrics/retrieval_recall.py</code> <pre><code>def compute(self) -&gt; torch.Tensor:\n    \"\"\"Compute the metric.\n\n    Returns\n    -------\n    torch.Tensor\n        The computed metric.\n    \"\"\"\n    x = dim_zero_cat(self.x)\n    y = dim_zero_cat(self.y)\n\n    # normalize embeddings\n    x /= x.norm(dim=-1, p=2, keepdim=True)\n    y /= y.norm(dim=-1, p=2, keepdim=True)\n\n    # instantiate reduction function\n    reduction_mapping: Dict[\n        Optional[str], Callable[[torch.Tensor], torch.Tensor]\n    ] = {\n        \"sum\": partial(torch.sum, dim=-1),\n        \"mean\": partial(torch.mean, dim=-1),\n        \"none\": lambda x: x,\n        None: lambda x: x,\n    }\n\n    # concatenate indexes of true pairs\n    indexes = dim_zero_cat(self.indexes)\n\n    results: list[torch.Tensor] = []\n    with concurrent.futures.ThreadPoolExecutor(\n        max_workers=os.cpu_count() or 1  # use all available CPUs\n    ) as executor:\n        futures = [\n            executor.submit(\n                self._process_batch,\n                start,\n                x,\n                y,\n                indexes,\n                reduction_mapping,\n                self.top_k,\n            )\n            for start in tqdm(\n                range(0, len(x), self._batch_size), desc=f\"Recall@{self.top_k}\"\n            )\n        ]\n        for future in concurrent.futures.as_completed(futures):\n            results.append(future.result())\n\n    return _retrieval_aggregate(\n        (torch.cat([x.float() for x in results]) &gt; 0).float(), self.aggregation\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.metrics.RetrievalRecallAtK.forward","title":"forward","text":"<pre><code>forward(*args, **kwargs)\n</code></pre> <p>Forward method is not supported.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>The forward method is not supported for this metric.</p> Source code in <code>mmlearn/modules/metrics/retrieval_recall.py</code> <pre><code>def forward(self, *args: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"Forward method is not supported.\n\n    Raises\n    ------\n    NotImplementedError\n        The forward method is not supported for this metric.\n    \"\"\"\n    raise NotImplementedError(\n        \"RetrievalRecallAtK metric does not support forward method\"\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.metrics.retrieval_recall","title":"retrieval_recall","text":"<p>Retrieval Recall@K metric.</p>"},{"location":"api/#mmlearn.modules.metrics.retrieval_recall.RetrievalRecallAtK","title":"RetrievalRecallAtK","text":"<p>               Bases: <code>Metric</code></p> <p>Retrieval Recall@K metric.</p> <p>Computes the Recall@K for retrieval tasks. The metric is computed as follows:</p> <ol> <li>Compute the cosine similarity between the query and the database.</li> <li>For each query, sort the database in decreasing order of similarity.</li> <li>Compute the Recall@K as the number of true positives among the top K elements.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>top_k</code> <code>int</code> <p>The number of top elements to consider for computing the Recall@K.</p> required <code>reduction</code> <code>(mean, sum, none, None)</code> <p>Specifies the reduction to apply after computing the pairwise cosine similarity scores.</p> <code>\"mean\"</code> <code>aggregation</code> <code>(mean, median, min, max)</code> <p>Specifies the aggregation function to apply to the Recall@K values computed in batches. If a callable is provided, it should accept a tensor of values and a keyword argument <code>'dim'</code> and return a single scalar value.</p> <code>\"mean\"</code> <code>kwargs</code> <code>Any</code> <p>Additional arguments to be passed to the class:<code>torchmetrics.Metric</code> class.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <ul> <li>If the <code>top_k</code> is not a positive integer or None.</li> <li>If the <code>reduction</code> is not one of {\"mean\", \"sum\", \"none\", None}.</li> <li>If the <code>aggregation</code> is not one of {\"mean\", \"median\", \"min\", \"max\"} or a   custom callable function.</li> </ul> Source code in <code>mmlearn/modules/metrics/retrieval_recall.py</code> <pre><code>@store(group=\"modules/metrics\", provider=\"mmlearn\")\nclass RetrievalRecallAtK(Metric):\n    \"\"\"Retrieval Recall@K metric.\n\n    Computes the Recall@K for retrieval tasks. The metric is computed as follows:\n\n    1. Compute the cosine similarity between the query and the database.\n    2. For each query, sort the database in decreasing order of similarity.\n    3. Compute the Recall@K as the number of true positives among the top K elements.\n\n    Parameters\n    ----------\n    top_k : int\n        The number of top elements to consider for computing the Recall@K.\n    reduction : {\"mean\", \"sum\", \"none\", None}, optional, default=\"sum\"\n        Specifies the reduction to apply after computing the pairwise cosine similarity\n        scores.\n    aggregation : {\"mean\", \"median\", \"min\", \"max\"} or callable, default=\"mean\"\n        Specifies the aggregation function to apply to the Recall@K values computed\n        in batches. If a callable is provided, it should accept a tensor of values\n        and a keyword argument ``'dim'`` and return a single scalar value.\n    kwargs : Any\n        Additional arguments to be passed to the :py:class:`torchmetrics.Metric` class.\n\n    Raises\n    ------\n    ValueError\n\n        - If the `top_k` is not a positive integer or None.\n        - If the `reduction` is not one of {\"mean\", \"sum\", \"none\", None}.\n        - If the `aggregation` is not one of {\"mean\", \"median\", \"min\", \"max\"} or a\n          custom callable function.\n\n    \"\"\"\n\n    is_differentiable: bool = False\n    higher_is_better: bool = True\n    full_state_update: bool = False\n\n    indexes: list[torch.Tensor]\n    x: list[torch.Tensor]\n    y: list[torch.Tensor]\n    num_samples: torch.Tensor\n\n    def __init__(\n        self,\n        top_k: int,\n        reduction: Optional[Literal[\"mean\", \"sum\", \"none\"]] = \"sum\",\n        aggregation: Union[\n            Literal[\"mean\", \"median\", \"min\", \"max\"],\n            Callable[[torch.Tensor, int], torch.Tensor],\n        ] = \"mean\",\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Initialize the metric.\"\"\"\n        super().__init__(**kwargs)\n\n        if top_k is not None and not (isinstance(top_k, int) and top_k &gt; 0):\n            raise ValueError(\"`top_k` has to be a positive integer or None\")\n        self.top_k = top_k\n\n        allowed_reduction = (\"sum\", \"mean\", \"none\", None)\n        if reduction not in allowed_reduction:\n            raise ValueError(\n                f\"Expected argument `reduction` to be one of {allowed_reduction} but got {reduction}\"\n            )\n        self.reduction = reduction\n\n        if not (\n            aggregation in (\"mean\", \"median\", \"min\", \"max\") or callable(aggregation)\n        ):\n            raise ValueError(\n                \"Argument `aggregation` must be one of `mean`, `median`, `min`, `max` or a custom callable function\"\n                f\"which takes tensor of values, but got {aggregation}.\"\n            )\n        self.aggregation = aggregation\n\n        self.add_state(\"x\", default=[], dist_reduce_fx=\"cat\")\n        self.add_state(\"y\", default=[], dist_reduce_fx=\"cat\")\n        self.add_state(\"indexes\", default=[], dist_reduce_fx=\"cat\")\n        self.add_state(\"num_samples\", default=torch.tensor(0), dist_reduce_fx=\"cat\")\n\n        self._batch_size: int = -1\n\n        self.compute_on_cpu = True\n        self.sync_on_compute = False\n        self.dist_sync_on_step = False\n        self._to_sync = self.sync_on_compute\n        self._should_unsync = False\n\n    def update(self, x: torch.Tensor, y: torch.Tensor, indexes: torch.Tensor) -&gt; None:\n        \"\"\"Check shape, convert dtypes and add to accumulators.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Embeddings (unnormalized) of shape ``(N, D)`` where ``N`` is the number\n            of samples and `D` is the number of dimensions.\n        y : torch.Tensor\n            Embeddings (unnormalized) of shape ``(M, D)`` where ``M`` is the number\n            of samples and ``D`` is the number of dimensions.\n        indexes : torch.Tensor\n            Index tensor of shape ``(N,)`` where ``N`` is the number of samples.\n            This specifies which sample in ``y`` is the positive match for each\n            sample in ``x``.\n\n        Raises\n        ------\n        ValueError\n            If `indexes` is None.\n\n        \"\"\"\n        if indexes is None:\n            raise ValueError(\"Argument `indexes` cannot be None\")\n\n        x, y, indexes = _update_batch_inputs(x.clone(), y.clone(), indexes.clone())\n\n        # offset batch indexes by the number of samples seen so far\n        indexes += self.num_samples\n\n        local_batch_size = torch.zeros_like(self.num_samples) + x.size(0)\n        if self._is_distributed():\n            x = dim_zero_cat(gather_all_tensors(x, self.process_group))\n            y = dim_zero_cat(gather_all_tensors(y, self.process_group))\n            indexes = dim_zero_cat(\n                gather_all_tensors(indexes.clone(), self.process_group)\n            )\n\n            # offset indexes for each device\n            bsz_per_device = dim_zero_cat(\n                gather_all_tensors(local_batch_size, self.process_group)\n            )\n            cum_local_bsz = torch.cumsum(bsz_per_device, dim=0)\n            for device_idx in range(1, bsz_per_device.numel()):\n                indexes[cum_local_bsz[device_idx - 1] : cum_local_bsz[device_idx]] += (\n                    cum_local_bsz[device_idx - 1]\n                )\n\n            # update the global sample count\n            self.num_samples += cum_local_bsz[-1]\n\n            self._is_synced = True\n        else:\n            self.num_samples += x.size(0)\n\n        self.x.append(x)\n        self.y.append(y)\n        self.indexes.append(indexes)\n\n        if self._batch_size == -1:\n            self._batch_size = x.size(0)  # global batch size\n\n    def compute(self) -&gt; torch.Tensor:\n        \"\"\"Compute the metric.\n\n        Returns\n        -------\n        torch.Tensor\n            The computed metric.\n        \"\"\"\n        x = dim_zero_cat(self.x)\n        y = dim_zero_cat(self.y)\n\n        # normalize embeddings\n        x /= x.norm(dim=-1, p=2, keepdim=True)\n        y /= y.norm(dim=-1, p=2, keepdim=True)\n\n        # instantiate reduction function\n        reduction_mapping: Dict[\n            Optional[str], Callable[[torch.Tensor], torch.Tensor]\n        ] = {\n            \"sum\": partial(torch.sum, dim=-1),\n            \"mean\": partial(torch.mean, dim=-1),\n            \"none\": lambda x: x,\n            None: lambda x: x,\n        }\n\n        # concatenate indexes of true pairs\n        indexes = dim_zero_cat(self.indexes)\n\n        results: list[torch.Tensor] = []\n        with concurrent.futures.ThreadPoolExecutor(\n            max_workers=os.cpu_count() or 1  # use all available CPUs\n        ) as executor:\n            futures = [\n                executor.submit(\n                    self._process_batch,\n                    start,\n                    x,\n                    y,\n                    indexes,\n                    reduction_mapping,\n                    self.top_k,\n                )\n                for start in tqdm(\n                    range(0, len(x), self._batch_size), desc=f\"Recall@{self.top_k}\"\n                )\n            ]\n            for future in concurrent.futures.as_completed(futures):\n                results.append(future.result())\n\n        return _retrieval_aggregate(\n            (torch.cat([x.float() for x in results]) &gt; 0).float(), self.aggregation\n        )\n\n    def forward(self, *args: Any, **kwargs: Any) -&gt; Any:\n        \"\"\"Forward method is not supported.\n\n        Raises\n        ------\n        NotImplementedError\n            The forward method is not supported for this metric.\n        \"\"\"\n        raise NotImplementedError(\n            \"RetrievalRecallAtK metric does not support forward method\"\n        )\n\n    def _is_distributed(self) -&gt; bool:\n        if self.distributed_available_fn is not None:\n            distributed_available = self.distributed_available_fn\n\n        return distributed_available() if callable(distributed_available) else False\n\n    def _process_batch(\n        self,\n        start: int,\n        x_norm: torch.Tensor,\n        y_norm: torch.Tensor,\n        indexes: torch.Tensor,\n        reduction_mapping: Dict[Optional[str], Callable[[torch.Tensor], torch.Tensor]],\n        top_k: int,\n    ) -&gt; torch.Tensor:\n        \"\"\"Compute the Recall@K for a batch of samples.\"\"\"\n        end = start + self._batch_size\n        x_norm_batch = x_norm[start:end]\n        indexes_batch = indexes[start:end]\n\n        similarity = _safe_matmul(x_norm_batch, y_norm)\n        scores: torch.Tensor = reduction_mapping[self.reduction](similarity)\n\n        with torch.inference_mode():\n            positive_pairs = torch.zeros_like(scores, dtype=torch.bool)\n            positive_pairs[torch.arange(len(scores)), indexes_batch] = True\n\n        return _recall_at_k(scores, positive_pairs, top_k)\n</code></pre>"},{"location":"api/#mmlearn.modules.metrics.retrieval_recall.RetrievalRecallAtK.__init__","title":"__init__","text":"<pre><code>__init__(\n    top_k, reduction=\"sum\", aggregation=\"mean\", **kwargs\n)\n</code></pre> <p>Initialize the metric.</p> Source code in <code>mmlearn/modules/metrics/retrieval_recall.py</code> <pre><code>def __init__(\n    self,\n    top_k: int,\n    reduction: Optional[Literal[\"mean\", \"sum\", \"none\"]] = \"sum\",\n    aggregation: Union[\n        Literal[\"mean\", \"median\", \"min\", \"max\"],\n        Callable[[torch.Tensor, int], torch.Tensor],\n    ] = \"mean\",\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Initialize the metric.\"\"\"\n    super().__init__(**kwargs)\n\n    if top_k is not None and not (isinstance(top_k, int) and top_k &gt; 0):\n        raise ValueError(\"`top_k` has to be a positive integer or None\")\n    self.top_k = top_k\n\n    allowed_reduction = (\"sum\", \"mean\", \"none\", None)\n    if reduction not in allowed_reduction:\n        raise ValueError(\n            f\"Expected argument `reduction` to be one of {allowed_reduction} but got {reduction}\"\n        )\n    self.reduction = reduction\n\n    if not (\n        aggregation in (\"mean\", \"median\", \"min\", \"max\") or callable(aggregation)\n    ):\n        raise ValueError(\n            \"Argument `aggregation` must be one of `mean`, `median`, `min`, `max` or a custom callable function\"\n            f\"which takes tensor of values, but got {aggregation}.\"\n        )\n    self.aggregation = aggregation\n\n    self.add_state(\"x\", default=[], dist_reduce_fx=\"cat\")\n    self.add_state(\"y\", default=[], dist_reduce_fx=\"cat\")\n    self.add_state(\"indexes\", default=[], dist_reduce_fx=\"cat\")\n    self.add_state(\"num_samples\", default=torch.tensor(0), dist_reduce_fx=\"cat\")\n\n    self._batch_size: int = -1\n\n    self.compute_on_cpu = True\n    self.sync_on_compute = False\n    self.dist_sync_on_step = False\n    self._to_sync = self.sync_on_compute\n    self._should_unsync = False\n</code></pre>"},{"location":"api/#mmlearn.modules.metrics.retrieval_recall.RetrievalRecallAtK.update","title":"update","text":"<pre><code>update(x, y, indexes)\n</code></pre> <p>Check shape, convert dtypes and add to accumulators.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Embeddings (unnormalized) of shape <code>(N, D)</code> where <code>N</code> is the number of samples and <code>D</code> is the number of dimensions.</p> required <code>y</code> <code>Tensor</code> <p>Embeddings (unnormalized) of shape <code>(M, D)</code> where <code>M</code> is the number of samples and <code>D</code> is the number of dimensions.</p> required <code>indexes</code> <code>Tensor</code> <p>Index tensor of shape <code>(N,)</code> where <code>N</code> is the number of samples. This specifies which sample in <code>y</code> is the positive match for each sample in <code>x</code>.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>indexes</code> is None.</p> Source code in <code>mmlearn/modules/metrics/retrieval_recall.py</code> <pre><code>def update(self, x: torch.Tensor, y: torch.Tensor, indexes: torch.Tensor) -&gt; None:\n    \"\"\"Check shape, convert dtypes and add to accumulators.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Embeddings (unnormalized) of shape ``(N, D)`` where ``N`` is the number\n        of samples and `D` is the number of dimensions.\n    y : torch.Tensor\n        Embeddings (unnormalized) of shape ``(M, D)`` where ``M`` is the number\n        of samples and ``D`` is the number of dimensions.\n    indexes : torch.Tensor\n        Index tensor of shape ``(N,)`` where ``N`` is the number of samples.\n        This specifies which sample in ``y`` is the positive match for each\n        sample in ``x``.\n\n    Raises\n    ------\n    ValueError\n        If `indexes` is None.\n\n    \"\"\"\n    if indexes is None:\n        raise ValueError(\"Argument `indexes` cannot be None\")\n\n    x, y, indexes = _update_batch_inputs(x.clone(), y.clone(), indexes.clone())\n\n    # offset batch indexes by the number of samples seen so far\n    indexes += self.num_samples\n\n    local_batch_size = torch.zeros_like(self.num_samples) + x.size(0)\n    if self._is_distributed():\n        x = dim_zero_cat(gather_all_tensors(x, self.process_group))\n        y = dim_zero_cat(gather_all_tensors(y, self.process_group))\n        indexes = dim_zero_cat(\n            gather_all_tensors(indexes.clone(), self.process_group)\n        )\n\n        # offset indexes for each device\n        bsz_per_device = dim_zero_cat(\n            gather_all_tensors(local_batch_size, self.process_group)\n        )\n        cum_local_bsz = torch.cumsum(bsz_per_device, dim=0)\n        for device_idx in range(1, bsz_per_device.numel()):\n            indexes[cum_local_bsz[device_idx - 1] : cum_local_bsz[device_idx]] += (\n                cum_local_bsz[device_idx - 1]\n            )\n\n        # update the global sample count\n        self.num_samples += cum_local_bsz[-1]\n\n        self._is_synced = True\n    else:\n        self.num_samples += x.size(0)\n\n    self.x.append(x)\n    self.y.append(y)\n    self.indexes.append(indexes)\n\n    if self._batch_size == -1:\n        self._batch_size = x.size(0)  # global batch size\n</code></pre>"},{"location":"api/#mmlearn.modules.metrics.retrieval_recall.RetrievalRecallAtK.compute","title":"compute","text":"<pre><code>compute()\n</code></pre> <p>Compute the metric.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>The computed metric.</p> Source code in <code>mmlearn/modules/metrics/retrieval_recall.py</code> <pre><code>def compute(self) -&gt; torch.Tensor:\n    \"\"\"Compute the metric.\n\n    Returns\n    -------\n    torch.Tensor\n        The computed metric.\n    \"\"\"\n    x = dim_zero_cat(self.x)\n    y = dim_zero_cat(self.y)\n\n    # normalize embeddings\n    x /= x.norm(dim=-1, p=2, keepdim=True)\n    y /= y.norm(dim=-1, p=2, keepdim=True)\n\n    # instantiate reduction function\n    reduction_mapping: Dict[\n        Optional[str], Callable[[torch.Tensor], torch.Tensor]\n    ] = {\n        \"sum\": partial(torch.sum, dim=-1),\n        \"mean\": partial(torch.mean, dim=-1),\n        \"none\": lambda x: x,\n        None: lambda x: x,\n    }\n\n    # concatenate indexes of true pairs\n    indexes = dim_zero_cat(self.indexes)\n\n    results: list[torch.Tensor] = []\n    with concurrent.futures.ThreadPoolExecutor(\n        max_workers=os.cpu_count() or 1  # use all available CPUs\n    ) as executor:\n        futures = [\n            executor.submit(\n                self._process_batch,\n                start,\n                x,\n                y,\n                indexes,\n                reduction_mapping,\n                self.top_k,\n            )\n            for start in tqdm(\n                range(0, len(x), self._batch_size), desc=f\"Recall@{self.top_k}\"\n            )\n        ]\n        for future in concurrent.futures.as_completed(futures):\n            results.append(future.result())\n\n    return _retrieval_aggregate(\n        (torch.cat([x.float() for x in results]) &gt; 0).float(), self.aggregation\n    )\n</code></pre>"},{"location":"api/#mmlearn.modules.metrics.retrieval_recall.RetrievalRecallAtK.forward","title":"forward","text":"<pre><code>forward(*args, **kwargs)\n</code></pre> <p>Forward method is not supported.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>The forward method is not supported for this metric.</p> Source code in <code>mmlearn/modules/metrics/retrieval_recall.py</code> <pre><code>def forward(self, *args: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"Forward method is not supported.\n\n    Raises\n    ------\n    NotImplementedError\n        The forward method is not supported for this metric.\n    \"\"\"\n    raise NotImplementedError(\n        \"RetrievalRecallAtK metric does not support forward method\"\n    )\n</code></pre>"},{"location":"api/#tasks","title":"Tasks","text":""},{"location":"api/#mmlearn.tasks","title":"mmlearn.tasks","text":"<p>Modules for pretraining, downstream and evaluation tasks.</p>"},{"location":"api/#mmlearn.tasks.ContrastivePretraining","title":"ContrastivePretraining","text":"<p>               Bases: <code>TrainingTask</code></p> <p>Contrastive pretraining task.</p> <p>This class supports contrastive pretraining with <code>N</code> modalities of data. It allows the sharing of encoders, heads, and postprocessors across modalities. It also supports computing the contrastive loss between specified pairs of modalities, as well as training auxiliary tasks alongside the main contrastive pretraining task.</p> <p>Parameters:</p> Name Type Description Default <code>encoders</code> <code>dict[str, Module]</code> <p>A dictionary of encoders. The keys can be any string, including the names of any supported modalities. If the keys are not supported modalities, the <code>modality_module_mapping</code> parameter must be provided to map the encoders to specific modalities. The encoders are expected to take a dictionary of input values and return a list-like object with the first element being the encoded values. This first element is passed on to the heads or postprocessors and the remaining elements are ignored.</p> required <code>heads</code> <code>Optional[dict[str, Union[Module, dict[str, Module]]]]</code> <p>A dictionary of modules that process the encoder outputs, usually projection heads. If the keys do not correspond to the name of a supported modality, the <code>modality_module_mapping</code> parameter must be provided. If any of the values are dictionaries, they will be wrapped in a class:<code>torch.nn.Sequential</code> module. All head modules are expected to take a single input tensor and return a single output tensor.</p> <code>None</code> <code>postprocessors</code> <code>Optional[dict[str, Union[Module, dict[str, Module]]]]</code> <p>A dictionary of modules that process the head outputs. If the keys do not correspond to the name of a supported modality, the <code>modality_module_mapping</code> parameter must be provided. If any of the values are dictionaries, they will be wrapped in a <code>nn.Sequential</code> module. All postprocessor modules are expected to take a single input tensor and return a single output tensor.</p> <code>None</code> <code>modality_module_mapping</code> <code>Optional[dict[str, ModuleKeySpec]]</code> <p>A dictionary mapping modalities to encoders, heads, and postprocessors. Useful for reusing the same instance of a module across multiple modalities.</p> <code>None</code> <code>optimizer</code> <code>Optional[partial[Optimizer]]</code> <p>The optimizer to use for training. This is expected to be a func:<code>~functools.partial</code> function that takes the model parameters as the only required argument. If not provided, training will continue without an optimizer.</p> <code>None</code> <code>lr_scheduler</code> <code>Optional[Union[dict[str, Union[partial[LRScheduler], Any]], partial[LRScheduler]]]</code> <p>The learning rate scheduler to use for training. This can be a func:<code>~functools.partial</code> function that takes the optimizer as the only required argument or a dictionary with a <code>'scheduler'</code> key that specifies the scheduler and an optional <code>'extras'</code> key that specifies additional arguments to pass to the scheduler. If not provided, the learning rate will not be adjusted during training.</p> <code>None</code> <code>init_logit_scale</code> <code>float</code> <p>The initial value of the logit scale parameter. This is the log of the scale factor applied to the logits before computing the contrastive loss.</p> <code>1 / 0.07</code> <code>max_logit_scale</code> <code>float</code> <p>The maximum value of the logit scale parameter. The logit scale parameter is clamped to the range <code>[0, log(max_logit_scale)]</code>.</p> <code>100</code> <code>learnable_logit_scale</code> <code>bool</code> <p>Whether the logit scale parameter is learnable. If set to False, the logit scale parameter is treated as a constant.</p> <code>True</code> <code>loss</code> <code>Optional[Module]</code> <p>The loss function to use.</p> <code>None</code> <code>modality_loss_pairs</code> <code>Optional[list[LossPairSpec]]</code> <p>A list of pairs of modalities to compute the contrastive loss between and the weight to apply to each pair.</p> <code>None</code> <code>auxiliary_tasks</code> <code>dict[str, AuxiliaryTaskSpec]</code> <p>Auxiliary tasks to run alongside the main contrastive pretraining task.</p> <ul> <li>The auxiliary task module is expected to be a partially-initialized instance   of a class:<code>~lightning.pytorch.core.LightningModule</code> created using   func:<code>functools.partial</code>, such that an initialized encoder can be   passed as the only argument.</li> <li>The <code>modality</code> parameter specifies the modality of the encoder to use   for the auxiliary task. The <code>loss_weight</code> parameter specifies the weight   to apply to the auxiliary task loss.</li> </ul> <code>None</code> <code>log_auxiliary_tasks_loss</code> <code>bool</code> <p>Whether to log the loss of auxiliary tasks to the main logger.</p> <code>False</code> <code>compute_validation_loss</code> <code>bool</code> <p>Whether to compute the validation loss if a validation dataloader is provided. The loss function must be provided to compute the validation loss.</p> <code>True</code> <code>compute_test_loss</code> <code>bool</code> <p>Whether to compute the test loss if a test dataloader is provided. The loss function must be provided to compute the test loss.</p> <code>True</code> <code>evaluation_tasks</code> <code>Optional[dict[str, EvaluationSpec]]</code> <p>Evaluation tasks to run during validation, while training, and during testing.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <ul> <li>If the loss function is not provided and either the validation or test loss   needs to be computed.</li> <li>If the given modality is not supported.</li> <li>If the encoder, head, or postprocessor is not mapped to a modality.</li> <li>If an unsupported modality is found in the loss pair specification.</li> <li>If an unsupported modality is found in the auxiliary tasks.</li> <li>If the auxiliary task is not a partial function.</li> <li>If the evaluation task is not an instance of class:<code>~mmlearn.tasks.hooks.EvaluationHooks</code>.</li> </ul> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>@store(group=\"task\", provider=\"mmlearn\")\nclass ContrastivePretraining(TrainingTask):\n    \"\"\"Contrastive pretraining task.\n\n    This class supports contrastive pretraining with ``N`` modalities of data. It\n    allows the sharing of encoders, heads, and postprocessors across modalities.\n    It also supports computing the contrastive loss between specified pairs of\n    modalities, as well as training auxiliary tasks alongside the main contrastive\n    pretraining task.\n\n    Parameters\n    ----------\n    encoders : dict[str, torch.nn.Module]\n        A dictionary of encoders. The keys can be any string, including the names of\n        any supported modalities. If the keys are not supported modalities, the\n        ``modality_module_mapping`` parameter must be provided to map the encoders to\n        specific modalities. The encoders are expected to take a dictionary of input\n        values and return a list-like object with the first element being the encoded\n        values. This first element is passed on to the heads or postprocessors and\n        the remaining elements are ignored.\n    heads : Optional[dict[str, Union[torch.nn.Module, dict[str, torch.nn.Module]]]], optional, default=None\n        A dictionary of modules that process the encoder outputs, usually projection\n        heads. If the keys do not correspond to the name of a supported modality,\n        the ``modality_module_mapping`` parameter must be provided. If any of the values\n        are dictionaries, they will be wrapped in a :py:class:`torch.nn.Sequential`\n        module. All head modules are expected to take a single input tensor and\n        return a single output tensor.\n    postprocessors : Optional[dict[str, Union[torch.nn.Module, dict[str, torch.nn.Module]]]], optional, default=None\n        A dictionary of modules that process the head outputs. If the keys do not\n        correspond to the name of a supported modality, the `modality_module_mapping`\n        parameter must be provided. If any of the values are dictionaries, they will\n        be wrapped in a `nn.Sequential` module. All postprocessor modules are expected\n        to take a single input tensor and return a single output tensor.\n    modality_module_mapping : Optional[dict[str, ModuleKeySpec]], optional, default=None\n        A dictionary mapping modalities to encoders, heads, and postprocessors.\n        Useful for reusing the same instance of a module across multiple modalities.\n    optimizer : Optional[partial[torch.optim.Optimizer]], optional, default=None\n        The optimizer to use for training. This is expected to be a :py:func:`~functools.partial`\n        function that takes the model parameters as the only required argument.\n        If not provided, training will continue without an optimizer.\n    lr_scheduler : Optional[Union[dict[str, Union[partial[torch.optim.lr_scheduler.LRScheduler], Any]], partial[torch.optim.lr_scheduler.LRScheduler]]], optional, default=None\n        The learning rate scheduler to use for training. This can be a\n        :py:func:`~functools.partial` function that takes the optimizer as the only\n        required argument or a dictionary with a ``'scheduler'`` key that specifies\n        the scheduler and an optional ``'extras'`` key that specifies additional\n        arguments to pass to the scheduler. If not provided, the learning rate will\n        not be adjusted during training.\n    init_logit_scale : float, optional, default=1 / 0.07\n        The initial value of the logit scale parameter. This is the log of the scale\n        factor applied to the logits before computing the contrastive loss.\n    max_logit_scale : float, optional, default=100\n        The maximum value of the logit scale parameter. The logit scale parameter\n        is clamped to the range ``[0, log(max_logit_scale)]``.\n    learnable_logit_scale : bool, optional, default=True\n        Whether the logit scale parameter is learnable. If set to False, the logit\n        scale parameter is treated as a constant.\n    loss : Optional[torch.nn.Module], optional, default=None\n        The loss function to use.\n    modality_loss_pairs : Optional[list[LossPairSpec]], optional, default=None\n        A list of pairs of modalities to compute the contrastive loss between and\n        the weight to apply to each pair.\n    auxiliary_tasks : dict[str, AuxiliaryTaskSpec], optional, default=None\n        Auxiliary tasks to run alongside the main contrastive pretraining task.\n\n        - The auxiliary task module is expected to be a partially-initialized instance\n          of a :py:class:`~lightning.pytorch.core.LightningModule` created using\n          :py:func:`functools.partial`, such that an initialized encoder can be\n          passed as the only argument.\n        - The ``modality`` parameter specifies the modality of the encoder to use\n          for the auxiliary task. The ``loss_weight`` parameter specifies the weight\n          to apply to the auxiliary task loss.\n    log_auxiliary_tasks_loss : bool, optional, default=False\n        Whether to log the loss of auxiliary tasks to the main logger.\n    compute_validation_loss : bool, optional, default=True\n        Whether to compute the validation loss if a validation dataloader is provided.\n        The loss function must be provided to compute the validation loss.\n    compute_test_loss : bool, optional, default=True\n        Whether to compute the test loss if a test dataloader is provided. The loss\n        function must be provided to compute the test loss.\n    evaluation_tasks : Optional[dict[str, EvaluationSpec]], optional, default=None\n        Evaluation tasks to run during validation, while training, and during testing.\n\n    Raises\n    ------\n    ValueError\n\n        - If the loss function is not provided and either the validation or test loss\n          needs to be computed.\n        - If the given modality is not supported.\n        - If the encoder, head, or postprocessor is not mapped to a modality.\n        - If an unsupported modality is found in the loss pair specification.\n        - If an unsupported modality is found in the auxiliary tasks.\n        - If the auxiliary task is not a partial function.\n        - If the evaluation task is not an instance of :py:class:`~mmlearn.tasks.hooks.EvaluationHooks`.\n\n    \"\"\"  # noqa: W505\n\n    def __init__(  # noqa: PLR0912, PLR0915\n        self,\n        encoders: dict[str, nn.Module],\n        heads: Optional[dict[str, Union[nn.Module, dict[str, nn.Module]]]] = None,\n        postprocessors: Optional[\n            dict[str, Union[nn.Module, dict[str, nn.Module]]]\n        ] = None,\n        modality_module_mapping: Optional[dict[str, ModuleKeySpec]] = None,\n        optimizer: Optional[partial[torch.optim.Optimizer]] = None,\n        lr_scheduler: Optional[\n            Union[\n                dict[str, Union[partial[torch.optim.lr_scheduler.LRScheduler], Any]],\n                partial[torch.optim.lr_scheduler.LRScheduler],\n            ]\n        ] = None,\n        init_logit_scale: float = 1 / 0.07,\n        max_logit_scale: float = 100,\n        learnable_logit_scale: bool = True,\n        loss: Optional[nn.Module] = None,\n        modality_loss_pairs: Optional[list[LossPairSpec]] = None,\n        auxiliary_tasks: Optional[dict[str, AuxiliaryTaskSpec]] = None,\n        log_auxiliary_tasks_loss: bool = False,\n        compute_validation_loss: bool = True,\n        compute_test_loss: bool = True,\n        evaluation_tasks: Optional[dict[str, EvaluationSpec]] = None,\n    ) -&gt; None:\n        super().__init__(\n            optimizer=optimizer,\n            lr_scheduler=lr_scheduler,\n            loss_fn=loss,\n            compute_validation_loss=compute_validation_loss,\n            compute_test_loss=compute_test_loss,\n        )\n\n        self.save_hyperparameters(\n            ignore=[\n                \"encoders\",\n                \"heads\",\n                \"postprocessors\",\n                \"modality_module_mapping\",\n                \"loss\",\n                \"auxiliary_tasks\",\n                \"evaluation_tasks\",\n                \"modality_loss_pairs\",\n            ]\n        )\n\n        if modality_module_mapping is None:\n            # assume all the module dictionaries use the same keys corresponding\n            # to modalities\n            modality_module_mapping = {}\n            for key in encoders:\n                modality_module_mapping[key] = ModuleKeySpec(\n                    encoder_key=key,\n                    head_key=key,\n                    postprocessor_key=key,\n                )\n\n        # match modalities to encoders, heads, and postprocessors\n        modality_encoder_mapping: dict[str, Optional[str]] = {}\n        modality_head_mapping: dict[str, Optional[str]] = {}\n        modality_postprocessor_mapping: dict[str, Optional[str]] = {}\n        for modality_key, module_mapping in modality_module_mapping.items():\n            if not Modalities.has_modality(modality_key):\n                raise ValueError(_unsupported_modality_error.format(modality_key))\n            modality_encoder_mapping[modality_key] = module_mapping.encoder_key\n            modality_head_mapping[modality_key] = module_mapping.head_key\n            modality_postprocessor_mapping[modality_key] = (\n                module_mapping.postprocessor_key\n            )\n\n        # ensure all modules are mapped to a modality\n        for key in encoders:\n            if key not in modality_encoder_mapping.values():\n                if not Modalities.has_modality(key):\n                    raise ValueError(_unsupported_modality_error.format(key))\n                modality_encoder_mapping[key] = key\n\n        if heads is not None:\n            for key in heads:\n                if key not in modality_head_mapping.values():\n                    if not Modalities.has_modality(key):\n                        raise ValueError(_unsupported_modality_error.format(key))\n                    modality_head_mapping[key] = key\n\n        if postprocessors is not None:\n            for key in postprocessors:\n                if key not in modality_postprocessor_mapping.values():\n                    if not Modalities.has_modality(key):\n                        raise ValueError(_unsupported_modality_error.format(key))\n                    modality_postprocessor_mapping[key] = key\n\n        self._available_modalities: list[Modality] = [\n            Modalities.get_modality(modality_key)\n            for modality_key in modality_encoder_mapping\n        ]\n        assert len(self._available_modalities) &gt;= 2, (\n            \"Expected at least two modalities to be available. \"\n        )\n\n        #: A :py:class:`~torch.nn.ModuleDict`, where the keys are the names of the\n        #: modalities and the values are the encoder modules.\n        self.encoders = nn.ModuleDict(\n            {\n                Modalities.get_modality(modality_key).name: encoders[encoder_key]\n                for modality_key, encoder_key in modality_encoder_mapping.items()\n                if encoder_key is not None\n            }\n        )\n\n        #: A :py:class:`~torch.nn.ModuleDict`, where the keys are the names of the\n        #: modalities and the values are the projection head modules. This can be\n        #: ``None`` if no heads modules are provided.\n        self.heads = None\n        if heads is not None:\n            self.heads = nn.ModuleDict(\n                {\n                    Modalities.get_modality(modality_key).name: heads[head_key]\n                    if isinstance(heads[head_key], nn.Module)\n                    else nn.Sequential(*heads[head_key].values())\n                    for modality_key, head_key in modality_head_mapping.items()\n                    if head_key is not None and head_key in heads\n                }\n            )\n\n        #: A :py:class:`~torch.nn.ModuleDict`, where the keys are the names of the\n        #: modalities and the values are the postprocessor modules. This can be\n        #: ``None`` if no postprocessor modules are provided.\n        self.postprocessors = None\n        if postprocessors is not None:\n            self.postprocessors = nn.ModuleDict(\n                {\n                    Modalities.get_modality(modality_key).name: postprocessors[\n                        postprocessor_key\n                    ]\n                    if isinstance(postprocessors[postprocessor_key], nn.Module)\n                    else nn.Sequential(*postprocessors[postprocessor_key].values())\n                    for modality_key, postprocessor_key in modality_postprocessor_mapping.items()\n                    if postprocessor_key is not None\n                    and postprocessor_key in postprocessors\n                }\n            )\n\n        # set up logit scaling\n        log_logit_scale = torch.ones([]) * np.log(init_logit_scale)\n        self.max_logit_scale = max_logit_scale\n        self.learnable_logit_scale = learnable_logit_scale\n\n        if self.learnable_logit_scale:\n            self.log_logit_scale = torch.nn.Parameter(\n                log_logit_scale, requires_grad=True\n            )\n        else:\n            self.register_buffer(\"log_logit_scale\", log_logit_scale)\n\n        # set up contrastive loss pairs\n        if modality_loss_pairs is None:\n            modality_loss_pairs = [\n                LossPairSpec(modalities=(m1.name, m2.name))\n                for m1, m2 in itertools.combinations(self._available_modalities, 2)\n            ]\n\n        for modality_pair in modality_loss_pairs:\n            if not all(\n                Modalities.get_modality(modality) in self._available_modalities\n                for modality in modality_pair.modalities\n            ):\n                raise ValueError(\n                    \"Found unspecified modality in the loss pair specification \"\n                    f\"{modality_pair.modalities}. Available modalities are \"\n                    f\"{self._available_modalities}.\"\n                )\n\n        #: A list :py:class:`LossPairSpec` instances specifying the pairs of\n        #: modalities to compute the contrastive loss between and the weight to\n        #: apply to each pair.\n        self.modality_loss_pairs = modality_loss_pairs\n\n        # set up auxiliary tasks\n        self.aux_task_specs = auxiliary_tasks or {}\n        self.auxiliary_tasks: nn.ModuleDict[str, L.LightningModule] = nn.ModuleDict()\n        for task_name, task_spec in self.aux_task_specs.items():\n            if not Modalities.has_modality(task_spec.modality):\n                raise ValueError(\n                    f\"Found unsupported modality `{task_spec.modality}` in the auxiliary tasks. \"\n                    f\"Available modalities are {self._available_modalities}.\"\n                )\n            if not isinstance(task_spec.task, partial):\n                raise TypeError(\n                    f\"Expected auxiliary task to be a partial function, but got {type(task_spec.task)}.\"\n                )\n\n            self.auxiliary_tasks[task_name] = task_spec.task(\n                self.encoders[Modalities.get_modality(task_spec.modality).name]\n            )\n\n        self.log_auxiliary_tasks_loss = log_auxiliary_tasks_loss\n\n        if evaluation_tasks is not None:\n            for eval_task_spec in evaluation_tasks.values():\n                if not isinstance(eval_task_spec.task, EvaluationHooks):\n                    raise TypeError(\n                        f\"Expected {eval_task_spec.task} to be an instance of `EvaluationHooks` \"\n                        f\"but got {type(eval_task_spec.task)}.\"\n                    )\n\n        #: A dictionary of evaluation tasks to run during validation, while training,\n        #: or during testing.\n        self.evaluation_tasks = evaluation_tasks\n\n    def configure_model(self) -&gt; None:\n        \"\"\"Configure the model.\"\"\"\n        if self.auxiliary_tasks:\n            for task_name in self.auxiliary_tasks:\n                self.auxiliary_tasks[task_name].configure_model()\n\n    def encode(\n        self, inputs: dict[str, Any], modality: Modality, normalize: bool = False\n    ) -&gt; torch.Tensor:\n        \"\"\"Encode the input values for the given modality.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            Input values.\n        modality : Modality\n            The modality to encode.\n        normalize : bool, optional, default=False\n            Whether to apply L2 normalization to the output (after the head and\n            postprocessor layers, if present).\n\n        Returns\n        -------\n        torch.Tensor\n            The encoded values for the specified modality.\n        \"\"\"\n        output = self.encoders[modality.name](inputs)[0]\n\n        if self.postprocessors and modality.name in self.postprocessors:\n            output = self.postprocessors[modality.name](output)\n\n        if self.heads and modality.name in self.heads:\n            output = self.heads[modality.name](output)\n\n        if normalize:\n            output = torch.nn.functional.normalize(output, p=2, dim=-1)\n\n        return output\n\n    def forward(self, inputs: dict[str, Any]) -&gt; dict[str, torch.Tensor]:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input tensors to encode.\n\n        Returns\n        -------\n        dict[str, torch.Tensor]\n            The encodings for each modality.\n        \"\"\"\n        outputs = {\n            modality.embedding: self.encode(inputs, modality, normalize=True)\n            for modality in self._available_modalities\n            if modality.name in inputs\n        }\n\n        if not all(\n            output.size(-1) == list(outputs.values())[0].size(-1)\n            for output in outputs.values()\n        ):\n            raise ValueError(\"Expected all model outputs to have the same dimension.\")\n\n        return outputs\n\n    def on_train_epoch_start(self) -&gt; None:\n        \"\"\"Prepare for the training epoch.\n\n        This method sets the modules to training mode.\n        \"\"\"\n        self.encoders.train()\n        if self.heads:\n            self.heads.train()\n        if self.postprocessors:\n            self.postprocessors.train()\n\n    def training_step(self, batch: dict[str, Any], batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"Compute the loss for the batch.\n\n        Parameters\n        ----------\n        batch : dict[str, Any]\n            The batch of data to process.\n        batch_idx : int\n            The index of the batch.\n\n        Returns\n        -------\n        torch.Tensor\n            The loss for the batch.\n        \"\"\"\n        outputs = self(batch)\n\n        with torch.no_grad():\n            self.log_logit_scale.clamp_(0, math.log(self.max_logit_scale))\n\n        loss = self._compute_loss(batch, batch_idx, outputs)\n\n        if loss is None:\n            raise ValueError(\"The loss function must be provided for training.\")\n\n        self.log(\"train/loss\", loss, prog_bar=True, sync_dist=True)\n        self.log(\n            \"train/logit_scale\",\n            self.log_logit_scale.exp(),\n            prog_bar=True,\n            on_step=True,\n            on_epoch=False,\n        )\n\n        return loss\n\n    def on_before_zero_grad(self, optimizer: torch.optim.Optimizer) -&gt; None:\n        \"\"\"Zero out the gradients of the model.\"\"\"\n        if self.auxiliary_tasks:\n            for task in self.auxiliary_tasks.values():\n                task.on_before_zero_grad(optimizer)\n\n    def on_validation_epoch_start(self) -&gt; None:\n        \"\"\"Prepare for the validation epoch.\n\n        This method sets the modules to evaluation mode and calls the\n        ``on_evaluation_epoch_start`` method of each evaluation task.\n        \"\"\"\n        self._on_eval_epoch_start(\"val\")\n\n    def validation_step(\n        self, batch: dict[str, torch.Tensor], batch_idx: int\n    ) -&gt; Optional[torch.Tensor]:\n        \"\"\"Run a single validation step.\n\n        Parameters\n        ----------\n        batch : dict[str, torch.Tensor]\n            The batch of data to process.\n        batch_idx : int\n            The index of the batch.\n\n        Returns\n        -------\n        Optional[torch.Tensor]\n            The loss for the batch or ``None`` if the loss function is not provided.\n        \"\"\"\n        return self._shared_eval_step(batch, batch_idx, \"val\")\n\n    def on_validation_epoch_end(self) -&gt; None:\n        \"\"\"Compute and log epoch-level metrics at the end of the validation epoch.\"\"\"\n        self._on_eval_epoch_end(\"val\")\n\n    def on_test_epoch_start(self) -&gt; None:\n        \"\"\"Prepare for the test epoch.\"\"\"\n        self._on_eval_epoch_start(\"test\")\n\n    def test_step(\n        self, batch: dict[str, torch.Tensor], batch_idx: int\n    ) -&gt; Optional[torch.Tensor]:\n        \"\"\"Run a single test step.\n\n        Parameters\n        ----------\n        batch : dict[str, torch.Tensor]\n            The batch of data to process.\n        batch_idx : int\n            The index of the batch.\n\n        Returns\n        -------\n        Optional[torch.Tensor]\n            The loss for the batch or ``None`` if the loss function is not provided.\n        \"\"\"\n        return self._shared_eval_step(batch, batch_idx, \"test\")\n\n    def on_test_epoch_end(self) -&gt; None:\n        \"\"\"Compute and log epoch-level metrics at the end of the test epoch.\"\"\"\n        self._on_eval_epoch_end(\"test\")\n\n    def on_load_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n        \"\"\"Modify the model checkpoint after loading.\n\n        The `on_load_checkpoint` method of auxiliary tasks is called here to allow\n        them to modify the checkpoint after loading.\n\n        Parameters\n        ----------\n        checkpoint : Dict[str, Any]\n            The loaded checkpoint.\n        \"\"\"\n        if self.auxiliary_tasks:\n            for task in self.auxiliary_tasks.values():\n                task.on_load_checkpoint(checkpoint)\n\n    def on_save_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n        \"\"\"Modify the checkpoint before saving.\n\n        The `on_save_checkpoint` method of auxiliary tasks is called here to allow\n        them to modify the checkpoint before saving.\n\n        Parameters\n        ----------\n        checkpoint : Dict[str, Any]\n            The checkpoint to save.\n        \"\"\"\n        if self.auxiliary_tasks:\n            for task in self.auxiliary_tasks.values():\n                task.on_save_checkpoint(checkpoint)\n\n    def _compute_loss(\n        self, batch: dict[str, Any], batch_idx: int, outputs: dict[str, torch.Tensor]\n    ) -&gt; Optional[torch.Tensor]:\n        if self.loss_fn is None:\n            return None\n\n        contrastive_loss = self.loss_fn(\n            outputs,\n            batch[\"example_ids\"],\n            self.log_logit_scale.exp(),\n            self.modality_loss_pairs,\n        )\n\n        auxiliary_losses: list[torch.Tensor] = []\n        if self.auxiliary_tasks:\n            for task_name, task_spec in self.aux_task_specs.items():\n                auxiliary_task_output = self.auxiliary_tasks[task_name].training_step(\n                    batch, batch_idx\n                )\n                if isinstance(auxiliary_task_output, torch.Tensor):\n                    auxiliary_task_loss = auxiliary_task_output\n                elif isinstance(auxiliary_task_output, Mapping):\n                    auxiliary_task_loss = auxiliary_task_output[\"loss\"]\n                else:\n                    raise ValueError(\n                        \"Expected auxiliary task output to be a tensor or a mapping \"\n                        f\"containing a 'loss' key, but got {type(auxiliary_task_output)}.\"\n                    )\n\n                auxiliary_task_loss *= task_spec.loss_weight\n                auxiliary_losses.append(auxiliary_task_loss)\n                if self.log_auxiliary_tasks_loss:\n                    self.log(\n                        f\"train/{task_name}_loss\", auxiliary_task_loss, sync_dist=True\n                    )\n\n        if not auxiliary_losses:\n            return contrastive_loss\n\n        return torch.stack(auxiliary_losses).sum() + contrastive_loss\n\n    def _on_eval_epoch_start(self, eval_type: Literal[\"val\", \"test\"]) -&gt; None:\n        \"\"\"Prepare for the evaluation epoch.\"\"\"\n        self.encoders.eval()\n        if self.heads:\n            self.heads.eval()\n        if self.postprocessors:\n            self.postprocessors.eval()\n        if self.evaluation_tasks:\n            for task_spec in self.evaluation_tasks.values():\n                if (eval_type == \"val\" and task_spec.run_on_validation) or (\n                    eval_type == \"test\" and task_spec.run_on_test\n                ):\n                    task_spec.task.on_evaluation_epoch_start(self)\n\n    def _shared_eval_step(\n        self,\n        batch: dict[str, torch.Tensor],\n        batch_idx: int,\n        eval_type: Literal[\"val\", \"test\"],\n    ) -&gt; Optional[torch.Tensor]:\n        \"\"\"Run a single evaluation step.\n\n        Parameters\n        ----------\n        batch : dict[str, torch.Tensor]\n            The batch of data to process.\n        batch_idx : int\n            The index of the batch.\n\n        Returns\n        -------\n        Optional[torch.Tensor]\n            The loss for the batch or ``None`` if the loss function is not provided.\n        \"\"\"\n        loss: Optional[torch.Tensor] = None\n        if (eval_type == \"val\" and self.compute_validation_loss) or (\n            eval_type == \"test\" and self.compute_test_loss\n        ):\n            outputs = self(batch)\n            loss = self._compute_loss(batch, batch_idx, outputs)\n            if loss is not None and not self.trainer.sanity_checking:\n                self.log(f\"{eval_type}/loss\", loss, prog_bar=True, sync_dist=True)\n\n        if self.evaluation_tasks:\n            for task_spec in self.evaluation_tasks.values():\n                if (eval_type == \"val\" and task_spec.run_on_validation) or (\n                    eval_type == \"test\" and task_spec.run_on_test\n                ):\n                    task_spec.task.evaluation_step(self, batch, batch_idx)\n\n        return loss\n\n    def _on_eval_epoch_end(self, eval_type: Literal[\"val\", \"test\"]) -&gt; None:\n        \"\"\"Compute and log epoch-level metrics at the end of the evaluation epoch.\"\"\"\n        if self.evaluation_tasks:\n            for task_spec in self.evaluation_tasks.values():\n                if (eval_type == \"val\" and task_spec.run_on_validation) or (\n                    eval_type == \"test\" and task_spec.run_on_test\n                ):\n                    task_spec.task.on_evaluation_epoch_end(self)\n</code></pre>"},{"location":"api/#mmlearn.tasks.ContrastivePretraining.configure_model","title":"configure_model","text":"<pre><code>configure_model()\n</code></pre> <p>Configure the model.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def configure_model(self) -&gt; None:\n    \"\"\"Configure the model.\"\"\"\n    if self.auxiliary_tasks:\n        for task_name in self.auxiliary_tasks:\n            self.auxiliary_tasks[task_name].configure_model()\n</code></pre>"},{"location":"api/#mmlearn.tasks.ContrastivePretraining.encode","title":"encode","text":"<pre><code>encode(inputs, modality, normalize=False)\n</code></pre> <p>Encode the input values for the given modality.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>Input values.</p> required <code>modality</code> <code>Modality</code> <p>The modality to encode.</p> required <code>normalize</code> <code>bool</code> <p>Whether to apply L2 normalization to the output (after the head and postprocessor layers, if present).</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The encoded values for the specified modality.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def encode(\n    self, inputs: dict[str, Any], modality: Modality, normalize: bool = False\n) -&gt; torch.Tensor:\n    \"\"\"Encode the input values for the given modality.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        Input values.\n    modality : Modality\n        The modality to encode.\n    normalize : bool, optional, default=False\n        Whether to apply L2 normalization to the output (after the head and\n        postprocessor layers, if present).\n\n    Returns\n    -------\n    torch.Tensor\n        The encoded values for the specified modality.\n    \"\"\"\n    output = self.encoders[modality.name](inputs)[0]\n\n    if self.postprocessors and modality.name in self.postprocessors:\n        output = self.postprocessors[modality.name](output)\n\n    if self.heads and modality.name in self.heads:\n        output = self.heads[modality.name](output)\n\n    if normalize:\n        output = torch.nn.functional.normalize(output, p=2, dim=-1)\n\n    return output\n</code></pre>"},{"location":"api/#mmlearn.tasks.ContrastivePretraining.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input tensors to encode.</p> required <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>The encodings for each modality.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; dict[str, torch.Tensor]:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input tensors to encode.\n\n    Returns\n    -------\n    dict[str, torch.Tensor]\n        The encodings for each modality.\n    \"\"\"\n    outputs = {\n        modality.embedding: self.encode(inputs, modality, normalize=True)\n        for modality in self._available_modalities\n        if modality.name in inputs\n    }\n\n    if not all(\n        output.size(-1) == list(outputs.values())[0].size(-1)\n        for output in outputs.values()\n    ):\n        raise ValueError(\"Expected all model outputs to have the same dimension.\")\n\n    return outputs\n</code></pre>"},{"location":"api/#mmlearn.tasks.ContrastivePretraining.on_train_epoch_start","title":"on_train_epoch_start","text":"<pre><code>on_train_epoch_start()\n</code></pre> <p>Prepare for the training epoch.</p> <p>This method sets the modules to training mode.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def on_train_epoch_start(self) -&gt; None:\n    \"\"\"Prepare for the training epoch.\n\n    This method sets the modules to training mode.\n    \"\"\"\n    self.encoders.train()\n    if self.heads:\n        self.heads.train()\n    if self.postprocessors:\n        self.postprocessors.train()\n</code></pre>"},{"location":"api/#mmlearn.tasks.ContrastivePretraining.training_step","title":"training_step","text":"<pre><code>training_step(batch, batch_idx)\n</code></pre> <p>Compute the loss for the batch.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict[str, Any]</code> <p>The batch of data to process.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The loss for the batch.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def training_step(self, batch: dict[str, Any], batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"Compute the loss for the batch.\n\n    Parameters\n    ----------\n    batch : dict[str, Any]\n        The batch of data to process.\n    batch_idx : int\n        The index of the batch.\n\n    Returns\n    -------\n    torch.Tensor\n        The loss for the batch.\n    \"\"\"\n    outputs = self(batch)\n\n    with torch.no_grad():\n        self.log_logit_scale.clamp_(0, math.log(self.max_logit_scale))\n\n    loss = self._compute_loss(batch, batch_idx, outputs)\n\n    if loss is None:\n        raise ValueError(\"The loss function must be provided for training.\")\n\n    self.log(\"train/loss\", loss, prog_bar=True, sync_dist=True)\n    self.log(\n        \"train/logit_scale\",\n        self.log_logit_scale.exp(),\n        prog_bar=True,\n        on_step=True,\n        on_epoch=False,\n    )\n\n    return loss\n</code></pre>"},{"location":"api/#mmlearn.tasks.ContrastivePretraining.on_before_zero_grad","title":"on_before_zero_grad","text":"<pre><code>on_before_zero_grad(optimizer)\n</code></pre> <p>Zero out the gradients of the model.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def on_before_zero_grad(self, optimizer: torch.optim.Optimizer) -&gt; None:\n    \"\"\"Zero out the gradients of the model.\"\"\"\n    if self.auxiliary_tasks:\n        for task in self.auxiliary_tasks.values():\n            task.on_before_zero_grad(optimizer)\n</code></pre>"},{"location":"api/#mmlearn.tasks.ContrastivePretraining.on_validation_epoch_start","title":"on_validation_epoch_start","text":"<pre><code>on_validation_epoch_start()\n</code></pre> <p>Prepare for the validation epoch.</p> <p>This method sets the modules to evaluation mode and calls the <code>on_evaluation_epoch_start</code> method of each evaluation task.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def on_validation_epoch_start(self) -&gt; None:\n    \"\"\"Prepare for the validation epoch.\n\n    This method sets the modules to evaluation mode and calls the\n    ``on_evaluation_epoch_start`` method of each evaluation task.\n    \"\"\"\n    self._on_eval_epoch_start(\"val\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.ContrastivePretraining.validation_step","title":"validation_step","text":"<pre><code>validation_step(batch, batch_idx)\n</code></pre> <p>Run a single validation step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict[str, Tensor]</code> <p>The batch of data to process.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required <p>Returns:</p> Type Description <code>Optional[Tensor]</code> <p>The loss for the batch or <code>None</code> if the loss function is not provided.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def validation_step(\n    self, batch: dict[str, torch.Tensor], batch_idx: int\n) -&gt; Optional[torch.Tensor]:\n    \"\"\"Run a single validation step.\n\n    Parameters\n    ----------\n    batch : dict[str, torch.Tensor]\n        The batch of data to process.\n    batch_idx : int\n        The index of the batch.\n\n    Returns\n    -------\n    Optional[torch.Tensor]\n        The loss for the batch or ``None`` if the loss function is not provided.\n    \"\"\"\n    return self._shared_eval_step(batch, batch_idx, \"val\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.ContrastivePretraining.on_validation_epoch_end","title":"on_validation_epoch_end","text":"<pre><code>on_validation_epoch_end()\n</code></pre> <p>Compute and log epoch-level metrics at the end of the validation epoch.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def on_validation_epoch_end(self) -&gt; None:\n    \"\"\"Compute and log epoch-level metrics at the end of the validation epoch.\"\"\"\n    self._on_eval_epoch_end(\"val\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.ContrastivePretraining.on_test_epoch_start","title":"on_test_epoch_start","text":"<pre><code>on_test_epoch_start()\n</code></pre> <p>Prepare for the test epoch.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def on_test_epoch_start(self) -&gt; None:\n    \"\"\"Prepare for the test epoch.\"\"\"\n    self._on_eval_epoch_start(\"test\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.ContrastivePretraining.test_step","title":"test_step","text":"<pre><code>test_step(batch, batch_idx)\n</code></pre> <p>Run a single test step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict[str, Tensor]</code> <p>The batch of data to process.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required <p>Returns:</p> Type Description <code>Optional[Tensor]</code> <p>The loss for the batch or <code>None</code> if the loss function is not provided.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def test_step(\n    self, batch: dict[str, torch.Tensor], batch_idx: int\n) -&gt; Optional[torch.Tensor]:\n    \"\"\"Run a single test step.\n\n    Parameters\n    ----------\n    batch : dict[str, torch.Tensor]\n        The batch of data to process.\n    batch_idx : int\n        The index of the batch.\n\n    Returns\n    -------\n    Optional[torch.Tensor]\n        The loss for the batch or ``None`` if the loss function is not provided.\n    \"\"\"\n    return self._shared_eval_step(batch, batch_idx, \"test\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.ContrastivePretraining.on_test_epoch_end","title":"on_test_epoch_end","text":"<pre><code>on_test_epoch_end()\n</code></pre> <p>Compute and log epoch-level metrics at the end of the test epoch.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def on_test_epoch_end(self) -&gt; None:\n    \"\"\"Compute and log epoch-level metrics at the end of the test epoch.\"\"\"\n    self._on_eval_epoch_end(\"test\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.ContrastivePretraining.on_load_checkpoint","title":"on_load_checkpoint","text":"<pre><code>on_load_checkpoint(checkpoint)\n</code></pre> <p>Modify the model checkpoint after loading.</p> <p>The <code>on_load_checkpoint</code> method of auxiliary tasks is called here to allow them to modify the checkpoint after loading.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <code>Dict[str, Any]</code> <p>The loaded checkpoint.</p> required Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def on_load_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n    \"\"\"Modify the model checkpoint after loading.\n\n    The `on_load_checkpoint` method of auxiliary tasks is called here to allow\n    them to modify the checkpoint after loading.\n\n    Parameters\n    ----------\n    checkpoint : Dict[str, Any]\n        The loaded checkpoint.\n    \"\"\"\n    if self.auxiliary_tasks:\n        for task in self.auxiliary_tasks.values():\n            task.on_load_checkpoint(checkpoint)\n</code></pre>"},{"location":"api/#mmlearn.tasks.ContrastivePretraining.on_save_checkpoint","title":"on_save_checkpoint","text":"<pre><code>on_save_checkpoint(checkpoint)\n</code></pre> <p>Modify the checkpoint before saving.</p> <p>The <code>on_save_checkpoint</code> method of auxiliary tasks is called here to allow them to modify the checkpoint before saving.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <code>Dict[str, Any]</code> <p>The checkpoint to save.</p> required Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def on_save_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n    \"\"\"Modify the checkpoint before saving.\n\n    The `on_save_checkpoint` method of auxiliary tasks is called here to allow\n    them to modify the checkpoint before saving.\n\n    Parameters\n    ----------\n    checkpoint : Dict[str, Any]\n        The checkpoint to save.\n    \"\"\"\n    if self.auxiliary_tasks:\n        for task in self.auxiliary_tasks.values():\n            task.on_save_checkpoint(checkpoint)\n</code></pre>"},{"location":"api/#mmlearn.tasks.IJEPA","title":"IJEPA","text":"<p>               Bases: <code>TrainingTask</code></p> <p>Pretraining module for IJEPA.</p> <p>This class implements the IJEPA (Image Joint-Embedding Predictive Architecture) pretraining task using PyTorch Lightning. It trains an encoder and a predictor to reconstruct masked regions of an image based on its unmasked context.</p> <p>Parameters:</p> Name Type Description Default <code>encoder</code> <code>VisionTransformer</code> <p>Vision transformer encoder.</p> required <code>predictor</code> <code>VisionTransformerPredictor</code> <p>Vision transformer predictor.</p> required <code>optimizer</code> <code>Optional[partial[Optimizer]]</code> <p>The optimizer to use for training. This is expected to be a func:<code>~functools.partial</code> function that takes the model parameters as the only required argument. If not provided, training will continue without an optimizer.</p> <code>None</code> <code>lr_scheduler</code> <code>Optional[Union[dict[str, Union[partial[LRScheduler], Any]], partial[LRScheduler]]]</code> <p>The learning rate scheduler to use for training. This can be a func:<code>~functools.partial</code> function that takes the optimizer as the only required argument or a dictionary with a <code>'scheduler'</code> key that specifies the scheduler and an optional <code>'extras'</code> key that specifies additional arguments to pass to the scheduler. If not provided, the learning rate will not be adjusted during training.</p> <code>None</code> <code>ema_decay</code> <code>float</code> <p>Initial momentum for EMA of target encoder.</p> <code>0.996</code> <code>ema_decay_end</code> <code>float</code> <p>Final momentum for EMA of target encoder.</p> <code>1.0</code> <code>ema_anneal_end_step</code> <code>int</code> <p>Number of steps to anneal EMA momentum to <code>ema_decay_end</code>.</p> <code>1000</code> <code>loss_fn</code> <code>Optional[Callable[[Tensor, Tensor], Tensor]]</code> <p>Loss function to use. If not provided, defaults to func:<code>~torch.nn.functional.smooth_l1_loss</code>.</p> <code>None</code> <code>compute_validation_loss</code> <code>bool</code> <p>Whether to compute validation loss.</p> <code>True</code> <code>compute_test_loss</code> <code>bool</code> <p>Whether to compute test loss.</p> <code>True</code> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>@store(group=\"task\", provider=\"mmlearn\", zen_partial=False)\nclass IJEPA(TrainingTask):\n    \"\"\"Pretraining module for IJEPA.\n\n    This class implements the IJEPA (Image Joint-Embedding Predictive Architecture)\n    pretraining task using PyTorch Lightning. It trains an encoder and a predictor to\n    reconstruct masked regions of an image based on its unmasked context.\n\n    Parameters\n    ----------\n    encoder : VisionTransformer\n        Vision transformer encoder.\n    predictor : VisionTransformerPredictor\n        Vision transformer predictor.\n    optimizer : Optional[partial[torch.optim.Optimizer]], optional, default=None\n        The optimizer to use for training. This is expected to be a :py:func:`~functools.partial`\n        function that takes the model parameters as the only required argument.\n        If not provided, training will continue without an optimizer.\n    lr_scheduler : Optional[Union[dict[str, Union[partial[torch.optim.lr_scheduler.LRScheduler], Any]], partial[torch.optim.lr_scheduler.LRScheduler]]], optional, default=None\n        The learning rate scheduler to use for training. This can be a\n        :py:func:`~functools.partial` function that takes the optimizer as the only\n        required argument or a dictionary with a ``'scheduler'`` key that specifies\n        the scheduler and an optional ``'extras'`` key that specifies additional\n        arguments to pass to the scheduler. If not provided, the learning rate will\n        not be adjusted during training.\n    ema_decay : float, optional, default=0.996\n        Initial momentum for EMA of target encoder.\n    ema_decay_end : float, optional, default=1.0\n        Final momentum for EMA of target encoder.\n    ema_anneal_end_step : int, optional, default=1000\n        Number of steps to anneal EMA momentum to ``ema_decay_end``.\n    loss_fn : Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]], optional\n        Loss function to use. If not provided, defaults to\n        :py:func:`~torch.nn.functional.smooth_l1_loss`.\n    compute_validation_loss : bool, optional, default=True\n        Whether to compute validation loss.\n    compute_test_loss : bool, optional, default=True\n        Whether to compute test loss.\n    \"\"\"  # noqa: W505\n\n    def __init__(\n        self,\n        encoder: VisionTransformer,\n        predictor: VisionTransformerPredictor,\n        modality: str = \"RGB\",\n        optimizer: Optional[partial[torch.optim.Optimizer]] = None,\n        lr_scheduler: Optional[\n            Union[\n                dict[str, Union[partial[torch.optim.lr_scheduler.LRScheduler], Any]],\n                partial[torch.optim.lr_scheduler.LRScheduler],\n            ]\n        ] = None,\n        ema_decay: float = 0.996,\n        ema_decay_end: float = 1.0,\n        ema_anneal_end_step: int = 1000,\n        loss_fn: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None,\n        compute_validation_loss: bool = True,\n        compute_test_loss: bool = True,\n    ):\n        super().__init__(\n            optimizer=optimizer,\n            lr_scheduler=lr_scheduler,\n            loss_fn=loss_fn if loss_fn is not None else F.smooth_l1_loss,\n            compute_validation_loss=compute_validation_loss,\n            compute_test_loss=compute_test_loss,\n        )\n        self.modality = Modalities.get_modality(modality)\n        self.mask_generator = IJEPAMaskGenerator()\n\n        self.encoder = encoder\n        self.predictor = predictor\n\n        self.predictor.num_patches = encoder.patch_embed.num_patches\n        self.predictor.embed_dim = encoder.embed_dim\n        self.predictor.num_heads = encoder.num_heads\n\n        self.target_encoder = ExponentialMovingAverage(\n            self.encoder, ema_decay, ema_decay_end, ema_anneal_end_step\n        )\n\n    def configure_model(self) -&gt; None:\n        \"\"\"Configure the model.\"\"\"\n        self.target_encoder.configure_model(self.device)\n\n    def on_before_zero_grad(self, optimizer: torch.optim.Optimizer) -&gt; None:\n        \"\"\"Perform exponential moving average update of target encoder.\n\n        This is done right after the ``optimizer.step()`, which comes just before\n        ``optimizer.zero_grad()`` to account for gradient accumulation.\n        \"\"\"\n        if self.target_encoder is not None:\n            self.target_encoder.step(self.encoder)\n\n    def training_step(self, batch: dict[str, Any], batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"Perform a single training step.\n\n        Parameters\n        ----------\n        batch : dict[str, Any]\n            A batch of data.\n        batch_idx : int\n            Index of the batch.\n\n        Returns\n        -------\n        torch.Tensor\n            Loss value.\n        \"\"\"\n        return self._shared_step(batch, batch_idx, step_type=\"train\")\n\n    def validation_step(\n        self, batch: dict[str, Any], batch_idx: int\n    ) -&gt; Optional[torch.Tensor]:\n        \"\"\"Run a single validation step.\n\n        Parameters\n        ----------\n        batch : dict[str, Any]\n            A batch of data.\n        batch_idx : int\n            Index of the batch.\n\n        Returns\n        -------\n        Optional[torch.Tensor]\n            Loss value or ``None`` if no loss is computed.\n        \"\"\"\n        return self._shared_step(batch, batch_idx, step_type=\"val\")\n\n    def test_step(\n        self, batch: dict[str, Any], batch_idx: int\n    ) -&gt; Optional[torch.Tensor]:\n        \"\"\"Run a single test step.\n\n        Parameters\n        ----------\n        batch : dict[str, Any]\n            A batch of data.\n        batch_idx : int\n            Index of the batch.\n\n        Returns\n        -------\n        Optional[torch.Tensor]\n            Loss value or ``None`` if no loss is computed\n        \"\"\"\n        return self._shared_step(batch, batch_idx, step_type=\"test\")\n\n    def on_validation_epoch_start(self) -&gt; None:\n        \"\"\"Prepare for the validation epoch.\"\"\"\n        self._on_eval_epoch_start(\"val\")\n\n    def on_validation_epoch_end(self) -&gt; None:\n        \"\"\"Actions at the end of the validation epoch.\"\"\"\n        self._on_eval_epoch_end(\"val\")\n\n    def on_test_epoch_start(self) -&gt; None:\n        \"\"\"Prepare for the test epoch.\"\"\"\n        self._on_eval_epoch_start(\"test\")\n\n    def on_test_epoch_end(self) -&gt; None:\n        \"\"\"Actions at the end of the test epoch.\"\"\"\n        self._on_eval_epoch_end(\"test\")\n\n    def on_save_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n        \"\"\"Add relevant EMA state to the checkpoint.\n\n        Parameters\n        ----------\n        checkpoint : dict[str, Any]\n            The state dictionary to save the EMA state to.\n        \"\"\"\n        if self.target_encoder is not None:\n            checkpoint[\"ema_params\"] = {\n                \"decay\": self.target_encoder.decay,\n                \"num_updates\": self.target_encoder.num_updates,\n            }\n\n    def on_load_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n        \"\"\"Restore EMA state from the checkpoint.\n\n        Parameters\n        ----------\n        checkpoint : dict[str, Any]\n            The state dictionary to restore the EMA state from.\n        \"\"\"\n        if \"ema_params\" in checkpoint and self.target_encoder is not None:\n            ema_params = checkpoint.pop(\"ema_params\")\n            self.target_encoder.decay = ema_params[\"decay\"]\n            self.target_encoder.num_updates = ema_params[\"num_updates\"]\n\n            self.target_encoder.restore(self.encoder)\n\n    def _shared_step(\n        self, batch: dict[str, Any], batch_idx: int, step_type: str\n    ) -&gt; Optional[torch.Tensor]:\n        images = batch[self.modality.name]\n\n        # Generate masks\n        batch_size = images.size(0)\n        mask_info = self.mask_generator(batch_size=batch_size)\n\n        # Extract masks and move to device\n        device = images.device\n        encoder_masks = [mask.to(device) for mask in mask_info[\"encoder_masks\"]]\n        predictor_masks = [mask.to(device) for mask in mask_info[\"predictor_masks\"]]\n\n        # Forward pass through target encoder to get h\n        with torch.no_grad():\n            h = self.target_encoder.model(batch)[0]\n            h = F.layer_norm(h, h.size()[-1:])\n            h_masked = apply_masks(h, predictor_masks)\n            h_masked = repeat_interleave_batch(\n                h_masked, images.size(0), repeat=len(encoder_masks)\n            )\n\n        # Forward pass through encoder with encoder_masks\n        batch[self.modality.mask] = encoder_masks\n        z = self.encoder(batch)[0]\n\n        # Pass z through predictor with encoder_masks and predictor_masks\n        z_pred = self.predictor(z, encoder_masks, predictor_masks)\n\n        if step_type == \"train\":\n            self.log(\"train/ema_decay\", self.target_encoder.decay, prog_bar=True)\n\n        if self.loss_fn is not None and (\n            step_type == \"train\"\n            or (step_type == \"val\" and self.compute_validation_loss)\n            or (step_type == \"test\" and self.compute_test_loss)\n        ):\n            # Compute loss between z_pred and h_masked\n            loss = self.loss_fn(z_pred, h_masked)\n\n            # Log loss\n            self.log(f\"{step_type}/loss\", loss, prog_bar=True, sync_dist=True)\n\n            return loss\n\n        return None\n\n    def _on_eval_epoch_start(self, step_type: str) -&gt; None:\n        \"\"\"Initialize states or configurations at the start of an evaluation epoch.\n\n        Parameters\n        ----------\n        step_type : str\n            Type of the evaluation phase (\"val\" or \"test\").\n        \"\"\"\n        if (\n            step_type == \"val\"\n            and self.compute_validation_loss\n            or step_type == \"test\"\n            and self.compute_test_loss\n        ):\n            self.log(f\"{step_type}/start\", 1, prog_bar=True, sync_dist=True)\n\n    def _on_eval_epoch_end(self, step_type: str) -&gt; None:\n        \"\"\"Finalize states or logging at the end of an evaluation epoch.\n\n        Parameters\n        ----------\n        step_type : str\n            Type of the evaluation phase (\"val\" or \"test\").\n        \"\"\"\n        if (\n            step_type == \"val\"\n            and self.compute_validation_loss\n            or step_type == \"test\"\n            and self.compute_test_loss\n        ):\n            self.log(f\"{step_type}/end\", 1, prog_bar=True, sync_dist=True)\n</code></pre>"},{"location":"api/#mmlearn.tasks.IJEPA.configure_model","title":"configure_model","text":"<pre><code>configure_model()\n</code></pre> <p>Configure the model.</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def configure_model(self) -&gt; None:\n    \"\"\"Configure the model.\"\"\"\n    self.target_encoder.configure_model(self.device)\n</code></pre>"},{"location":"api/#mmlearn.tasks.IJEPA.on_before_zero_grad","title":"on_before_zero_grad","text":"<pre><code>on_before_zero_grad(optimizer)\n</code></pre> <p>Perform exponential moving average update of target encoder.</p> <p>This is done right after the <code>optimizer.step()`, which comes just before</code>optimizer.zero_grad()`` to account for gradient accumulation.</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def on_before_zero_grad(self, optimizer: torch.optim.Optimizer) -&gt; None:\n    \"\"\"Perform exponential moving average update of target encoder.\n\n    This is done right after the ``optimizer.step()`, which comes just before\n    ``optimizer.zero_grad()`` to account for gradient accumulation.\n    \"\"\"\n    if self.target_encoder is not None:\n        self.target_encoder.step(self.encoder)\n</code></pre>"},{"location":"api/#mmlearn.tasks.IJEPA.training_step","title":"training_step","text":"<pre><code>training_step(batch, batch_idx)\n</code></pre> <p>Perform a single training step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict[str, Any]</code> <p>A batch of data.</p> required <code>batch_idx</code> <code>int</code> <p>Index of the batch.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Loss value.</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def training_step(self, batch: dict[str, Any], batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"Perform a single training step.\n\n    Parameters\n    ----------\n    batch : dict[str, Any]\n        A batch of data.\n    batch_idx : int\n        Index of the batch.\n\n    Returns\n    -------\n    torch.Tensor\n        Loss value.\n    \"\"\"\n    return self._shared_step(batch, batch_idx, step_type=\"train\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.IJEPA.validation_step","title":"validation_step","text":"<pre><code>validation_step(batch, batch_idx)\n</code></pre> <p>Run a single validation step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict[str, Any]</code> <p>A batch of data.</p> required <code>batch_idx</code> <code>int</code> <p>Index of the batch.</p> required <p>Returns:</p> Type Description <code>Optional[Tensor]</code> <p>Loss value or <code>None</code> if no loss is computed.</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def validation_step(\n    self, batch: dict[str, Any], batch_idx: int\n) -&gt; Optional[torch.Tensor]:\n    \"\"\"Run a single validation step.\n\n    Parameters\n    ----------\n    batch : dict[str, Any]\n        A batch of data.\n    batch_idx : int\n        Index of the batch.\n\n    Returns\n    -------\n    Optional[torch.Tensor]\n        Loss value or ``None`` if no loss is computed.\n    \"\"\"\n    return self._shared_step(batch, batch_idx, step_type=\"val\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.IJEPA.test_step","title":"test_step","text":"<pre><code>test_step(batch, batch_idx)\n</code></pre> <p>Run a single test step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict[str, Any]</code> <p>A batch of data.</p> required <code>batch_idx</code> <code>int</code> <p>Index of the batch.</p> required <p>Returns:</p> Type Description <code>Optional[Tensor]</code> <p>Loss value or <code>None</code> if no loss is computed</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def test_step(\n    self, batch: dict[str, Any], batch_idx: int\n) -&gt; Optional[torch.Tensor]:\n    \"\"\"Run a single test step.\n\n    Parameters\n    ----------\n    batch : dict[str, Any]\n        A batch of data.\n    batch_idx : int\n        Index of the batch.\n\n    Returns\n    -------\n    Optional[torch.Tensor]\n        Loss value or ``None`` if no loss is computed\n    \"\"\"\n    return self._shared_step(batch, batch_idx, step_type=\"test\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.IJEPA.on_validation_epoch_start","title":"on_validation_epoch_start","text":"<pre><code>on_validation_epoch_start()\n</code></pre> <p>Prepare for the validation epoch.</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def on_validation_epoch_start(self) -&gt; None:\n    \"\"\"Prepare for the validation epoch.\"\"\"\n    self._on_eval_epoch_start(\"val\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.IJEPA.on_validation_epoch_end","title":"on_validation_epoch_end","text":"<pre><code>on_validation_epoch_end()\n</code></pre> <p>Actions at the end of the validation epoch.</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def on_validation_epoch_end(self) -&gt; None:\n    \"\"\"Actions at the end of the validation epoch.\"\"\"\n    self._on_eval_epoch_end(\"val\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.IJEPA.on_test_epoch_start","title":"on_test_epoch_start","text":"<pre><code>on_test_epoch_start()\n</code></pre> <p>Prepare for the test epoch.</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def on_test_epoch_start(self) -&gt; None:\n    \"\"\"Prepare for the test epoch.\"\"\"\n    self._on_eval_epoch_start(\"test\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.IJEPA.on_test_epoch_end","title":"on_test_epoch_end","text":"<pre><code>on_test_epoch_end()\n</code></pre> <p>Actions at the end of the test epoch.</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def on_test_epoch_end(self) -&gt; None:\n    \"\"\"Actions at the end of the test epoch.\"\"\"\n    self._on_eval_epoch_end(\"test\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.IJEPA.on_save_checkpoint","title":"on_save_checkpoint","text":"<pre><code>on_save_checkpoint(checkpoint)\n</code></pre> <p>Add relevant EMA state to the checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <code>dict[str, Any]</code> <p>The state dictionary to save the EMA state to.</p> required Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def on_save_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n    \"\"\"Add relevant EMA state to the checkpoint.\n\n    Parameters\n    ----------\n    checkpoint : dict[str, Any]\n        The state dictionary to save the EMA state to.\n    \"\"\"\n    if self.target_encoder is not None:\n        checkpoint[\"ema_params\"] = {\n            \"decay\": self.target_encoder.decay,\n            \"num_updates\": self.target_encoder.num_updates,\n        }\n</code></pre>"},{"location":"api/#mmlearn.tasks.IJEPA.on_load_checkpoint","title":"on_load_checkpoint","text":"<pre><code>on_load_checkpoint(checkpoint)\n</code></pre> <p>Restore EMA state from the checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <code>dict[str, Any]</code> <p>The state dictionary to restore the EMA state from.</p> required Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def on_load_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n    \"\"\"Restore EMA state from the checkpoint.\n\n    Parameters\n    ----------\n    checkpoint : dict[str, Any]\n        The state dictionary to restore the EMA state from.\n    \"\"\"\n    if \"ema_params\" in checkpoint and self.target_encoder is not None:\n        ema_params = checkpoint.pop(\"ema_params\")\n        self.target_encoder.decay = ema_params[\"decay\"]\n        self.target_encoder.num_updates = ema_params[\"num_updates\"]\n\n        self.target_encoder.restore(self.encoder)\n</code></pre>"},{"location":"api/#mmlearn.tasks.ZeroShotClassification","title":"ZeroShotClassification","text":"<p>               Bases: <code>EvaluationHooks</code></p> <p>Zero-shot classification evaluation task.</p> <p>This task evaluates the zero-shot classification performance.</p> <p>Parameters:</p> Name Type Description Default <code>task_specs</code> <code>list[ClassificationTaskSpec]</code> <p>A list of classification task specifications.</p> required <code>tokenizer</code> <code>Callable[[Union[str, list[str]]], Union[Tensor, dict[str, Tensor]]]</code> <p>A function to tokenize text inputs.</p> required Source code in <code>mmlearn/tasks/zero_shot_classification.py</code> <pre><code>@store(group=\"eval_task\", provider=\"mmlearn\")\nclass ZeroShotClassification(EvaluationHooks):\n    \"\"\"Zero-shot classification evaluation task.\n\n    This task evaluates the zero-shot classification performance.\n\n    Parameters\n    ----------\n    task_specs : list[ClassificationTaskSpec]\n        A list of classification task specifications.\n    tokenizer : Callable[[Union[str, list[str]]], Union[torch.Tensor, dict[str, torch.Tensor]]]\n        A function to tokenize text inputs.\n    \"\"\"  # noqa: W505\n\n    def __init__(\n        self,\n        task_specs: list[ClassificationTaskSpec],\n        tokenizer: Callable[\n            [Union[str, list[str]]], Union[torch.Tensor, dict[str, torch.Tensor]]\n        ],\n    ) -&gt; None:\n        super().__init__()\n        self.tokenizer = tokenizer\n        self.task_specs = task_specs\n        for spec in self.task_specs:\n            assert Modalities.has_modality(spec.query_modality)\n\n        self.metrics: dict[tuple[str, int], MetricCollection] = {}\n        self._embeddings_store: dict[int, torch.Tensor] = {}\n\n    def on_evaluation_epoch_start(self, pl_module: LightningModule) -&gt; None:\n        \"\"\"Set up the evaluation task.\n\n        Parameters\n        ----------\n        pl_module : pl.LightningModule\n            A reference to the Lightning module being evaluated.\n\n        Raises\n        ------\n        ValueError\n            - If the task is not being run for validation or testing.\n            - If the dataset does not have the required attributes to perform zero-shot\n              classification (i.e ``id2label`` and ``zero_shot_prompt_templates``).\n        \"\"\"\n        if pl_module.trainer.validating:\n            eval_dataset: CombinedDataset = pl_module.trainer.val_dataloaders.dataset\n        elif pl_module.trainer.testing:\n            eval_dataset = pl_module.trainer.test_dataloaders.dataset\n        else:\n            raise ValueError(\n                \"ZeroShotClassification task is only supported for validation and testing.\"\n            )\n\n        self.all_dataset_info = {}\n\n        # create metrics for each dataset/query_modality combination\n        if not self.metrics:\n            for dataset_index, dataset in enumerate(eval_dataset.datasets):\n                dataset_name = getattr(dataset, \"name\", dataset.__class__.__name__)\n                try:\n                    id2label: dict[int, str] = dataset.id2label\n                except AttributeError:\n                    raise ValueError(\n                        f\"Dataset '{dataset_name}' must have a `id2label` attribute \"\n                        \"to perform zero-shot classification.\"\n                    ) from None\n\n                try:\n                    zero_shot_prompt_templates: list[str] = (\n                        dataset.zero_shot_prompt_templates\n                    )\n                except AttributeError:\n                    raise ValueError(\n                        \"Dataset must have a `zero_shot_prompt_templates` attribute to perform zero-shot classification.\"\n                    ) from None\n\n                num_classes = len(id2label)\n\n                self.all_dataset_info[dataset_index] = {\n                    \"name\": dataset_name,\n                    \"id2label\": id2label,\n                    \"prompt_templates\": zero_shot_prompt_templates,\n                    \"num_classes\": num_classes,\n                }\n\n                for spec in self.task_specs:\n                    query_modality = Modalities.get_modality(spec.query_modality).name\n                    self.metrics[(query_modality, dataset_index)] = (\n                        self._create_metrics(\n                            num_classes,\n                            spec.top_k,\n                            prefix=f\"{dataset_name}/{query_modality}_\",\n                            postfix=\"\",\n                        )\n                    )\n\n        for metric in self.metrics.values():\n            metric.to(pl_module.device)\n\n        for dataset_index, dataset_info in self.all_dataset_info.items():\n            id2label = dataset_info[\"id2label\"]\n            prompt_templates: list[str] = dataset_info[\"prompt_templates\"]\n            labels = list(id2label.values())\n\n            with torch.no_grad():\n                chunk_size = 10\n                all_embeddings = []\n\n                for i in tqdm(\n                    range(0, len(labels), chunk_size),\n                    desc=\"Encoding class descriptions\",\n                ):\n                    batch_labels = labels[i : min(i + chunk_size, len(labels))]\n                    descriptions = [\n                        template.format(label)\n                        for label in batch_labels\n                        for template in prompt_templates\n                    ]\n                    tokenized_descriptions = move_data_to_device(\n                        self.tokenizer(descriptions),\n                        pl_module.device,\n                    )\n\n                    # Encode the chunk using the pl_module's encode method\n                    chunk_embeddings = pl_module.encode(\n                        tokenized_descriptions, Modalities.TEXT\n                    )  # shape: [chunk_size x len(prompt_templates), embed_dim]\n                    chunk_embeddings /= chunk_embeddings.norm(p=2, dim=-1, keepdim=True)\n                    chunk_embeddings = chunk_embeddings.reshape(\n                        len(batch_labels), len(prompt_templates), -1\n                    ).mean(dim=1)  # shape: [chunk_size, embed_dim]\n                    chunk_embeddings /= chunk_embeddings.norm(p=2, dim=-1, keepdim=True)\n\n                    # Append the chunk embeddings to the list\n                    all_embeddings.append(chunk_embeddings)\n\n                # Concatenate all chunk embeddings into a single tensor\n                class_embeddings = torch.cat(all_embeddings, dim=0)\n\n            self._embeddings_store[dataset_index] = class_embeddings\n\n    def evaluation_step(\n        self, pl_module: LightningModule, batch: dict[str, torch.Tensor], batch_idx: int\n    ) -&gt; None:\n        \"\"\"Compute logits and update metrics.\n\n        Parameters\n        ----------\n        pl_module : pl.LightningModule\n            A reference to the Lightning module being evaluated.\n        batch : dict[str, torch.Tensor]\n            A batch of data.\n        batch_idx : int\n            The index of the batch.\n        \"\"\"\n        if pl_module.trainer.sanity_checking:\n            return\n\n        for (query_modality, dataset_index), metric_collection in self.metrics.items():\n            matching_indices = torch.where(batch[\"dataset_index\"] == dataset_index)[0]\n\n            if not matching_indices.numel():\n                continue\n\n            class_embeddings = self._embeddings_store[dataset_index]\n            query_embeddings: torch.Tensor = pl_module.encode(\n                batch, Modalities.get_modality(query_modality)\n            )\n            query_embeddings /= query_embeddings.norm(p=2, dim=-1, keepdim=True)\n            query_embeddings = query_embeddings[matching_indices]\n\n            if self.all_dataset_info[dataset_index][\"num_classes\"] == 2:\n                softmax_output = _safe_matmul(\n                    query_embeddings, class_embeddings\n                ).softmax(dim=-1)\n                logits = softmax_output[:, 1] - softmax_output[:, 0]\n            else:\n                logits = 100.0 * _safe_matmul(query_embeddings, class_embeddings)\n            targets = batch[Modalities.get_modality(query_modality).target][\n                matching_indices\n            ]\n\n            metric_collection.update(logits, targets)\n\n    def on_evaluation_epoch_end(self, pl_module: LightningModule) -&gt; dict[str, Any]:\n        \"\"\"Compute and reset metrics.\n\n        Parameters\n        ----------\n        pl_module : pl.LightningModule\n            A reference to the Lightning module being evaluated.\n\n        Returns\n        -------\n        dict[str, Any]\n            The computed metrics.\n        \"\"\"\n        results = {}\n        for metric_collection in self.metrics.values():\n            results.update(metric_collection.compute())\n            metric_collection.reset()\n\n        self._embeddings_store.clear()\n\n        eval_type = \"val\" if pl_module.trainer.validating else \"test\"\n        for key, value in results.items():\n            pl_module.log(f\"{eval_type}/{key}\", value)\n\n        return results\n\n    @staticmethod\n    def _create_metrics(\n        num_classes: int, top_k: list[int], prefix: str, postfix: str\n    ) -&gt; MetricCollection:\n        \"\"\"Create a collection of classification metrics.\"\"\"\n        task_type = \"binary\" if num_classes == 2 else \"multiclass\"\n        acc_metrics = (\n            {\n                f\"top{k}_accuracy\": Accuracy(\n                    task=task_type, num_classes=num_classes, top_k=k, average=\"micro\"\n                )\n                for k in top_k\n            }\n            if num_classes &gt; 2\n            else {\"accuracy\": Accuracy(task=task_type, num_classes=num_classes)}\n        )\n        return MetricCollection(\n            {\n                \"precision\": Precision(\n                    task=task_type,\n                    num_classes=num_classes,\n                    average=\"macro\" if num_classes &gt; 2 else \"micro\",\n                ),\n                \"recall\": Recall(\n                    task=task_type,\n                    num_classes=num_classes,\n                    average=\"macro\" if num_classes &gt; 2 else \"micro\",\n                ),\n                \"f1_score_macro\": F1Score(\n                    task=task_type,\n                    num_classes=num_classes,\n                    average=\"macro\" if num_classes &gt; 2 else \"micro\",\n                ),\n                \"aucroc\": AUROC(task=task_type, num_classes=num_classes),\n                **acc_metrics,\n            },\n            prefix=prefix,\n            postfix=postfix,\n            compute_groups=True,\n        )\n</code></pre>"},{"location":"api/#mmlearn.tasks.ZeroShotClassification.on_evaluation_epoch_start","title":"on_evaluation_epoch_start","text":"<pre><code>on_evaluation_epoch_start(pl_module)\n</code></pre> <p>Set up the evaluation task.</p> <p>Parameters:</p> Name Type Description Default <code>pl_module</code> <code>LightningModule</code> <p>A reference to the Lightning module being evaluated.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <ul> <li>If the task is not being run for validation or testing.</li> <li>If the dataset does not have the required attributes to perform zero-shot   classification (i.e <code>id2label</code> and <code>zero_shot_prompt_templates</code>).</li> </ul> Source code in <code>mmlearn/tasks/zero_shot_classification.py</code> <pre><code>def on_evaluation_epoch_start(self, pl_module: LightningModule) -&gt; None:\n    \"\"\"Set up the evaluation task.\n\n    Parameters\n    ----------\n    pl_module : pl.LightningModule\n        A reference to the Lightning module being evaluated.\n\n    Raises\n    ------\n    ValueError\n        - If the task is not being run for validation or testing.\n        - If the dataset does not have the required attributes to perform zero-shot\n          classification (i.e ``id2label`` and ``zero_shot_prompt_templates``).\n    \"\"\"\n    if pl_module.trainer.validating:\n        eval_dataset: CombinedDataset = pl_module.trainer.val_dataloaders.dataset\n    elif pl_module.trainer.testing:\n        eval_dataset = pl_module.trainer.test_dataloaders.dataset\n    else:\n        raise ValueError(\n            \"ZeroShotClassification task is only supported for validation and testing.\"\n        )\n\n    self.all_dataset_info = {}\n\n    # create metrics for each dataset/query_modality combination\n    if not self.metrics:\n        for dataset_index, dataset in enumerate(eval_dataset.datasets):\n            dataset_name = getattr(dataset, \"name\", dataset.__class__.__name__)\n            try:\n                id2label: dict[int, str] = dataset.id2label\n            except AttributeError:\n                raise ValueError(\n                    f\"Dataset '{dataset_name}' must have a `id2label` attribute \"\n                    \"to perform zero-shot classification.\"\n                ) from None\n\n            try:\n                zero_shot_prompt_templates: list[str] = (\n                    dataset.zero_shot_prompt_templates\n                )\n            except AttributeError:\n                raise ValueError(\n                    \"Dataset must have a `zero_shot_prompt_templates` attribute to perform zero-shot classification.\"\n                ) from None\n\n            num_classes = len(id2label)\n\n            self.all_dataset_info[dataset_index] = {\n                \"name\": dataset_name,\n                \"id2label\": id2label,\n                \"prompt_templates\": zero_shot_prompt_templates,\n                \"num_classes\": num_classes,\n            }\n\n            for spec in self.task_specs:\n                query_modality = Modalities.get_modality(spec.query_modality).name\n                self.metrics[(query_modality, dataset_index)] = (\n                    self._create_metrics(\n                        num_classes,\n                        spec.top_k,\n                        prefix=f\"{dataset_name}/{query_modality}_\",\n                        postfix=\"\",\n                    )\n                )\n\n    for metric in self.metrics.values():\n        metric.to(pl_module.device)\n\n    for dataset_index, dataset_info in self.all_dataset_info.items():\n        id2label = dataset_info[\"id2label\"]\n        prompt_templates: list[str] = dataset_info[\"prompt_templates\"]\n        labels = list(id2label.values())\n\n        with torch.no_grad():\n            chunk_size = 10\n            all_embeddings = []\n\n            for i in tqdm(\n                range(0, len(labels), chunk_size),\n                desc=\"Encoding class descriptions\",\n            ):\n                batch_labels = labels[i : min(i + chunk_size, len(labels))]\n                descriptions = [\n                    template.format(label)\n                    for label in batch_labels\n                    for template in prompt_templates\n                ]\n                tokenized_descriptions = move_data_to_device(\n                    self.tokenizer(descriptions),\n                    pl_module.device,\n                )\n\n                # Encode the chunk using the pl_module's encode method\n                chunk_embeddings = pl_module.encode(\n                    tokenized_descriptions, Modalities.TEXT\n                )  # shape: [chunk_size x len(prompt_templates), embed_dim]\n                chunk_embeddings /= chunk_embeddings.norm(p=2, dim=-1, keepdim=True)\n                chunk_embeddings = chunk_embeddings.reshape(\n                    len(batch_labels), len(prompt_templates), -1\n                ).mean(dim=1)  # shape: [chunk_size, embed_dim]\n                chunk_embeddings /= chunk_embeddings.norm(p=2, dim=-1, keepdim=True)\n\n                # Append the chunk embeddings to the list\n                all_embeddings.append(chunk_embeddings)\n\n            # Concatenate all chunk embeddings into a single tensor\n            class_embeddings = torch.cat(all_embeddings, dim=0)\n\n        self._embeddings_store[dataset_index] = class_embeddings\n</code></pre>"},{"location":"api/#mmlearn.tasks.ZeroShotClassification.evaluation_step","title":"evaluation_step","text":"<pre><code>evaluation_step(pl_module, batch, batch_idx)\n</code></pre> <p>Compute logits and update metrics.</p> <p>Parameters:</p> Name Type Description Default <code>pl_module</code> <code>LightningModule</code> <p>A reference to the Lightning module being evaluated.</p> required <code>batch</code> <code>dict[str, Tensor]</code> <p>A batch of data.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required Source code in <code>mmlearn/tasks/zero_shot_classification.py</code> <pre><code>def evaluation_step(\n    self, pl_module: LightningModule, batch: dict[str, torch.Tensor], batch_idx: int\n) -&gt; None:\n    \"\"\"Compute logits and update metrics.\n\n    Parameters\n    ----------\n    pl_module : pl.LightningModule\n        A reference to the Lightning module being evaluated.\n    batch : dict[str, torch.Tensor]\n        A batch of data.\n    batch_idx : int\n        The index of the batch.\n    \"\"\"\n    if pl_module.trainer.sanity_checking:\n        return\n\n    for (query_modality, dataset_index), metric_collection in self.metrics.items():\n        matching_indices = torch.where(batch[\"dataset_index\"] == dataset_index)[0]\n\n        if not matching_indices.numel():\n            continue\n\n        class_embeddings = self._embeddings_store[dataset_index]\n        query_embeddings: torch.Tensor = pl_module.encode(\n            batch, Modalities.get_modality(query_modality)\n        )\n        query_embeddings /= query_embeddings.norm(p=2, dim=-1, keepdim=True)\n        query_embeddings = query_embeddings[matching_indices]\n\n        if self.all_dataset_info[dataset_index][\"num_classes\"] == 2:\n            softmax_output = _safe_matmul(\n                query_embeddings, class_embeddings\n            ).softmax(dim=-1)\n            logits = softmax_output[:, 1] - softmax_output[:, 0]\n        else:\n            logits = 100.0 * _safe_matmul(query_embeddings, class_embeddings)\n        targets = batch[Modalities.get_modality(query_modality).target][\n            matching_indices\n        ]\n\n        metric_collection.update(logits, targets)\n</code></pre>"},{"location":"api/#mmlearn.tasks.ZeroShotClassification.on_evaluation_epoch_end","title":"on_evaluation_epoch_end","text":"<pre><code>on_evaluation_epoch_end(pl_module)\n</code></pre> <p>Compute and reset metrics.</p> <p>Parameters:</p> Name Type Description Default <code>pl_module</code> <code>LightningModule</code> <p>A reference to the Lightning module being evaluated.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>The computed metrics.</p> Source code in <code>mmlearn/tasks/zero_shot_classification.py</code> <pre><code>def on_evaluation_epoch_end(self, pl_module: LightningModule) -&gt; dict[str, Any]:\n    \"\"\"Compute and reset metrics.\n\n    Parameters\n    ----------\n    pl_module : pl.LightningModule\n        A reference to the Lightning module being evaluated.\n\n    Returns\n    -------\n    dict[str, Any]\n        The computed metrics.\n    \"\"\"\n    results = {}\n    for metric_collection in self.metrics.values():\n        results.update(metric_collection.compute())\n        metric_collection.reset()\n\n    self._embeddings_store.clear()\n\n    eval_type = \"val\" if pl_module.trainer.validating else \"test\"\n    for key, value in results.items():\n        pl_module.log(f\"{eval_type}/{key}\", value)\n\n    return results\n</code></pre>"},{"location":"api/#mmlearn.tasks.ZeroShotCrossModalRetrieval","title":"ZeroShotCrossModalRetrieval","text":"<p>               Bases: <code>EvaluationHooks</code></p> <p>Zero-shot cross-modal retrieval evaluation task.</p> <p>This task evaluates the retrieval performance of a model on a set of query-target pairs. The model is expected to produce embeddings for both the query and target modalities. The task computes the retrieval recall at <code>k</code> for each pair of modalities.</p> <p>Parameters:</p> Name Type Description Default <code>task_specs</code> <code>list[RetrievalTaskSpec]</code> <p>A list of retrieval task specifications. Each specification defines the query and target modalities, as well as the top-k values for which to compute the retrieval recall metrics.</p> required Source code in <code>mmlearn/tasks/zero_shot_retrieval.py</code> <pre><code>@store(group=\"eval_task\", provider=\"mmlearn\")\nclass ZeroShotCrossModalRetrieval(EvaluationHooks):\n    \"\"\"Zero-shot cross-modal retrieval evaluation task.\n\n    This task evaluates the retrieval performance of a model on a set of query-target\n    pairs. The model is expected to produce embeddings for both the query and target\n    modalities. The task computes the retrieval recall at `k` for each pair of\n    modalities.\n\n    Parameters\n    ----------\n    task_specs : list[RetrievalTaskSpec]\n        A list of retrieval task specifications. Each specification defines the query\n        and target modalities, as well as the top-k values for which to compute the\n        retrieval recall metrics.\n\n    \"\"\"\n\n    def __init__(self, task_specs: list[RetrievalTaskSpec]) -&gt; None:\n        super().__init__()\n\n        self.task_specs = task_specs\n        self.metrics: dict[tuple[str, str], MetricCollection] = {}\n        self._available_modalities = set()\n\n        for spec in self.task_specs:\n            query_modality = spec.query_modality\n            target_modality = spec.target_modality\n            assert Modalities.has_modality(query_modality)\n            assert Modalities.has_modality(target_modality)\n\n            self.metrics[(query_modality, target_modality)] = MetricCollection(\n                {\n                    f\"{query_modality}_to_{target_modality}_R@{k}\": RetrievalRecallAtK(\n                        top_k=k, aggregation=\"mean\", reduction=\"none\"\n                    )\n                    for k in spec.top_k\n                }\n            )\n            self._available_modalities.add(query_modality)\n            self._available_modalities.add(target_modality)\n\n    def on_evaluation_epoch_start(self, pl_module: pl.LightningModule) -&gt; None:\n        \"\"\"Move the metrics to the device of the Lightning module.\"\"\"\n        for metric in self.metrics.values():\n            metric.to(pl_module.device)\n\n    def evaluation_step(\n        self,\n        pl_module: pl.LightningModule,\n        batch: dict[str, torch.Tensor],\n        batch_idx: int,\n    ) -&gt; None:\n        \"\"\"Run the forward pass and update retrieval recall metrics.\n\n        Parameters\n        ----------\n        pl_module : pl.LightningModule\n            A reference to the Lightning module being evaluated.\n        batch : dict[str, torch.Tensor]\n            A dictionary of batched input tensors.\n        batch_idx : int\n            The index of the batch.\n\n        \"\"\"\n        if pl_module.trainer.sanity_checking:\n            return\n\n        outputs: dict[str, Any] = {}\n        for modality_name in self._available_modalities:\n            if modality_name in batch:\n                outputs[modality_name] = pl_module.encode(\n                    batch, Modalities.get_modality(modality_name), normalize=False\n                )\n        for (query_modality, target_modality), metric in self.metrics.items():\n            if query_modality not in outputs or target_modality not in outputs:\n                continue\n            query_embeddings: torch.Tensor = outputs[query_modality]\n            target_embeddings: torch.Tensor = outputs[target_modality]\n            indexes = torch.arange(query_embeddings.size(0), device=pl_module.device)\n\n            metric.update(query_embeddings, target_embeddings, indexes)\n\n    def on_evaluation_epoch_end(\n        self, pl_module: pl.LightningModule\n    ) -&gt; Optional[dict[str, Any]]:\n        \"\"\"Compute the retrieval recall metrics.\n\n        Parameters\n        ----------\n        pl_module : pl.LightningModule\n            A reference to the Lightning module being evaluated.\n\n        Returns\n        -------\n        Optional[dict[str, Any]]\n            A dictionary of evaluation results or `None` if no results are available.\n        \"\"\"\n        if pl_module.trainer.sanity_checking:\n            return None\n\n        results = {}\n        for metric in self.metrics.values():\n            results.update(metric.compute())\n            metric.reset()\n\n        eval_type = \"val\" if pl_module.trainer.validating else \"test\"\n\n        for key, value in results.items():\n            pl_module.log(f\"{eval_type}/{key}\", value)\n\n        return results\n</code></pre>"},{"location":"api/#mmlearn.tasks.ZeroShotCrossModalRetrieval.on_evaluation_epoch_start","title":"on_evaluation_epoch_start","text":"<pre><code>on_evaluation_epoch_start(pl_module)\n</code></pre> <p>Move the metrics to the device of the Lightning module.</p> Source code in <code>mmlearn/tasks/zero_shot_retrieval.py</code> <pre><code>def on_evaluation_epoch_start(self, pl_module: pl.LightningModule) -&gt; None:\n    \"\"\"Move the metrics to the device of the Lightning module.\"\"\"\n    for metric in self.metrics.values():\n        metric.to(pl_module.device)\n</code></pre>"},{"location":"api/#mmlearn.tasks.ZeroShotCrossModalRetrieval.evaluation_step","title":"evaluation_step","text":"<pre><code>evaluation_step(pl_module, batch, batch_idx)\n</code></pre> <p>Run the forward pass and update retrieval recall metrics.</p> <p>Parameters:</p> Name Type Description Default <code>pl_module</code> <code>LightningModule</code> <p>A reference to the Lightning module being evaluated.</p> required <code>batch</code> <code>dict[str, Tensor]</code> <p>A dictionary of batched input tensors.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required Source code in <code>mmlearn/tasks/zero_shot_retrieval.py</code> <pre><code>def evaluation_step(\n    self,\n    pl_module: pl.LightningModule,\n    batch: dict[str, torch.Tensor],\n    batch_idx: int,\n) -&gt; None:\n    \"\"\"Run the forward pass and update retrieval recall metrics.\n\n    Parameters\n    ----------\n    pl_module : pl.LightningModule\n        A reference to the Lightning module being evaluated.\n    batch : dict[str, torch.Tensor]\n        A dictionary of batched input tensors.\n    batch_idx : int\n        The index of the batch.\n\n    \"\"\"\n    if pl_module.trainer.sanity_checking:\n        return\n\n    outputs: dict[str, Any] = {}\n    for modality_name in self._available_modalities:\n        if modality_name in batch:\n            outputs[modality_name] = pl_module.encode(\n                batch, Modalities.get_modality(modality_name), normalize=False\n            )\n    for (query_modality, target_modality), metric in self.metrics.items():\n        if query_modality not in outputs or target_modality not in outputs:\n            continue\n        query_embeddings: torch.Tensor = outputs[query_modality]\n        target_embeddings: torch.Tensor = outputs[target_modality]\n        indexes = torch.arange(query_embeddings.size(0), device=pl_module.device)\n\n        metric.update(query_embeddings, target_embeddings, indexes)\n</code></pre>"},{"location":"api/#mmlearn.tasks.ZeroShotCrossModalRetrieval.on_evaluation_epoch_end","title":"on_evaluation_epoch_end","text":"<pre><code>on_evaluation_epoch_end(pl_module)\n</code></pre> <p>Compute the retrieval recall metrics.</p> <p>Parameters:</p> Name Type Description Default <code>pl_module</code> <code>LightningModule</code> <p>A reference to the Lightning module being evaluated.</p> required <p>Returns:</p> Type Description <code>Optional[dict[str, Any]]</code> <p>A dictionary of evaluation results or <code>None</code> if no results are available.</p> Source code in <code>mmlearn/tasks/zero_shot_retrieval.py</code> <pre><code>def on_evaluation_epoch_end(\n    self, pl_module: pl.LightningModule\n) -&gt; Optional[dict[str, Any]]:\n    \"\"\"Compute the retrieval recall metrics.\n\n    Parameters\n    ----------\n    pl_module : pl.LightningModule\n        A reference to the Lightning module being evaluated.\n\n    Returns\n    -------\n    Optional[dict[str, Any]]\n        A dictionary of evaluation results or `None` if no results are available.\n    \"\"\"\n    if pl_module.trainer.sanity_checking:\n        return None\n\n    results = {}\n    for metric in self.metrics.values():\n        results.update(metric.compute())\n        metric.reset()\n\n    eval_type = \"val\" if pl_module.trainer.validating else \"test\"\n\n    for key, value in results.items():\n        pl_module.log(f\"{eval_type}/{key}\", value)\n\n    return results\n</code></pre>"},{"location":"api/#mmlearn.tasks.base","title":"base","text":"<p>Base class for all tasks in mmlearn that require training.</p>"},{"location":"api/#mmlearn.tasks.base.TrainingTask","title":"TrainingTask","text":"<p>               Bases: <code>LightningModule</code></p> <p>Base class for all tasks in mmlearn that require training.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>Optional[partial[Optimizer]]</code> <p>The optimizer to use for training. This is expected to be a partial function, created using <code>functools.partial</code>, that takes the model parameters as the only required argument. If not provided, training will continue without an optimizer.</p> <code>None</code> <code>lr_scheduler</code> <code>Optional[Union[dict[str, Union[partial[LRScheduler], Any]], partial[LRScheduler]]]</code> <p>The learning rate scheduler to use for training. This can be a partial function that takes the optimizer as the only required argument or a dictionary with a <code>scheduler</code> key that specifies the scheduler and an optional <code>extras</code> key that specifies additional arguments to pass to the scheduler. If not provided, the learning rate will not be adjusted during training.</p> <code>None</code> <code>loss_fn</code> <code>Optional[Module]</code> <p>Loss function to use for training.</p> <code>None</code> <code>compute_validation_loss</code> <code>bool</code> <p>Whether to compute the validation loss if a validation dataloader is provided. The loss function must be provided to compute the validation loss.</p> <code>True</code> <code>compute_test_loss</code> <code>bool</code> <p>Whether to compute the test loss if a test dataloader is provided. The loss function must be provided to compute the test loss.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the loss function is not provided and either the validation or test loss needs to be computed.</p> Source code in <code>mmlearn/tasks/base.py</code> <pre><code>class TrainingTask(L.LightningModule):\n    \"\"\"Base class for all tasks in mmlearn that require training.\n\n    Parameters\n    ----------\n    optimizer : Optional[partial[torch.optim.Optimizer]], optional, default=None\n        The optimizer to use for training. This is expected to be a partial function,\n        created using `functools.partial`, that takes the model parameters as the\n        only required argument. If not provided, training will continue without an\n        optimizer.\n    lr_scheduler : Optional[Union[dict[str, Union[partial[torch.optim.lr_scheduler.LRScheduler], Any]], partial[torch.optim.lr_scheduler.LRScheduler]]], optional, default=None\n        The learning rate scheduler to use for training. This can be a partial function\n        that takes the optimizer as the only required argument or a dictionary with\n        a `scheduler` key that specifies the scheduler and an optional `extras` key\n        that specifies additional arguments to pass to the scheduler. If not provided,\n        the learning rate will not be adjusted during training.\n    loss_fn : Optional[torch.nn.Module], optional, default=None\n        Loss function to use for training.\n    compute_validation_loss : bool, optional, default=True\n        Whether to compute the validation loss if a validation dataloader is provided.\n        The loss function must be provided to compute the validation loss.\n    compute_test_loss : bool, optional, default=True\n        Whether to compute the test loss if a test dataloader is provided. The loss\n        function must be provided to compute the test loss.\n\n    Raises\n    ------\n    ValueError\n        If the loss function is not provided and either the validation or test loss\n        needs to be computed.\n    \"\"\"  # noqa: W505\n\n    def __init__(\n        self,\n        optimizer: Optional[partial[torch.optim.Optimizer]] = None,\n        lr_scheduler: Optional[\n            Union[\n                dict[str, Union[partial[torch.optim.lr_scheduler.LRScheduler], Any]],\n                partial[torch.optim.lr_scheduler.LRScheduler],\n            ]\n        ] = None,\n        loss_fn: Optional[torch.nn.Module] = None,\n        compute_validation_loss: bool = True,\n        compute_test_loss: bool = True,\n    ):\n        super().__init__()\n        if loss_fn is None and (compute_validation_loss or compute_test_loss):\n            raise ValueError(\n                \"Loss function must be provided to compute validation or test loss.\"\n            )\n\n        self.optimizer = optimizer\n        self.lr_scheduler = lr_scheduler\n        self.loss_fn = loss_fn\n        self.compute_validation_loss = compute_validation_loss\n        self.compute_test_loss = compute_test_loss\n\n    def configure_optimizers(self) -&gt; OptimizerLRScheduler:  # noqa: PLR0912\n        \"\"\"Configure the optimizer and learning rate scheduler.\"\"\"\n        if self.optimizer is None:\n            rank_zero_warn(\n                \"Optimizer not provided. Training will continue without an optimizer. \"\n                \"LR scheduler will not be used.\",\n            )\n            return None\n\n        weight_decay: Optional[float] = self.optimizer.keywords.get(\n            \"weight_decay\", None\n        )\n        if weight_decay is None:  # try getting default value\n            kw_param = inspect.signature(self.optimizer.func).parameters.get(\n                \"weight_decay\"\n            )\n            if kw_param is not None and kw_param.default != inspect.Parameter.empty:\n                weight_decay = kw_param.default\n\n        parameters = [param for param in self.parameters() if param.requires_grad]\n\n        if weight_decay is not None:\n            decay_params = []\n            no_decay_params = []\n\n            for param in self.parameters():\n                if not param.requires_grad:\n                    continue\n\n                if param.ndim &lt; 2:  # includes all bias and normalization parameters\n                    no_decay_params.append(param)\n                else:\n                    decay_params.append(param)\n\n            parameters = [\n                {\n                    \"params\": decay_params,\n                    \"weight_decay\": weight_decay,\n                    \"name\": \"weight_decay_params\",\n                },\n                {\n                    \"params\": no_decay_params,\n                    \"weight_decay\": 0.0,\n                    \"name\": \"no_weight_decay_params\",\n                },\n            ]\n\n        optimizer = self.optimizer(parameters)\n        if not isinstance(optimizer, torch.optim.Optimizer):\n            raise TypeError(\n                \"Expected optimizer to be an instance of `torch.optim.Optimizer`, \"\n                f\"but got {type(optimizer)}.\",\n            )\n\n        if self.lr_scheduler is not None:\n            if isinstance(self.lr_scheduler, dict):\n                if \"scheduler\" not in self.lr_scheduler:\n                    raise ValueError(\n                        \"Expected 'scheduler' key in the learning rate scheduler dictionary.\",\n                    )\n\n                lr_scheduler = self.lr_scheduler[\"scheduler\"](optimizer)\n                if not isinstance(lr_scheduler, torch.optim.lr_scheduler.LRScheduler):\n                    raise TypeError(\n                        \"Expected scheduler to be an instance of `torch.optim.lr_scheduler.LRScheduler`, \"\n                        f\"but got {type(lr_scheduler)}.\",\n                    )\n                lr_scheduler_dict: dict[\n                    str, Union[torch.optim.lr_scheduler.LRScheduler, Any]\n                ] = {\"scheduler\": lr_scheduler}\n\n                if self.lr_scheduler.get(\"extras\"):\n                    lr_scheduler_dict.update(self.lr_scheduler[\"extras\"])\n                return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler_dict}\n\n            lr_scheduler = self.lr_scheduler(optimizer)\n            if not isinstance(lr_scheduler, torch.optim.lr_scheduler.LRScheduler):\n                raise TypeError(\n                    \"Expected scheduler to be an instance of `torch.optim.lr_scheduler.LRScheduler`, \"\n                    f\"but got {type(lr_scheduler)}.\",\n                )\n            return [optimizer], [lr_scheduler]\n\n        return optimizer\n</code></pre>"},{"location":"api/#mmlearn.tasks.base.TrainingTask.configure_optimizers","title":"configure_optimizers","text":"<pre><code>configure_optimizers()\n</code></pre> <p>Configure the optimizer and learning rate scheduler.</p> Source code in <code>mmlearn/tasks/base.py</code> <pre><code>def configure_optimizers(self) -&gt; OptimizerLRScheduler:  # noqa: PLR0912\n    \"\"\"Configure the optimizer and learning rate scheduler.\"\"\"\n    if self.optimizer is None:\n        rank_zero_warn(\n            \"Optimizer not provided. Training will continue without an optimizer. \"\n            \"LR scheduler will not be used.\",\n        )\n        return None\n\n    weight_decay: Optional[float] = self.optimizer.keywords.get(\n        \"weight_decay\", None\n    )\n    if weight_decay is None:  # try getting default value\n        kw_param = inspect.signature(self.optimizer.func).parameters.get(\n            \"weight_decay\"\n        )\n        if kw_param is not None and kw_param.default != inspect.Parameter.empty:\n            weight_decay = kw_param.default\n\n    parameters = [param for param in self.parameters() if param.requires_grad]\n\n    if weight_decay is not None:\n        decay_params = []\n        no_decay_params = []\n\n        for param in self.parameters():\n            if not param.requires_grad:\n                continue\n\n            if param.ndim &lt; 2:  # includes all bias and normalization parameters\n                no_decay_params.append(param)\n            else:\n                decay_params.append(param)\n\n        parameters = [\n            {\n                \"params\": decay_params,\n                \"weight_decay\": weight_decay,\n                \"name\": \"weight_decay_params\",\n            },\n            {\n                \"params\": no_decay_params,\n                \"weight_decay\": 0.0,\n                \"name\": \"no_weight_decay_params\",\n            },\n        ]\n\n    optimizer = self.optimizer(parameters)\n    if not isinstance(optimizer, torch.optim.Optimizer):\n        raise TypeError(\n            \"Expected optimizer to be an instance of `torch.optim.Optimizer`, \"\n            f\"but got {type(optimizer)}.\",\n        )\n\n    if self.lr_scheduler is not None:\n        if isinstance(self.lr_scheduler, dict):\n            if \"scheduler\" not in self.lr_scheduler:\n                raise ValueError(\n                    \"Expected 'scheduler' key in the learning rate scheduler dictionary.\",\n                )\n\n            lr_scheduler = self.lr_scheduler[\"scheduler\"](optimizer)\n            if not isinstance(lr_scheduler, torch.optim.lr_scheduler.LRScheduler):\n                raise TypeError(\n                    \"Expected scheduler to be an instance of `torch.optim.lr_scheduler.LRScheduler`, \"\n                    f\"but got {type(lr_scheduler)}.\",\n                )\n            lr_scheduler_dict: dict[\n                str, Union[torch.optim.lr_scheduler.LRScheduler, Any]\n            ] = {\"scheduler\": lr_scheduler}\n\n            if self.lr_scheduler.get(\"extras\"):\n                lr_scheduler_dict.update(self.lr_scheduler[\"extras\"])\n            return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler_dict}\n\n        lr_scheduler = self.lr_scheduler(optimizer)\n        if not isinstance(lr_scheduler, torch.optim.lr_scheduler.LRScheduler):\n            raise TypeError(\n                \"Expected scheduler to be an instance of `torch.optim.lr_scheduler.LRScheduler`, \"\n                f\"but got {type(lr_scheduler)}.\",\n            )\n        return [optimizer], [lr_scheduler]\n\n    return optimizer\n</code></pre>"},{"location":"api/#mmlearn.tasks.contrastive_pretraining","title":"contrastive_pretraining","text":"<p>Contrastive pretraining task.</p>"},{"location":"api/#mmlearn.tasks.contrastive_pretraining.ModuleKeySpec","title":"ModuleKeySpec  <code>dataclass</code>","text":"<p>Module key specification for mapping modules to modalities.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>@dataclass\nclass ModuleKeySpec:\n    \"\"\"Module key specification for mapping modules to modalities.\"\"\"\n\n    #: The key of the encoder module. If not provided, the modality name is used.\n    encoder_key: Optional[str] = None\n\n    #: The key of the head module. If not provided, the modality name is used.\n    head_key: Optional[str] = None\n\n    #: The key of the postprocessor module. If not provided, the modality name is used.\n    postprocessor_key: Optional[str] = None\n</code></pre>"},{"location":"api/#mmlearn.tasks.contrastive_pretraining.LossPairSpec","title":"LossPairSpec  <code>dataclass</code>","text":"<p>Specification for a pair of modalities to compute the contrastive loss.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>@dataclass\nclass LossPairSpec:\n    \"\"\"Specification for a pair of modalities to compute the contrastive loss.\"\"\"\n\n    #: The pair of modalities to compute the contrastive loss between.\n    modalities: tuple[str, str]\n\n    #: The weight to apply to the contrastive loss for the pair of modalities.\n    weight: float = 1.0\n</code></pre>"},{"location":"api/#mmlearn.tasks.contrastive_pretraining.AuxiliaryTaskSpec","title":"AuxiliaryTaskSpec  <code>dataclass</code>","text":"<p>Specification for an auxiliary task to run alongside the main task.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>@dataclass\nclass AuxiliaryTaskSpec:\n    \"\"\"Specification for an auxiliary task to run alongside the main task.\"\"\"\n\n    #: The modality of the encoder to use for the auxiliary task.\n    modality: str\n\n    #: The auxiliary task module. This is expected to be a partially-initialized\n    #: instance of a :py:class:`~lightning.pytorch.core.LightningModule` created\n    #: using :py:func:`functools.partial`, such that an initialized encoder can be\n    #: passed as the only argument.\n    task: Any  # `functools.partial[L.LightningModule]` expected\n\n    #: The weight to apply to the auxiliary task loss.\n    loss_weight: float = 1.0\n</code></pre>"},{"location":"api/#mmlearn.tasks.contrastive_pretraining.EvaluationSpec","title":"EvaluationSpec  <code>dataclass</code>","text":"<p>Specification for an evaluation task.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>@dataclass\nclass EvaluationSpec:\n    \"\"\"Specification for an evaluation task.\"\"\"\n\n    #: The evaluation task module. This is expected to be an instance of\n    #: :py:class:`~mmlearn.tasks.hooks.EvaluationHooks`.\n    task: Any  # `EvaluationHooks` expected\n\n    #: Whether to run the evaluation task during validation.\n    run_on_validation: bool = True\n\n    #: Whether to run the evaluation task during training.\n    run_on_test: bool = True\n</code></pre>"},{"location":"api/#mmlearn.tasks.contrastive_pretraining.ContrastivePretraining","title":"ContrastivePretraining","text":"<p>               Bases: <code>TrainingTask</code></p> <p>Contrastive pretraining task.</p> <p>This class supports contrastive pretraining with <code>N</code> modalities of data. It allows the sharing of encoders, heads, and postprocessors across modalities. It also supports computing the contrastive loss between specified pairs of modalities, as well as training auxiliary tasks alongside the main contrastive pretraining task.</p> <p>Parameters:</p> Name Type Description Default <code>encoders</code> <code>dict[str, Module]</code> <p>A dictionary of encoders. The keys can be any string, including the names of any supported modalities. If the keys are not supported modalities, the <code>modality_module_mapping</code> parameter must be provided to map the encoders to specific modalities. The encoders are expected to take a dictionary of input values and return a list-like object with the first element being the encoded values. This first element is passed on to the heads or postprocessors and the remaining elements are ignored.</p> required <code>heads</code> <code>Optional[dict[str, Union[Module, dict[str, Module]]]]</code> <p>A dictionary of modules that process the encoder outputs, usually projection heads. If the keys do not correspond to the name of a supported modality, the <code>modality_module_mapping</code> parameter must be provided. If any of the values are dictionaries, they will be wrapped in a class:<code>torch.nn.Sequential</code> module. All head modules are expected to take a single input tensor and return a single output tensor.</p> <code>None</code> <code>postprocessors</code> <code>Optional[dict[str, Union[Module, dict[str, Module]]]]</code> <p>A dictionary of modules that process the head outputs. If the keys do not correspond to the name of a supported modality, the <code>modality_module_mapping</code> parameter must be provided. If any of the values are dictionaries, they will be wrapped in a <code>nn.Sequential</code> module. All postprocessor modules are expected to take a single input tensor and return a single output tensor.</p> <code>None</code> <code>modality_module_mapping</code> <code>Optional[dict[str, ModuleKeySpec]]</code> <p>A dictionary mapping modalities to encoders, heads, and postprocessors. Useful for reusing the same instance of a module across multiple modalities.</p> <code>None</code> <code>optimizer</code> <code>Optional[partial[Optimizer]]</code> <p>The optimizer to use for training. This is expected to be a func:<code>~functools.partial</code> function that takes the model parameters as the only required argument. If not provided, training will continue without an optimizer.</p> <code>None</code> <code>lr_scheduler</code> <code>Optional[Union[dict[str, Union[partial[LRScheduler], Any]], partial[LRScheduler]]]</code> <p>The learning rate scheduler to use for training. This can be a func:<code>~functools.partial</code> function that takes the optimizer as the only required argument or a dictionary with a <code>'scheduler'</code> key that specifies the scheduler and an optional <code>'extras'</code> key that specifies additional arguments to pass to the scheduler. If not provided, the learning rate will not be adjusted during training.</p> <code>None</code> <code>init_logit_scale</code> <code>float</code> <p>The initial value of the logit scale parameter. This is the log of the scale factor applied to the logits before computing the contrastive loss.</p> <code>1 / 0.07</code> <code>max_logit_scale</code> <code>float</code> <p>The maximum value of the logit scale parameter. The logit scale parameter is clamped to the range <code>[0, log(max_logit_scale)]</code>.</p> <code>100</code> <code>learnable_logit_scale</code> <code>bool</code> <p>Whether the logit scale parameter is learnable. If set to False, the logit scale parameter is treated as a constant.</p> <code>True</code> <code>loss</code> <code>Optional[Module]</code> <p>The loss function to use.</p> <code>None</code> <code>modality_loss_pairs</code> <code>Optional[list[LossPairSpec]]</code> <p>A list of pairs of modalities to compute the contrastive loss between and the weight to apply to each pair.</p> <code>None</code> <code>auxiliary_tasks</code> <code>dict[str, AuxiliaryTaskSpec]</code> <p>Auxiliary tasks to run alongside the main contrastive pretraining task.</p> <ul> <li>The auxiliary task module is expected to be a partially-initialized instance   of a class:<code>~lightning.pytorch.core.LightningModule</code> created using   func:<code>functools.partial</code>, such that an initialized encoder can be   passed as the only argument.</li> <li>The <code>modality</code> parameter specifies the modality of the encoder to use   for the auxiliary task. The <code>loss_weight</code> parameter specifies the weight   to apply to the auxiliary task loss.</li> </ul> <code>None</code> <code>log_auxiliary_tasks_loss</code> <code>bool</code> <p>Whether to log the loss of auxiliary tasks to the main logger.</p> <code>False</code> <code>compute_validation_loss</code> <code>bool</code> <p>Whether to compute the validation loss if a validation dataloader is provided. The loss function must be provided to compute the validation loss.</p> <code>True</code> <code>compute_test_loss</code> <code>bool</code> <p>Whether to compute the test loss if a test dataloader is provided. The loss function must be provided to compute the test loss.</p> <code>True</code> <code>evaluation_tasks</code> <code>Optional[dict[str, EvaluationSpec]]</code> <p>Evaluation tasks to run during validation, while training, and during testing.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <ul> <li>If the loss function is not provided and either the validation or test loss   needs to be computed.</li> <li>If the given modality is not supported.</li> <li>If the encoder, head, or postprocessor is not mapped to a modality.</li> <li>If an unsupported modality is found in the loss pair specification.</li> <li>If an unsupported modality is found in the auxiliary tasks.</li> <li>If the auxiliary task is not a partial function.</li> <li>If the evaluation task is not an instance of class:<code>~mmlearn.tasks.hooks.EvaluationHooks</code>.</li> </ul> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>@store(group=\"task\", provider=\"mmlearn\")\nclass ContrastivePretraining(TrainingTask):\n    \"\"\"Contrastive pretraining task.\n\n    This class supports contrastive pretraining with ``N`` modalities of data. It\n    allows the sharing of encoders, heads, and postprocessors across modalities.\n    It also supports computing the contrastive loss between specified pairs of\n    modalities, as well as training auxiliary tasks alongside the main contrastive\n    pretraining task.\n\n    Parameters\n    ----------\n    encoders : dict[str, torch.nn.Module]\n        A dictionary of encoders. The keys can be any string, including the names of\n        any supported modalities. If the keys are not supported modalities, the\n        ``modality_module_mapping`` parameter must be provided to map the encoders to\n        specific modalities. The encoders are expected to take a dictionary of input\n        values and return a list-like object with the first element being the encoded\n        values. This first element is passed on to the heads or postprocessors and\n        the remaining elements are ignored.\n    heads : Optional[dict[str, Union[torch.nn.Module, dict[str, torch.nn.Module]]]], optional, default=None\n        A dictionary of modules that process the encoder outputs, usually projection\n        heads. If the keys do not correspond to the name of a supported modality,\n        the ``modality_module_mapping`` parameter must be provided. If any of the values\n        are dictionaries, they will be wrapped in a :py:class:`torch.nn.Sequential`\n        module. All head modules are expected to take a single input tensor and\n        return a single output tensor.\n    postprocessors : Optional[dict[str, Union[torch.nn.Module, dict[str, torch.nn.Module]]]], optional, default=None\n        A dictionary of modules that process the head outputs. If the keys do not\n        correspond to the name of a supported modality, the `modality_module_mapping`\n        parameter must be provided. If any of the values are dictionaries, they will\n        be wrapped in a `nn.Sequential` module. All postprocessor modules are expected\n        to take a single input tensor and return a single output tensor.\n    modality_module_mapping : Optional[dict[str, ModuleKeySpec]], optional, default=None\n        A dictionary mapping modalities to encoders, heads, and postprocessors.\n        Useful for reusing the same instance of a module across multiple modalities.\n    optimizer : Optional[partial[torch.optim.Optimizer]], optional, default=None\n        The optimizer to use for training. This is expected to be a :py:func:`~functools.partial`\n        function that takes the model parameters as the only required argument.\n        If not provided, training will continue without an optimizer.\n    lr_scheduler : Optional[Union[dict[str, Union[partial[torch.optim.lr_scheduler.LRScheduler], Any]], partial[torch.optim.lr_scheduler.LRScheduler]]], optional, default=None\n        The learning rate scheduler to use for training. This can be a\n        :py:func:`~functools.partial` function that takes the optimizer as the only\n        required argument or a dictionary with a ``'scheduler'`` key that specifies\n        the scheduler and an optional ``'extras'`` key that specifies additional\n        arguments to pass to the scheduler. If not provided, the learning rate will\n        not be adjusted during training.\n    init_logit_scale : float, optional, default=1 / 0.07\n        The initial value of the logit scale parameter. This is the log of the scale\n        factor applied to the logits before computing the contrastive loss.\n    max_logit_scale : float, optional, default=100\n        The maximum value of the logit scale parameter. The logit scale parameter\n        is clamped to the range ``[0, log(max_logit_scale)]``.\n    learnable_logit_scale : bool, optional, default=True\n        Whether the logit scale parameter is learnable. If set to False, the logit\n        scale parameter is treated as a constant.\n    loss : Optional[torch.nn.Module], optional, default=None\n        The loss function to use.\n    modality_loss_pairs : Optional[list[LossPairSpec]], optional, default=None\n        A list of pairs of modalities to compute the contrastive loss between and\n        the weight to apply to each pair.\n    auxiliary_tasks : dict[str, AuxiliaryTaskSpec], optional, default=None\n        Auxiliary tasks to run alongside the main contrastive pretraining task.\n\n        - The auxiliary task module is expected to be a partially-initialized instance\n          of a :py:class:`~lightning.pytorch.core.LightningModule` created using\n          :py:func:`functools.partial`, such that an initialized encoder can be\n          passed as the only argument.\n        - The ``modality`` parameter specifies the modality of the encoder to use\n          for the auxiliary task. The ``loss_weight`` parameter specifies the weight\n          to apply to the auxiliary task loss.\n    log_auxiliary_tasks_loss : bool, optional, default=False\n        Whether to log the loss of auxiliary tasks to the main logger.\n    compute_validation_loss : bool, optional, default=True\n        Whether to compute the validation loss if a validation dataloader is provided.\n        The loss function must be provided to compute the validation loss.\n    compute_test_loss : bool, optional, default=True\n        Whether to compute the test loss if a test dataloader is provided. The loss\n        function must be provided to compute the test loss.\n    evaluation_tasks : Optional[dict[str, EvaluationSpec]], optional, default=None\n        Evaluation tasks to run during validation, while training, and during testing.\n\n    Raises\n    ------\n    ValueError\n\n        - If the loss function is not provided and either the validation or test loss\n          needs to be computed.\n        - If the given modality is not supported.\n        - If the encoder, head, or postprocessor is not mapped to a modality.\n        - If an unsupported modality is found in the loss pair specification.\n        - If an unsupported modality is found in the auxiliary tasks.\n        - If the auxiliary task is not a partial function.\n        - If the evaluation task is not an instance of :py:class:`~mmlearn.tasks.hooks.EvaluationHooks`.\n\n    \"\"\"  # noqa: W505\n\n    def __init__(  # noqa: PLR0912, PLR0915\n        self,\n        encoders: dict[str, nn.Module],\n        heads: Optional[dict[str, Union[nn.Module, dict[str, nn.Module]]]] = None,\n        postprocessors: Optional[\n            dict[str, Union[nn.Module, dict[str, nn.Module]]]\n        ] = None,\n        modality_module_mapping: Optional[dict[str, ModuleKeySpec]] = None,\n        optimizer: Optional[partial[torch.optim.Optimizer]] = None,\n        lr_scheduler: Optional[\n            Union[\n                dict[str, Union[partial[torch.optim.lr_scheduler.LRScheduler], Any]],\n                partial[torch.optim.lr_scheduler.LRScheduler],\n            ]\n        ] = None,\n        init_logit_scale: float = 1 / 0.07,\n        max_logit_scale: float = 100,\n        learnable_logit_scale: bool = True,\n        loss: Optional[nn.Module] = None,\n        modality_loss_pairs: Optional[list[LossPairSpec]] = None,\n        auxiliary_tasks: Optional[dict[str, AuxiliaryTaskSpec]] = None,\n        log_auxiliary_tasks_loss: bool = False,\n        compute_validation_loss: bool = True,\n        compute_test_loss: bool = True,\n        evaluation_tasks: Optional[dict[str, EvaluationSpec]] = None,\n    ) -&gt; None:\n        super().__init__(\n            optimizer=optimizer,\n            lr_scheduler=lr_scheduler,\n            loss_fn=loss,\n            compute_validation_loss=compute_validation_loss,\n            compute_test_loss=compute_test_loss,\n        )\n\n        self.save_hyperparameters(\n            ignore=[\n                \"encoders\",\n                \"heads\",\n                \"postprocessors\",\n                \"modality_module_mapping\",\n                \"loss\",\n                \"auxiliary_tasks\",\n                \"evaluation_tasks\",\n                \"modality_loss_pairs\",\n            ]\n        )\n\n        if modality_module_mapping is None:\n            # assume all the module dictionaries use the same keys corresponding\n            # to modalities\n            modality_module_mapping = {}\n            for key in encoders:\n                modality_module_mapping[key] = ModuleKeySpec(\n                    encoder_key=key,\n                    head_key=key,\n                    postprocessor_key=key,\n                )\n\n        # match modalities to encoders, heads, and postprocessors\n        modality_encoder_mapping: dict[str, Optional[str]] = {}\n        modality_head_mapping: dict[str, Optional[str]] = {}\n        modality_postprocessor_mapping: dict[str, Optional[str]] = {}\n        for modality_key, module_mapping in modality_module_mapping.items():\n            if not Modalities.has_modality(modality_key):\n                raise ValueError(_unsupported_modality_error.format(modality_key))\n            modality_encoder_mapping[modality_key] = module_mapping.encoder_key\n            modality_head_mapping[modality_key] = module_mapping.head_key\n            modality_postprocessor_mapping[modality_key] = (\n                module_mapping.postprocessor_key\n            )\n\n        # ensure all modules are mapped to a modality\n        for key in encoders:\n            if key not in modality_encoder_mapping.values():\n                if not Modalities.has_modality(key):\n                    raise ValueError(_unsupported_modality_error.format(key))\n                modality_encoder_mapping[key] = key\n\n        if heads is not None:\n            for key in heads:\n                if key not in modality_head_mapping.values():\n                    if not Modalities.has_modality(key):\n                        raise ValueError(_unsupported_modality_error.format(key))\n                    modality_head_mapping[key] = key\n\n        if postprocessors is not None:\n            for key in postprocessors:\n                if key not in modality_postprocessor_mapping.values():\n                    if not Modalities.has_modality(key):\n                        raise ValueError(_unsupported_modality_error.format(key))\n                    modality_postprocessor_mapping[key] = key\n\n        self._available_modalities: list[Modality] = [\n            Modalities.get_modality(modality_key)\n            for modality_key in modality_encoder_mapping\n        ]\n        assert len(self._available_modalities) &gt;= 2, (\n            \"Expected at least two modalities to be available. \"\n        )\n\n        #: A :py:class:`~torch.nn.ModuleDict`, where the keys are the names of the\n        #: modalities and the values are the encoder modules.\n        self.encoders = nn.ModuleDict(\n            {\n                Modalities.get_modality(modality_key).name: encoders[encoder_key]\n                for modality_key, encoder_key in modality_encoder_mapping.items()\n                if encoder_key is not None\n            }\n        )\n\n        #: A :py:class:`~torch.nn.ModuleDict`, where the keys are the names of the\n        #: modalities and the values are the projection head modules. This can be\n        #: ``None`` if no heads modules are provided.\n        self.heads = None\n        if heads is not None:\n            self.heads = nn.ModuleDict(\n                {\n                    Modalities.get_modality(modality_key).name: heads[head_key]\n                    if isinstance(heads[head_key], nn.Module)\n                    else nn.Sequential(*heads[head_key].values())\n                    for modality_key, head_key in modality_head_mapping.items()\n                    if head_key is not None and head_key in heads\n                }\n            )\n\n        #: A :py:class:`~torch.nn.ModuleDict`, where the keys are the names of the\n        #: modalities and the values are the postprocessor modules. This can be\n        #: ``None`` if no postprocessor modules are provided.\n        self.postprocessors = None\n        if postprocessors is not None:\n            self.postprocessors = nn.ModuleDict(\n                {\n                    Modalities.get_modality(modality_key).name: postprocessors[\n                        postprocessor_key\n                    ]\n                    if isinstance(postprocessors[postprocessor_key], nn.Module)\n                    else nn.Sequential(*postprocessors[postprocessor_key].values())\n                    for modality_key, postprocessor_key in modality_postprocessor_mapping.items()\n                    if postprocessor_key is not None\n                    and postprocessor_key in postprocessors\n                }\n            )\n\n        # set up logit scaling\n        log_logit_scale = torch.ones([]) * np.log(init_logit_scale)\n        self.max_logit_scale = max_logit_scale\n        self.learnable_logit_scale = learnable_logit_scale\n\n        if self.learnable_logit_scale:\n            self.log_logit_scale = torch.nn.Parameter(\n                log_logit_scale, requires_grad=True\n            )\n        else:\n            self.register_buffer(\"log_logit_scale\", log_logit_scale)\n\n        # set up contrastive loss pairs\n        if modality_loss_pairs is None:\n            modality_loss_pairs = [\n                LossPairSpec(modalities=(m1.name, m2.name))\n                for m1, m2 in itertools.combinations(self._available_modalities, 2)\n            ]\n\n        for modality_pair in modality_loss_pairs:\n            if not all(\n                Modalities.get_modality(modality) in self._available_modalities\n                for modality in modality_pair.modalities\n            ):\n                raise ValueError(\n                    \"Found unspecified modality in the loss pair specification \"\n                    f\"{modality_pair.modalities}. Available modalities are \"\n                    f\"{self._available_modalities}.\"\n                )\n\n        #: A list :py:class:`LossPairSpec` instances specifying the pairs of\n        #: modalities to compute the contrastive loss between and the weight to\n        #: apply to each pair.\n        self.modality_loss_pairs = modality_loss_pairs\n\n        # set up auxiliary tasks\n        self.aux_task_specs = auxiliary_tasks or {}\n        self.auxiliary_tasks: nn.ModuleDict[str, L.LightningModule] = nn.ModuleDict()\n        for task_name, task_spec in self.aux_task_specs.items():\n            if not Modalities.has_modality(task_spec.modality):\n                raise ValueError(\n                    f\"Found unsupported modality `{task_spec.modality}` in the auxiliary tasks. \"\n                    f\"Available modalities are {self._available_modalities}.\"\n                )\n            if not isinstance(task_spec.task, partial):\n                raise TypeError(\n                    f\"Expected auxiliary task to be a partial function, but got {type(task_spec.task)}.\"\n                )\n\n            self.auxiliary_tasks[task_name] = task_spec.task(\n                self.encoders[Modalities.get_modality(task_spec.modality).name]\n            )\n\n        self.log_auxiliary_tasks_loss = log_auxiliary_tasks_loss\n\n        if evaluation_tasks is not None:\n            for eval_task_spec in evaluation_tasks.values():\n                if not isinstance(eval_task_spec.task, EvaluationHooks):\n                    raise TypeError(\n                        f\"Expected {eval_task_spec.task} to be an instance of `EvaluationHooks` \"\n                        f\"but got {type(eval_task_spec.task)}.\"\n                    )\n\n        #: A dictionary of evaluation tasks to run during validation, while training,\n        #: or during testing.\n        self.evaluation_tasks = evaluation_tasks\n\n    def configure_model(self) -&gt; None:\n        \"\"\"Configure the model.\"\"\"\n        if self.auxiliary_tasks:\n            for task_name in self.auxiliary_tasks:\n                self.auxiliary_tasks[task_name].configure_model()\n\n    def encode(\n        self, inputs: dict[str, Any], modality: Modality, normalize: bool = False\n    ) -&gt; torch.Tensor:\n        \"\"\"Encode the input values for the given modality.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            Input values.\n        modality : Modality\n            The modality to encode.\n        normalize : bool, optional, default=False\n            Whether to apply L2 normalization to the output (after the head and\n            postprocessor layers, if present).\n\n        Returns\n        -------\n        torch.Tensor\n            The encoded values for the specified modality.\n        \"\"\"\n        output = self.encoders[modality.name](inputs)[0]\n\n        if self.postprocessors and modality.name in self.postprocessors:\n            output = self.postprocessors[modality.name](output)\n\n        if self.heads and modality.name in self.heads:\n            output = self.heads[modality.name](output)\n\n        if normalize:\n            output = torch.nn.functional.normalize(output, p=2, dim=-1)\n\n        return output\n\n    def forward(self, inputs: dict[str, Any]) -&gt; dict[str, torch.Tensor]:\n        \"\"\"Run the forward pass.\n\n        Parameters\n        ----------\n        inputs : dict[str, Any]\n            The input tensors to encode.\n\n        Returns\n        -------\n        dict[str, torch.Tensor]\n            The encodings for each modality.\n        \"\"\"\n        outputs = {\n            modality.embedding: self.encode(inputs, modality, normalize=True)\n            for modality in self._available_modalities\n            if modality.name in inputs\n        }\n\n        if not all(\n            output.size(-1) == list(outputs.values())[0].size(-1)\n            for output in outputs.values()\n        ):\n            raise ValueError(\"Expected all model outputs to have the same dimension.\")\n\n        return outputs\n\n    def on_train_epoch_start(self) -&gt; None:\n        \"\"\"Prepare for the training epoch.\n\n        This method sets the modules to training mode.\n        \"\"\"\n        self.encoders.train()\n        if self.heads:\n            self.heads.train()\n        if self.postprocessors:\n            self.postprocessors.train()\n\n    def training_step(self, batch: dict[str, Any], batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"Compute the loss for the batch.\n\n        Parameters\n        ----------\n        batch : dict[str, Any]\n            The batch of data to process.\n        batch_idx : int\n            The index of the batch.\n\n        Returns\n        -------\n        torch.Tensor\n            The loss for the batch.\n        \"\"\"\n        outputs = self(batch)\n\n        with torch.no_grad():\n            self.log_logit_scale.clamp_(0, math.log(self.max_logit_scale))\n\n        loss = self._compute_loss(batch, batch_idx, outputs)\n\n        if loss is None:\n            raise ValueError(\"The loss function must be provided for training.\")\n\n        self.log(\"train/loss\", loss, prog_bar=True, sync_dist=True)\n        self.log(\n            \"train/logit_scale\",\n            self.log_logit_scale.exp(),\n            prog_bar=True,\n            on_step=True,\n            on_epoch=False,\n        )\n\n        return loss\n\n    def on_before_zero_grad(self, optimizer: torch.optim.Optimizer) -&gt; None:\n        \"\"\"Zero out the gradients of the model.\"\"\"\n        if self.auxiliary_tasks:\n            for task in self.auxiliary_tasks.values():\n                task.on_before_zero_grad(optimizer)\n\n    def on_validation_epoch_start(self) -&gt; None:\n        \"\"\"Prepare for the validation epoch.\n\n        This method sets the modules to evaluation mode and calls the\n        ``on_evaluation_epoch_start`` method of each evaluation task.\n        \"\"\"\n        self._on_eval_epoch_start(\"val\")\n\n    def validation_step(\n        self, batch: dict[str, torch.Tensor], batch_idx: int\n    ) -&gt; Optional[torch.Tensor]:\n        \"\"\"Run a single validation step.\n\n        Parameters\n        ----------\n        batch : dict[str, torch.Tensor]\n            The batch of data to process.\n        batch_idx : int\n            The index of the batch.\n\n        Returns\n        -------\n        Optional[torch.Tensor]\n            The loss for the batch or ``None`` if the loss function is not provided.\n        \"\"\"\n        return self._shared_eval_step(batch, batch_idx, \"val\")\n\n    def on_validation_epoch_end(self) -&gt; None:\n        \"\"\"Compute and log epoch-level metrics at the end of the validation epoch.\"\"\"\n        self._on_eval_epoch_end(\"val\")\n\n    def on_test_epoch_start(self) -&gt; None:\n        \"\"\"Prepare for the test epoch.\"\"\"\n        self._on_eval_epoch_start(\"test\")\n\n    def test_step(\n        self, batch: dict[str, torch.Tensor], batch_idx: int\n    ) -&gt; Optional[torch.Tensor]:\n        \"\"\"Run a single test step.\n\n        Parameters\n        ----------\n        batch : dict[str, torch.Tensor]\n            The batch of data to process.\n        batch_idx : int\n            The index of the batch.\n\n        Returns\n        -------\n        Optional[torch.Tensor]\n            The loss for the batch or ``None`` if the loss function is not provided.\n        \"\"\"\n        return self._shared_eval_step(batch, batch_idx, \"test\")\n\n    def on_test_epoch_end(self) -&gt; None:\n        \"\"\"Compute and log epoch-level metrics at the end of the test epoch.\"\"\"\n        self._on_eval_epoch_end(\"test\")\n\n    def on_load_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n        \"\"\"Modify the model checkpoint after loading.\n\n        The `on_load_checkpoint` method of auxiliary tasks is called here to allow\n        them to modify the checkpoint after loading.\n\n        Parameters\n        ----------\n        checkpoint : Dict[str, Any]\n            The loaded checkpoint.\n        \"\"\"\n        if self.auxiliary_tasks:\n            for task in self.auxiliary_tasks.values():\n                task.on_load_checkpoint(checkpoint)\n\n    def on_save_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n        \"\"\"Modify the checkpoint before saving.\n\n        The `on_save_checkpoint` method of auxiliary tasks is called here to allow\n        them to modify the checkpoint before saving.\n\n        Parameters\n        ----------\n        checkpoint : Dict[str, Any]\n            The checkpoint to save.\n        \"\"\"\n        if self.auxiliary_tasks:\n            for task in self.auxiliary_tasks.values():\n                task.on_save_checkpoint(checkpoint)\n\n    def _compute_loss(\n        self, batch: dict[str, Any], batch_idx: int, outputs: dict[str, torch.Tensor]\n    ) -&gt; Optional[torch.Tensor]:\n        if self.loss_fn is None:\n            return None\n\n        contrastive_loss = self.loss_fn(\n            outputs,\n            batch[\"example_ids\"],\n            self.log_logit_scale.exp(),\n            self.modality_loss_pairs,\n        )\n\n        auxiliary_losses: list[torch.Tensor] = []\n        if self.auxiliary_tasks:\n            for task_name, task_spec in self.aux_task_specs.items():\n                auxiliary_task_output = self.auxiliary_tasks[task_name].training_step(\n                    batch, batch_idx\n                )\n                if isinstance(auxiliary_task_output, torch.Tensor):\n                    auxiliary_task_loss = auxiliary_task_output\n                elif isinstance(auxiliary_task_output, Mapping):\n                    auxiliary_task_loss = auxiliary_task_output[\"loss\"]\n                else:\n                    raise ValueError(\n                        \"Expected auxiliary task output to be a tensor or a mapping \"\n                        f\"containing a 'loss' key, but got {type(auxiliary_task_output)}.\"\n                    )\n\n                auxiliary_task_loss *= task_spec.loss_weight\n                auxiliary_losses.append(auxiliary_task_loss)\n                if self.log_auxiliary_tasks_loss:\n                    self.log(\n                        f\"train/{task_name}_loss\", auxiliary_task_loss, sync_dist=True\n                    )\n\n        if not auxiliary_losses:\n            return contrastive_loss\n\n        return torch.stack(auxiliary_losses).sum() + contrastive_loss\n\n    def _on_eval_epoch_start(self, eval_type: Literal[\"val\", \"test\"]) -&gt; None:\n        \"\"\"Prepare for the evaluation epoch.\"\"\"\n        self.encoders.eval()\n        if self.heads:\n            self.heads.eval()\n        if self.postprocessors:\n            self.postprocessors.eval()\n        if self.evaluation_tasks:\n            for task_spec in self.evaluation_tasks.values():\n                if (eval_type == \"val\" and task_spec.run_on_validation) or (\n                    eval_type == \"test\" and task_spec.run_on_test\n                ):\n                    task_spec.task.on_evaluation_epoch_start(self)\n\n    def _shared_eval_step(\n        self,\n        batch: dict[str, torch.Tensor],\n        batch_idx: int,\n        eval_type: Literal[\"val\", \"test\"],\n    ) -&gt; Optional[torch.Tensor]:\n        \"\"\"Run a single evaluation step.\n\n        Parameters\n        ----------\n        batch : dict[str, torch.Tensor]\n            The batch of data to process.\n        batch_idx : int\n            The index of the batch.\n\n        Returns\n        -------\n        Optional[torch.Tensor]\n            The loss for the batch or ``None`` if the loss function is not provided.\n        \"\"\"\n        loss: Optional[torch.Tensor] = None\n        if (eval_type == \"val\" and self.compute_validation_loss) or (\n            eval_type == \"test\" and self.compute_test_loss\n        ):\n            outputs = self(batch)\n            loss = self._compute_loss(batch, batch_idx, outputs)\n            if loss is not None and not self.trainer.sanity_checking:\n                self.log(f\"{eval_type}/loss\", loss, prog_bar=True, sync_dist=True)\n\n        if self.evaluation_tasks:\n            for task_spec in self.evaluation_tasks.values():\n                if (eval_type == \"val\" and task_spec.run_on_validation) or (\n                    eval_type == \"test\" and task_spec.run_on_test\n                ):\n                    task_spec.task.evaluation_step(self, batch, batch_idx)\n\n        return loss\n\n    def _on_eval_epoch_end(self, eval_type: Literal[\"val\", \"test\"]) -&gt; None:\n        \"\"\"Compute and log epoch-level metrics at the end of the evaluation epoch.\"\"\"\n        if self.evaluation_tasks:\n            for task_spec in self.evaluation_tasks.values():\n                if (eval_type == \"val\" and task_spec.run_on_validation) or (\n                    eval_type == \"test\" and task_spec.run_on_test\n                ):\n                    task_spec.task.on_evaluation_epoch_end(self)\n</code></pre>"},{"location":"api/#mmlearn.tasks.contrastive_pretraining.ContrastivePretraining.configure_model","title":"configure_model","text":"<pre><code>configure_model()\n</code></pre> <p>Configure the model.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def configure_model(self) -&gt; None:\n    \"\"\"Configure the model.\"\"\"\n    if self.auxiliary_tasks:\n        for task_name in self.auxiliary_tasks:\n            self.auxiliary_tasks[task_name].configure_model()\n</code></pre>"},{"location":"api/#mmlearn.tasks.contrastive_pretraining.ContrastivePretraining.encode","title":"encode","text":"<pre><code>encode(inputs, modality, normalize=False)\n</code></pre> <p>Encode the input values for the given modality.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>Input values.</p> required <code>modality</code> <code>Modality</code> <p>The modality to encode.</p> required <code>normalize</code> <code>bool</code> <p>Whether to apply L2 normalization to the output (after the head and postprocessor layers, if present).</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The encoded values for the specified modality.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def encode(\n    self, inputs: dict[str, Any], modality: Modality, normalize: bool = False\n) -&gt; torch.Tensor:\n    \"\"\"Encode the input values for the given modality.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        Input values.\n    modality : Modality\n        The modality to encode.\n    normalize : bool, optional, default=False\n        Whether to apply L2 normalization to the output (after the head and\n        postprocessor layers, if present).\n\n    Returns\n    -------\n    torch.Tensor\n        The encoded values for the specified modality.\n    \"\"\"\n    output = self.encoders[modality.name](inputs)[0]\n\n    if self.postprocessors and modality.name in self.postprocessors:\n        output = self.postprocessors[modality.name](output)\n\n    if self.heads and modality.name in self.heads:\n        output = self.heads[modality.name](output)\n\n    if normalize:\n        output = torch.nn.functional.normalize(output, p=2, dim=-1)\n\n    return output\n</code></pre>"},{"location":"api/#mmlearn.tasks.contrastive_pretraining.ContrastivePretraining.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> <p>Run the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict[str, Any]</code> <p>The input tensors to encode.</p> required <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>The encodings for each modality.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def forward(self, inputs: dict[str, Any]) -&gt; dict[str, torch.Tensor]:\n    \"\"\"Run the forward pass.\n\n    Parameters\n    ----------\n    inputs : dict[str, Any]\n        The input tensors to encode.\n\n    Returns\n    -------\n    dict[str, torch.Tensor]\n        The encodings for each modality.\n    \"\"\"\n    outputs = {\n        modality.embedding: self.encode(inputs, modality, normalize=True)\n        for modality in self._available_modalities\n        if modality.name in inputs\n    }\n\n    if not all(\n        output.size(-1) == list(outputs.values())[0].size(-1)\n        for output in outputs.values()\n    ):\n        raise ValueError(\"Expected all model outputs to have the same dimension.\")\n\n    return outputs\n</code></pre>"},{"location":"api/#mmlearn.tasks.contrastive_pretraining.ContrastivePretraining.on_train_epoch_start","title":"on_train_epoch_start","text":"<pre><code>on_train_epoch_start()\n</code></pre> <p>Prepare for the training epoch.</p> <p>This method sets the modules to training mode.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def on_train_epoch_start(self) -&gt; None:\n    \"\"\"Prepare for the training epoch.\n\n    This method sets the modules to training mode.\n    \"\"\"\n    self.encoders.train()\n    if self.heads:\n        self.heads.train()\n    if self.postprocessors:\n        self.postprocessors.train()\n</code></pre>"},{"location":"api/#mmlearn.tasks.contrastive_pretraining.ContrastivePretraining.training_step","title":"training_step","text":"<pre><code>training_step(batch, batch_idx)\n</code></pre> <p>Compute the loss for the batch.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict[str, Any]</code> <p>The batch of data to process.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The loss for the batch.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def training_step(self, batch: dict[str, Any], batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"Compute the loss for the batch.\n\n    Parameters\n    ----------\n    batch : dict[str, Any]\n        The batch of data to process.\n    batch_idx : int\n        The index of the batch.\n\n    Returns\n    -------\n    torch.Tensor\n        The loss for the batch.\n    \"\"\"\n    outputs = self(batch)\n\n    with torch.no_grad():\n        self.log_logit_scale.clamp_(0, math.log(self.max_logit_scale))\n\n    loss = self._compute_loss(batch, batch_idx, outputs)\n\n    if loss is None:\n        raise ValueError(\"The loss function must be provided for training.\")\n\n    self.log(\"train/loss\", loss, prog_bar=True, sync_dist=True)\n    self.log(\n        \"train/logit_scale\",\n        self.log_logit_scale.exp(),\n        prog_bar=True,\n        on_step=True,\n        on_epoch=False,\n    )\n\n    return loss\n</code></pre>"},{"location":"api/#mmlearn.tasks.contrastive_pretraining.ContrastivePretraining.on_before_zero_grad","title":"on_before_zero_grad","text":"<pre><code>on_before_zero_grad(optimizer)\n</code></pre> <p>Zero out the gradients of the model.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def on_before_zero_grad(self, optimizer: torch.optim.Optimizer) -&gt; None:\n    \"\"\"Zero out the gradients of the model.\"\"\"\n    if self.auxiliary_tasks:\n        for task in self.auxiliary_tasks.values():\n            task.on_before_zero_grad(optimizer)\n</code></pre>"},{"location":"api/#mmlearn.tasks.contrastive_pretraining.ContrastivePretraining.on_validation_epoch_start","title":"on_validation_epoch_start","text":"<pre><code>on_validation_epoch_start()\n</code></pre> <p>Prepare for the validation epoch.</p> <p>This method sets the modules to evaluation mode and calls the <code>on_evaluation_epoch_start</code> method of each evaluation task.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def on_validation_epoch_start(self) -&gt; None:\n    \"\"\"Prepare for the validation epoch.\n\n    This method sets the modules to evaluation mode and calls the\n    ``on_evaluation_epoch_start`` method of each evaluation task.\n    \"\"\"\n    self._on_eval_epoch_start(\"val\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.contrastive_pretraining.ContrastivePretraining.validation_step","title":"validation_step","text":"<pre><code>validation_step(batch, batch_idx)\n</code></pre> <p>Run a single validation step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict[str, Tensor]</code> <p>The batch of data to process.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required <p>Returns:</p> Type Description <code>Optional[Tensor]</code> <p>The loss for the batch or <code>None</code> if the loss function is not provided.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def validation_step(\n    self, batch: dict[str, torch.Tensor], batch_idx: int\n) -&gt; Optional[torch.Tensor]:\n    \"\"\"Run a single validation step.\n\n    Parameters\n    ----------\n    batch : dict[str, torch.Tensor]\n        The batch of data to process.\n    batch_idx : int\n        The index of the batch.\n\n    Returns\n    -------\n    Optional[torch.Tensor]\n        The loss for the batch or ``None`` if the loss function is not provided.\n    \"\"\"\n    return self._shared_eval_step(batch, batch_idx, \"val\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.contrastive_pretraining.ContrastivePretraining.on_validation_epoch_end","title":"on_validation_epoch_end","text":"<pre><code>on_validation_epoch_end()\n</code></pre> <p>Compute and log epoch-level metrics at the end of the validation epoch.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def on_validation_epoch_end(self) -&gt; None:\n    \"\"\"Compute and log epoch-level metrics at the end of the validation epoch.\"\"\"\n    self._on_eval_epoch_end(\"val\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.contrastive_pretraining.ContrastivePretraining.on_test_epoch_start","title":"on_test_epoch_start","text":"<pre><code>on_test_epoch_start()\n</code></pre> <p>Prepare for the test epoch.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def on_test_epoch_start(self) -&gt; None:\n    \"\"\"Prepare for the test epoch.\"\"\"\n    self._on_eval_epoch_start(\"test\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.contrastive_pretraining.ContrastivePretraining.test_step","title":"test_step","text":"<pre><code>test_step(batch, batch_idx)\n</code></pre> <p>Run a single test step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict[str, Tensor]</code> <p>The batch of data to process.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required <p>Returns:</p> Type Description <code>Optional[Tensor]</code> <p>The loss for the batch or <code>None</code> if the loss function is not provided.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def test_step(\n    self, batch: dict[str, torch.Tensor], batch_idx: int\n) -&gt; Optional[torch.Tensor]:\n    \"\"\"Run a single test step.\n\n    Parameters\n    ----------\n    batch : dict[str, torch.Tensor]\n        The batch of data to process.\n    batch_idx : int\n        The index of the batch.\n\n    Returns\n    -------\n    Optional[torch.Tensor]\n        The loss for the batch or ``None`` if the loss function is not provided.\n    \"\"\"\n    return self._shared_eval_step(batch, batch_idx, \"test\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.contrastive_pretraining.ContrastivePretraining.on_test_epoch_end","title":"on_test_epoch_end","text":"<pre><code>on_test_epoch_end()\n</code></pre> <p>Compute and log epoch-level metrics at the end of the test epoch.</p> Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def on_test_epoch_end(self) -&gt; None:\n    \"\"\"Compute and log epoch-level metrics at the end of the test epoch.\"\"\"\n    self._on_eval_epoch_end(\"test\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.contrastive_pretraining.ContrastivePretraining.on_load_checkpoint","title":"on_load_checkpoint","text":"<pre><code>on_load_checkpoint(checkpoint)\n</code></pre> <p>Modify the model checkpoint after loading.</p> <p>The <code>on_load_checkpoint</code> method of auxiliary tasks is called here to allow them to modify the checkpoint after loading.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <code>Dict[str, Any]</code> <p>The loaded checkpoint.</p> required Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def on_load_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n    \"\"\"Modify the model checkpoint after loading.\n\n    The `on_load_checkpoint` method of auxiliary tasks is called here to allow\n    them to modify the checkpoint after loading.\n\n    Parameters\n    ----------\n    checkpoint : Dict[str, Any]\n        The loaded checkpoint.\n    \"\"\"\n    if self.auxiliary_tasks:\n        for task in self.auxiliary_tasks.values():\n            task.on_load_checkpoint(checkpoint)\n</code></pre>"},{"location":"api/#mmlearn.tasks.contrastive_pretraining.ContrastivePretraining.on_save_checkpoint","title":"on_save_checkpoint","text":"<pre><code>on_save_checkpoint(checkpoint)\n</code></pre> <p>Modify the checkpoint before saving.</p> <p>The <code>on_save_checkpoint</code> method of auxiliary tasks is called here to allow them to modify the checkpoint before saving.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <code>Dict[str, Any]</code> <p>The checkpoint to save.</p> required Source code in <code>mmlearn/tasks/contrastive_pretraining.py</code> <pre><code>def on_save_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n    \"\"\"Modify the checkpoint before saving.\n\n    The `on_save_checkpoint` method of auxiliary tasks is called here to allow\n    them to modify the checkpoint before saving.\n\n    Parameters\n    ----------\n    checkpoint : Dict[str, Any]\n        The checkpoint to save.\n    \"\"\"\n    if self.auxiliary_tasks:\n        for task in self.auxiliary_tasks.values():\n            task.on_save_checkpoint(checkpoint)\n</code></pre>"},{"location":"api/#mmlearn.tasks.hooks","title":"hooks","text":"<p>Task-related hooks for Lightning modules.</p>"},{"location":"api/#mmlearn.tasks.hooks.EvaluationHooks","title":"EvaluationHooks","text":"<p>Hooks for evaluation.</p> Source code in <code>mmlearn/tasks/hooks.py</code> <pre><code>class EvaluationHooks:\n    \"\"\"Hooks for evaluation.\"\"\"\n\n    def on_evaluation_epoch_start(self, pl_module: pl.LightningModule) -&gt; None:\n        \"\"\"Prepare the evaluation loop.\n\n        Parameters\n        ----------\n        pl_module : pl.LightningModule\n            A reference to the Lightning module being evaluated.\n        \"\"\"\n\n    def evaluation_step(\n        self, pl_module: pl.LightningModule, batch: Any, batch_idx: int\n    ) -&gt; Optional[Mapping[str, Any]]:\n        \"\"\"Run a single evaluation step.\n\n        Parameters\n        ----------\n        pl_module : pl.LightningModule\n            A reference to the Lightning module being evaluated.\n        batch : Any\n            A batch of data.\n        batch_idx : int\n            The index of the batch.\n\n        Returns\n        -------\n        Optional[Mapping[str, Any]]\n            A dictionary of evaluation results for the batch or ``None`` if no\n            batch results are available.\n\n        \"\"\"\n        rank_zero_warn(\n            f\"`evaluation_step` must be implemented to use {self.__class__.__name__} for evaluation.\"\n        )\n        return None\n\n    def on_evaluation_epoch_end(\n        self, pl_module: pl.LightningModule\n    ) -&gt; Optional[Union[Mapping[str, Any]]]:\n        \"\"\"Run after the evaluation epoch.\n\n        Parameters\n        ----------\n        pl_module : pl.LightningModule\n            A reference to the Lightning module being evaluated.\n\n        Returns\n        -------\n        Optional[Union[Mapping[str, Any]]]\n            A dictionary of evaluation results or ``None`` if no results are available.\n        \"\"\"\n</code></pre>"},{"location":"api/#mmlearn.tasks.hooks.EvaluationHooks.on_evaluation_epoch_start","title":"on_evaluation_epoch_start","text":"<pre><code>on_evaluation_epoch_start(pl_module)\n</code></pre> <p>Prepare the evaluation loop.</p> <p>Parameters:</p> Name Type Description Default <code>pl_module</code> <code>LightningModule</code> <p>A reference to the Lightning module being evaluated.</p> required Source code in <code>mmlearn/tasks/hooks.py</code> <pre><code>def on_evaluation_epoch_start(self, pl_module: pl.LightningModule) -&gt; None:\n    \"\"\"Prepare the evaluation loop.\n\n    Parameters\n    ----------\n    pl_module : pl.LightningModule\n        A reference to the Lightning module being evaluated.\n    \"\"\"\n</code></pre>"},{"location":"api/#mmlearn.tasks.hooks.EvaluationHooks.evaluation_step","title":"evaluation_step","text":"<pre><code>evaluation_step(pl_module, batch, batch_idx)\n</code></pre> <p>Run a single evaluation step.</p> <p>Parameters:</p> Name Type Description Default <code>pl_module</code> <code>LightningModule</code> <p>A reference to the Lightning module being evaluated.</p> required <code>batch</code> <code>Any</code> <p>A batch of data.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required <p>Returns:</p> Type Description <code>Optional[Mapping[str, Any]]</code> <p>A dictionary of evaluation results for the batch or <code>None</code> if no batch results are available.</p> Source code in <code>mmlearn/tasks/hooks.py</code> <pre><code>def evaluation_step(\n    self, pl_module: pl.LightningModule, batch: Any, batch_idx: int\n) -&gt; Optional[Mapping[str, Any]]:\n    \"\"\"Run a single evaluation step.\n\n    Parameters\n    ----------\n    pl_module : pl.LightningModule\n        A reference to the Lightning module being evaluated.\n    batch : Any\n        A batch of data.\n    batch_idx : int\n        The index of the batch.\n\n    Returns\n    -------\n    Optional[Mapping[str, Any]]\n        A dictionary of evaluation results for the batch or ``None`` if no\n        batch results are available.\n\n    \"\"\"\n    rank_zero_warn(\n        f\"`evaluation_step` must be implemented to use {self.__class__.__name__} for evaluation.\"\n    )\n    return None\n</code></pre>"},{"location":"api/#mmlearn.tasks.hooks.EvaluationHooks.on_evaluation_epoch_end","title":"on_evaluation_epoch_end","text":"<pre><code>on_evaluation_epoch_end(pl_module)\n</code></pre> <p>Run after the evaluation epoch.</p> <p>Parameters:</p> Name Type Description Default <code>pl_module</code> <code>LightningModule</code> <p>A reference to the Lightning module being evaluated.</p> required <p>Returns:</p> Type Description <code>Optional[Union[Mapping[str, Any]]]</code> <p>A dictionary of evaluation results or <code>None</code> if no results are available.</p> Source code in <code>mmlearn/tasks/hooks.py</code> <pre><code>def on_evaluation_epoch_end(\n    self, pl_module: pl.LightningModule\n) -&gt; Optional[Union[Mapping[str, Any]]]:\n    \"\"\"Run after the evaluation epoch.\n\n    Parameters\n    ----------\n    pl_module : pl.LightningModule\n        A reference to the Lightning module being evaluated.\n\n    Returns\n    -------\n    Optional[Union[Mapping[str, Any]]]\n        A dictionary of evaluation results or ``None`` if no results are available.\n    \"\"\"\n</code></pre>"},{"location":"api/#mmlearn.tasks.ijepa","title":"ijepa","text":"<p>IJEPA (Image Joint-Embedding Predictive Architecture) pretraining task.</p>"},{"location":"api/#mmlearn.tasks.ijepa.IJEPA","title":"IJEPA","text":"<p>               Bases: <code>TrainingTask</code></p> <p>Pretraining module for IJEPA.</p> <p>This class implements the IJEPA (Image Joint-Embedding Predictive Architecture) pretraining task using PyTorch Lightning. It trains an encoder and a predictor to reconstruct masked regions of an image based on its unmasked context.</p> <p>Parameters:</p> Name Type Description Default <code>encoder</code> <code>VisionTransformer</code> <p>Vision transformer encoder.</p> required <code>predictor</code> <code>VisionTransformerPredictor</code> <p>Vision transformer predictor.</p> required <code>optimizer</code> <code>Optional[partial[Optimizer]]</code> <p>The optimizer to use for training. This is expected to be a func:<code>~functools.partial</code> function that takes the model parameters as the only required argument. If not provided, training will continue without an optimizer.</p> <code>None</code> <code>lr_scheduler</code> <code>Optional[Union[dict[str, Union[partial[LRScheduler], Any]], partial[LRScheduler]]]</code> <p>The learning rate scheduler to use for training. This can be a func:<code>~functools.partial</code> function that takes the optimizer as the only required argument or a dictionary with a <code>'scheduler'</code> key that specifies the scheduler and an optional <code>'extras'</code> key that specifies additional arguments to pass to the scheduler. If not provided, the learning rate will not be adjusted during training.</p> <code>None</code> <code>ema_decay</code> <code>float</code> <p>Initial momentum for EMA of target encoder.</p> <code>0.996</code> <code>ema_decay_end</code> <code>float</code> <p>Final momentum for EMA of target encoder.</p> <code>1.0</code> <code>ema_anneal_end_step</code> <code>int</code> <p>Number of steps to anneal EMA momentum to <code>ema_decay_end</code>.</p> <code>1000</code> <code>loss_fn</code> <code>Optional[Callable[[Tensor, Tensor], Tensor]]</code> <p>Loss function to use. If not provided, defaults to func:<code>~torch.nn.functional.smooth_l1_loss</code>.</p> <code>None</code> <code>compute_validation_loss</code> <code>bool</code> <p>Whether to compute validation loss.</p> <code>True</code> <code>compute_test_loss</code> <code>bool</code> <p>Whether to compute test loss.</p> <code>True</code> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>@store(group=\"task\", provider=\"mmlearn\", zen_partial=False)\nclass IJEPA(TrainingTask):\n    \"\"\"Pretraining module for IJEPA.\n\n    This class implements the IJEPA (Image Joint-Embedding Predictive Architecture)\n    pretraining task using PyTorch Lightning. It trains an encoder and a predictor to\n    reconstruct masked regions of an image based on its unmasked context.\n\n    Parameters\n    ----------\n    encoder : VisionTransformer\n        Vision transformer encoder.\n    predictor : VisionTransformerPredictor\n        Vision transformer predictor.\n    optimizer : Optional[partial[torch.optim.Optimizer]], optional, default=None\n        The optimizer to use for training. This is expected to be a :py:func:`~functools.partial`\n        function that takes the model parameters as the only required argument.\n        If not provided, training will continue without an optimizer.\n    lr_scheduler : Optional[Union[dict[str, Union[partial[torch.optim.lr_scheduler.LRScheduler], Any]], partial[torch.optim.lr_scheduler.LRScheduler]]], optional, default=None\n        The learning rate scheduler to use for training. This can be a\n        :py:func:`~functools.partial` function that takes the optimizer as the only\n        required argument or a dictionary with a ``'scheduler'`` key that specifies\n        the scheduler and an optional ``'extras'`` key that specifies additional\n        arguments to pass to the scheduler. If not provided, the learning rate will\n        not be adjusted during training.\n    ema_decay : float, optional, default=0.996\n        Initial momentum for EMA of target encoder.\n    ema_decay_end : float, optional, default=1.0\n        Final momentum for EMA of target encoder.\n    ema_anneal_end_step : int, optional, default=1000\n        Number of steps to anneal EMA momentum to ``ema_decay_end``.\n    loss_fn : Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]], optional\n        Loss function to use. If not provided, defaults to\n        :py:func:`~torch.nn.functional.smooth_l1_loss`.\n    compute_validation_loss : bool, optional, default=True\n        Whether to compute validation loss.\n    compute_test_loss : bool, optional, default=True\n        Whether to compute test loss.\n    \"\"\"  # noqa: W505\n\n    def __init__(\n        self,\n        encoder: VisionTransformer,\n        predictor: VisionTransformerPredictor,\n        modality: str = \"RGB\",\n        optimizer: Optional[partial[torch.optim.Optimizer]] = None,\n        lr_scheduler: Optional[\n            Union[\n                dict[str, Union[partial[torch.optim.lr_scheduler.LRScheduler], Any]],\n                partial[torch.optim.lr_scheduler.LRScheduler],\n            ]\n        ] = None,\n        ema_decay: float = 0.996,\n        ema_decay_end: float = 1.0,\n        ema_anneal_end_step: int = 1000,\n        loss_fn: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None,\n        compute_validation_loss: bool = True,\n        compute_test_loss: bool = True,\n    ):\n        super().__init__(\n            optimizer=optimizer,\n            lr_scheduler=lr_scheduler,\n            loss_fn=loss_fn if loss_fn is not None else F.smooth_l1_loss,\n            compute_validation_loss=compute_validation_loss,\n            compute_test_loss=compute_test_loss,\n        )\n        self.modality = Modalities.get_modality(modality)\n        self.mask_generator = IJEPAMaskGenerator()\n\n        self.encoder = encoder\n        self.predictor = predictor\n\n        self.predictor.num_patches = encoder.patch_embed.num_patches\n        self.predictor.embed_dim = encoder.embed_dim\n        self.predictor.num_heads = encoder.num_heads\n\n        self.target_encoder = ExponentialMovingAverage(\n            self.encoder, ema_decay, ema_decay_end, ema_anneal_end_step\n        )\n\n    def configure_model(self) -&gt; None:\n        \"\"\"Configure the model.\"\"\"\n        self.target_encoder.configure_model(self.device)\n\n    def on_before_zero_grad(self, optimizer: torch.optim.Optimizer) -&gt; None:\n        \"\"\"Perform exponential moving average update of target encoder.\n\n        This is done right after the ``optimizer.step()`, which comes just before\n        ``optimizer.zero_grad()`` to account for gradient accumulation.\n        \"\"\"\n        if self.target_encoder is not None:\n            self.target_encoder.step(self.encoder)\n\n    def training_step(self, batch: dict[str, Any], batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"Perform a single training step.\n\n        Parameters\n        ----------\n        batch : dict[str, Any]\n            A batch of data.\n        batch_idx : int\n            Index of the batch.\n\n        Returns\n        -------\n        torch.Tensor\n            Loss value.\n        \"\"\"\n        return self._shared_step(batch, batch_idx, step_type=\"train\")\n\n    def validation_step(\n        self, batch: dict[str, Any], batch_idx: int\n    ) -&gt; Optional[torch.Tensor]:\n        \"\"\"Run a single validation step.\n\n        Parameters\n        ----------\n        batch : dict[str, Any]\n            A batch of data.\n        batch_idx : int\n            Index of the batch.\n\n        Returns\n        -------\n        Optional[torch.Tensor]\n            Loss value or ``None`` if no loss is computed.\n        \"\"\"\n        return self._shared_step(batch, batch_idx, step_type=\"val\")\n\n    def test_step(\n        self, batch: dict[str, Any], batch_idx: int\n    ) -&gt; Optional[torch.Tensor]:\n        \"\"\"Run a single test step.\n\n        Parameters\n        ----------\n        batch : dict[str, Any]\n            A batch of data.\n        batch_idx : int\n            Index of the batch.\n\n        Returns\n        -------\n        Optional[torch.Tensor]\n            Loss value or ``None`` if no loss is computed\n        \"\"\"\n        return self._shared_step(batch, batch_idx, step_type=\"test\")\n\n    def on_validation_epoch_start(self) -&gt; None:\n        \"\"\"Prepare for the validation epoch.\"\"\"\n        self._on_eval_epoch_start(\"val\")\n\n    def on_validation_epoch_end(self) -&gt; None:\n        \"\"\"Actions at the end of the validation epoch.\"\"\"\n        self._on_eval_epoch_end(\"val\")\n\n    def on_test_epoch_start(self) -&gt; None:\n        \"\"\"Prepare for the test epoch.\"\"\"\n        self._on_eval_epoch_start(\"test\")\n\n    def on_test_epoch_end(self) -&gt; None:\n        \"\"\"Actions at the end of the test epoch.\"\"\"\n        self._on_eval_epoch_end(\"test\")\n\n    def on_save_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n        \"\"\"Add relevant EMA state to the checkpoint.\n\n        Parameters\n        ----------\n        checkpoint : dict[str, Any]\n            The state dictionary to save the EMA state to.\n        \"\"\"\n        if self.target_encoder is not None:\n            checkpoint[\"ema_params\"] = {\n                \"decay\": self.target_encoder.decay,\n                \"num_updates\": self.target_encoder.num_updates,\n            }\n\n    def on_load_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n        \"\"\"Restore EMA state from the checkpoint.\n\n        Parameters\n        ----------\n        checkpoint : dict[str, Any]\n            The state dictionary to restore the EMA state from.\n        \"\"\"\n        if \"ema_params\" in checkpoint and self.target_encoder is not None:\n            ema_params = checkpoint.pop(\"ema_params\")\n            self.target_encoder.decay = ema_params[\"decay\"]\n            self.target_encoder.num_updates = ema_params[\"num_updates\"]\n\n            self.target_encoder.restore(self.encoder)\n\n    def _shared_step(\n        self, batch: dict[str, Any], batch_idx: int, step_type: str\n    ) -&gt; Optional[torch.Tensor]:\n        images = batch[self.modality.name]\n\n        # Generate masks\n        batch_size = images.size(0)\n        mask_info = self.mask_generator(batch_size=batch_size)\n\n        # Extract masks and move to device\n        device = images.device\n        encoder_masks = [mask.to(device) for mask in mask_info[\"encoder_masks\"]]\n        predictor_masks = [mask.to(device) for mask in mask_info[\"predictor_masks\"]]\n\n        # Forward pass through target encoder to get h\n        with torch.no_grad():\n            h = self.target_encoder.model(batch)[0]\n            h = F.layer_norm(h, h.size()[-1:])\n            h_masked = apply_masks(h, predictor_masks)\n            h_masked = repeat_interleave_batch(\n                h_masked, images.size(0), repeat=len(encoder_masks)\n            )\n\n        # Forward pass through encoder with encoder_masks\n        batch[self.modality.mask] = encoder_masks\n        z = self.encoder(batch)[0]\n\n        # Pass z through predictor with encoder_masks and predictor_masks\n        z_pred = self.predictor(z, encoder_masks, predictor_masks)\n\n        if step_type == \"train\":\n            self.log(\"train/ema_decay\", self.target_encoder.decay, prog_bar=True)\n\n        if self.loss_fn is not None and (\n            step_type == \"train\"\n            or (step_type == \"val\" and self.compute_validation_loss)\n            or (step_type == \"test\" and self.compute_test_loss)\n        ):\n            # Compute loss between z_pred and h_masked\n            loss = self.loss_fn(z_pred, h_masked)\n\n            # Log loss\n            self.log(f\"{step_type}/loss\", loss, prog_bar=True, sync_dist=True)\n\n            return loss\n\n        return None\n\n    def _on_eval_epoch_start(self, step_type: str) -&gt; None:\n        \"\"\"Initialize states or configurations at the start of an evaluation epoch.\n\n        Parameters\n        ----------\n        step_type : str\n            Type of the evaluation phase (\"val\" or \"test\").\n        \"\"\"\n        if (\n            step_type == \"val\"\n            and self.compute_validation_loss\n            or step_type == \"test\"\n            and self.compute_test_loss\n        ):\n            self.log(f\"{step_type}/start\", 1, prog_bar=True, sync_dist=True)\n\n    def _on_eval_epoch_end(self, step_type: str) -&gt; None:\n        \"\"\"Finalize states or logging at the end of an evaluation epoch.\n\n        Parameters\n        ----------\n        step_type : str\n            Type of the evaluation phase (\"val\" or \"test\").\n        \"\"\"\n        if (\n            step_type == \"val\"\n            and self.compute_validation_loss\n            or step_type == \"test\"\n            and self.compute_test_loss\n        ):\n            self.log(f\"{step_type}/end\", 1, prog_bar=True, sync_dist=True)\n</code></pre>"},{"location":"api/#mmlearn.tasks.ijepa.IJEPA.configure_model","title":"configure_model","text":"<pre><code>configure_model()\n</code></pre> <p>Configure the model.</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def configure_model(self) -&gt; None:\n    \"\"\"Configure the model.\"\"\"\n    self.target_encoder.configure_model(self.device)\n</code></pre>"},{"location":"api/#mmlearn.tasks.ijepa.IJEPA.on_before_zero_grad","title":"on_before_zero_grad","text":"<pre><code>on_before_zero_grad(optimizer)\n</code></pre> <p>Perform exponential moving average update of target encoder.</p> <p>This is done right after the <code>optimizer.step()`, which comes just before</code>optimizer.zero_grad()`` to account for gradient accumulation.</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def on_before_zero_grad(self, optimizer: torch.optim.Optimizer) -&gt; None:\n    \"\"\"Perform exponential moving average update of target encoder.\n\n    This is done right after the ``optimizer.step()`, which comes just before\n    ``optimizer.zero_grad()`` to account for gradient accumulation.\n    \"\"\"\n    if self.target_encoder is not None:\n        self.target_encoder.step(self.encoder)\n</code></pre>"},{"location":"api/#mmlearn.tasks.ijepa.IJEPA.training_step","title":"training_step","text":"<pre><code>training_step(batch, batch_idx)\n</code></pre> <p>Perform a single training step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict[str, Any]</code> <p>A batch of data.</p> required <code>batch_idx</code> <code>int</code> <p>Index of the batch.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Loss value.</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def training_step(self, batch: dict[str, Any], batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"Perform a single training step.\n\n    Parameters\n    ----------\n    batch : dict[str, Any]\n        A batch of data.\n    batch_idx : int\n        Index of the batch.\n\n    Returns\n    -------\n    torch.Tensor\n        Loss value.\n    \"\"\"\n    return self._shared_step(batch, batch_idx, step_type=\"train\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.ijepa.IJEPA.validation_step","title":"validation_step","text":"<pre><code>validation_step(batch, batch_idx)\n</code></pre> <p>Run a single validation step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict[str, Any]</code> <p>A batch of data.</p> required <code>batch_idx</code> <code>int</code> <p>Index of the batch.</p> required <p>Returns:</p> Type Description <code>Optional[Tensor]</code> <p>Loss value or <code>None</code> if no loss is computed.</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def validation_step(\n    self, batch: dict[str, Any], batch_idx: int\n) -&gt; Optional[torch.Tensor]:\n    \"\"\"Run a single validation step.\n\n    Parameters\n    ----------\n    batch : dict[str, Any]\n        A batch of data.\n    batch_idx : int\n        Index of the batch.\n\n    Returns\n    -------\n    Optional[torch.Tensor]\n        Loss value or ``None`` if no loss is computed.\n    \"\"\"\n    return self._shared_step(batch, batch_idx, step_type=\"val\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.ijepa.IJEPA.test_step","title":"test_step","text":"<pre><code>test_step(batch, batch_idx)\n</code></pre> <p>Run a single test step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict[str, Any]</code> <p>A batch of data.</p> required <code>batch_idx</code> <code>int</code> <p>Index of the batch.</p> required <p>Returns:</p> Type Description <code>Optional[Tensor]</code> <p>Loss value or <code>None</code> if no loss is computed</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def test_step(\n    self, batch: dict[str, Any], batch_idx: int\n) -&gt; Optional[torch.Tensor]:\n    \"\"\"Run a single test step.\n\n    Parameters\n    ----------\n    batch : dict[str, Any]\n        A batch of data.\n    batch_idx : int\n        Index of the batch.\n\n    Returns\n    -------\n    Optional[torch.Tensor]\n        Loss value or ``None`` if no loss is computed\n    \"\"\"\n    return self._shared_step(batch, batch_idx, step_type=\"test\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.ijepa.IJEPA.on_validation_epoch_start","title":"on_validation_epoch_start","text":"<pre><code>on_validation_epoch_start()\n</code></pre> <p>Prepare for the validation epoch.</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def on_validation_epoch_start(self) -&gt; None:\n    \"\"\"Prepare for the validation epoch.\"\"\"\n    self._on_eval_epoch_start(\"val\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.ijepa.IJEPA.on_validation_epoch_end","title":"on_validation_epoch_end","text":"<pre><code>on_validation_epoch_end()\n</code></pre> <p>Actions at the end of the validation epoch.</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def on_validation_epoch_end(self) -&gt; None:\n    \"\"\"Actions at the end of the validation epoch.\"\"\"\n    self._on_eval_epoch_end(\"val\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.ijepa.IJEPA.on_test_epoch_start","title":"on_test_epoch_start","text":"<pre><code>on_test_epoch_start()\n</code></pre> <p>Prepare for the test epoch.</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def on_test_epoch_start(self) -&gt; None:\n    \"\"\"Prepare for the test epoch.\"\"\"\n    self._on_eval_epoch_start(\"test\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.ijepa.IJEPA.on_test_epoch_end","title":"on_test_epoch_end","text":"<pre><code>on_test_epoch_end()\n</code></pre> <p>Actions at the end of the test epoch.</p> Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def on_test_epoch_end(self) -&gt; None:\n    \"\"\"Actions at the end of the test epoch.\"\"\"\n    self._on_eval_epoch_end(\"test\")\n</code></pre>"},{"location":"api/#mmlearn.tasks.ijepa.IJEPA.on_save_checkpoint","title":"on_save_checkpoint","text":"<pre><code>on_save_checkpoint(checkpoint)\n</code></pre> <p>Add relevant EMA state to the checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <code>dict[str, Any]</code> <p>The state dictionary to save the EMA state to.</p> required Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def on_save_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n    \"\"\"Add relevant EMA state to the checkpoint.\n\n    Parameters\n    ----------\n    checkpoint : dict[str, Any]\n        The state dictionary to save the EMA state to.\n    \"\"\"\n    if self.target_encoder is not None:\n        checkpoint[\"ema_params\"] = {\n            \"decay\": self.target_encoder.decay,\n            \"num_updates\": self.target_encoder.num_updates,\n        }\n</code></pre>"},{"location":"api/#mmlearn.tasks.ijepa.IJEPA.on_load_checkpoint","title":"on_load_checkpoint","text":"<pre><code>on_load_checkpoint(checkpoint)\n</code></pre> <p>Restore EMA state from the checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <code>dict[str, Any]</code> <p>The state dictionary to restore the EMA state from.</p> required Source code in <code>mmlearn/tasks/ijepa.py</code> <pre><code>def on_load_checkpoint(self, checkpoint: dict[str, Any]) -&gt; None:\n    \"\"\"Restore EMA state from the checkpoint.\n\n    Parameters\n    ----------\n    checkpoint : dict[str, Any]\n        The state dictionary to restore the EMA state from.\n    \"\"\"\n    if \"ema_params\" in checkpoint and self.target_encoder is not None:\n        ema_params = checkpoint.pop(\"ema_params\")\n        self.target_encoder.decay = ema_params[\"decay\"]\n        self.target_encoder.num_updates = ema_params[\"num_updates\"]\n\n        self.target_encoder.restore(self.encoder)\n</code></pre>"},{"location":"api/#mmlearn.tasks.zero_shot_classification","title":"zero_shot_classification","text":"<p>Zero-shot classification evaluation task.</p>"},{"location":"api/#mmlearn.tasks.zero_shot_classification.ClassificationTaskSpec","title":"ClassificationTaskSpec  <code>dataclass</code>","text":"<p>Specification for a classification task.</p> Source code in <code>mmlearn/tasks/zero_shot_classification.py</code> <pre><code>@dataclass\nclass ClassificationTaskSpec:\n    \"\"\"Specification for a classification task.\"\"\"\n\n    #: The modality of the query input.\n    query_modality: str\n\n    #: The top-k values for which to compute the classification metrics like accuracy.\n    top_k: list[int]\n</code></pre>"},{"location":"api/#mmlearn.tasks.zero_shot_classification.ZeroShotClassification","title":"ZeroShotClassification","text":"<p>               Bases: <code>EvaluationHooks</code></p> <p>Zero-shot classification evaluation task.</p> <p>This task evaluates the zero-shot classification performance.</p> <p>Parameters:</p> Name Type Description Default <code>task_specs</code> <code>list[ClassificationTaskSpec]</code> <p>A list of classification task specifications.</p> required <code>tokenizer</code> <code>Callable[[Union[str, list[str]]], Union[Tensor, dict[str, Tensor]]]</code> <p>A function to tokenize text inputs.</p> required Source code in <code>mmlearn/tasks/zero_shot_classification.py</code> <pre><code>@store(group=\"eval_task\", provider=\"mmlearn\")\nclass ZeroShotClassification(EvaluationHooks):\n    \"\"\"Zero-shot classification evaluation task.\n\n    This task evaluates the zero-shot classification performance.\n\n    Parameters\n    ----------\n    task_specs : list[ClassificationTaskSpec]\n        A list of classification task specifications.\n    tokenizer : Callable[[Union[str, list[str]]], Union[torch.Tensor, dict[str, torch.Tensor]]]\n        A function to tokenize text inputs.\n    \"\"\"  # noqa: W505\n\n    def __init__(\n        self,\n        task_specs: list[ClassificationTaskSpec],\n        tokenizer: Callable[\n            [Union[str, list[str]]], Union[torch.Tensor, dict[str, torch.Tensor]]\n        ],\n    ) -&gt; None:\n        super().__init__()\n        self.tokenizer = tokenizer\n        self.task_specs = task_specs\n        for spec in self.task_specs:\n            assert Modalities.has_modality(spec.query_modality)\n\n        self.metrics: dict[tuple[str, int], MetricCollection] = {}\n        self._embeddings_store: dict[int, torch.Tensor] = {}\n\n    def on_evaluation_epoch_start(self, pl_module: LightningModule) -&gt; None:\n        \"\"\"Set up the evaluation task.\n\n        Parameters\n        ----------\n        pl_module : pl.LightningModule\n            A reference to the Lightning module being evaluated.\n\n        Raises\n        ------\n        ValueError\n            - If the task is not being run for validation or testing.\n            - If the dataset does not have the required attributes to perform zero-shot\n              classification (i.e ``id2label`` and ``zero_shot_prompt_templates``).\n        \"\"\"\n        if pl_module.trainer.validating:\n            eval_dataset: CombinedDataset = pl_module.trainer.val_dataloaders.dataset\n        elif pl_module.trainer.testing:\n            eval_dataset = pl_module.trainer.test_dataloaders.dataset\n        else:\n            raise ValueError(\n                \"ZeroShotClassification task is only supported for validation and testing.\"\n            )\n\n        self.all_dataset_info = {}\n\n        # create metrics for each dataset/query_modality combination\n        if not self.metrics:\n            for dataset_index, dataset in enumerate(eval_dataset.datasets):\n                dataset_name = getattr(dataset, \"name\", dataset.__class__.__name__)\n                try:\n                    id2label: dict[int, str] = dataset.id2label\n                except AttributeError:\n                    raise ValueError(\n                        f\"Dataset '{dataset_name}' must have a `id2label` attribute \"\n                        \"to perform zero-shot classification.\"\n                    ) from None\n\n                try:\n                    zero_shot_prompt_templates: list[str] = (\n                        dataset.zero_shot_prompt_templates\n                    )\n                except AttributeError:\n                    raise ValueError(\n                        \"Dataset must have a `zero_shot_prompt_templates` attribute to perform zero-shot classification.\"\n                    ) from None\n\n                num_classes = len(id2label)\n\n                self.all_dataset_info[dataset_index] = {\n                    \"name\": dataset_name,\n                    \"id2label\": id2label,\n                    \"prompt_templates\": zero_shot_prompt_templates,\n                    \"num_classes\": num_classes,\n                }\n\n                for spec in self.task_specs:\n                    query_modality = Modalities.get_modality(spec.query_modality).name\n                    self.metrics[(query_modality, dataset_index)] = (\n                        self._create_metrics(\n                            num_classes,\n                            spec.top_k,\n                            prefix=f\"{dataset_name}/{query_modality}_\",\n                            postfix=\"\",\n                        )\n                    )\n\n        for metric in self.metrics.values():\n            metric.to(pl_module.device)\n\n        for dataset_index, dataset_info in self.all_dataset_info.items():\n            id2label = dataset_info[\"id2label\"]\n            prompt_templates: list[str] = dataset_info[\"prompt_templates\"]\n            labels = list(id2label.values())\n\n            with torch.no_grad():\n                chunk_size = 10\n                all_embeddings = []\n\n                for i in tqdm(\n                    range(0, len(labels), chunk_size),\n                    desc=\"Encoding class descriptions\",\n                ):\n                    batch_labels = labels[i : min(i + chunk_size, len(labels))]\n                    descriptions = [\n                        template.format(label)\n                        for label in batch_labels\n                        for template in prompt_templates\n                    ]\n                    tokenized_descriptions = move_data_to_device(\n                        self.tokenizer(descriptions),\n                        pl_module.device,\n                    )\n\n                    # Encode the chunk using the pl_module's encode method\n                    chunk_embeddings = pl_module.encode(\n                        tokenized_descriptions, Modalities.TEXT\n                    )  # shape: [chunk_size x len(prompt_templates), embed_dim]\n                    chunk_embeddings /= chunk_embeddings.norm(p=2, dim=-1, keepdim=True)\n                    chunk_embeddings = chunk_embeddings.reshape(\n                        len(batch_labels), len(prompt_templates), -1\n                    ).mean(dim=1)  # shape: [chunk_size, embed_dim]\n                    chunk_embeddings /= chunk_embeddings.norm(p=2, dim=-1, keepdim=True)\n\n                    # Append the chunk embeddings to the list\n                    all_embeddings.append(chunk_embeddings)\n\n                # Concatenate all chunk embeddings into a single tensor\n                class_embeddings = torch.cat(all_embeddings, dim=0)\n\n            self._embeddings_store[dataset_index] = class_embeddings\n\n    def evaluation_step(\n        self, pl_module: LightningModule, batch: dict[str, torch.Tensor], batch_idx: int\n    ) -&gt; None:\n        \"\"\"Compute logits and update metrics.\n\n        Parameters\n        ----------\n        pl_module : pl.LightningModule\n            A reference to the Lightning module being evaluated.\n        batch : dict[str, torch.Tensor]\n            A batch of data.\n        batch_idx : int\n            The index of the batch.\n        \"\"\"\n        if pl_module.trainer.sanity_checking:\n            return\n\n        for (query_modality, dataset_index), metric_collection in self.metrics.items():\n            matching_indices = torch.where(batch[\"dataset_index\"] == dataset_index)[0]\n\n            if not matching_indices.numel():\n                continue\n\n            class_embeddings = self._embeddings_store[dataset_index]\n            query_embeddings: torch.Tensor = pl_module.encode(\n                batch, Modalities.get_modality(query_modality)\n            )\n            query_embeddings /= query_embeddings.norm(p=2, dim=-1, keepdim=True)\n            query_embeddings = query_embeddings[matching_indices]\n\n            if self.all_dataset_info[dataset_index][\"num_classes\"] == 2:\n                softmax_output = _safe_matmul(\n                    query_embeddings, class_embeddings\n                ).softmax(dim=-1)\n                logits = softmax_output[:, 1] - softmax_output[:, 0]\n            else:\n                logits = 100.0 * _safe_matmul(query_embeddings, class_embeddings)\n            targets = batch[Modalities.get_modality(query_modality).target][\n                matching_indices\n            ]\n\n            metric_collection.update(logits, targets)\n\n    def on_evaluation_epoch_end(self, pl_module: LightningModule) -&gt; dict[str, Any]:\n        \"\"\"Compute and reset metrics.\n\n        Parameters\n        ----------\n        pl_module : pl.LightningModule\n            A reference to the Lightning module being evaluated.\n\n        Returns\n        -------\n        dict[str, Any]\n            The computed metrics.\n        \"\"\"\n        results = {}\n        for metric_collection in self.metrics.values():\n            results.update(metric_collection.compute())\n            metric_collection.reset()\n\n        self._embeddings_store.clear()\n\n        eval_type = \"val\" if pl_module.trainer.validating else \"test\"\n        for key, value in results.items():\n            pl_module.log(f\"{eval_type}/{key}\", value)\n\n        return results\n\n    @staticmethod\n    def _create_metrics(\n        num_classes: int, top_k: list[int], prefix: str, postfix: str\n    ) -&gt; MetricCollection:\n        \"\"\"Create a collection of classification metrics.\"\"\"\n        task_type = \"binary\" if num_classes == 2 else \"multiclass\"\n        acc_metrics = (\n            {\n                f\"top{k}_accuracy\": Accuracy(\n                    task=task_type, num_classes=num_classes, top_k=k, average=\"micro\"\n                )\n                for k in top_k\n            }\n            if num_classes &gt; 2\n            else {\"accuracy\": Accuracy(task=task_type, num_classes=num_classes)}\n        )\n        return MetricCollection(\n            {\n                \"precision\": Precision(\n                    task=task_type,\n                    num_classes=num_classes,\n                    average=\"macro\" if num_classes &gt; 2 else \"micro\",\n                ),\n                \"recall\": Recall(\n                    task=task_type,\n                    num_classes=num_classes,\n                    average=\"macro\" if num_classes &gt; 2 else \"micro\",\n                ),\n                \"f1_score_macro\": F1Score(\n                    task=task_type,\n                    num_classes=num_classes,\n                    average=\"macro\" if num_classes &gt; 2 else \"micro\",\n                ),\n                \"aucroc\": AUROC(task=task_type, num_classes=num_classes),\n                **acc_metrics,\n            },\n            prefix=prefix,\n            postfix=postfix,\n            compute_groups=True,\n        )\n</code></pre>"},{"location":"api/#mmlearn.tasks.zero_shot_classification.ZeroShotClassification.on_evaluation_epoch_start","title":"on_evaluation_epoch_start","text":"<pre><code>on_evaluation_epoch_start(pl_module)\n</code></pre> <p>Set up the evaluation task.</p> <p>Parameters:</p> Name Type Description Default <code>pl_module</code> <code>LightningModule</code> <p>A reference to the Lightning module being evaluated.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <ul> <li>If the task is not being run for validation or testing.</li> <li>If the dataset does not have the required attributes to perform zero-shot   classification (i.e <code>id2label</code> and <code>zero_shot_prompt_templates</code>).</li> </ul> Source code in <code>mmlearn/tasks/zero_shot_classification.py</code> <pre><code>def on_evaluation_epoch_start(self, pl_module: LightningModule) -&gt; None:\n    \"\"\"Set up the evaluation task.\n\n    Parameters\n    ----------\n    pl_module : pl.LightningModule\n        A reference to the Lightning module being evaluated.\n\n    Raises\n    ------\n    ValueError\n        - If the task is not being run for validation or testing.\n        - If the dataset does not have the required attributes to perform zero-shot\n          classification (i.e ``id2label`` and ``zero_shot_prompt_templates``).\n    \"\"\"\n    if pl_module.trainer.validating:\n        eval_dataset: CombinedDataset = pl_module.trainer.val_dataloaders.dataset\n    elif pl_module.trainer.testing:\n        eval_dataset = pl_module.trainer.test_dataloaders.dataset\n    else:\n        raise ValueError(\n            \"ZeroShotClassification task is only supported for validation and testing.\"\n        )\n\n    self.all_dataset_info = {}\n\n    # create metrics for each dataset/query_modality combination\n    if not self.metrics:\n        for dataset_index, dataset in enumerate(eval_dataset.datasets):\n            dataset_name = getattr(dataset, \"name\", dataset.__class__.__name__)\n            try:\n                id2label: dict[int, str] = dataset.id2label\n            except AttributeError:\n                raise ValueError(\n                    f\"Dataset '{dataset_name}' must have a `id2label` attribute \"\n                    \"to perform zero-shot classification.\"\n                ) from None\n\n            try:\n                zero_shot_prompt_templates: list[str] = (\n                    dataset.zero_shot_prompt_templates\n                )\n            except AttributeError:\n                raise ValueError(\n                    \"Dataset must have a `zero_shot_prompt_templates` attribute to perform zero-shot classification.\"\n                ) from None\n\n            num_classes = len(id2label)\n\n            self.all_dataset_info[dataset_index] = {\n                \"name\": dataset_name,\n                \"id2label\": id2label,\n                \"prompt_templates\": zero_shot_prompt_templates,\n                \"num_classes\": num_classes,\n            }\n\n            for spec in self.task_specs:\n                query_modality = Modalities.get_modality(spec.query_modality).name\n                self.metrics[(query_modality, dataset_index)] = (\n                    self._create_metrics(\n                        num_classes,\n                        spec.top_k,\n                        prefix=f\"{dataset_name}/{query_modality}_\",\n                        postfix=\"\",\n                    )\n                )\n\n    for metric in self.metrics.values():\n        metric.to(pl_module.device)\n\n    for dataset_index, dataset_info in self.all_dataset_info.items():\n        id2label = dataset_info[\"id2label\"]\n        prompt_templates: list[str] = dataset_info[\"prompt_templates\"]\n        labels = list(id2label.values())\n\n        with torch.no_grad():\n            chunk_size = 10\n            all_embeddings = []\n\n            for i in tqdm(\n                range(0, len(labels), chunk_size),\n                desc=\"Encoding class descriptions\",\n            ):\n                batch_labels = labels[i : min(i + chunk_size, len(labels))]\n                descriptions = [\n                    template.format(label)\n                    for label in batch_labels\n                    for template in prompt_templates\n                ]\n                tokenized_descriptions = move_data_to_device(\n                    self.tokenizer(descriptions),\n                    pl_module.device,\n                )\n\n                # Encode the chunk using the pl_module's encode method\n                chunk_embeddings = pl_module.encode(\n                    tokenized_descriptions, Modalities.TEXT\n                )  # shape: [chunk_size x len(prompt_templates), embed_dim]\n                chunk_embeddings /= chunk_embeddings.norm(p=2, dim=-1, keepdim=True)\n                chunk_embeddings = chunk_embeddings.reshape(\n                    len(batch_labels), len(prompt_templates), -1\n                ).mean(dim=1)  # shape: [chunk_size, embed_dim]\n                chunk_embeddings /= chunk_embeddings.norm(p=2, dim=-1, keepdim=True)\n\n                # Append the chunk embeddings to the list\n                all_embeddings.append(chunk_embeddings)\n\n            # Concatenate all chunk embeddings into a single tensor\n            class_embeddings = torch.cat(all_embeddings, dim=0)\n\n        self._embeddings_store[dataset_index] = class_embeddings\n</code></pre>"},{"location":"api/#mmlearn.tasks.zero_shot_classification.ZeroShotClassification.evaluation_step","title":"evaluation_step","text":"<pre><code>evaluation_step(pl_module, batch, batch_idx)\n</code></pre> <p>Compute logits and update metrics.</p> <p>Parameters:</p> Name Type Description Default <code>pl_module</code> <code>LightningModule</code> <p>A reference to the Lightning module being evaluated.</p> required <code>batch</code> <code>dict[str, Tensor]</code> <p>A batch of data.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required Source code in <code>mmlearn/tasks/zero_shot_classification.py</code> <pre><code>def evaluation_step(\n    self, pl_module: LightningModule, batch: dict[str, torch.Tensor], batch_idx: int\n) -&gt; None:\n    \"\"\"Compute logits and update metrics.\n\n    Parameters\n    ----------\n    pl_module : pl.LightningModule\n        A reference to the Lightning module being evaluated.\n    batch : dict[str, torch.Tensor]\n        A batch of data.\n    batch_idx : int\n        The index of the batch.\n    \"\"\"\n    if pl_module.trainer.sanity_checking:\n        return\n\n    for (query_modality, dataset_index), metric_collection in self.metrics.items():\n        matching_indices = torch.where(batch[\"dataset_index\"] == dataset_index)[0]\n\n        if not matching_indices.numel():\n            continue\n\n        class_embeddings = self._embeddings_store[dataset_index]\n        query_embeddings: torch.Tensor = pl_module.encode(\n            batch, Modalities.get_modality(query_modality)\n        )\n        query_embeddings /= query_embeddings.norm(p=2, dim=-1, keepdim=True)\n        query_embeddings = query_embeddings[matching_indices]\n\n        if self.all_dataset_info[dataset_index][\"num_classes\"] == 2:\n            softmax_output = _safe_matmul(\n                query_embeddings, class_embeddings\n            ).softmax(dim=-1)\n            logits = softmax_output[:, 1] - softmax_output[:, 0]\n        else:\n            logits = 100.0 * _safe_matmul(query_embeddings, class_embeddings)\n        targets = batch[Modalities.get_modality(query_modality).target][\n            matching_indices\n        ]\n\n        metric_collection.update(logits, targets)\n</code></pre>"},{"location":"api/#mmlearn.tasks.zero_shot_classification.ZeroShotClassification.on_evaluation_epoch_end","title":"on_evaluation_epoch_end","text":"<pre><code>on_evaluation_epoch_end(pl_module)\n</code></pre> <p>Compute and reset metrics.</p> <p>Parameters:</p> Name Type Description Default <code>pl_module</code> <code>LightningModule</code> <p>A reference to the Lightning module being evaluated.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>The computed metrics.</p> Source code in <code>mmlearn/tasks/zero_shot_classification.py</code> <pre><code>def on_evaluation_epoch_end(self, pl_module: LightningModule) -&gt; dict[str, Any]:\n    \"\"\"Compute and reset metrics.\n\n    Parameters\n    ----------\n    pl_module : pl.LightningModule\n        A reference to the Lightning module being evaluated.\n\n    Returns\n    -------\n    dict[str, Any]\n        The computed metrics.\n    \"\"\"\n    results = {}\n    for metric_collection in self.metrics.values():\n        results.update(metric_collection.compute())\n        metric_collection.reset()\n\n    self._embeddings_store.clear()\n\n    eval_type = \"val\" if pl_module.trainer.validating else \"test\"\n    for key, value in results.items():\n        pl_module.log(f\"{eval_type}/{key}\", value)\n\n    return results\n</code></pre>"},{"location":"api/#mmlearn.tasks.zero_shot_retrieval","title":"zero_shot_retrieval","text":"<p>Zero-shot cross-modal retrieval evaluation task.</p>"},{"location":"api/#mmlearn.tasks.zero_shot_retrieval.RetrievalTaskSpec","title":"RetrievalTaskSpec  <code>dataclass</code>","text":"<p>Specification for a retrieval task.</p> Source code in <code>mmlearn/tasks/zero_shot_retrieval.py</code> <pre><code>@dataclass\nclass RetrievalTaskSpec:\n    \"\"\"Specification for a retrieval task.\"\"\"\n\n    #: The query modality.\n    query_modality: str\n\n    #: The target modality.\n    target_modality: str\n\n    #: The top-k values for which to compute the retrieval recall metrics.\n    top_k: list[int]\n</code></pre>"},{"location":"api/#mmlearn.tasks.zero_shot_retrieval.ZeroShotCrossModalRetrieval","title":"ZeroShotCrossModalRetrieval","text":"<p>               Bases: <code>EvaluationHooks</code></p> <p>Zero-shot cross-modal retrieval evaluation task.</p> <p>This task evaluates the retrieval performance of a model on a set of query-target pairs. The model is expected to produce embeddings for both the query and target modalities. The task computes the retrieval recall at <code>k</code> for each pair of modalities.</p> <p>Parameters:</p> Name Type Description Default <code>task_specs</code> <code>list[RetrievalTaskSpec]</code> <p>A list of retrieval task specifications. Each specification defines the query and target modalities, as well as the top-k values for which to compute the retrieval recall metrics.</p> required Source code in <code>mmlearn/tasks/zero_shot_retrieval.py</code> <pre><code>@store(group=\"eval_task\", provider=\"mmlearn\")\nclass ZeroShotCrossModalRetrieval(EvaluationHooks):\n    \"\"\"Zero-shot cross-modal retrieval evaluation task.\n\n    This task evaluates the retrieval performance of a model on a set of query-target\n    pairs. The model is expected to produce embeddings for both the query and target\n    modalities. The task computes the retrieval recall at `k` for each pair of\n    modalities.\n\n    Parameters\n    ----------\n    task_specs : list[RetrievalTaskSpec]\n        A list of retrieval task specifications. Each specification defines the query\n        and target modalities, as well as the top-k values for which to compute the\n        retrieval recall metrics.\n\n    \"\"\"\n\n    def __init__(self, task_specs: list[RetrievalTaskSpec]) -&gt; None:\n        super().__init__()\n\n        self.task_specs = task_specs\n        self.metrics: dict[tuple[str, str], MetricCollection] = {}\n        self._available_modalities = set()\n\n        for spec in self.task_specs:\n            query_modality = spec.query_modality\n            target_modality = spec.target_modality\n            assert Modalities.has_modality(query_modality)\n            assert Modalities.has_modality(target_modality)\n\n            self.metrics[(query_modality, target_modality)] = MetricCollection(\n                {\n                    f\"{query_modality}_to_{target_modality}_R@{k}\": RetrievalRecallAtK(\n                        top_k=k, aggregation=\"mean\", reduction=\"none\"\n                    )\n                    for k in spec.top_k\n                }\n            )\n            self._available_modalities.add(query_modality)\n            self._available_modalities.add(target_modality)\n\n    def on_evaluation_epoch_start(self, pl_module: pl.LightningModule) -&gt; None:\n        \"\"\"Move the metrics to the device of the Lightning module.\"\"\"\n        for metric in self.metrics.values():\n            metric.to(pl_module.device)\n\n    def evaluation_step(\n        self,\n        pl_module: pl.LightningModule,\n        batch: dict[str, torch.Tensor],\n        batch_idx: int,\n    ) -&gt; None:\n        \"\"\"Run the forward pass and update retrieval recall metrics.\n\n        Parameters\n        ----------\n        pl_module : pl.LightningModule\n            A reference to the Lightning module being evaluated.\n        batch : dict[str, torch.Tensor]\n            A dictionary of batched input tensors.\n        batch_idx : int\n            The index of the batch.\n\n        \"\"\"\n        if pl_module.trainer.sanity_checking:\n            return\n\n        outputs: dict[str, Any] = {}\n        for modality_name in self._available_modalities:\n            if modality_name in batch:\n                outputs[modality_name] = pl_module.encode(\n                    batch, Modalities.get_modality(modality_name), normalize=False\n                )\n        for (query_modality, target_modality), metric in self.metrics.items():\n            if query_modality not in outputs or target_modality not in outputs:\n                continue\n            query_embeddings: torch.Tensor = outputs[query_modality]\n            target_embeddings: torch.Tensor = outputs[target_modality]\n            indexes = torch.arange(query_embeddings.size(0), device=pl_module.device)\n\n            metric.update(query_embeddings, target_embeddings, indexes)\n\n    def on_evaluation_epoch_end(\n        self, pl_module: pl.LightningModule\n    ) -&gt; Optional[dict[str, Any]]:\n        \"\"\"Compute the retrieval recall metrics.\n\n        Parameters\n        ----------\n        pl_module : pl.LightningModule\n            A reference to the Lightning module being evaluated.\n\n        Returns\n        -------\n        Optional[dict[str, Any]]\n            A dictionary of evaluation results or `None` if no results are available.\n        \"\"\"\n        if pl_module.trainer.sanity_checking:\n            return None\n\n        results = {}\n        for metric in self.metrics.values():\n            results.update(metric.compute())\n            metric.reset()\n\n        eval_type = \"val\" if pl_module.trainer.validating else \"test\"\n\n        for key, value in results.items():\n            pl_module.log(f\"{eval_type}/{key}\", value)\n\n        return results\n</code></pre>"},{"location":"api/#mmlearn.tasks.zero_shot_retrieval.ZeroShotCrossModalRetrieval.on_evaluation_epoch_start","title":"on_evaluation_epoch_start","text":"<pre><code>on_evaluation_epoch_start(pl_module)\n</code></pre> <p>Move the metrics to the device of the Lightning module.</p> Source code in <code>mmlearn/tasks/zero_shot_retrieval.py</code> <pre><code>def on_evaluation_epoch_start(self, pl_module: pl.LightningModule) -&gt; None:\n    \"\"\"Move the metrics to the device of the Lightning module.\"\"\"\n    for metric in self.metrics.values():\n        metric.to(pl_module.device)\n</code></pre>"},{"location":"api/#mmlearn.tasks.zero_shot_retrieval.ZeroShotCrossModalRetrieval.evaluation_step","title":"evaluation_step","text":"<pre><code>evaluation_step(pl_module, batch, batch_idx)\n</code></pre> <p>Run the forward pass and update retrieval recall metrics.</p> <p>Parameters:</p> Name Type Description Default <code>pl_module</code> <code>LightningModule</code> <p>A reference to the Lightning module being evaluated.</p> required <code>batch</code> <code>dict[str, Tensor]</code> <p>A dictionary of batched input tensors.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required Source code in <code>mmlearn/tasks/zero_shot_retrieval.py</code> <pre><code>def evaluation_step(\n    self,\n    pl_module: pl.LightningModule,\n    batch: dict[str, torch.Tensor],\n    batch_idx: int,\n) -&gt; None:\n    \"\"\"Run the forward pass and update retrieval recall metrics.\n\n    Parameters\n    ----------\n    pl_module : pl.LightningModule\n        A reference to the Lightning module being evaluated.\n    batch : dict[str, torch.Tensor]\n        A dictionary of batched input tensors.\n    batch_idx : int\n        The index of the batch.\n\n    \"\"\"\n    if pl_module.trainer.sanity_checking:\n        return\n\n    outputs: dict[str, Any] = {}\n    for modality_name in self._available_modalities:\n        if modality_name in batch:\n            outputs[modality_name] = pl_module.encode(\n                batch, Modalities.get_modality(modality_name), normalize=False\n            )\n    for (query_modality, target_modality), metric in self.metrics.items():\n        if query_modality not in outputs or target_modality not in outputs:\n            continue\n        query_embeddings: torch.Tensor = outputs[query_modality]\n        target_embeddings: torch.Tensor = outputs[target_modality]\n        indexes = torch.arange(query_embeddings.size(0), device=pl_module.device)\n\n        metric.update(query_embeddings, target_embeddings, indexes)\n</code></pre>"},{"location":"api/#mmlearn.tasks.zero_shot_retrieval.ZeroShotCrossModalRetrieval.on_evaluation_epoch_end","title":"on_evaluation_epoch_end","text":"<pre><code>on_evaluation_epoch_end(pl_module)\n</code></pre> <p>Compute the retrieval recall metrics.</p> <p>Parameters:</p> Name Type Description Default <code>pl_module</code> <code>LightningModule</code> <p>A reference to the Lightning module being evaluated.</p> required <p>Returns:</p> Type Description <code>Optional[dict[str, Any]]</code> <p>A dictionary of evaluation results or <code>None</code> if no results are available.</p> Source code in <code>mmlearn/tasks/zero_shot_retrieval.py</code> <pre><code>def on_evaluation_epoch_end(\n    self, pl_module: pl.LightningModule\n) -&gt; Optional[dict[str, Any]]:\n    \"\"\"Compute the retrieval recall metrics.\n\n    Parameters\n    ----------\n    pl_module : pl.LightningModule\n        A reference to the Lightning module being evaluated.\n\n    Returns\n    -------\n    Optional[dict[str, Any]]\n        A dictionary of evaluation results or `None` if no results are available.\n    \"\"\"\n    if pl_module.trainer.sanity_checking:\n        return None\n\n    results = {}\n    for metric in self.metrics.values():\n        results.update(metric.compute())\n        metric.reset()\n\n    eval_type = \"val\" if pl_module.trainer.validating else \"test\"\n\n    for key, value in results.items():\n        pl_module.log(f\"{eval_type}/{key}\", value)\n\n    return results\n</code></pre>"},{"location":"api/#utilities","title":"Utilities","text":""},{"location":"api/#mmlearn.hf_utils","title":"mmlearn.hf_utils","text":"<p>Utilities for loading components from the HuggingFace <code>transformers</code> library.</p>"},{"location":"api/#mmlearn.hf_utils.load_huggingface_model","title":"load_huggingface_model","text":"<pre><code>load_huggingface_model(\n    model_type,\n    model_name_or_path,\n    load_pretrained_weights=True,\n    get_model_attr=None,\n    model_config_kwargs=None,\n    config_type=None,\n)\n</code></pre> <p>Load a model from the HuggingFace <code>transformers</code> library.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>Type[_BaseAutoModelClass]</code> <p>The model class to instantiate e.g. <code>transformers.AutoModel</code>.</p> required <code>model_name_or_path</code> <code>str</code> <p>The model name or path to load the model from.</p> required <code>load_pretrained_weights</code> <code>bool</code> <p>Whether to load the pretrained weights or not. If false, the argument <code>pretrained_model_name_or_path</code> will be used to get the model configuration and the model will be initialized with random weights.</p> <code>True</code> <code>get_model_attr</code> <code>Optional[str]</code> <p>If not None, the attribute of the model to return. For example, if the model is an <code>transformers.AutoModel</code> and <code>get_model_attr='encoder'</code>, the encoder part of the model will be returned. If <code>None</code>, the full model will be returned.</p> <code>None</code> <code>model_config_kwargs</code> <code>Optional[dict[str, Any]]</code> <p>Additional keyword arguments to pass to the model configuration. The values in kwargs of any keys which are configuration attributes will be used to override the loaded values. Behavior concerning key/value pairs whose keys are not configuration attributes is controlled by the <code>return_unused_kwargs</code> keyword parameter.</p> <code>None</code> <code>config_type</code> <code>Optional[Type[PretrainedConfig]]</code> <p>The class of the configuration to use. If None, <code>transformers.AutoConfig</code> will be used.</p> <code>None</code> <p>Returns:</p> Type Description <code>Module</code> <p>The instantiated model.</p> Source code in <code>mmlearn/hf_utils.py</code> <pre><code>def load_huggingface_model(\n    model_type: Type[_BaseAutoModelClass],\n    model_name_or_path: str,\n    load_pretrained_weights: bool = True,\n    get_model_attr: Optional[str] = None,\n    model_config_kwargs: Optional[dict[str, Any]] = None,\n    config_type: Optional[Type[PretrainedConfig]] = None,\n) -&gt; nn.Module:\n    \"\"\"Load a model from the HuggingFace ``transformers`` library.\n\n    Parameters\n    ----------\n    model_type : Type[_BaseAutoModelClass]\n        The model class to instantiate e.g. ``transformers.AutoModel``.\n    model_name_or_path : str\n        The model name or path to load the model from.\n    load_pretrained_weights : bool, optional, default=True\n        Whether to load the pretrained weights or not. If false, the argument\n        ``pretrained_model_name_or_path`` will be used to get the model configuration\n        and the model will be initialized with random weights.\n    get_model_attr : Optional[str], optional, default=None\n        If not None, the attribute of the model to return. For example, if the model\n        is an ``transformers.AutoModel`` and ``get_model_attr='encoder'``, the\n        encoder part of the model will be returned. If ``None``, the full model\n        will be returned.\n    model_config_kwargs : Optional[dict[str, Any]], optional, default=None\n        Additional keyword arguments to pass to the model configuration.\n        The values in kwargs of any keys which are configuration attributes will\n        be used to override the loaded values. Behavior concerning key/value pairs\n        whose keys are *not* configuration attributes is controlled by the\n        ``return_unused_kwargs`` keyword parameter.\n    config_type : Optional[Type[PretrainedConfig]], optional, default=None\n        The class of the configuration to use. If None, ``transformers.AutoConfig``\n        will be used.\n\n    Returns\n    -------\n    torch.nn.Module\n        The instantiated model.\n    \"\"\"\n    model_config_kwargs = model_config_kwargs or {}\n    if load_pretrained_weights:\n        model = model_type.from_pretrained(model_name_or_path, **model_config_kwargs)\n    else:\n        if config_type is None:\n            config_type = AutoConfig\n        config, kwargs = config_type.from_pretrained(\n            pretrained_model_name_or_path=model_name_or_path,\n            return_unused_kwargs=True,\n            **model_config_kwargs,\n        )\n        model = model_type.from_config(config, **kwargs)\n\n    if get_model_attr is not None and hasattr(model, get_model_attr):\n        model = getattr(model, get_model_attr)\n\n    return model\n</code></pre>"},{"location":"api/#mmlearn.constants","title":"mmlearn.constants","text":"<p>Constants.</p>"},{"location":"contributing/","title":"Contributing to mmlearn","text":"<p>Thanks for your interest in contributing to mmlearn!</p> <p>To submit PRs, please fill out the PR template along with the PR. If the PR fixes an issue, please include a link to the PR to the issue. Below are some details around important things to consider before contributing to the library. A table of contents also appears below for navigation.</p> <ul> <li>Development Practices</li> <li>Development Requirements</li> <li>Coding Guidelines, Formatters, and Checks</li> <li>Code Documentation</li> <li>Tests</li> </ul>"},{"location":"contributing/#development-practices","title":"Development Practices","text":"<p>We use the standard git development flow of branch and merge to main with PRs on GitHub. At least one member of the core team needs to approve a PR before it can be merged into main. As mentioned above, tests are run automatically on PRs with a merge target of main. Furthermore, a suite of static code checkers and formatters are also run on said PRs. These also need to pass for a PR to be eligible for merging into the main branch of the library. Currently, such checks run on python3.9.</p>"},{"location":"contributing/#development-requirements","title":"Development Requirements","text":"<p>For development and testing, we use uv for dependency management. The library dependencies and those for development and testing are listed in the <code>pyproject.toml</code> file. You may use whatever virtual environment management tool that you would like. These include uv, conda, and virtualenv.</p> <p>The easiest way to create and activate a virtual environment for development using uv is: <pre><code>uv sync --dev --all-extras\n</code></pre></p> <p>Note that the with command is installing all libraries required for the full development workflow. See the <code>pyproject.toml</code> file for additional details as to what is installed with each of these options.</p> <p>If you need to update the environment libraries, you should change the requirements in the <code>pyproject.toml</code> and then update the <code>uv.lock</code> using the command <code>uv lock</code>.</p>"},{"location":"contributing/#coding-guidelines-formatters-and-checks","title":"Coding Guidelines, Formatters, and Checks","text":"<p>For code style, we recommend the PEP 8 style guide.</p> <p>We use ruff for code formatting and static code analysis. Ruff checks various rules including flake8. The pre-commit hooks show errors which you need to fix before submitting a PR.</p> <p>Last but not least, we use type hints in our code which are checked using mypy. The mypy checks are strictly enforced. That is, all mypy checks must pass or the associated PR will not be merge-able.</p> <p>The settings for <code>mypy</code> and <code>ruff</code> can be found the <code>pyproject.toml</code> files and some standard checks are defined directly in the <code>.pre-commit-config.yaml</code> settings.</p> <p>All of these checks and formatters are invoked by pre-commit hooks. These hooks are run remotely on GitHub. In order to ensure that your code conforms to these standards, and, therefore, passes the remote checks, you can install the pre-commit hooks to be run locally. This is done by running (with your environment active)</p> <pre><code>pre-commit install\n</code></pre> <p>To run the checks, some of which will automatically re-format your code to fit the standards, you can run <pre><code>pre-commit run --all-files\n</code></pre> It can also be run on a subset of files by omitting the <code>--all-files</code> option and pointing to specific files or folders.</p> <p>If you're using VS Code for development, pre-commit should setup git hooks that execute the pre-commit checks each time you check code into your branch through the integrated source-control as well. This will ensure that each of your commits conform to the desired format before they are run remotely and without needing to remember to run the checks before pushing to a remote. If this isn't done automatically, you can find instructions for setting up these hooks manually online.</p>"},{"location":"contributing/#code-documentation","title":"Code Documentation","text":"<p>For code documentation, we try to adhere to the numpy format. For development, any non-trivial or non-obvious methods added to the library should have a doc string. For our library this applies only to code added to the main library in <code>mmlearn</code>. Code outside the core library folder, such as tests, need not incorporate the strict rules of documentation, though clarifying and helpful comments in that code is also strongly encouraged.</p> <p>NOTE: As a matter of convention choice, classes are documented at the \"class\" level rather than through their <code>__init__</code> functions.</p> <p>If you are using VS Code a very helpful integration is available to facilitate the creation of properly formatted doc-strings called autoDocstring VS Code Page and Documentation. This tool will automatically generate a docstring template when starting a docstring with triple quotation marks (<code>\"\"\"</code>). To get the correct format, the following settings should be prescribed in your VS Code settings JSON:</p> <pre><code>{\n    \"autoDocstring.customTemplatePath\": \"\",\n    \"autoDocstring.docstringFormat\": \"numpy\",\n    \"autoDocstring.generateDocstringOnEnter\": true,\n    \"autoDocstring.guessTypes\": true,\n    \"autoDocstring.includeExtendedSummary\": false,\n    \"autoDocstring.includeName\": false,\n    \"autoDocstring.logLevel\": \"Info\",\n    \"autoDocstring.quoteStyle\": \"\\\"\\\"\\\"\",\n    \"autoDocstring.startOnNewLine\": true\n}\n</code></pre>"},{"location":"contributing/#tests","title":"Tests","text":"<p>All tests for the library are housed in the <code>tests</code> folder. The unit and integration tests are run using <code>pytest</code>. These tests are automatically run through GitHub integrations on PRs to the main branch of this repository. PRs that fail any of the tests will not be eligible to be merged until they are fixed.</p> <p>To run all tests in the tests folder one only needs to run (with the venv active) <pre><code>pytest .\n</code></pre> To run a specific test with pytest, one runs <pre><code>pytest tests/datasets/test_combined_dataset.py\n</code></pre></p>"},{"location":"installation/","title":"Installation","text":"<p>Prerequisites</p> <p>For local development, it is generally recommended to install mmlearn in a non-global environment (e.g. venv or conda). This will allow you to use different versions of mmlearn for different projects.</p> <p>You can create a virtual environment using venv with the following command: <pre><code>python3 -m venv /path/to/new/virtual/environment\nsource /path/to/new/virtual/environment/bin/activate\n</code></pre></p>"},{"location":"installation/#installing-from-pypi","title":"Installing from PyPI","text":"<p>mmlearn is published on the Python Package Index and can be installed using pip.</p> <p>Run the following command to install the library:</p> <pre><code>python3 -m pip install mmlearn\n</code></pre> <p>Note</p> <p><code>mmlearn</code> has several optional dependencies that are used for specific functionality. For example, the peft library for parameter-efficient finetuning. Hence, <code>peft</code> can be installed using:</p> <pre><code>python3 -m pip install mmlearn[peft]\n</code></pre> <p>Specific sets of dependencies are listed below.</p> Dependency pip extra Notes torchvision, timm, opencv-python vision Allows use of computer vision models and image processing functionality torchaudio audio Allows use of audio processing and audio model functionality peft peft Allows use of parameter-efficient fine-tuning methods"},{"location":"installation/#installing-from-source","title":"Installing from source","text":"<p>You can install mmlearn directly from a clone of the Git repository. This can be done either by cloning the repo and installing from the local clone, or simply installing directly via git.</p> <pre><code>git clone https://github.com/VectorInstitute/mmlearn.git\ncd mmlearn\npython3 -m pip install -e .\n</code></pre> <pre><code>pip install git+https://github.com/VectorInstitute/mmlearn.git\n</code></pre>"},{"location":"user_guide/","title":"User Guide","text":"<p>mmlearn contains a collection of tools and utilities to help researchers and practitioners easily set up and run training or evaluation experiments for multimodal representation learning methods. The toolkit is designed to be modular and extensible. We aim to provide a high degree of flexibility in using existing methods, while also allowing users to easily add support for new modalities of data, datasets, models and pretraining or evaluation methods.</p> <p>Much of the power and flexibility of mmlearn comes from building on top of the PyTorch Lightning framework and using Hydra and hydra-zen for configuration management. Together, these tools make it easy to define and run experiments with different configurations, and to scale up experiments to run on a SLURM cluster.</p> <p>The goal of this guide is to give you a brief overview of what mmlearn is and how you can get started using it.</p> <p>Note</p> <p>mmlearn currently only supports training and evaluation of encoder-only models.</p> <p>For more detailed information on the features and capabilities of mmlearn, please refer to the API Reference.</p>"},{"location":"user_guide/#defining-a-dataset","title":"Defining a Dataset","text":"<p>Datasets in mmlearn can be defined using PyTorch's <code>torch.utils.data.Dataset</code> or <code>torch.utils.data.IterableDataset</code> classes. However, there are two additional requirements for datasets in mmlearn:</p> <ol> <li>The dataset must return an instance of <code>mmlearn.datasets.core.example.Example</code> from the <code>__getitem__</code>    method or the <code>__iter__</code> method.</li> <li>The <code>Example</code> object returned by the dataset must contain the key <code>'example_index'</code>    and use modality-specific keys from the <code>Modalities</code> registry    to store the data.</li> </ol> <p>Example 1: Defining a map-style dataset in mmlearn:</p> <pre><code>from torch.utils.data.dataset import Dataset\n\nfrom mmlearn.datasets.core import Example, Modalities\nfrom mmlearn.constants import EXAMPLE_INDEX_KEY\n\n\nclass MyMapStyleDataset(Dataset[Example]):\n   ...\n   def __getitem__(self, idx: int) -&gt; Example:\n      ...\n      return Example(\n         {\n            EXAMPLE_INDEX_KEY: idx,\n            Modalities.TEXT.name: ...,\n            Modalities.RGB.name: ...,\n            Modalities.RGB.target: ...,\n            Modalities.TEXT.mask: ...,\n            ...\n         }\n      )\n</code></pre> <p>Example 2: Defining an iterable-style dataset in mmlearn:</p> <pre><code>from torch.utils.data.dataset import IterableDataset\n\nfrom mmlearn.datasets.core import Example, Modalities\nfrom mmlearn.constants import EXAMPLE_INDEX_KEY\n\n\nclass MyIterableStyleDataset(IterableDataset[Example]):\n   ...\n   def __iter__(self) -&gt; Generator[Example, None, None]:\n      ...\n      idx = 0\n      for item in items:\n         yield Example(\n            {\n               EXAMPLE_INDEX_KEY: idx,\n               Modalities.TEXT.name: ...,\n               Modalities.AUDIO.name: ...,\n               Modalities.TEXT.mask: ...,\n               Modalities.AUDIO.mask: ...,\n               ...\n            }\n         )\n         idx += 1\n</code></pre> <p>The <code>Example</code> class represents a single example in the dataset and all the attributes associated with it. The class is an extension of the <code>collections.OrderedDict</code> class that provides attribute-style access to the dictionary values and handles the creation of the <code>'example_ids'</code> tuple, combining the <code>'example_index'</code> and <code>'dataset_index'</code> values. The <code>'example_index'</code> key is created by the dataset object for each example returned by the dataset. On the other hand, the <code>'dataset_index'</code> key is created by the <code>CombinedDataset</code> each <code>Example</code> object returned by the dataset.</p> <p>Note</p> <p>All dataset objects in mmlearn are wrapped in the <code>CombinedDataset</code> class, which is a subclass of <code>torch.utils.data.Dataset</code>. As such, the user almost never has to add/define the <code>'dataset_index'</code> key explicitly.</p> <p>Since batching typically combines data from the same modality into one tensor, both the <code>'example_index'</code> and <code>'dataset_index'</code> keys are essential for uniquely identifying paired examples across different modalities from the same dataset. The <code>find_matching_indices</code> function does exactly this by finding the indices of the examples in a batch that have the same <code>'example_ids'</code> tuple.</p> <p><code>Modalities</code> is an instance of <code>ModalityRegistry</code> singleton class that serves as a global registry for all the modalities supported by mmlearn. It allows dot-style access registered modalities and their properties. For example, the <code>'RGB'</code> modality can be accessed using <code>Modalities.RGB</code> (returns string <code>'rgb'</code>) and the <code>'target'</code> property of the <code>'RGB'</code> modality can be accessed using <code>Modalities.RGB.target</code> (returns the string <code>'rgb_target'</code>). It also provides a method to register new modalities and their properties. For example, the following code snippet shows how to register a new <code>'DNA'</code> modality:</p> <pre><code>from mmlearn.datasets.core import Modalities\n\nModalities.register_modality(\"dna\")\n</code></pre>"},{"location":"user_guide/#adding-new-modules","title":"Adding New Modules","text":"<p>Modules are building blocks for models and tasks in mmlearn. They can be anything from encoders, layers, losses, optimizers, learning rate schedulers, metrics, etc. Modules in mmlearn are generally defined by extending PyTorch's <code>torch.nn.Module</code> class.</p> <p>Users have the flexibility to design new modules according to their requirements, with the exception of encoder modules and modules associated with specific pre-defined tasks (e.g., loss functions for the <code>ContrastivePretraining</code> task). The forward method of encoder modules must accept a dictionary as input, where the keys are the names of the modalities and the values are the corresponding (batched) tensors/data. This format makes it easier to reuse the encoder with different modalities and different tasks. In addition, the forward method must return a list-like object where the first element is the last layer's output. The following code snippet shows how to define a new text encoder module:</p> <pre><code>import torch\nfrom torch import nn\n\nfrom mmlearn.datasets.core import Modalities\n\n\nclass MyTextEncoder(nn.Module):\n   def __init__(self, input_dim: int, output_dim: int):\n      super().__init__()\n      self.encoder = ...\n\n   def forward(self, inputs: dict[str, torch.Tensor]) -&gt; tuple[torch.Tensor]:\n      out = self.encoder(\n         inputs[Modalities.TEXT.name],\n         inputs.get(\n            \"attention_mask\", inputs.get(Modalities.TEXT.attention_mask, None)\n         ),\n      )\n      return (out,)\n</code></pre> <p>For modules associated with pre-defined tasks, the new modules must adhere to the same function signature as the existing modules for that task. For instance, the forward method of a new loss function for the <code>ContrastivePretraining</code> task must have the following signature to be compatible with the existing loss functions for the task:</p> <pre><code>import torch\n\nfrom mmlearn.tasks.contrastive_pretraining import LossPairSpec\n\ndef my_contrastive_loss(\n   embeddings: dict[str, torch.Tensor],\n   example_ids: dict[str, torch.Tensor],\n   logit_scale: torch.Tensor,\n   modality_loss_pairs: list[LossPairSpec],\n) -&gt; torch.Tensor:\n   ...\n</code></pre>"},{"location":"user_guide/#adding-new-tasks","title":"Adding New Tasks","text":"<p>Tasks in mmlearn represent the different training and/or evaluation objectives that can be performed on the data using the different modules. Tasks that require training should extend the <code>TrainingTask</code> class, while tasks involving only evaluation should extend the <code>EvaluationHooks</code> class.</p>"},{"location":"user_guide/#training-tasks","title":"Training Tasks","text":"<p>The <code>TrainingTask</code> class is an extension of the <code>lightning.pytorch.core.LightningModule</code> class, which itself is an extension of the <code>torch.nn.Module</code> class. The class provides a common interface for training tasks in mmlearn. It allows users to define the training loop, validation loop, test loop, and the setup for the model, optimizer, learning rate scheduler and loss function, all in one place (a functionality inherited from PyTorch Lightning). The class also provides hooks for customizing the training loop, validation loop, and test loop, as well as a suite of other functionalities like logging, checkpointing and handling distributed training.</p> <p>Seealso</p> <p>For more information on the features and capabilities of the <code>TrainingTask</code> class inherited from PyTorch Lightning, please refer to the PyTorch Lightning documentation.</p> <p>To be used with the PyTorch Lightning Trainer, extensions of the <code>TrainingTask</code> class must define a <code>training_step</code> method. The following code snippet shows the minimum requirements for defining a new task in mmlearn:</p> <pre><code>from typing import Any, Optional, Union\nfrom functools import partial\n\nimport torch\n\nfrom mmlearn.tasks.base import TrainingTask\n\nclass MyTask(TrainingTask):\n   def __init__(\n      self,\n      optimizer: Optional[partial[torch.optim.Optimizer]],\n      loss_fn: Optional[torch.nn.Module],\n      lr_scheduler: Optional[\n         Union[\n            dict[str, Union[partial[torch.optim.lr_scheduler.LRScheduler], Any]],\n            partial[torch.optim.lr_scheduler.LRScheduler],\n         ]\n      ] = None,\n   ) -&gt; None:\n      super().__init__(optimizer=optimizer, loss_fn=loss_fn, lr_scheduler=lr_scheduler)\n\n      # Since this class also inherits from torch.nn.Module, we can define the\n      # model and its components directly in the constructor and also define\n      # a forward method for the model as an instance method of this class.\n      # Alternatively, we can pass the model as an argument to the constructor\n      # and assign it to an instance variable.\n      self.model = ...\n\n   def training_step(self, batch: dict[str, Any], batch_idx: int) -&gt; torch.Tensor:\n      outputs = self.model(batch) # or self(batch) if a forward method is defined in this class\n\n      # maybe process outputs here\n\n      loss = self.loss_fn(outputs, ...)\n      return loss\n</code></pre>"},{"location":"user_guide/#evaluation-tasks","title":"Evaluation Tasks","text":"<p>The <code>EvaluationHooks</code> class is intended to be used for evaluation tasks that don't require training, e.g. zero-shot evaluation tasks (as opposed to evaluation tasks like linear probing, which require training). The class provides an interface for defining and customizing the evaluation loop.</p> <p>Classes that inherit from <code>EvaluationHooks</code> cannot be run/used on their own. They must be used in conjunction with a training task, which will call the hooks defined in the evaluation task during the evaluation phase. This way, multiple evaluation tasks can be defined and used with the same training task. The model to be evaluated is provided by the training task to the evaluation task.</p> <p>Training tasks that wish to use one or more evaluation tasks must accept an instance of the evaluation task(s) as an argument to the constructor and must define a <code>validation_step</code> and/or <code>test_step</code> method that calls the <code>evaluation_step</code> method of the evaluation task(s).</p>"},{"location":"user_guide/#creating-and-configuring-a-project","title":"Creating and Configuring a Project","text":"<p>A project in mmlearn can be thought of as a collection of related experiments. Within a project, you can reuse components from mmlearn (e.g., datasets, models, tasks) or define new ones and use them all together for experiments.</p> <p>To create a new project, create a new directory following the structure below:</p> <pre><code>my_project/\n\u251c\u2500\u2500 configs/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 experiment/\n\u2502       \u251c\u2500\u2500 my_experiment.yaml\n\u251c\u2500\u2500 README.md (optional)\n\u251c\u2500\u2500 requirements.txt (optional)\n</code></pre> <p>The <code>configs/</code> directory contains all the configurations, both structured configs and YAML config files for the experiments in the project. The <code>configs/experiment/</code> directory contains the <code>.yaml</code> files for the experiments associated with the project. These <code>.yaml</code> files use the Hydra configuration format, which also allows overriding the configuration options/values from the command line.</p> <p>The <code>__init__.py</code> file in the <code>configs/</code> directory is required to make the <code>configs/</code> directory a Python package, allowing hydra to compose configurations from <code>.yaml</code> files as well as structured configs from python modules. More on this in the next section.</p> <p>Optionally, you can also include a <code>README.md</code> file with a brief description of the project and a <code>requirements.txt</code> file with the dependencies required to run the project.</p>"},{"location":"user_guide/#specifying-configurable-components","title":"Specifying Configurable Components","text":"<p>One of the key features of the Hydra configuration system is the ability to compose configurations from multiple sources, including the command line, <code>.yaml</code> files and structured configs from Python modules. Structured Configs in Hydra use Python <code>dataclasses.dataclass</code> to define the configuration schema. This allows for both static and runtime type-checking of the configuration. Hydra-zen extends Hydra to makes it easy to dynamically generate dataclass-backed configurations for any class or function simply by adding a decorator to the class or function.</p> <p>mmlearn provides a pre-populated config store, <code>mmlearn.conf.external_store</code>, which can be used as a decorator to register configurable components. This config store already contains configurations for common components like PyTorch optimizers, learning rate schedulers, loss functions and samplers, as well as PyTorch Lightning's Trainer callbacks and loggers. To dynamically add new configurable components to the store, simply add the <code>mmlearn.conf.external_store</code> decorator to the class or function definition.</p> <p>For example, the following code snippet shows how to register a new dataset class:</p> <pre><code>from torch.utils.data.dataset import Dataset\n\nfrom mmlearn.conf import external_store\nfrom mmlearn.constants import EXAMPLE_INDEX_KEY\nfrom mmlearn.datasets.core import Example, Modalities\n\n\n@external_store(group=\"datasets\")\nclass MyMapStyleDataset(Dataset[Example]):\n   ...\n   def __getitem__(self, idx: int) -&gt; Example:\n      ...\n      return Example(\n         {\n            EXAMPLE_INDEX_KEY: idx,\n            Modalities.TEXT.name: ...,\n            Modalities.RGB.name: ...,\n            Modalities.RGB.target: ...,\n            Modalities.TEXT.mask: ...,\n            ...\n         }\n      )\n</code></pre> <p>The <code>mmlearn.conf.external_store</code> decorator immediately add the class to the config store once the Python interpreter loads the module containing the class. This is why the <code>configs/</code> directory must be a Python package and why modules containing user-defined configurable components must be imported in the <code>configs/__init__.py</code> file.</p> <p>The <code>group</code> argument specifies the config group under which the configurable component will be registered. This allows users to easily reference the component in the configurations using the group name and the class name. The available config groups in mmlearn are:</p> <ul> <li><code>datasets</code>: Contains all the dataset classes.</li> <li><code>datasets/masking</code>: Contains all the configurable classes and functions for masking input data.</li> <li><code>datasets/tokenizers</code>: Contains all the configurable classes and functions for converting raw inputs to tokens.</li> <li><code>datasets/transforms</code>: Contains all the configurable classes and functions for transforming input data.</li> <li><code>dataloader/sampler</code>: Contains all the dataloader sampler classes.</li> <li><code>modules/encoders</code>: Contains all the encoder modules.</li> <li><code>modules/layers</code>: For layers that can be used independent of the model.</li> <li><code>modules/losses</code>: Contains all the loss functions.</li> <li><code>modules/optimizers</code>: Contains all the optimizers.</li> <li><code>modules/lr_schedulers</code>: Contains all the learning rate schedulers.</li> <li><code>modules/metrics</code>: Contains all the evaluation metrics.</li> <li><code>tasks</code>: Contains all the task classes.</li> <li><code>trainer/callbacks</code>: Contains all the PyTorch Lightning Trainer callbacks.</li> <li><code>trainer/logger</code>: Contains all the PyTorch Lightning Trainer loggers.</li> </ul>"},{"location":"user_guide/#the-base-configuration","title":"The Base Configuration","text":"<p>The base configuration for all experiments in mmlearn are defined in the <code>MMLearnConf</code> dataclass. This serves as the base configuration for all experiments and can be extended to include additional configuration options, following Hydra's override syntax.</p> <p>The base configuration for mmlearn is shown below:</p> <pre><code>experiment_name: ???\njob_type: train\nseed: null\ndatasets:\n   train: null\n   val: null\n   test: null\ndataloader:\n   train:\n      _target_: torch.utils.data.dataloader.DataLoader\n      _convert_: object\n      dataset: ???\n      batch_size: 1\n      shuffle: null\n      sampler: null\n      batch_sampler: null\n      num_workers: 0\n      collate_fn:\n         _target_: mmlearn.datasets.core.data_collator.DefaultDataCollator\n         batch_processors: null\n      pin_memory: true\n      drop_last: false\n      timeout: 0.0\n      worker_init_fn: null\n      multiprocessing_context: null\n      generator: null\n      prefetch_factor: null\n      persistent_workers: false\n      pin_memory_device: ''\n   val:\n      _target_: torch.utils.data.dataloader.DataLoader\n      _convert_: object\n      dataset: ???\n      batch_size: 1\n      shuffle: null\n      sampler: null\n      batch_sampler: null\n      num_workers: 0\n      collate_fn:\n         _target_: mmlearn.datasets.core.data_collator.DefaultDataCollator\n         batch_processors: null\n      pin_memory: true\n      drop_last: false\n      timeout: 0.0\n      worker_init_fn: null\n      multiprocessing_context: null\n      generator: null\n      prefetch_factor: null\n      persistent_workers: false\n      pin_memory_device: ''\n   test:\n      _target_: torch.utils.data.dataloader.DataLoader\n      _convert_: object\n      dataset: ???\n      batch_size: 1\n      shuffle: null\n      sampler: null\n      batch_sampler: null\n      num_workers: 0\n      collate_fn:\n         _target_: mmlearn.datasets.core.data_collator.DefaultDataCollator\n         batch_processors: null\n      pin_memory: true\n      drop_last: false\n      timeout: 0.0\n      worker_init_fn: null\n      multiprocessing_context: null\n      generator: null\n      prefetch_factor: null\n      persistent_workers: false\n      pin_memory_device: ''\ntask: ???\ntrainer:\n   _target_: lightning.pytorch.trainer.trainer.Trainer\n   accelerator: auto\n   strategy: auto\n   devices: auto\n   num_nodes: 1\n   precision: null\n   logger: null\n   callbacks: null\n   fast_dev_run: false\n   max_epochs: null\n   min_epochs: null\n   max_steps: -1\n   min_steps: null\n   max_time: null\n   limit_train_batches: null\n   limit_val_batches: null\n   limit_test_batches: null\n   limit_predict_batches: null\n   overfit_batches: 0.0\n   val_check_interval: null\n   check_val_every_n_epoch: 1\n   num_sanity_val_steps: null\n   log_every_n_steps: null\n   enable_checkpointing: true\n   enable_progress_bar: true\n   enable_model_summary: true\n   accumulate_grad_batches: 1\n   gradient_clip_val: null\n   gradient_clip_algorithm: null\n   deterministic: null\n   benchmark: null\n   inference_mode: true\n   use_distributed_sampler: true\n   profiler: null\n   detect_anomaly: false\n   barebones: false\n   plugins: null\n   sync_batchnorm: false\n   reload_dataloaders_every_n_epochs: 0\n   default_root_dir: ${hydra:runtime.output_dir}/checkpoints\ntags:\n   - ${experiment_name}\nresume_from_checkpoint: null\nstrict_loading: true\ntorch_compile_kwargs:\n   disable: true\n   fullgraph: false\n   dynamic: null\n   backend: inductor\n   mode: null\n   options: null\n</code></pre> <p>The config keys with a value of <code>???</code> are placeholders that must be overridden in the experiment configurations. While the <code>dataset</code> key in the <code>dataloader</code> group is also a placeholder, it should not be provided as it will be automatically filled in from the <code>datasets</code> group.</p>"},{"location":"user_guide/#configuring-an-experiment","title":"Configuring an Experiment","text":"<p>To configure an experiment, create a new <code>.yaml</code> file in the <code>configs/experiment/</code> directory of the project. The configuration file should define the experiment-specific configuration options and override the base configuration options as needed. Configurable components from the config store can be referenced by name in the configuration file under the defaults list. The following code snippet shows an example configuration file for an experiment:</p> <pre><code># @package _global_\n\ndefaults:\n- /datasets@datasets.train.my_iterable: MyIterableStyleDataset\n- /datasets@datasets.train.my_map: MyMapStyleDataset\n- /modules/encoders@task.encoders.text: MyTextEncoder\n- /modules/encoders@task.encoders.rgb: MyRGBEncoder\n- /modules/losses@task.loss: ContrastiveLoss\n- /modules/optimizers@task.optimizer: AdamW\n- /modules/lr_schedulers@task.lr_scheduler.scheduler: CosineAnnealingLR\n- /eval_task@task.evaluation_tasks.retrieval.task: ZeroShotCrossModalRetrieval\n- /trainer/callbacks@trainer.callbacks.lr_monitor: LearningRateMonitor\n- /trainer/callbacks@trainer.callbacks.model_checkpoint: ModelCheckpoint\n- /trainer/callbacks@trainer.callbacks.early_stopping: EarlyStopping\n- /trainer/callbacks@trainer.callbacks.model_summary: ModelSummary\n- /trainer/logger@trainer.logger.wandb: WandbLogger\n- override /task: ContrastivePretraining\n- _self_\n\nseed: 42\n\ndatasets:\n   train:\n      my_iterable:\n         my_iterable_arg1: ...\n      my_map:\n         my_map_arg1: ...\n\ndataloader:\n   train:\n      batch_size: 64\n\ntask:\n   encoders:\n      text:\n         text_arg1: ...\n      rgb:\n         rgb_arg1: ...\n   evaluation_tasks:\n      retrieval:\n         task:\n         task_specs:\n            - query_modality: text\n               target_modality: rgb\n               top_k: [10, 200]\n            - query_modality: rgb\n               target_modality: text\n               top_k: [10, 200]\n         run_on_validation: false\n         run_on_test: true\n</code></pre>"},{"location":"user_guide/#running-an-experiment","title":"Running an Experiment","text":"<p>To run an experiment locally, use the following command:</p> <pre><code>mmlearn_run 'hydra.searchpath=[pkg://path.to.my_project.configs]' \\\n   +experiment=my_experiment \\\n   experiment_name=my_experiment_name\n</code></pre> <p>Tip</p> <p>You can see the full config for an experiment without running it by adding the <code>--help</code> flag to the command.</p> <pre><code>mmlearn_run 'hydra.searchpath=[pkg://path.to.my_project.configs]' \\\n   +experiment=my_experiment \\\n   experiment_name=my_experiment_name \\\n   task=my_task \\ # required for the command to run\n   --help\n</code></pre> <p>To run the experiment on a SLURM cluster, use the following command:</p> <pre><code>mmlearn_run --multirun \\\n   hydra.launcher.mem_per_cpu=5G \\\n   hydra.launcher.qos=your_qos \\\n   hydra.launcher.partition=your_partition \\\n   hydra.launcher.gres=gpu:4 \\\n   hydra.launcher.cpus_per_task=8 \\\n   hydra.launcher.tasks_per_node=4 \\\n   hydra.launcher.nodes=1 \\\n   hydra.launcher.stderr_to_stdout=true \\\n   hydra.launcher.timeout_min=720 \\\n   'hydra.searchpath=[pkg://path.to.my_project.configs]' \\\n   +experiment=my_experiment \\\n   experiment_name=my_experiment_name\n</code></pre> <p>This uses the submitit launcher plugin built into Hydra to submit the experiment to the SLURM scheduler with the specified resources.</p> <p>Note</p> <p>After the job is submitted, it is okay to cancel the program with <code>Ctrl+C</code>. The job will continue running on the cluster. You can also add <code>&amp;</code> at the end of the command to run it in the background.</p>"}]}